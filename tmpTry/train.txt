start Time:  1641198714999
init
self.action_var shape:  torch.Size([1])
self.action_var shape:  torch.Size([1])
selectttttt
FloatTensor_reshaped at 1641198717557
state222:  tensor([[0., 0., 0., 0.]], device='cuda:0')
policy_old.forwarded at 1641198717564
give action 0============================
log_to_linear:  tensor([[[0.5067]]], device='cuda:0') tensor([[[1.0106]]], device='cuda:0')
log_to_linear action at 1641198717566
bwe changes from to:  [300000, tensor([[[303186.5938]]])]
step into gymStat at 1641198717566
wait for recv string at 1641198717566
recved string at 1641198718796
1
wait for recv [self.estimator, stat] at 1641198718796
recved [self.estimator, stat] at 1641198718796
send bwe to appRecv at 1641198718796
sent bwe to appRecv at 1641198718796
wait for recv string at 1641198718796
recved string at 1641198719434
1
wait for recv [self.estimator, stat] at 1641198719434
recved [self.estimator, stat] at 1641198719435
sorted packlist at 1641198719435
packetSeq:  3224
packetSeq:  3225
packetSeq:  3226
packetSeq:  3227
packetSeq:  3228
packetSeq:  3229
packetSeq:  3230
processed packlist at 1641198719435
receiving_rate:  6440.0
delay:  742.0
loss_ratio:  0.0
processed state0-2 at 1641198719435
avgFrameBetween:  3
psnrStat:  [[395661, 382314, 378257]]
delayStat:  [[72, 67, 33]]
skipStat:  [[1, 1, 1]]
skipCount:  0
state:  [385410.6666666667] [57.333333333333336] [0]
processed state3-5 at 1641198719435
liner_to_log:  tensor([[[1.0106]]]) tensor([[[0.5067]]])
linear_to_log at 1641198719435
listState:  [0.00161, 0.49466666666666664, 0.0, 0.3854106666666667, 0.05733333333333333, 0.0, tensor([[[0.5067]]])]
state_clone_detach at 1641198719436
reward: -1.468739719245914
state tensor([0.0016, 0.4947, 0.0000, 0.5067], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198719436
state222:  tensor([[0.0016, 0.4947, 0.0000, 0.5067]], device='cuda:0')
policy_old.forwarded at 1641198719438
give action 1============================
log_to_linear:  tensor([[[0.5770]]], device='cuda:0') tensor([[[1.1274]]], device='cuda:0')
log_to_linear action at 1641198719439
bwe changes from to:  [tensor([[[303186.5938]]]), tensor([[[341815.5000]]])]
step into gymStat at 1641198719439
send bwe to appRecv at 1641198719439
sent bwe to appRecv at 1641198719439
wait for recv string at 1641198719439
recved string at 1641198719637
1
wait for recv [self.estimator, stat] at 1641198719637
recved [self.estimator, stat] at 1641198719637
sorted packlist at 1641198719637
packetSeq:  3231
packetSeq:  3232
packetSeq:  3233
packetSeq:  3234
packetSeq:  3235
packetSeq:  3236
packetSeq:  3237
packetSeq:  3238
packetSeq:  3239
packetSeq:  3240
packetSeq:  3241
packetSeq:  3242
packetSeq:  3243
packetSeq:  3244
packetSeq:  3245
processed packlist at 1641198719637
receiving_rate:  375560.0
delay:  577.4666666666667
loss_ratio:  0.0
processed state0-2 at 1641198719637
avgFrameBetween:  4
psnrStat:  [[377306, 377529, 380325, 381282, 382853, 383668]]
delayStat:  [[571, 650, 616, 583, 551, 517]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [380493.8333333333] [581.3333333333334] [0]
processed state3-5 at 1641198719637
liner_to_log:  tensor([[[1.1274]]]) tensor([[[0.5770]]])
linear_to_log at 1641198719638
listState:  [0.09389, 0.3849777777777778, 0.0, 0.3804938333333333, 0.5813333333333334, 0.0, tensor([[[0.5770]]])]
state_clone_detach at 1641198719638
reward: -0.6046215349098722
state tensor([0.0939, 0.3850, 0.0000, 0.5770], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198719639
state222:  tensor([[0.0939, 0.3850, 0.0000, 0.5770]], device='cuda:0')
policy_old.forwarded at 1641198719640
give action 2============================
log_to_linear:  tensor([[[0.3262]]], device='cuda:0') tensor([[[0.7541]]], device='cuda:0')
log_to_linear action at 1641198719641
bwe changes from to:  [tensor([[[341815.5000]]]), tensor([[[257753.1719]]])]
step into gymStat at 1641198719641
send bwe to appRecv at 1641198719641
sent bwe to appRecv at 1641198719641
wait for recv string at 1641198719641
recved string at 1641198719856
1
wait for recv [self.estimator, stat] at 1641198719856
recved [self.estimator, stat] at 1641198719856
sorted packlist at 1641198719856
packetSeq:  3246
packetSeq:  3247
packetSeq:  3248
packetSeq:  3249
packetSeq:  3250
packetSeq:  3251
packetSeq:  3252
packetSeq:  3253
packetSeq:  3254
packetSeq:  3255
packetSeq:  3256
packetSeq:  3257
processed packlist at 1641198719856
receiving_rate:  362720.0
delay:  334.3636363636364
loss_ratio:  0.0
processed state0-2 at 1641198719856
avgFrameBetween:  7
psnrStat:  [[385902, 388119, 389977, 389904, 392116, 391944, 394442, 394624, 395210, 396241, 397871, 399726, 398993, 400831]]
delayStat:  [[573, 541, 507, 474, 444, 465, 432, 413, 426, 393, 371, 357, 340, 323]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [393992.85714285716] [432.7857142857143] [0]
processed state3-5 at 1641198719856
liner_to_log:  tensor([[[0.7541]]]) tensor([[[0.3262]]])
linear_to_log at 1641198719857
listState:  [0.09068, 0.22290909090909092, 0.0, 0.39399285714285714, 0.4327857142857143, 0.0, tensor([[[0.3262]]])]
state_clone_detach at 1641198719857
reward: -0.13085898648381789
state tensor([0.0907, 0.2229, 0.0000, 0.3262], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198719858
state222:  tensor([[0.0907, 0.2229, 0.0000, 0.3262]], device='cuda:0')
policy_old.forwarded at 1641198719859
give action 3============================
log_to_linear:  tensor([[[0.3593]]], device='cuda:0') tensor([[[0.7961]]], device='cuda:0')
log_to_linear action at 1641198719860
bwe changes from to:  [tensor([[[257753.1719]]]), tensor([[[205194.1719]]])]
step into gymStat at 1641198719860
send bwe to appRecv at 1641198719860
sent bwe to appRecv at 1641198719860
wait for recv string at 1641198719861
recved string at 1641198720060
1
wait for recv [self.estimator, stat] at 1641198720060
recved [self.estimator, stat] at 1641198720060
sorted packlist at 1641198720060
packetSeq:  3258
packetSeq:  3259
packetSeq:  3260
packetSeq:  3261
packetSeq:  3262
packetSeq:  3263
packetSeq:  3264
packetSeq:  3265
packetSeq:  3266
packetSeq:  3267
packetSeq:  3268
packetSeq:  3269
packetSeq:  3270
processed packlist at 1641198720060
receiving_rate:  374920.0
delay:  218.76923076923077
loss_ratio:  0.0
processed state0-2 at 1641198720060
avgFrameBetween:  8
psnrStat:  [[402097, 401771, 404631, 404649, 406997, 408439, 410293, 416104, 417061, 416556, 418159]]
delayStat:  [[325, 291, 292, 274, 257, 240, 224, 221, 192, 188, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [409705.1818181818] [243.27272727272728] [0]
processed state3-5 at 1641198720060
liner_to_log:  tensor([[[0.7961]]]) tensor([[[0.3593]]])
linear_to_log at 1641198720061
listState:  [0.09373, 0.14584615384615385, 0.0, 0.40970518181818183, 0.24327272727272728, 0.0, tensor([[[0.3593]]])]
state_clone_detach at 1641198720061
reward: 0.11215917267041187
state tensor([0.0937, 0.1458, 0.0000, 0.3593], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198720061
state222:  tensor([[0.0937, 0.1458, 0.0000, 0.3593]], device='cuda:0')
policy_old.forwarded at 1641198720063
give action 4============================
log_to_linear:  tensor([[[0.1002]]], device='cuda:0') tensor([[[0.5391]]], device='cuda:0')
log_to_linear action at 1641198720064
bwe changes from to:  [tensor([[[205194.1719]]]), tensor([[[110625.8672]]])]
step into gymStat at 1641198720064
send bwe to appRecv at 1641198720064
sent bwe to appRecv at 1641198720064
wait for recv string at 1641198720064
recved string at 1641198720264
1
wait for recv [self.estimator, stat] at 1641198720264
recved [self.estimator, stat] at 1641198720264
sorted packlist at 1641198720264
packetSeq:  3271
packetSeq:  3272
packetSeq:  3273
packetSeq:  3274
packetSeq:  3275
packetSeq:  3276
packetSeq:  3277
packetSeq:  3278
packetSeq:  3279
packetSeq:  3280
processed packlist at 1641198720264
receiving_rate:  280400.0
delay:  196.4
loss_ratio:  0.0
processed state0-2 at 1641198720264
avgFrameBetween:  8
psnrStat:  [[416708, 416906, 417117, 417536, 416324, 416990, 417003, 417051, 415829]]
delayStat:  [[168, 153, 138, 133, 118, 107, 101, 89, 79]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [416829.3333333333] [120.66666666666667] [0]
processed state3-5 at 1641198720264
liner_to_log:  tensor([[[0.5391]]]) tensor([[[0.1002]]])
linear_to_log at 1641198720264
listState:  [0.0701, 0.13093333333333335, 0.0, 0.41682933333333333, 0.12066666666666667, 0.0, tensor([[[0.1002]]])]
state_clone_detach at 1641198720265
reward: 0.0585023126056437
state tensor([0.0701, 0.1309, 0.0000, 0.1002], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198720265
state222:  tensor([[0.0701, 0.1309, 0.0000, 0.1002]], device='cuda:0')
policy_old.forwarded at 1641198720267
give action 5============================
log_to_linear:  tensor([[[0.4624]]], device='cuda:0') tensor([[[0.9417]]], device='cuda:0')
log_to_linear action at 1641198720268
bwe changes from to:  [tensor([[[110625.8672]]]), tensor([[[104176.2109]]])]
step into gymStat at 1641198720268
send bwe to appRecv at 1641198720268
sent bwe to appRecv at 1641198720268
wait for recv string at 1641198720268
recved string at 1641198720496
1
wait for recv [self.estimator, stat] at 1641198720496
recved [self.estimator, stat] at 1641198720496
sorted packlist at 1641198720496
packetSeq:  3281
packetSeq:  3282
packetSeq:  3283
packetSeq:  3284
packetSeq:  3285
packetSeq:  3286
packetSeq:  3287
packetSeq:  3288
packetSeq:  3289
packetSeq:  3290
processed packlist at 1641198720496
receiving_rate:  263880.0
delay:  197.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198720496
avgFrameBetween:  8
psnrStat:  [[417067, 417121, 416155, 417081, 418185, 417538, 417137, 418839]]
delayStat:  [[73, 63, 55, 46, 38, 31, 24, 18]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [417390.375] [43.5] [0]
processed state3-5 at 1641198720496
liner_to_log:  tensor([[[0.9417]]]) tensor([[[0.4624]]])
linear_to_log at 1641198720497
listState:  [0.06597, 0.13148148148148148, 0.0, 0.417390375, 0.0435, 0.0, tensor([[[0.4624]]])]
state_clone_detach at 1641198720497
reward: 0.03787286993625305
state tensor([0.0660, 0.1315, 0.0000, 0.4624], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198720498
state222:  tensor([[0.0660, 0.1315, 0.0000, 0.4624]], device='cuda:0')
policy_old.forwarded at 1641198720499
give action 6============================
log_to_linear:  tensor([[[0.3697]]], device='cuda:0') tensor([[[0.8099]]], device='cuda:0')
log_to_linear action at 1641198720500
bwe changes from to:  [tensor([[[104176.2109]]]), tensor([[[84367.2109]]])]
step into gymStat at 1641198720500
send bwe to appRecv at 1641198720500
sent bwe to appRecv at 1641198720500
wait for recv string at 1641198720500
recved string at 1641198720727
1
wait for recv [self.estimator, stat] at 1641198720727
recved [self.estimator, stat] at 1641198720728
sorted packlist at 1641198720728
packetSeq:  3291
packetSeq:  3292
packetSeq:  3293
packetSeq:  3294
packetSeq:  3295
packetSeq:  3296
packetSeq:  3297
packetSeq:  3298
packetSeq:  3299
packetSeq:  3300
processed packlist at 1641198720728
receiving_rate:  262640.0
delay:  197.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198720728
avgFrameBetween:  8
psnrStat:  [[417157, 418217, 417380, 418029, 417870, 417520, 416842]]
delayStat:  [[23, 13, 16, 25, 18, 16, 15]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [417573.5714285714] [18.0] [1]
processed state3-5 at 1641198720728
liner_to_log:  tensor([[[0.8099]]]) tensor([[[0.3697]]])
linear_to_log at 1641198720728
listState:  [0.06566, 0.13148148148148148, 0.0, 0.41757357142857143, 0.018, 0.03125, tensor([[[0.3697]]])]
state_clone_detach at 1641198720728
reward: 0.036423311502171485
state tensor([0.0657, 0.1315, 0.0000, 0.3697], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198720729
state222:  tensor([[0.0657, 0.1315, 0.0000, 0.3697]], device='cuda:0')
policy_old.forwarded at 1641198720731
give action 7============================
log_to_linear:  tensor([[[0.1311]]], device='cuda:0') tensor([[[0.5599]]], device='cuda:0')
log_to_linear action at 1641198720731
bwe changes from to:  [tensor([[[84367.2109]]]), tensor([[[47239.0977]]])]
step into gymStat at 1641198720732
send bwe to appRecv at 1641198720732
sent bwe to appRecv at 1641198720732
wait for recv string at 1641198720732
recved string at 1641198720958
1
wait for recv [self.estimator, stat] at 1641198720958
recved [self.estimator, stat] at 1641198720958
sorted packlist at 1641198720958
packetSeq:  3301
packetSeq:  3302
packetSeq:  3303
packetSeq:  3304
packetSeq:  3305
packetSeq:  3306
packetSeq:  3307
packetSeq:  3308
packetSeq:  3309
processed packlist at 1641198720958
receiving_rate:  287520.0
delay:  197.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198720958
avgFrameBetween:  8
psnrStat:  [[417325, 417111, 417449, 416452, 416472, 419218, 417384]]
delayStat:  [[21, 15, 21, 13, 13, 19, 13]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  1
state:  [417347.6666666667] [15.666666666666666] [2]
processed state3-5 at 1641198720958
liner_to_log:  tensor([[[0.5599]]]) tensor([[[0.1311]]])
linear_to_log at 1641198720958
listState:  [0.07188, 0.13192592592592592, 0.0, 0.4173476666666667, 0.015666666666666666, 0.0625, tensor([[[0.1311]]])]
state_clone_detach at 1641198720958
reward: 0.06352525267201109
state tensor([0.0719, 0.1319, 0.0000, 0.1311], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198720959
state222:  tensor([[0.0719, 0.1319, 0.0000, 0.1311]], device='cuda:0')
policy_old.forwarded at 1641198720961
give action 8============================
log_to_linear:  tensor([[[0.5043]]], device='cuda:0') tensor([[[1.0069]]], device='cuda:0')
log_to_linear action at 1641198720962
bwe changes from to:  [tensor([[[47239.0977]]]), tensor([[[47565.4961]]])]
step into gymStat at 1641198720962
send bwe to appRecv at 1641198720962
sent bwe to appRecv at 1641198720962
wait for recv string at 1641198720962
recved string at 1641198721160
1
wait for recv [self.estimator, stat] at 1641198721160
recved [self.estimator, stat] at 1641198721160
sorted packlist at 1641198721160
packetSeq:  3310
packetSeq:  3311
packetSeq:  3312
packetSeq:  3313
packetSeq:  3314
packetSeq:  3315
packetSeq:  3316
packetSeq:  3317
processed packlist at 1641198721160
receiving_rate:  242640.0
delay:  197.125
loss_ratio:  0.0
processed state0-2 at 1641198721160
avgFrameBetween:  7
psnrStat:  [[417517, 418015, 418328, 419955, 421630, 423029]]
delayStat:  [[18, 18, 15, 15, 14, 14]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  2
state:  [420735.5] [14.5] [4]
processed state3-5 at 1641198721160
liner_to_log:  tensor([[[1.0069]]]) tensor([[[0.5043]]])
linear_to_log at 1641198721161
listState:  [0.06066, 0.13141666666666665, 0.0, 0.4207355, 0.0145, 0.125, tensor([[[0.5043]]])]
state_clone_detach at 1641198721161
reward: 0.012740659788366193
state tensor([0.0607, 0.1314, 0.0000, 0.5043], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198721162
state222:  tensor([[0.0607, 0.1314, 0.0000, 0.5043]], device='cuda:0')
policy_old.forwarded at 1641198721163
give action 9============================
log_to_linear:  tensor([[[0.7176]]], device='cuda:0') tensor([[[1.3865]]], device='cuda:0')
log_to_linear action at 1641198721164
bwe changes from to:  [tensor([[[47565.4961]]]), tensor([[[65951.4844]]])]
step into gymStat at 1641198721164
send bwe to appRecv at 1641198721164
sent bwe to appRecv at 1641198721165
wait for recv string at 1641198721165
recved string at 1641198721363
1
wait for recv [self.estimator, stat] at 1641198721363
recved [self.estimator, stat] at 1641198721363
sorted packlist at 1641198721363
packetSeq:  3318
packetSeq:  3319
packetSeq:  3320
packetSeq:  3321
packetSeq:  3322
packetSeq:  3323
packetSeq:  3324
packetSeq:  3325
packetSeq:  3326
processed packlist at 1641198721363
receiving_rate:  281360.0
delay:  198.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198721363
avgFrameBetween:  7
psnrStat:  [[424115, 424744, 426113, 426199, 428935, 427021]]
delayStat:  [[20, 13, 17, 16, 24, 16]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  3
state:  [427385.0] [18.666666666666668] [8]
processed state3-5 at 1641198721363
liner_to_log:  tensor([[[1.3865]]]) tensor([[[0.7176]]])
linear_to_log at 1641198721364
listState:  [0.07034, 0.13237037037037036, 0.0, 0.427385, 0.018666666666666668, 0.25, tensor([[[0.7176]]])]
state_clone_detach at 1641198721364
reward: 0.05527619274860296
state tensor([0.0703, 0.1324, 0.0000, 0.7176], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198721364
state222:  tensor([[0.0703, 0.1324, 0.0000, 0.7176]], device='cuda:0')
policy_old.forwarded at 1641198721366
give action 10============================
log_to_linear:  tensor([[[0.2519]]], device='cuda:0') tensor([[[0.6686]]], device='cuda:0')
log_to_linear action at 1641198721367
bwe changes from to:  [tensor([[[65951.4844]]]), tensor([[[44097.3438]]])]
step into gymStat at 1641198721367
send bwe to appRecv at 1641198721367
sent bwe to appRecv at 1641198721367
wait for recv string at 1641198721367
recved string at 1641198721565
1
wait for recv [self.estimator, stat] at 1641198721565
recved [self.estimator, stat] at 1641198721565
sorted packlist at 1641198721565
packetSeq:  3327
packetSeq:  3328
packetSeq:  3329
packetSeq:  3330
packetSeq:  3331
packetSeq:  3332
packetSeq:  3333
packetSeq:  3334
packetSeq:  3335
packetSeq:  3336
packetSeq:  3337
processed packlist at 1641198721565
receiving_rate:  309640.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198721565
avgFrameBetween:  7
psnrStat:  [[428653, 425898, 427893, 426962, 427285, 425262]]
delayStat:  [[23, 17, 18, 20, 22, 17]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  4
state:  [426273.5] [19.5] [16]
processed state3-5 at 1641198721565
liner_to_log:  tensor([[[0.6686]]]) tensor([[[0.2519]]])
linear_to_log at 1641198721566
listState:  [0.07741, 0.132, 0.0, 0.4262735, 0.0195, 0.5, tensor([[[0.2519]]])]
state_clone_detach at 1641198721566
reward: 0.08749968120170326
state tensor([0.0774, 0.1320, 0.0000, 0.2519], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198721566
state222:  tensor([[0.0774, 0.1320, 0.0000, 0.2519]], device='cuda:0')
policy_old.forwarded at 1641198721568
give action 11============================
log_to_linear:  tensor([[[0.4879]]], device='cuda:0') tensor([[[0.9810]]], device='cuda:0')
log_to_linear action at 1641198721569
bwe changes from to:  [tensor([[[44097.3438]]]), tensor([[[43258.2227]]])]
step into gymStat at 1641198721569
send bwe to appRecv at 1641198721569
sent bwe to appRecv at 1641198721569
wait for recv string at 1641198721569
recved string at 1641198721789
1
wait for recv [self.estimator, stat] at 1641198721789
recved [self.estimator, stat] at 1641198721789
sorted packlist at 1641198721789
packetSeq:  3338
packetSeq:  3339
packetSeq:  3340
packetSeq:  3341
packetSeq:  3342
packetSeq:  3343
packetSeq:  3344
packetSeq:  3345
packetSeq:  3346
packetSeq:  3347
packetSeq:  3348
packetSeq:  3349
processed packlist at 1641198721789
receiving_rate:  283360.0
delay:  197.36363636363637
loss_ratio:  0.0
processed state0-2 at 1641198721790
avgFrameBetween:  7
psnrStat:  [[425551, 422533, 423324, 421834, 423378, 422020, 422820]]
delayStat:  [[25, 19, 21, 17, 21, 17, 18]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  5
state:  [422420.0] [17.5] [16]
processed state3-5 at 1641198721790
liner_to_log:  tensor([[[0.9810]]]) tensor([[[0.4879]]])
linear_to_log at 1641198721790
listState:  [0.07084, 0.1315757575757576, 0.0, 0.42242, 0.0175, 0.5, tensor([[[0.4879]]])]
state_clone_detach at 1641198721790
reward: 0.059914147541262175
state tensor([0.0708, 0.1316, 0.0000, 0.4879], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198721791
state222:  tensor([[0.0708, 0.1316, 0.0000, 0.4879]], device='cuda:0')
policy_old.forwarded at 1641198721792
give action 12============================
log_to_linear:  tensor([[[0.2463]]], device='cuda:0') tensor([[[0.6628]]], device='cuda:0')
log_to_linear action at 1641198721793
bwe changes from to:  [tensor([[[43258.2227]]]), tensor([[[28671.7793]]])]
step into gymStat at 1641198721794
send bwe to appRecv at 1641198721794
sent bwe to appRecv at 1641198721794
wait for recv string at 1641198721794
recved string at 1641198722000
1
wait for recv [self.estimator, stat] at 1641198722000
recved [self.estimator, stat] at 1641198722000
sorted packlist at 1641198722000
packetSeq:  3350
packetSeq:  3351
packetSeq:  3352
packetSeq:  3353
packetSeq:  3354
packetSeq:  3355
packetSeq:  3356
packetSeq:  3357
packetSeq:  3358
packetSeq:  3359
packetSeq:  3360
processed packlist at 1641198722000
receiving_rate:  280960.0
delay:  198.1
loss_ratio:  0.0
processed state0-2 at 1641198722000
avgFrameBetween:  7
psnrStat:  [[422758, 419486, 420375, 422510, 422568, 421105]]
delayStat:  [[21, 13, 17, 20, 21, 13]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  5
state:  [421105.0] [13.0] [32]
processed state3-5 at 1641198722000
liner_to_log:  tensor([[[0.6628]]]) tensor([[[0.2463]]])
linear_to_log at 1641198722001
listState:  [0.07024, 0.13206666666666667, 0.0, 0.421105, 0.013, 1.0, tensor([[[0.2463]]])]
state_clone_detach at 1641198722001
reward: 0.05573546258525386
state tensor([0.0702, 0.1321, 0.0000, 0.2463], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198722001
state222:  tensor([[0.0702, 0.1321, 0.0000, 0.2463]], device='cuda:0')
policy_old.forwarded at 1641198722003
give action 13============================
log_to_linear:  tensor([[[0.5759]]], device='cuda:0') tensor([[[1.1255]]], device='cuda:0')
log_to_linear action at 1641198722004
bwe changes from to:  [tensor([[[28671.7793]]]), tensor([[[32269.5234]]])]
step into gymStat at 1641198722004
send bwe to appRecv at 1641198722004
sent bwe to appRecv at 1641198722004
wait for recv string at 1641198722004
recved string at 1641198722220
1
wait for recv [self.estimator, stat] at 1641198722220
recved [self.estimator, stat] at 1641198722220
sorted packlist at 1641198722220
packetSeq:  3361
packetSeq:  3362
packetSeq:  3363
packetSeq:  3364
packetSeq:  3365
packetSeq:  3366
packetSeq:  3367
processed packlist at 1641198722220
receiving_rate:  247920.0
delay:  197.71428571428572
loss_ratio:  0.0
processed state0-2 at 1641198722220
avgFrameBetween:  7
psnrStat:  [[422140, 419696, 421163, 422151, 423288, 423515, 424232]]
delayStat:  [[27, 14, 15, 13, 13, 14, 16]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [424232.0] [16.0] [32]
processed state3-5 at 1641198722220
liner_to_log:  tensor([[[1.1255]]]) tensor([[[0.5759]]])
linear_to_log at 1641198722220
listState:  [0.06198, 0.13180952380952382, 0.0, 0.424232, 0.016, 1.0, tensor([[[0.5759]]])]
state_clone_detach at 1641198722221
reward: 0.017958530784063953
state tensor([0.0620, 0.1318, 0.0000, 0.5759], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198722221
state222:  tensor([[0.0620, 0.1318, 0.0000, 0.5759]], device='cuda:0')
policy_old.forwarded at 1641198722223
give action 14============================
log_to_linear:  tensor([[[0.9700]]], device='cuda:0') tensor([[[1.9294]]], device='cuda:0')
log_to_linear action at 1641198722224
bwe changes from to:  [tensor([[[32269.5234]]]), tensor([[[62259.9961]]])]
step into gymStat at 1641198722224
send bwe to appRecv at 1641198722224
sent bwe to appRecv at 1641198722224
wait for recv string at 1641198722224
recved string at 1641198722427
1
wait for recv [self.estimator, stat] at 1641198722427
recved [self.estimator, stat] at 1641198722427
sorted packlist at 1641198722427
packetSeq:  3368
packetSeq:  3369
packetSeq:  3370
packetSeq:  3371
packetSeq:  3372
packetSeq:  3373
packetSeq:  3374
packetSeq:  3375
packetSeq:  3376
packetSeq:  3377
packetSeq:  3378
processed packlist at 1641198722428
receiving_rate:  306760.0
delay:  198.0909090909091
loss_ratio:  0.0
processed state0-2 at 1641198722428
avgFrameBetween:  7
psnrStat:  [[424278, 423418, 425008, 425962, 426239, 426006]]
delayStat:  [[18, 15, 19, 21, 15, 21]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [425151.8333333333] [18.166666666666668] [64]
processed state3-5 at 1641198722428
liner_to_log:  tensor([[[1.9294]]]) tensor([[[0.9700]]])
linear_to_log at 1641198722428
listState:  [0.07669, 0.13206060606060607, 0.0, 0.42515183333333334, 0.018166666666666668, 1, tensor([[[0.9700]]])]
state_clone_detach at 1641198722428
reward: 0.08422219151635374
state tensor([0.0767, 0.1321, 0.0000, 0.9700], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198722429
state222:  tensor([[0.0767, 0.1321, 0.0000, 0.9700]], device='cuda:0')
policy_old.forwarded at 1641198722431
give action 15============================
log_to_linear:  tensor([[[0.6739]]], device='cuda:0') tensor([[[1.3024]]], device='cuda:0')
log_to_linear action at 1641198722431
bwe changes from to:  [tensor([[[62259.9961]]]), tensor([[[81090.0547]]])]
step into gymStat at 1641198722432
send bwe to appRecv at 1641198722432
sent bwe to appRecv at 1641198722432
wait for recv string at 1641198722432
recved string at 1641198722652
1
wait for recv [self.estimator, stat] at 1641198722652
recved [self.estimator, stat] at 1641198722652
sorted packlist at 1641198722652
packetSeq:  3379
packetSeq:  3380
packetSeq:  3381
packetSeq:  3382
packetSeq:  3383
packetSeq:  3384
packetSeq:  3385
packetSeq:  3386
packetSeq:  3387
processed packlist at 1641198722652
receiving_rate:  316680.0
delay:  196.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198722652
avgFrameBetween:  7
psnrStat:  [[425810, 427349, 426745, 427973, 427444, 428365, 427469]]
delayStat:  [[21, 20, 15, 14, 17, 11, 9]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [427307.85714285716] [15.285714285714286] [64]
processed state3-5 at 1641198722652
liner_to_log:  tensor([[[1.3024]]]) tensor([[[0.6739]]])
linear_to_log at 1641198722653
listState:  [0.07917, 0.13118518518518518, 0.0, 0.4273078571428572, 0.015285714285714286, 1, tensor([[[0.6739]]])]
state_clone_detach at 1641198722653
reward: 0.09744475603378677
state tensor([0.0792, 0.1312, 0.0000, 0.6739], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198722653
state222:  tensor([[0.0792, 0.1312, 0.0000, 0.6739]], device='cuda:0')
policy_old.forwarded at 1641198722655
give action 16============================
log_to_linear:  tensor([[[0.1536]]], device='cuda:0') tensor([[[0.5770]]], device='cuda:0')
log_to_linear action at 1641198722656
bwe changes from to:  [tensor([[[81090.0547]]]), tensor([[[46791.9922]]])]
step into gymStat at 1641198722656
send bwe to appRecv at 1641198722656
sent bwe to appRecv at 1641198722656
wait for recv string at 1641198722656
recved string at 1641198722887
1
wait for recv [self.estimator, stat] at 1641198722887
recved [self.estimator, stat] at 1641198722887
sorted packlist at 1641198722887
packetSeq:  3388
packetSeq:  3389
packetSeq:  3390
packetSeq:  3391
packetSeq:  3392
packetSeq:  3393
packetSeq:  3394
packetSeq:  3395
packetSeq:  3396
packetSeq:  3397
processed packlist at 1641198722888
receiving_rate:  206200.0
delay:  191.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198722888
avgFrameBetween:  7
psnrStat:  [[425860, 425860, 424904, 424029, 422819, 423378, 425568]]
delayStat:  [[13, 37, 13, 5, 6, 3, 6]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [424631.14285714284] [11.857142857142858] [64]
processed state3-5 at 1641198722888
liner_to_log:  tensor([[[0.5770]]]) tensor([[[0.1536]]])
linear_to_log at 1641198722888
listState:  [0.05155, 0.12762962962962965, 0.0, 0.4246311428571428, 0.011857142857142858, 1, tensor([[[0.1536]]])]
state_clone_detach at 1641198722888
reward: -0.022006653696064193
state tensor([0.0516, 0.1276, 0.0000, 0.1536], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198722889
state222:  tensor([[0.0516, 0.1276, 0.0000, 0.1536]], device='cuda:0')
policy_old.forwarded at 1641198722891
give action 17============================
log_to_linear:  tensor([[[0.6280]]], device='cuda:0') tensor([[[1.2177]]], device='cuda:0')
log_to_linear action at 1641198722891
bwe changes from to:  [tensor([[[46791.9922]]]), tensor([[[56976.5000]]])]
step into gymStat at 1641198722892
send bwe to appRecv at 1641198722892
sent bwe to appRecv at 1641198722892
wait for recv string at 1641198722892
recved string at 1641198723122
1
wait for recv [self.estimator, stat] at 1641198723122
recved [self.estimator, stat] at 1641198723122
sorted packlist at 1641198723122
packetSeq:  3398
packetSeq:  3399
packetSeq:  3400
packetSeq:  3401
packetSeq:  3402
packetSeq:  3403
packetSeq:  3404
processed packlist at 1641198723122
receiving_rate:  187200.0
delay:  192.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198723122
avgFrameBetween:  7
psnrStat:  [[429112, 431793, 435329, 438599, 442432, 445228, 448053]]
delayStat:  [[14, 6, 9, 7, 8, 6, 8]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [438649.4285714286] [8.285714285714286] [64]
processed state3-5 at 1641198723122
liner_to_log:  tensor([[[1.2177]]]) tensor([[[0.6280]]])
linear_to_log at 1641198723122
listState:  [0.0468, 0.12844444444444444, 0.0, 0.4386494285714286, 0.008285714285714287, 1, tensor([[[0.6280]]])]
state_clone_detach at 1641198723123
reward: -0.04997543026925105
state tensor([0.0468, 0.1284, 0.0000, 0.6280], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198723123
state222:  tensor([[0.0468, 0.1284, 0.0000, 0.6280]], device='cuda:0')
policy_old.forwarded at 1641198723125
give action 18============================
log_to_linear:  tensor([[[0.6404]]], device='cuda:0') tensor([[[1.2401]]], device='cuda:0')
log_to_linear action at 1641198723126
bwe changes from to:  [tensor([[[56976.5000]]]), tensor([[[70656.2812]]])]
step into gymStat at 1641198723126
send bwe to appRecv at 1641198723126
sent bwe to appRecv at 1641198723126
wait for recv string at 1641198723126
recved string at 1641198723349
1
wait for recv [self.estimator, stat] at 1641198723349
recved [self.estimator, stat] at 1641198723349
sorted packlist at 1641198723349
packetSeq:  3405
packetSeq:  3406
packetSeq:  3407
packetSeq:  3408
packetSeq:  3409
packetSeq:  3410
packetSeq:  3411
processed packlist at 1641198723349
receiving_rate:  236720.0
delay:  192.57142857142858
loss_ratio:  0.0
processed state0-2 at 1641198723350
avgFrameBetween:  7
psnrStat:  [[450868, 452244, 453154, 455941, 456649, 458947, 459787]]
delayStat:  [[15, 8, 8, 7, 8, 7, 9]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [455370.0] [8.857142857142858] [64]
processed state3-5 at 1641198723350
liner_to_log:  tensor([[[1.2401]]]) tensor([[[0.6404]]])
linear_to_log at 1641198723350
listState:  [0.05918, 0.1283809523809524, 0.0, 0.45537, 0.008857142857142857, 1, tensor([[[0.6404]]])]
state_clone_detach at 1641198723350
reward: 0.014593997582351803
state tensor([0.0592, 0.1284, 0.0000, 0.6404], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198723351
state222:  tensor([[0.0592, 0.1284, 0.0000, 0.6404]], device='cuda:0')
policy_old.forwarded at 1641198723353
give action 19============================
log_to_linear:  tensor([[[0.7263]]], device='cuda:0') tensor([[[1.4036]]], device='cuda:0')
log_to_linear action at 1641198723353
bwe changes from to:  [tensor([[[70656.2812]]]), tensor([[[99171.4766]]])]
step into gymStat at 1641198723354
send bwe to appRecv at 1641198723354
sent bwe to appRecv at 1641198723354
wait for recv string at 1641198723354
recved string at 1641198723550
1
wait for recv [self.estimator, stat] at 1641198723550
recved [self.estimator, stat] at 1641198723550
sorted packlist at 1641198723550
packetSeq:  3412
packetSeq:  3413
packetSeq:  3414
packetSeq:  3415
packetSeq:  3416
packetSeq:  3417
packetSeq:  3418
packetSeq:  3419
processed packlist at 1641198723550
receiving_rate:  253520.0
delay:  204.25
loss_ratio:  ./peerconnection_serverless.origin ./rtcGym/alphartc_gym/json/receiver_pyinfer8001.json
recvf lines:  494
got request at 1641198718796
processed allFrame at 1641198718796
send 'asking for bwe' at 1641198718796
sent 'asking for bwe' at 1641198718796
send [estimator, stat] at 1641198718796
sent [estimator, stat] at 1641198718796
pc wait for bwe at 1641198718796
pc got bwe at 1641198718796
bandwidth:  300000
pc flushed at 1641198718796
Bwe Sent: 0 at 1641198718796
got request at 1641198719434
processed allFrame at 1641198719434
send 'asking for bwe' at 1641198719434
sent 'asking for bwe' at 1641198719434
send [estimator, stat] at 1641198719434
sent [estimator, stat] at 1641198719434
pc wait for bwe at 1641198719435
pc got bwe at 1641198719439
bandwidth:  300000
pc flushed at 1641198719439
Bwe Sent: 5 at 1641198719439
got request at 1641198719637
processed allFrame at 1641198719637
send 'asking for bwe' at 1641198719637
sent 'asking for bwe' at 1641198719637
send [estimator, stat] at 1641198719637
sent [estimator, stat] at 1641198719637
pc wait for bwe at 1641198719637
pc got bwe at 1641198719641
bandwidth:  300000
pc flushed at 1641198719641
Bwe Sent: 4 at 1641198719641
got request at 1641198719856
processed allFrame at 1641198719856
send 'asking for bwe' at 1641198719856
sent 'asking for bwe' at 1641198719856
send [estimator, stat] at 1641198719856
sent [estimator, stat] at 1641198719856
pc wait for bwe at 1641198719856
pc got bwe at 1641198719861
bandwidth:  300000
pc flushed at 1641198719861
Bwe Sent: 5 at 1641198719861
got request at 1641198720060
processed allFrame at 1641198720060
send 'asking for bwe' at 1641198720060
sent 'asking for bwe' at 1641198720060
send [estimator, stat] at 1641198720060
sent [estimator, stat] at 1641198720060
pc wait for bwe at 1641198720060
pc got bwe at 1641198720064
bandwidth:  300000
pc flushed at 1641198720064
Bwe Sent: 4 at 1641198720064
got request at 1641198720264
processed allFrame at 1641198720264
send 'asking for bwe' at 1641198720264
sent 'asking for bwe' at 1641198720264
send [estimator, stat] at 1641198720264
sent [estimator, stat] at 1641198720264
pc wait for bwe at 1641198720264
pc got bwe at 1641198720268
bandwidth:  300000
pc flushed at 1641198720268
Bwe Sent: 4 at 1641198720268
got request at 1641198720496
processed allFrame at 1641198720496
send 'asking for bwe' at 1641198720496
sent 'asking for bwe' at 1641198720496
send [estimator, stat] at 1641198720496
sent [estimator, stat] at 1641198720496
pc wait for bwe at 1641198720496
pc got bwe at 1641198720501
bandwidth:  300000
pc flushed at 1641198720501
Bwe Sent: 5 at 1641198720501
got request at 1641198720727
processed allFrame at 1641198720727
send 'asking for bwe' at 1641198720727
sent 'asking for bwe' at 1641198720727
send [estimator, stat] at 1641198720727
sent [estimator, stat] at 1641198720728
pc wait for bwe at 1641198720728
pc got bwe at 1641198720732
bandwidth:  300000
pc flushed at 1641198720732
Bwe Sent: 5 at 1641198720732
got request at 1641198720957
processed allFrame at 1641198720957
send 'asking for bwe' at 1641198720958
sent 'asking for bwe' at 1641198720958
send [estimator, stat] at 1641198720958
sent [estimator, stat] at 1641198720958
pc wait for bwe at 1641198720958
pc got bwe at 1641198720962
bandwidth:  300000
pc flushed at 1641198720962
Bwe Sent: 5 at 1641198720962
got request at 1641198721160
processed allFrame at 1641198721160
send 'asking for bwe' at 1641198721160
sent 'asking for bwe' at 1641198721160
send [estimator, stat] at 1641198721160
sent [estimator, stat] at 1641198721160
pc wait for bwe at 1641198721160
pc got bwe at 1641198721165
bandwidth:  300000
pc flushed at 1641198721165
Bwe Sent: 5 at 1641198721165
got request at 1641198721363
processed allFrame at 1641198721363
send 'asking for bwe' at 1641198721363
sent 'asking for bwe' at 1641198721363
send [estimator, stat] at 1641198721363
sent [estimator, stat] at 1641198721363
pc wait for bwe at 1641198721363
pc got bwe at 1641198721367
bandwidth:  300000
pc flushed at 1641198721367
Bwe Sent: 4 at 1641198721367
got request at 1641198721565
processed allFrame at 1641198721565
send 'asking for bwe' at 1641198721565
sent 'asking for bwe' at 1641198721565
send [estimator, stat] at 1641198721565
sent [estimator, stat] at 1641198721565
pc wait for bwe at 1641198721565
pc got bwe at 1641198721569
bandwidth:  300000
pc flushed at 1641198721569
Bwe Sent: 4 at 1641198721569
got request at 1641198721789
processed allFrame at 1641198721789
send 'asking for bwe' at 1641198721789
sent 'asking for bwe' at 1641198721789
send [estimator, stat] at 1641198721789
sent [estimator, stat] at 1641198721789
pc wait for bwe at 1641198721789
pc got bwe at 1641198721794
bandwidth:  300000
pc flushed at 1641198721794
Bwe Sent: 5 at 1641198721794
got request at 1641198722000
processed allFrame at 1641198722000
send 'asking for bwe' at 1641198722000
sent 'asking for bwe' at 1641198722000
send [estimator, stat] at 1641198722000
sent [estimator, stat] at 1641198722000
pc wait for bwe at 1641198722000
pc got bwe at 1641198722004
bandwidth:  300000
pc flushed at 1641198722004
Bwe Sent: 4 at 1641198722004
got request at 1641198722220
processed allFrame at 1641198722220
send 'asking for bwe' at 1641198722220
sent 'asking for bwe' at 1641198722220
send [estimator, stat] at 1641198722220
sent [estimator, stat] at 1641198722220
pc wait for bwe at 1641198722220
pc got bwe at 1641198722224
bandwidth:  300000
pc flushed at 1641198722224
Bwe Sent: 4 at 1641198722224
got request at 1641198722427
processed allFrame at 1641198722427
send 'asking for bwe' at 1641198722427
sent 'asking for bwe' at 1641198722427
send [estimator, stat] at 1641198722427
sent [estimator, stat] at 1641198722427
pc wait for bwe at 1641198722427
pc got bwe at 1641198722432
bandwidth:  300000
pc flushed at 1641198722432
Bwe Sent: 5 at 1641198722432
got request at 1641198722652
processed allFrame at 1641198722652
send 'asking for bwe' at 1641198722652
sent 'asking for bwe' at 1641198722652
send [estimator, stat] at 1641198722652
sent [estimator, stat] at 1641198722652
pc wait for bwe at 1641198722652
pc got bwe at 1641198722656
bandwidth:  300000
pc flushed at 1641198722656
Bwe Sent: 4 at 1641198722656
got request at 1641198722887
processed allFrame at 1641198722887
send 'asking for bwe' at 1641198722887
sent 'asking for bwe' at 1641198722887
send [estimator, stat] at 1641198722887
sent [estimator, stat] at 1641198722887
pc wait for bwe at 1641198722887
pc got bwe at 1641198722892
bandwidth:  300000
pc flushed at 1641198722892
Bwe Sent: 5 at 1641198722892
got request at 1641198723122
processed allFrame at 1641198723122
send 'asking for bwe' at 1641198723122
sent 'asking for bwe' at 1641198723122
send [estimator, stat] at 1641198723122
sent [estimator, stat] at 1641198723122
pc wait for bwe at 1641198723122
pc got bwe at 1641198723126
bandwidth:  300000
pc flushed at 1641198723126
Bwe Sent: 4 at 1641198723126
got request at 1641198723349
processed allFrame at 1641198723349
send 'asking for bwe' at 1641198723349
sent 'asking for bwe' at 1641198723349
send [estimator, stat] at 1641198723349
sent [estimator, stat] at 1641198723349
pc wait for bwe at 1641198723349
pc got bwe at 1641198723354
bandwidth:  300000
pc flushed at 1641198723354
Bwe Sent: 5 at 1641198723354
got request at 1641198723550
processed allFrame at 1641198723550
send 'asking for bwe' at 1641198723550
sent 'asking for bwe' at 1641198723550
send [estimator, stat] at 1641198723550
sent [estimator, stat] at 1641198723550
pc wait for bwe at 1641198723550
pc got bwe at 1641198723555
bandwidth:  300000
pc flushed at 1641198723555
Bwe Sent: 5 at 1641198723555
got request at 1641198723752
processed allFrame at 1641198723752
send 'asking for bwe' at 1641198723752
sent 'asking for bwe' at 1641198723752
send [estimator, stat] at 1641198723752
sent [estimator, stat] at 1641198723752
pc wait for bwe at 1641198723752
pc got bwe at 1641198723756
bandwidth:  300000
pc flushed at 1641198723756
Bwe Sent: 4 at 1641198723756
got request at 1641198724209
processed allFrame at 1641198724209
send 'asking for bwe' at 1641198724209
sent 'asking for bwe' at 1641198724209
0.0
processed state0-2 at 1641198723550
avgFrameBetween:  7
psnrStat:  [[461538, 462436, 462653, 464692, 465851, 465966]]
delayStat:  [[10, 49, 38, 20, 24, 9]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [463856.0] [25.0] [64]
processed state3-5 at 1641198723551
liner_to_log:  tensor([[[1.4036]]]) tensor([[[0.7263]]])
linear_to_log at 1641198723551
listState:  [0.06338, 0.13616666666666666, 0.0, 0.463856, 0.025, 1, tensor([[[0.7263]]])]
state_clone_detach at 1641198723551
reward: 0.011597509272310125
state tensor([0.0634, 0.1362, 0.0000, 0.7263], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198723552
state222:  tensor([[0.0634, 0.1362, 0.0000, 0.7263]], device='cuda:0')
policy_old.forwarded at 1641198723553
give action 20============================
log_to_linear:  tensor([[[0.6351]]], device='cuda:0') tensor([[[1.2304]]], device='cuda:0')
log_to_linear action at 1641198723554
bwe changes from to:  [tensor([[[99171.4766]]]), tensor([[[122022.2969]]])]
step into gymStat at 1641198723555
send bwe to appRecv at 1641198723555
sent bwe to appRecv at 1641198723555
wait for recv string at 1641198723555
recved string at 1641198723752
1
wait for recv [self.estimator, stat] at 1641198723752
recved [self.estimator, stat] at 1641198723752
sorted packlist at 1641198723752
packetSeq:  3420
packetSeq:  3421
packetSeq:  3422
packetSeq:  3423
packetSeq:  3424
packetSeq:  3425
packetSeq:  3426
packetSeq:  3427
packetSeq:  3428
packetSeq:  3429
processed packlist at 1641198723752
receiving_rate:  267360.0
delay:  191.6
loss_ratio:  0.0
processed state0-2 at 1641198723752
avgFrameBetween:  7
psnrStat:  [[467670, 467822, 468711, 469950, 470596, 471407]]
delayStat:  [[11, 12, 10, 10, 11, 11]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  8
state:  [469359.3333333333] [10.833333333333334] [64]
processed state3-5 at 1641198723752
liner_to_log:  tensor([[[1.2304]]]) tensor([[[0.6351]]])
linear_to_log at 1641198723753
listState:  [0.06684, 0.12773333333333334, 0.0, 0.4693593333333333, 0.010833333333333334, 1, tensor([[[0.6351]]])]
state_clone_detach at 1641198723753
reward: 0.05316682356565461
state tensor([0.0668, 0.1277, 0.0000, 0.6351], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198723753
state222:  tensor([[0.0668, 0.1277, 0.0000, 0.6351]], device='cuda:0')
policy_old.forwarded at 1641198723755
give action 21============================
log_to_linear:  tensor([[[0.3844]]], device='cuda:0') tensor([[[0.8296]]], device='cuda:0')
log_to_linear action at 1641198723756
bwe changes from to:  [tensor([[[122022.2969]]]), tensor([[[101230.3906]]])]
step into gymStat at 1641198723756
send bwe to appRecv at 1641198723756
sent bwe to appRecv at 1641198723756
wait for recv string at 1641198723756
recved string at 1641198724209
1
wait for recv [self.estimator, stat] at 1641198724209
recved [self.estimator, stat] at 1641198724209
sorted packlist at 1641198724209
packetSeq:  3430
packetSeq:  3431
packetSeq:  3432
packetSeq:  3433
packetSeq:  3434
packetSeq:  3435
processed packlist at 1641198724209
receiving_rate:  43480.0
delay:  481.0
loss_ratio:  0.0
processed state0-2 at 1641198724210
avgFrameBetween:  7
psnrStat:  [[473222, 472799, 473552, 473614, 474572]]
delayStat:  [[12, 7, 9, 11, 8]]
skipStat:  [[1, 1, 1, 1, 1]]
skipCount:  9
state:  [473551.8] [9.4] [64]
processed state3-5 at 1641198724210
liner_to_log:  tensor([[[0.8296]]]) tensor([[[0.3844]]])
linear_to_log at 1641198724210
listState:  [0.01087, 0.32066666666666666, 0.0, 0.47355179999999997, 0.0094, 1, tensor([[[0.3844]]])]
state_clone_detach at 1641198724210
reward: -0.8657908544098717
state tensor([0.0109, 0.3207, 0.0000, 0.3844], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198724211
state222:  tensor([[0.0109, 0.3207, 0.0000, 0.3844]], device='cuda:0')
policy_old.forwarded at 1641198724213
give action 22============================
log_to_linear:  tensor([[[0.4349]]], device='cuda:0') tensor([[[0.9008]]], device='cuda:0')
log_to_linear action at 1641198724214
bwe changes from to:  [tensor([[[101230.3906]]]), tensor([[[91190.0938]]])]
step into gymStat at 1641198724214
send bwe to appRecv at 1641198724214
sent bwe to appRecv at 1641198724214
wait for recv string at 1641198724214
recved string at 1641198724419
1
wait for recv [self.estimator, stat] at 1641198724419
recved [self.estimator, stat] at 1641198724419
sorted packlist at 1641198724419
packetSeq:  3436
packetSeq:  3437
packetSeq:  3438
packetSeq:  3439
packetSeq:  3440
packetSeq:  3441
packetSeq:  3442
packetSeq:  3443
packetSeq:  3444
packetSeq:  3445
packetSeq:  3446
packetSeq:  3447
packetSeq:  3448
packetSeq:  3449
packetSeq:  3450
packetSeq:  3451
packetSeq:  3452
packetSeq:  3453
packetSeq:  3454
packetSeq:  3455
packetSeq:  3456
packetSeq:  3457
packetSeq:  3458
packetSeq:  3459
packetSeq:  3460
processed packlist at 1641198724420
receiving_rate:  654440.0
delay:  290.5416666666667
loss_ratio:  0.0
processed state0-2 at 1641198724420
avgFrameBetween:  7
psnrStat:  [[474637, 474996, 476112, 475455, 476255, 476342, 476925, 476621, 477391, 477628, 478655, 477489, 478470, 477866, 477252]]
delayStat:  [[303, 274, 253, 227, 205, 181, 157, 137, 114, 93, 72, 49, 29, 20, 20]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [477769.25] [29.5] [4]
processed state3-5 at 1641198724420
liner_to_log:  tensor([[[0.9008]]]) tensor([[[0.4349]]])
linear_to_log at 1641198724420
listState:  [0.16361, 0.19369444444444445, 0.0, 0.47776925, 0.0295, 0.125, tensor([[[0.4349]]])]
state_clone_detach at 1641198724420
reward: 0.19042969653534614
state tensor([0.1636, 0.1937, 0.0000, 0.4349], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198724421
state222:  tensor([[0.1636, 0.1937, 0.0000, 0.4349]], device='cuda:0')
policy_old.forwarded at 1641198724423
give action 23============================
log_to_linear:  tensor([[[0.5820]]], device='cuda:0') tensor([[[1.1361]]], device='cuda:0')
log_to_linear action at 1641198724423
bwe changes from to:  [tensor([[[91190.0938]]]), tensor([[[103605.5938]]])]
step into gymStat at 1641198724424
send bwe to appRecv at 1641198724424
sent bwe to appRecv at 1641198724424
wait for recv string at 1641198724424
recved string at 1641198724621
1
wait for recv [self.estimator, stat] at 1641198724621
recved [self.estimator, stat] at 1641198724622
sorted packlist at 1641198724622
packetSeq:  3461
packetSeq:  3462
packetSeq:  3463
packetSeq:  3464
packetSeq:  3465
packetSeq:  3466
packetSeq:  3467
packetSeq:  3468
packetSeq:  3469
packetSeq:  3470
packetSeq:  3471
processed packlist at 1641198724622
receiving_rate:  282920.0
delay:  191.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198724622
avgFrameBetween:  7
psnrStat:  [[477126, 476894, 475887, 474389, 474060, 473565]]
delayStat:  [[19, 19, 17, 18, 17, 17]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  3
state:  [474004.6666666667] [17.333333333333332] [8]
processed state3-5 at 1641198724622
liner_to_log:  tensor([[[1.1361]]]) tensor([[[0.5820]]])
linear_to_log at 1641198724622
listState:  [0.07073, 0.1276969696969697, 0.0, 0.4740046666666667, 0.017333333333333333, 0.25, tensor([[[0.5820]]])]
state_clone_detach at 1641198724622
reward: 0.07105533164694988
state tensor([0.0707, 0.1277, 0.0000, 0.5820], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198724623
state222:  tensor([[0.0707, 0.1277, 0.0000, 0.5820]], device='cuda:0')
policy_old.forwarded at 1641198724625
give action 24============================
log_to_linear:  tensor([[[0.3312]]], device='cuda:0') tensor([[[0.7603]]], device='cuda:0')
log_to_linear action at 1641198724625
bwe changes from to:  [tensor([[[103605.5938]]]), tensor([[[78769.7969]]])]
step into gymStat at 1641198724626
send bwe to appRecv at 1641198724626
sent bwe to appRecv at 1641198724626
wait for recv string at 1641198724626
recved string at 1641198724822
1
wait for recv [self.estimator, stat] at 1641198724822
recved [self.estimator, stat] at 1641198724822
sorted packlist at 1641198724822
packetSeq:  3472
packetSeq:  3473
packetSeq:  3474
packetSeq:  3475
packetSeq:  3476
packetSeq:  3477
packetSeq:  3478
packetSeq:  3479
processed packlist at 1641198724822
receiving_rate:  257320.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198724822
avgFrameBetween:  7
psnrStat:  [[472620, 472351, 472554, 473107, 473673]]
delayStat:  [[16, 15, 46, 46, 44]]
skipStat:  [[1, 1, 1, 1, 1]]
skipCount:  4
state:  [473673.0] [44.0] [32]
processed state3-5 at 1641198724823
liner_to_log:  tensor([[[0.7603]]]) tensor([[[0.3312]]])
linear_to_log at 1641198724823
listState:  [0.06433, 0.12816666666666668, 0.0, 0.473673, 0.044, 1.0, tensor([[[0.3312]]])]
state_clone_detach at 1641198724823
reward: 0.04010863020218941
state tensor([0.0643, 0.1282, 0.0000, 0.3312], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198724824
state222:  tensor([[0.0643, 0.1282, 0.0000, 0.3312]], device='cuda:0')
policy_old.forwarded at 1641198724826
give action 25============================
log_to_linear:  tensor([[[0.4572]]], device='cuda:0') tensor([[[0.9339]]], device='cuda:0')
log_to_linear action at 1641198724827
bwe changes from to:  [tensor([[[78769.7969]]]), tensor([[[73564.0781]]])]
step into gymStat at 1641198724827
send bwe to appRecv at 1641198724827
sent bwe to appRecv at 1641198724827
wait for recv string at 1641198724827
recved string at 1641198725025
1
wait for recv [self.estimator, stat] at 1641198725025
recved [self.estimator, stat] at 1641198725025
sorted packlist at 1641198725025
packetSeq:  3480
packetSeq:  3481
packetSeq:  3482
packetSeq:  3483
packetSeq:  3484
packetSeq:  3485
packetSeq:  3486
packetSeq:  3487
packetSeq:  3488
packetSeq:  3489
packetSeq:  3490
processed packlist at 1641198725025
receiving_rate:  272360.0
delay:  191.63636363636363
loss_ratio:  0.0
processed state0-2 at 1641198725025
avgFrameBetween:  7
psnrStat:  [[475071, 476034, 476566, 477678, 478964, 479460, 479211]]
delayStat:  [[45, 45, 44, 44, 43, 43, 43]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [479211.0] [43.0] [32]
processed state3-5 at 1641198725025
liner_to_log:  tensor([[[0.9339]]]) tensor([[[0.4572]]])
linear_to_log at 1641198725026
listState:  [0.06809, 0.12775757575757576, 0.0, 0.479211, 0.043, 1.0, tensor([[[0.4572]]])]
state_clone_detach at 1641198725026
reward: 0.0588649252212487
state tensor([0.0681, 0.1278, 0.0000, 0.4572], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198725026
state222:  tensor([[0.0681, 0.1278, 0.0000, 0.4572]], device='cuda:0')
policy_old.forwarded at 1641198725028
give action 26============================
log_to_linear:  tensor([[[0.7257]]], device='cuda:0') tensor([[[1.4023]]], device='cuda:0')
log_to_linear action at 1641198725029
bwe changes from to:  [tensor([[[73564.0781]]]), tensor([[[103158.9766]]])]
step into gymStat at 1641198725029
send bwe to appRecv at 1641198725029
sent bwe to appRecv at 1641198725029
wait for recv string at 1641198725029
recved string at 1641198725229
1
wait for recv [self.estimator, stat] at 1641198725229
recved [self.estimator, stat] at 1641198725229
sorted packlist at 1641198725229
packetSeq:  3491
packetSeq:  3492
packetSeq:  3493
packetSeq:  3494
packetSeq:  3495
packetSeq:  3496
packetSeq:  3497
packetSeq:  3498
packetSeq:  3499
packetSeq:  3500
packetSeq:  3501
processed packlist at 1641198725229
receiving_rate:  296600.0
delay:  192.0909090909091
loss_ratio:  0.0
processed state0-2 at 1641198725229
avgFrameBetween:  7
psnrStat:  [[479417, 479071, 479067, 477948, 477137, 476431]]
delayStat:  [[42, 42, 41, 42, 41, 40]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [478178.5] [41.333333333333336] [64]
processed state3-5 at 1641198725229
liner_to_log:  tensor([[[1.4023]]]) tensor([[[0.7257]]])
linear_to_log at 1641198725230
listState:  [0.07415, 0.12806060606060607, 0.0, 0.4781785, 0.04133333333333333, 1, tensor([[[0.7257]]])]
state_clone_detach at 1641198725230
reward: 0.08517199519721447
state tensor([0.0742, 0.1281, 0.0000, 0.7257], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198725230
state222:  tensor([[0.0742, 0.1281, 0.0000, 0.7257]], device='cuda:0')
policy_old.forwarded at 1641198725232
give action 27============================
log_to_linear:  tensor([[[0.4563]]], device='cuda:0') tensor([[[0.9325]]], device='cuda:0')
log_to_linear action at 1641198725233
bwe changes from to:  [tensor([[[103158.9766]]]), tensor([[[96199.4609]]])]
step into gymStat at 1641198725233
send bwe to appRecv at 1641198725233
sent bwe to appRecv at 1641198725233
wait for recv string at 1641198725233
recved string at 1641198725452
1
wait for recv [self.estimator, stat] at 1641198725452
recved [self.estimator, stat] at 1641198725452
sorted packlist at 1641198725452
packetSeq:  3502
packetSeq:  3503
packetSeq:  3504
packetSeq:  3505
packetSeq:  3506
packetSeq:  3507
packetSeq:  3508
packetSeq:  3509
packetSeq:  3510
packetSeq:  3511
packetSeq:  3512
processed packlist at 1641198725452
receiving_rate:  304240.0
delay:  192.1818181818182
loss_ratio:  0.0
processed state0-2 at 1641198725452
avgFrameBetween:  7
psnrStat:  [[475884, 475373, 474733, 474056, 472962, 474034]]
delayStat:  [[36, 41, 39, 40, 40, 40]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [474507.0] [39.333333333333336] [64]
processed state3-5 at 1641198725452
liner_to_log:  tensor([[[0.9325]]]) tensor([[[0.4563]]])
linear_to_log at 1641198725453
listState:  [0.07606, 0.12812121212121214, 0.0, 0.474507, 0.03933333333333334, 1, tensor([[[0.4563]]])]
state_clone_detach at 1641198725453
reward: 0.0933184964794252
state tensor([0.0761, 0.1281, 0.0000, 0.4563], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198725453
state222:  tensor([[0.0761, 0.1281, 0.0000, 0.4563]], device='cuda:0')
policy_old.forwarded at 1641198725455
give action 28============================
log_to_linear:  tensor([[[0.4922]]], device='cuda:0') tensor([[[0.9877]]], device='cuda:0')
log_to_linear action at 1641198725456
bwe changes from to:  [tensor([[[96199.4609]]]), tensor([[[95017.6953]]])]
step into gymStat at 1641198725456
send bwe to appRecv at 1641198725456
sent bwe to appRecv at 1641198725456
wait for recv string at 1641198725456
recved string at 1641198725655
1
wait for recv [self.estimator, stat] at 1641198725655
recved [self.estimator, stat] at 1641198725655
sorted packlist at 1641198725655
packetSeq:  3513
packetSeq:  3514
packetSeq:  3515
packetSeq:  3516
packetSeq:  3517
packetSeq:  3518
packetSeq:  3519
packetSeq:  3520
packetSeq:  3521
packetSeq:  3522
packetSeq:  3523
packetSeq:  3524
packetSeq:  3525
processed packlist at 1641198725655
receiving_rate:  318600.0
delay:  191.3846153846154
loss_ratio:  0.0
processed state0-2 at 1641198725655
avgFrameBetween:  7
psnrStat:  [[473358, 473753, 474260, 473571, 473847, 473027, 472308]]
delayStat:  [[39, 38, 39, 39, 38, 38, 38]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  8
state:  [473446.28571428574] [38.42857142857143] [64]
processed state3-5 at 1641198725655
liner_to_log:  tensor([[[0.9877]]]) tensor([[[0.4922]]])
linear_to_log at 1641198725655
listState:  [0.07965, 0.1275897435897436, 0.0, 0.47344628571428576, 0.03842857142857143, 1, tensor([[[0.4922]]])]
state_clone_detach at 1641198725656
reward: 0.11026053804356095
state tensor([0.0796, 0.1276, 0.0000, 0.4922], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198725656
state222:  tensor([[0.0796, 0.1276, 0.0000, 0.4922]], device='cuda:0')
policy_old.forwarded at 1641198725658
give action 29============================
log_to_linear:  tensor([[[1.0820]]], device='cuda:0') tensor([[[2.1995]]], device='cuda:0')
log_to_linear action at 1641198725659
bwe changes from to:  [tensor([[[95017.6953]]]), tensor([[[190035.3906]]])]
step into gymStat at 1641198725659
send bwe to appRecv at 1641198725659
sent bwe to appRecv at 1641198725659
wait for recv string at 1641198725659
recved string at 1641198725857
1
wait for recv [self.estimator, stat] at 1641198725857
recved [self.estimator, stat] at 1641198725857
sorted packlist at 1641198725857
packetSeq:  3526
packetSeq:  3527
packetSeq:  3528
packetSeq:  3529
packetSeq:  3530
packetSeq:  3531
packetSeq:  3532
packetSeq:  3533
packetSeq:  3534
packetSeq:  3535
processed packlist at 1641198725857
receiving_rate:  269440.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198725857
avgFrameBetween:  7
psnrStat:  [[473605, 471347, 471556, 470171]]
delayStat:  [[38, 98, 97, 98]]
skipStat:  [[1, 1, 1, 1]]
skipCount:  8
state:  [471669.75] [82.75] [64]
processed state3-5 at 1641198725857
liner_to_log:  tensor([[[2.]]]) tensor([[[1.]]])
linear_to_log at 1641198725857
listState:  [0.06736, 0.12793333333333334, 0.0, 0.47166975, 0.08275, 1, tensor([[[1.]]])]
state_clone_detach at 1641198725857
reward: 0.054974234202790906
state tensor([0.0674, 0.1279, 0.0000, 1.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198725858
state222:  tensor([[0.0674, 0.1279, 0.0000, 1.0000]], device='cuda:0')
policy_old.forwarded at 1641198725860
give action 30============================
log_to_linear:  tensor([[[0.6723]]], device='cuda:0') tensor([[[1.2994]]], device='cuda:0')
log_to_linear action at 1641198725861
bwe changes from to:  [tensor([[[190035.3906]]]), tensor([[[246940.9219]]])]
step into gymStat at 1641198725861
send bwe to appRecv at 1641198725861
sent bwe to appRecv at 1641198725861
wait for recv string at 1641198725861
recved string at 1641198726086
1
wait for recv [self.estimator, stat] at 1641198726086
recved [self.estimator, stat] at 1641198726087
sorted packlist at 1641198726087
packetSeq:  3536
packetSeq:  3537
packetSeq:  3538
packetSeq:  3539
packetSeq:  3540
packetSeq:  3541
packetSeq:  3542
packetSeq:  3543
packetSeq:  3544
packetSeq:  3545
processed packlist at 1641198726087
receiving_rate:  272600.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198726087
avgFrameBetween:  7
psnrStat:  [[470761, 469569, 469493, 469840, 470046, 469780, 469898]]
delayStat:  [[97, 97, 98, 96, 97, 97, 97]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [469912.4285714286] [97.0] [64]
processed state3-5 at 1641198726087
liner_to_log:  tensor([[[1.2994]]]) tensor([[[0.6723]]])
linear_to_log at 1641198726087
listState:  [0.06815, 0.12822222222222224, 0.0, 0.46991242857142856, 0.097, 1, tensor([[[0.6723]]])]
state_clone_detach at 1641198726087
reward: 0.057746594467892653
state tensor([0.0681, 0.1282, 0.0000, 0.6723], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198726088
state222:  tensor([[0.0681, 0.1282, 0.0000, 0.6723]], device='cuda:0')
policy_old.forwarded at 1641198726090
give action 31============================
log_to_linear:  tensor([[[0.7181]]], device='cuda:0') tensor([[[1.3875]]], device='cuda:0')
log_to_linear action at 1641198726090
bwe changes from to:  [tensor([[[246940.9219]]]), tensor([[[342642.5000]]])]
step into gymStat at 1641198726091
send bwe to appRecv at 1641198726091
sent bwe to appRecv at 1641198726091
wait for recv string at 1641198726091
recved string at 1641198726288
1
wait for recv [self.estimator, stat] at 1641198726288
recved [self.estimator, stat] at 1641198726289
sorted packlist at 1641198726289
packetSeq:  3546
packetSeq:  3547
packetSeq:  3548
packetSeq:  3549
packetSeq:  3550
packetSeq:  3551
packetSeq:  3552
packetSeq:  3553
packetSeq:  3554
packetSeq:  3555
packetSeq:  3556
processed packlist at 1641198726289
receiving_rate:  273400.0
delay:  191.63636363636363
loss_ratio:  0.0
processed state0-2 at 1641198726289
avgFrameBetween:  6
psnrStat:  [[471135, 472053, 471494, 471840, 471681, 471354]]
delayStat:  [[97, 96, 96, 95, 96, 95]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [471592.8333333333] [95.83333333333333] [32]
processed state3-5 at 1641198726289
liner_to_log:  tensor([[[1.3875]]]) tensor([[[0.7181]]])
linear_to_log at 1641198726289
listState:  [0.06835, 0.12775757575757576, 0.0, 0.4715928333333333, 0.09583333333333333, 1.0, tensor([[[0.7181]]])]
state_clone_detach at 1641198726289
reward: 0.060058316067552076
state tensor([0.0684, 0.1278, 0.0000, 0.7181], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198726290
state222:  tensor([[0.0684, 0.1278, 0.0000, 0.7181]], device='cuda:0')
policy_old.forwarded at 1641198726292
give action 32============================
log_to_linear:  tensor([[[0.1626]]], device='cuda:0') tensor([[[0.5843]]], device='cuda:0')
log_to_linear action at 1641198726292
bwe changes from to:  [tensor([[[342642.5000]]]), tensor([[[200199.3125]]])]
step into gymStat at 1641198726293
send bwe to appRecv at 1641198726293
sent bwe to appRecv at 1641198726293
wait for recv string at 1641198726293
recved string at 1641198726516
1
wait for recv [self.estimator, stat] at 1641198726516
recved [self.estimator, stat] at 1641198726516
sorted packlist at 1641198726516
packetSeq:  3557
packetSeq:  3558
packetSeq:  3559
packetSeq:  3560
packetSeq:  3561
packetSeq:  3562
packetSeq:  3563
packetSeq:  3564
packetSeq:  3565
packetSeq:  3566
processed packlist at 1641198726516
receiving_rate:  284280.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198726516
avgFrameBetween:  6
psnrStat:  [[471942, 471677, 472346, 472155, 470990, 473522, 473330]]
delayStat:  [[96, 96, 96, 95, 94, 95, 94]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [472280.28571428574] [95.14285714285714] [32]
processed state3-5 at 1641198726516
liner_to_log:  tensor([[[0.5843]]]) tensor([[[0.1626]]])
linear_to_log at 1641198726517
listState:  [0.07107, 0.12806666666666666, 0.0, 0.47228028571428576, 0.09514285714285714, 1.0, tensor([[[0.1626]]])]
state_clone_detach at 1641198726517
reward: 0.07147547694799489
state tensor([0.0711, 0.1281, 0.0000, 0.1626], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198726517
state222:  tensor([[0.0711, 0.1281, 0.0000, 0.1626]], device='cuda:0')
policy_old.forwarded at 1641198726519
give action 33============================
log_to_linear:  tensor([[[0.3958]]], device='cuda:0') tensor([[[0.8453]]], device='cuda:0')
log_to_linear action at 1641198726520
bwe changes from to:  [tensor([[[200199.3125]]]), tensor([[[169223.9688]]])]
step into gymStat at 1641198726520
send bwe to appRecv at 1641198726520
sent bwe to appRecv at 1641198726520
wait for recv string at 1641198726520
recved string at 1641198726721
1
wait for recv [self.estimator, stat] at 1641198726721
recved [self.estimator, stat] at 1641198726722
sorted packlist at 1641198726722
packetSeq:  3567
packetSeq:  3568
packetSeq:  3569
packetSeq:  3570
packetSeq:  3571
packetSeq:  3572
packetSeq:  3573
packetSeq:  3574
packetSeq:  3575
processed packlist at 1641198726722
receiving_rate:  269560.0
delay:  192.875
loss_ratio:  0.0
processed state0-2 at 1641198726722
avgFrameBetween:  6
psnrStat:  [[473084, 473023, 473398, 472980, 473551, 474264]]
delayStat:  [[95, 94, 95, 96, 94, 94]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  10
state:  [473383.3333333333] [94.66666666666667] [32]
processed state3-5 at 1641198726722
liner_to_log:  tensor([[[0.8453]]]) tensor([[[0.3958]]])
linear_to_log at 1641198726722
listState:  [0.06739, 0.12858333333333333, 0.0, 0.4733833333333333, 0.09466666666666668, 1.0, tensor([[[0.3958]]])]
state_clone_detach at 1641198726722
reward: 0.05316282900610214
state tensor([0.0674, 0.1286, 0.0000, 0.3958], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198726723
state222:  tensor([[0.0674, 0.1286, 0.0000, 0.3958]], device='cuda:0')
policy_old.forwarded at 1641198726725
give action 34============================
log_to_linear:  tensor([[[0.3726]]], device='cuda:0') tensor([[[0.8137]]], device='cuda:0')
log_to_linear action at 1641198726725
bwe changes from to:  [tensor([[[169223.9688]]]), tensor([[[137690.3750]]])]
step into gymStat at 1641198726726
send bwe to appRecv at 1641198726726
sent bwe to appRecv at 1641198726726
wait for recv string at 1641198726726
recved string at 1641198726924
1
wait for recv [self.estimator, stat] at 1641198726924
recved [self.estimator, stat] at 1641198726924
sorted packlist at 1641198726924
packetSeq:  3576
packetSeq:  3577
packetSeq:  3578
packetSeq:  3579
packetSeq:  3580
packetSeq:  3581
packetSeq:  3582
packetSeq:  3583
packetSeq:  3584
packetSeq:  3585
processed packlist at 1641198726924
receiving_rate:  278000.0
delay:  192.3
loss_ratio:  0.0
processed state0-2 at 1641198726924
avgFrameBetween:  6
psnrStat:  [[473146, 473982, 473332, 474667]]
delayStat:  [[147, 150, 149, 137]]
skipStat:  [[1, 1, 1, 1]]
skipCount:  10
state:  [473781.75] [145.75] [32]
processed state3-5 at 1641198726924
liner_to_log:  tensor([[[0.8137]]]) tensor([[[0.3726]]])
linear_to_log at 1641198726924
listState:  [0.0695, 0.1282, 0.0, 0.47378175, 0.14575, 1.0, tensor([[[0.3726]]])]
state_clone_detach at 1641198726925
reward: 0.06398121950640906
state tensor([0.0695, 0.1282, 0.0000, 0.3726], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198726925
state222:  tensor([[0.0695, 0.1282, 0.0000, 0.3726]], device='cuda:0')
policy_old.forwarded at 1641198726927
give action 35============================
log_to_linear:  tensor([[[0.4704]]], device='cuda:0') tensor([[[0.9540]]], device='cuda:0')
log_to_linear action at 1641198726928
bwe changes from to:  [tensor([[[137690.3750]]]), tensor([[[131353.7500]]])]
step into gymStat at 1641198726928
send bwe to appRecv at 1641198726928
sent bwe to appRecv at 1641198726928
wait for recv string at 1641198726928
recved string at 1641198727152
1
wait for recv [self.estimator, stat] at 1641198727152
recved [self.estimator, stat] at 1641198727152
sorted packlist at 1641198727152
packetSeq:  3586
packetSeq:  3587
packetSeq:  3588
packetSeq:  3589
packetSeq:  3590
packetSeq:  3591
packetSeq:  3592
packetSeq:  3593
packetSeq:  3594
processed packlist at 1641198727152
receiving_rate:  289760.0
delay:  192.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198727152
avgFrameBetween:  6
psnrStat:  [[474782, 474210, 474195, 474362, 473579, 473406, 472793]]
delayStat:  [[149, 147, 149, 148, 149, 149, 150]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  12
state:  [473903.85714285716] [148.71428571428572] [32]
processed state3-5 at 1641198727152
liner_to_log:  tensor([[[0.9540]]]) tensor([[[0.4704]]])
linear_to_log at 1641198727152
listState:  [0.07244, 0.1285185185185185, 0.0, 0.47390385714285715, 0.14871428571428572, 1.0, tensor([[[0.4704]]])]
state_clone_detach at 1641198727153
reward: 0.07624265151859855
state tensor([0.0724, 0.1285, 0.0000, 0.4704], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198727153
state222:  tensor([[0.0724, 0.1285, 0.0000, 0.4704]], device='cuda:0')
policy_old.forwarded at 1641198727155
give action 36============================
log_to_linear:  tensor([[[0.1525]]], device='cuda:0') tensor([[[0.5762]]], device='cuda:0')
log_to_linear action at 1641198727156
bwe changes from to:  [tensor([[[131353.7500]]]), tensor([[[75679.9062]]])]
step into gymStat at 1641198727156
send bwe to appRecv at 1641198727156
sent bwe to appRecv at 1641198727156
wait for recv string at 1641198727156
recved string at 1641198727353
1
wait for recv [self.estimator, stat] at 1641198727353
recved [self.estimator, stat] at 1641198727353
sorted packlist at 1641198727353
packetSeq:  3595
packetSeq:  3596
packetSeq:  3597
packetSeq:  3598
packetSeq:  3599
packetSeq:  3600
packetSeq:  3601
packetSeq:  3602
packetSeq:  3603
packetSeq:  3604
processed packlist at 1641198727353
receiving_rate:  285280.0
delay:  191.6
loss_ratio:  0.0
processed state0-2 at 1641198727354
avgFrameBetween:  6
psnrStat:  [[474458, 473317, 474021, 473353, 474309, 473137]]
delayStat:  [[149, 147, 148, 148, 148, 148]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [473765.8333333333] [148.0] [32]
processed state3-5 at 1641198727354
liner_to_log:  tensor([[[0.5762]]]) tensor([[[0.1525]]])
linear_to_log at 1641198727354
listState:  [0.07132, 0.12773333333333334, 0.0, 0.47376583333333333, 0.148, 1.0, tensor([[[0.1525]]])]
state_clone_detach at 1641198727354
reward: 0.07359743396738899
state tensor([0.0713, 0.1277, 0.0000, 0.1525], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198727355
state222:  tensor([[0.0713, 0.1277, 0.0000, 0.1525]], device='cuda:0')
policy_old.forwarded at 1641198727357
give action 37============================
log_to_linear:  tensor([[[0.5828]]], device='cuda:0') tensor([[[1.1375]]], device='cuda:0')
log_to_linear action at 1641198727357
bwe changes from to:  [tensor([[[75679.9062]]]), tensor([[[86084.0938]]])]
step into gymStat at 1641198727358
send bwe to appRecv at 1641198727358
sent bwe to appRecv at 1641198727358
wait for recv string at 1641198727358
recved string at 1641198727555
1
wait for recv [self.estimator, stat] at 1641198727555
recved [self.estimator, stat] at 1641198727555
sorted packlist at 1641198727555
packetSeq:  3605
packetSeq:  3606
packetSeq:  3607
packetSeq:  3608
packetSeq:  3609
packetSeq:  3610
packetSeq:  3611
packetSeq:  3612
packetSeq:  3613
packetSeq:  3614
processed packlist at 1641198727555
receiving_rate:  290880.0
delay:  192.2
loss_ratio:  0.0
processed state0-2 at 1641198727555
avgFrameBetween:  6
psnrStat:  [[473400, 473019, 472041, 471552, 470714, 470169]]
delayStat:  [[147, 148, 147, 148, 148, 147]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [471815.8333333333] [147.5] [32]
processed state3-5 at 1641198727555
liner_to_log:  tensor([[[1.1375]]]) tensor([[[0.5828]]])
linear_to_log at 1641198727556
listState:  [0.07272, 0.12813333333333332, 0.0, 0.4718158333333333, 0.1475, 1.0, tensor([[[0.5828]]])]
state_clone_detach at 1641198727556
reward: 0.07864191495210565
state tensor([0.0727, 0.1281, 0.0000, 0.5828], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198727556
state222:  tensor([[0.0727, 0.1281, 0.0000, 0.5828]], device='cuda:0')
policy_old.forwarded at 1641198727558
give action 38============================
log_to_linear:  tensor([[[0.3508]]], device='cuda:0') tensor([[[0.7852]]], device='cuda:0')
log_to_linear action at 1641198727559
bwe changes from to:  [tensor([[[86084.0938]]]), tensor([[[67591.3750]]])]
step into gymStat at 1641198727559
send bwe to appRecv at 1641198727559
sent bwe to appRecv at 1641198727559
wait for recv string at 1641198727559
recved string at 1641198727758
1
wait for recv [self.estimator, stat] at 1641198727758
recved [self.estimator, stat] at 1641198727758
sorted packlist at 1641198727758
packetSeq:  3615
packetSeq:  3616
packetSeq:  3617
packetSeq:  3618
packetSeq:  3619
packetSeq:  3620
packetSeq:  3621
packetSeq:  3622
packetSeq:  3623
packetSeq:  3624
packetSeq:  3625
processed packlist at 1641198727758
receiving_rate:  273160.0
delay:  196.0
loss_ratio:  0.0
processed state0-2 at 1641198727758
avgFrameBetween:  6
psnrStat:  [[471467, 470832, 470921, 470149, 470837]]
delayStat:  [[148, 147, 147, 147, 147]]
skipStat:  [[1, 1, 1, 1, 1]]
skipCount:  11
state:  [470841.2] [147.2] [32]
processed state3-5 at 1641198727758
liner_to_log:  tensor([[[0.7852]]]) tensor([[[0.3508]]])
linear_to_log at 1641198727759
listState:  [0.06829, 0.13066666666666665, 0.0, 0.4708412, 0.1472, 1.0, tensor([[[0.3508]]])]
state_clone_detach at 1641198727759
reward: 0.05105585605522317
state tensor([0.0683, 0.1307, 0.0000, 0.3508], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198727759
state222:  tensor([[0.0683, 0.1307, 0.0000, 0.3508]], device='cuda:0')
policy_old.forwarded at 1641198727761
give action 39============================
log_to_linear:  tensor([[[0.5499]]], device='cuda:0') tensor([[[1.0813]]], device='cuda:0')
log_to_linear action at 1641198727762
bwe changes from to:  [tensor([[[67591.3750]]]), tensor([[[73088.3828]]])]
step into gymStat at 1641198727762
send bwe to appRecv at 1641198727762
sent bwe to appRecv at 1641198727762
wait for recv string at 1641198727762
recved string at 1641198727960
1
wait for recv [self.estimator, stat] at 1641198727960
recved [self.estimator, stat] at 1641198727960
sorted packlist at 1641198727960
packetSeq:  3626
packetSeq:  3627
packetSeq:  3628
packetSeq:  3629
packetSeq:  3630
packetSeq:  3631
packetSeq:  3632
packetSeq:  3633
packetSeq:  3634
processed packlist at 1641198727960
receiving_rate:  265800.0
delay:  197.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198727960
avgFrameBetween:  6
psnrStat:  [[470782, 470604, 470009, 469870, 469908, 469222]]
delayStat:  [[187, 189, 189, 189, 189, 189]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  12
state:  [470065.8333333333] [188.66666666666666] [32]
processed state3-5 at 1641198727960
liner_to_log:  tensor([[[1.0813]]]) tensor([[[0.5499]]])
linear_to_log at 1641198727960
listState:  [0.06645, 0.13177777777777777, 0.0, 0.4700658333333333, 0.18866666666666665, 1.0, tensor([[[0.5499]]])]
state_clone_detach at 1641198727960
reward: 0.03922157201766552
state tensor([0.0664, 0.1318, 0.0000, 0.5499], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198727961
state222:  tensor([[0.0664, 0.1318, 0.0000, 0.5499]], device='cuda:0')
policy_old.forwarded at 1641198727963
give action 40============================
log_to_linear:  tensor([[[0.5099]]], device='cuda:0') tensor([[[1.0158]]], device='cuda:0')
log_to_linear action at 1641198727964
bwe changes from to:  [tensor([[[73088.3828]]]), tensor([[[74243.3672]]])]
step into gymStat at 1641198727964
send bwe to appRecv at 1641198727964
sent bwe to appRecv at 1641198727964
wait for recv string at 1641198727964
recved string at 1641198728191
1
wait for recv [self.estimator, stat] at 1641198728191
recved [self.estimator, stat] at 1641198728191
sorted packlist at 1641198728191
packetSeq:  3635
packetSeq:  3636
packetSeq:  3637
packetSeq:  3638
packetSeq:  3639
packetSeq:  3640
packetSeq:  3641
packetSeq:  3642
processed packlist at 1641198728191
receiving_rate:  288000.0
delay:  201.75
loss_ratio:  0.0
processed state0-2 at 1641198728191
avgFrameBetween:  6
psnrStat:  [[468957, 469137, 469356, 469754, 467790, 468506, 467604]]
delayStat:  [[188, 190, 188, 189, 189, 188, 189]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  12
state:  [468729.14285714284] [188.71428571428572] [32]
processed state3-5 at 1641198728191
liner_to_log:  tensor([[[1.0158]]]) tensor([[[0.5099]]])
linear_to_log at 1641198728192
listState:  [0.072, 0.1345, 0.0, 0.46872914285714284, 0.18871428571428572, 1.0, tensor([[[0.5099]]])]
state_clone_detach at 1641198728192
reward: 0.05633858537972275
state tensor([0.0720, 0.1345, 0.0000, 0.5099], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198728192
state222:  tensor([[0.0720, 0.1345, 0.0000, 0.5099]], device='cuda:0')
policy_old.forwarded at 1641198728194
give action 41============================
log_to_linear:  tensor([[[0.4723]]], device='cuda:0') tensor([[[0.9568]]], device='cuda:0')
log_to_linear action at 1641198728195
bwe changes from to:  [tensor([[[74243.3672]]]), tensor([[[71034.7500]]])]
step into gymStat at 1641198728195
send bwe to appRecv at 1641198728195
sent bwe to appRecv at 1641198728195
wait for recv string at 1641198728195
recved string at 1641198728391
1
wait for recv [self.estimator, stat] at 1641198728391
recved [self.estimator, stat] at 1641198728391
sorted packlist at 1641198728391
packetSeq:  3643
packetSeq:  3644
packetSeq:  3645
packetSeq:  3646
packetSeq:  3647
packetSeq:  3648
packetSeq:  3649
packetSeq:  3650
processed packlist at 1641198728391
receiving_rate:  251600.0
delay:  197.25
loss_ratio:  0.0
processed state0-2 at 1641198728392
avgFrameBetween:  6
psnrStat:  [[467342, 466546, 468548, 468150, 468768, 467696]]
delayStat:  [[188, 189, 190, 188, 189, 189]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [467841.6666666667] [188.83333333333334] [32]
processed state3-5 at 1641198728392
liner_to_log:  tensor([[[0.9568]]]) tensor([[[0.4723]]])
linear_to_log at 1641198728392
listState:  [0.0629, 0.1315, 0.0, 0.4678416666666667, 0.18883333333333335, 1.0, tensor([[[0.4723]]])]
state_clone_detach at 1641198728392
reward: 0.02330524287387875
state tensor([0.0629, 0.1315, 0.0000, 0.4723], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198728393
state222:  tensor([[0.0629, 0.1315, 0.0000, 0.4723]], device='cuda:0')
policy_old.forwarded at 1641198728394
give action 42============================
log_to_linear:  tensor([[[0.7926]]], device='cuda:0') tensor([[[1.5377]]], device='cuda:0')
log_to_linear action at 1641198728395
bwe changes from to:  [tensor([[[71034.7500]]]), tensor([[[109232.3125]]])]
step into gymStat at 1641198728396
send bwe to appRecv at 1641198728396
sent bwe to appRecv at 1641198728396
wait for recv string at 1641198728396
recved string at 1641198728599
1
wait for recv [self.estimator, stat] at 1641198728599
recved [self.estimator, stat] at 1641198728599
sorted packlist at 1641198728599
packetSeq:  3651
packetSeq:  3652
packetSeq:  3653
packetSeq:  3654
packetSeq:  3655
packetSeq:  3656
packetSeq:  3657
packetSeq:  3658
packetSeq:  3659
packetSeq:  3660
packetSeq:  3661
processed packlist at 1641198728599
receiving_rate:  281360.0
delay:  196.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198728599
avgFrameBetween:  6
psnrStat:  [[470105, 471426, 471449, 470991, 471767, 472962]]
delayStat:  [[190, 190, 189, 190, 189, 190]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [471450.0] [189.66666666666666] [32]
processed state3-5 at 1641198728599
liner_to_log:  tensor([[[1.5377]]]) tensor([[[0.7926]]])
linear_to_log at 1641198728599
listState:  [0.07034, 0.13127272727272726, 0.0, 0.47145, 0.18966666666666665, 1.0, tensor([[[0.7926]]])]
state_clone_detach at 1641198728599
reward: 0.05856912204153225
state tensor([0.0703, 0.1313, 0.0000, 0.7926], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198728600
state222:  tensor([[0.0703, 0.1313, 0.0000, 0.7926]], device='cuda:0')
policy_old.forwarded at 1641198728602
give action 43============================
log_to_linear:  tensor([[[0.2150]]], device='cuda:0') tensor([[[0.6313]]], device='cuda:0')
log_to_linear action at 1641198728603
bwe changes from to:  [tensor([[[109232.3125]]]), tensor([[[68953.2266]]])]
step into gymStat at 1641198728603
send bwe to appRecv at 1641198728603
sent bwe to appRecv at 1641198728603
wait for recv string at 1641198728603
recved string at 1641198728823
1
wait for recv [self.estimator, stat] at 1641198728823
recved [self.estimator, stat] at 1641198728823
sorted packlist at 1641198728823
packetSeq:  3662
packetSeq:  3663
packetSeq:  3664
packetSeq:  3665
packetSeq:  3666
packetSeq:  3667
packetSeq:  3668
packetSeq:  3669
packetSeq:  3670
processed packlist at 1641198728823
receiving_rate:  303320.0
delay:  197.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198728823
avgFrameBetween:  6
psnrStat:  [[474017, 473422, 474331, 474870, 474358, 473528]]
delayStat:  [[190, 189, 190, 189, 224, 223]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [474087.6666666667] [200.83333333333334] [32]
processed state3-5 at 1641198728823
liner_to_log:  tensor([[[0.6313]]]) tensor([[[0.2150]]])
linear_to_log at 1641198728824
listState:  [0.07583, 0.13192592592592592, 0.0, 0.4740876666666667, 0.20083333333333334, 1.0, tensor([[[0.2150]]])]
state_clone_detach at 1641198728824
reward: 0.08090756910884306
state tensor([0.0758, 0.1319, 0.0000, 0.2150], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198728824
state222:  tensor([[0.0758, 0.1319, 0.0000, 0.2150]], device='cuda:0')
policy_old.forwarded at 1641198728826
give action 44============================
log_to_linear:  tensor([[[0.4959]]], device='cuda:0') tensor([[[0.9935]]], device='cuda:0')
log_to_linear action at 1641198728827
bwe changes from to:  [tensor([[[68953.2266]]]), tensor([[[68506.9375]]])]
step into gymStat at 1641198728827
send [estimator, stat] at 1641198724209
sent [estimator, stat] at 1641198724209
pc wait for bwe at 1641198724209
pc got bwe at 1641198724214
bandwidth:  300000
pc flushed at 1641198724214
Bwe Sent: 5 at 1641198724214
got request at 1641198724419
processed allFrame at 1641198724419
send 'asking for bwe' at 1641198724419
sent 'asking for bwe' at 1641198724419
send [estimator, stat] at 1641198724419
sent [estimator, stat] at 1641198724419
pc wait for bwe at 1641198724419
pc got bwe at 1641198724424
bandwidth:  300000
pc flushed at 1641198724424
Bwe Sent: 5 at 1641198724424
got request at 1641198724621
processed allFrame at 1641198724621
send 'asking for bwe' at 1641198724621
sent 'asking for bwe' at 1641198724621
send [estimator, stat] at 1641198724621
sent [estimator, stat] at 1641198724621
pc wait for bwe at 1641198724622
pc got bwe at 1641198724626
bandwidth:  300000
pc flushed at 1641198724626
Bwe Sent: 5 at 1641198724626
got request at 1641198724822
processed allFrame at 1641198724822
send 'asking for bwe' at 1641198724822
sent 'asking for bwe' at 1641198724822
send [estimator, stat] at 1641198724822
sent [estimator, stat] at 1641198724822
pc wait for bwe at 1641198724822
pc got bwe at 1641198724827
bandwidth:  300000
pc flushed at 1641198724827
Bwe Sent: 5 at 1641198724827
got request at 1641198725025
processed allFrame at 1641198725025
send 'asking for bwe' at 1641198725025
sent 'asking for bwe' at 1641198725025
send [estimator, stat] at 1641198725025
sent [estimator, stat] at 1641198725025
pc wait for bwe at 1641198725025
pc got bwe at 1641198725029
bandwidth:  300000
pc flushed at 1641198725029
Bwe Sent: 4 at 1641198725029
got request at 1641198725229
processed allFrame at 1641198725229
send 'asking for bwe' at 1641198725229
sent 'asking for bwe' at 1641198725229
send [estimator, stat] at 1641198725229
sent [estimator, stat] at 1641198725229
pc wait for bwe at 1641198725229
pc got bwe at 1641198725233
bandwidth:  300000
pc flushed at 1641198725233
Bwe Sent: 4 at 1641198725233
got request at 1641198725452
processed allFrame at 1641198725452
send 'asking for bwe' at 1641198725452
sent 'asking for bwe' at 1641198725452
send [estimator, stat] at 1641198725452
sent [estimator, stat] at 1641198725452
pc wait for bwe at 1641198725452
pc got bwe at 1641198725456
bandwidth:  300000
pc flushed at 1641198725456
Bwe Sent: 4 at 1641198725456
got request at 1641198725655
processed allFrame at 1641198725655
send 'asking for bwe' at 1641198725655
sent 'asking for bwe' at 1641198725655
send [estimator, stat] at 1641198725655
sent [estimator, stat] at 1641198725655
pc wait for bwe at 1641198725655
pc got bwe at 1641198725659
bandwidth:  300000
pc flushed at 1641198725659
Bwe Sent: 4 at 1641198725659
got request at 1641198725856
processed allFrame at 1641198725856
send 'asking for bwe' at 1641198725856
sent 'asking for bwe' at 1641198725856
send [estimator, stat] at 1641198725856
sent [estimator, stat] at 1641198725857
pc wait for bwe at 1641198725857
pc got bwe at 1641198725861
bandwidth:  300000
pc flushed at 1641198725861
Bwe Sent: 5 at 1641198725861
got request at 1641198726086
processed allFrame at 1641198726086
send 'asking for bwe' at 1641198726086
sent 'asking for bwe' at 1641198726086
send [estimator, stat] at 1641198726086
sent [estimator, stat] at 1641198726086
pc wait for bwe at 1641198726087
pc got bwe at 1641198726091
bandwidth:  300000
pc flushed at 1641198726091
Bwe Sent: 5 at 1641198726091
got request at 1641198726288
processed allFrame at 1641198726288
send 'asking for bwe' at 1641198726288
sent 'asking for bwe' at 1641198726288
send [estimator, stat] at 1641198726288
sent [estimator, stat] at 1641198726289
pc wait for bwe at 1641198726289
pc got bwe at 1641198726293
bandwidth:  300000
pc flushed at 1641198726293
Bwe Sent: 5 at 1641198726293
got request at 1641198726516
processed allFrame at 1641198726516
send 'asking for bwe' at 1641198726516
sent 'asking for bwe' at 1641198726516
send [estimator, stat] at 1641198726516
sent [estimator, stat] at 1641198726516
pc wait for bwe at 1641198726516
pc got bwe at 1641198726520
bandwidth:  300000
pc flushed at 1641198726520
Bwe Sent: 4 at 1641198726520
got request at 1641198726721
processed allFrame at 1641198726721
send 'asking for bwe' at 1641198726721
sent 'asking for bwe' at 1641198726721
send [estimator, stat] at 1641198726721
sent [estimator, stat] at 1641198726721
pc wait for bwe at 1641198726722
pc got bwe at 1641198726726
bandwidth:  300000
pc flushed at 1641198726726
Bwe Sent: 5 at 1641198726726
got request at 1641198726924
processed allFrame at 1641198726924
send 'asking for bwe' at 1641198726924
sent 'asking for bwe' at 1641198726924
send [estimator, stat] at 1641198726924
sent [estimator, stat] at 1641198726924
pc wait for bwe at 1641198726924
pc got bwe at 1641198726928
bandwidth:  300000
pc flushed at 1641198726928
Bwe Sent: 4 at 1641198726928
got request at 1641198727151
processed allFrame at 1641198727152
send 'asking for bwe' at 1641198727152
sent 'asking for bwe' at 1641198727152
send [estimator, stat] at 1641198727152
sent [estimator, stat] at 1641198727152
pc wait for bwe at 1641198727152
pc got bwe at 1641198727156
bandwidth:  300000
pc flushed at 1641198727156
Bwe Sent: 4 at 1641198727156
got request at 1641198727353
processed allFrame at 1641198727353
send 'asking for bwe' at 1641198727353
sent 'asking for bwe' at 1641198727353
send [estimator, stat] at 1641198727353
sent [estimator, stat] at 1641198727353
pc wait for bwe at 1641198727353
pc got bwe at 1641198727358
bandwidth:  300000
pc flushed at 1641198727358
Bwe Sent: 5 at 1641198727358
got request at 1641198727555
processed allFrame at 1641198727555
send 'asking for bwe' at 1641198727555
sent 'asking for bwe' at 1641198727555
send [estimator, stat] at 1641198727555
sent [estimator, stat] at 1641198727555
pc wait for bwe at 1641198727555
pc got bwe at 1641198727559
bandwidth:  300000
pc flushed at 1641198727559
Bwe Sent: 4 at 1641198727559
got request at 1641198727758
processed allFrame at 1641198727758
send 'asking for bwe' at 1641198727758
sent 'asking for bwe' at 1641198727758
send [estimator, stat] at 1641198727758
sent [estimator, stat] at 1641198727758
pc wait for bwe at 1641198727758
pc got bwe at 1641198727762
bandwidth:  300000
pc flushed at 1641198727762
Bwe Sent: 4 at 1641198727762
got request at 1641198727959
processed allFrame at 1641198727959
send 'asking for bwe' at 1641198727959
sent 'asking for bwe' at 1641198727960
send [estimator, stat] at 1641198727960
sent [estimator, stat] at 1641198727960
pc wait for bwe at 1641198727960
pc got bwe at 1641198727964
bandwidth:  300000
pc flushed at 1641198727964
Bwe Sent: 5 at 1641198727964
got request at 1641198728191
processed allFrame at 1641198728191
send 'asking for bwe' at 1641198728191
sent 'asking for bwe' at 1641198728191
send [estimator, stat] at 1641198728191
sent [estimator, stat] at 1641198728191
pc wait for bwe at 1641198728191
pc got bwe at 1641198728195
bandwidth:  300000
pc flushed at 1641198728195
Bwe Sent: 4 at 1641198728195
got request at 1641198728391
processed allFrame at 1641198728391
send 'asking for bwe' at 1641198728391
sent 'asking for bwe' at 1641198728391
send [estimator, stat] at 1641198728391
sent [estimator, stat] at 1641198728391
pc wait for bwe at 1641198728391
pc got bwe at 1641198728396
bandwidth:  300000
pc flushed at 1641198728396
Bwe Sent: 5 at 1641198728396
got request at 1641198728598
processed allFrame at 1641198728598
send 'asking for bwe' at 1641198728598
sent 'asking for bwe' at 1641198728598
send [estimator, stat] at 1641198728598
sent [estimator, stat] at 1641198728599
pc wait for bwe at 1641198728599
pc got bwe at 1641198728603
bandwidth:  300000
pc flushed at 1641198728603
Bwe Sent: 5 at 1641198728603
got request at 1641198728823
processed allFrame at 1641198728823
send 'asking for bwe' at 1641198728823
sent 'asking for bwe' at 1641198728823
send [estimator, stat] at 1641198728823
sent [estimator, stat] at 1641198728823
pc wait for bwe at 1641198728823
pc got bwe at 1641198728827
bandwidth:  300000
pc flushed at 1641198728827
Bwe Sent: 4 at 1641198728827
got request at 1641198729024
send bwe to appRecv at 1641198728827
sent bwe to appRecv at 1641198728827
wait for recv string at 1641198728827
recved string at 1641198729024
1
wait for recv [self.estimator, stat] at 1641198729024
recved [self.estimator, stat] at 1641198729025
sorted packlist at 1641198729025
packetSeq:  3671
packetSeq:  3672
packetSeq:  3673
packetSeq:  3674
packetSeq:  3675
packetSeq:  3676
packetSeq:  3677
packetSeq:  3678
processed packlist at 1641198729025
receiving_rate:  286760.0
delay:  198.875
loss_ratio:  0.0
processed state0-2 at 1641198729025
avgFrameBetween:  6
psnrStat:  [[474023, 474045, 472732, 473015, 473478, 473478]]
delayStat:  [[224, 225, 224, 225, 225, 225]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [473461.8333333333] [224.66666666666666] [32]
processed state3-5 at 1641198729025
liner_to_log:  tensor([[[0.9935]]]) tensor([[[0.4959]]])
linear_to_log at 1641198729025
listState:  [0.07169, 0.13258333333333333, 0.0, 0.4734618333333333, 0.22466666666666665, 1.0, tensor([[[0.4959]]])]
state_clone_detach at 1641198729025
reward: 0.06070408962230639
state tensor([0.0717, 0.1326, 0.0000, 0.4959], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198729026
state222:  tensor([[0.0717, 0.1326, 0.0000, 0.4959]], device='cuda:0')
policy_old.forwarded at 1641198729028
give action 45============================
log_to_linear:  tensor([[[0.3862]]], device='cuda:0') tensor([[[0.8321]]], device='cuda:0')
log_to_linear action at 1641198729028
bwe changes from to:  [tensor([[[68506.9375]]]), tensor([[[57003.6250]]])]
step into gymStat at 1641198729029
send bwe to appRecv at 1641198729029
sent bwe to appRecv at 1641198729029
wait for recv string at 1641198729029
recved string at 1641198729232
1
wait for recv [self.estimator, stat] at 1641198729232
recved [self.estimator, stat] at 1641198729232
sorted packlist at 1641198729232
packetSeq:  3679
packetSeq:  3680
packetSeq:  3681
packetSeq:  3682
packetSeq:  3683
packetSeq:  3684
packetSeq:  3685
packetSeq:  3686
processed packlist at 1641198729232
receiving_rate:  252360.0
delay:  197.375
loss_ratio:  0.0
processed state0-2 at 1641198729232
avgFrameBetween:  6
psnrStat:  [[473450, 472406, 473733, 473062, 472414, 473349]]
delayStat:  [[226, 224, 225, 225, 225, 225]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [473069.0] [225.0] [32]
processed state3-5 at 1641198729232
liner_to_log:  tensor([[[0.8321]]]) tensor([[[0.3862]]])
linear_to_log at 1641198729233
listState:  [0.06309, 0.13158333333333333, 0.0, 0.473069, 0.225, 1.0, tensor([[[0.3862]]])]
state_clone_detach at 1641198729233
reward: 0.023963645772607678
state tensor([0.0631, 0.1316, 0.0000, 0.3862], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198729234
state222:  tensor([[0.0631, 0.1316, 0.0000, 0.3862]], device='cuda:0')
policy_old.forwarded at 1641198729235
give action 46============================
log_to_linear:  tensor([[[0.6804]]], device='cuda:0') tensor([[[1.3147]]], device='cuda:0')
log_to_linear action at 1641198729236
bwe changes from to:  [tensor([[[57003.6250]]]), tensor([[[74940.9062]]])]
step into gymStat at 1641198729236
send bwe to appRecv at 1641198729236
sent bwe to appRecv at 1641198729236
wait for recv string at 1641198729236
recved string at 1641198729458
1
wait for recv [self.estimator, stat] at 1641198729458
recved [self.estimator, stat] at 1641198729458
sorted packlist at 1641198729458
packetSeq:  3687
packetSeq:  3688
packetSeq:  3689
packetSeq:  3690
packetSeq:  3691
packetSeq:  3692
packetSeq:  3693
packetSeq:  3694
packetSeq:  3695
packetSeq:  3696
processed packlist at 1641198729458
receiving_rate:  267160.0
delay:  197.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198729458
avgFrameBetween:  6
psnrStat:  [[473687, 472984, 473485, 473266, 475217, 473108, 475576]]
delayStat:  [[224, 224, 224, 225, 225, 224, 225]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  11
state:  [473903.28571428574] [224.42857142857142] [32]
processed state3-5 at 1641198729458
liner_to_log:  tensor([[[1.3147]]]) tensor([[[0.6804]]])
linear_to_log at 1641198729458
listState:  [0.06679, 0.13155555555555556, 0.0, 0.47390328571428575, 0.22442857142857142, 1.0, tensor([[[0.6804]]])]
state_clone_detach at 1641198729458
reward: 0.04146816524779706
state tensor([0.0668, 0.1316, 0.0000, 0.6804], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198729459
state222:  tensor([[0.0668, 0.1316, 0.0000, 0.6804]], device='cuda:0')
policy_old.forwarded at 1641198729461
give action 47============================
log_to_linear:  tensor([[[0.3400]]], device='cuda:0') tensor([[[0.7713]]], device='cuda:0')
log_to_linear action at 1641198729461
bwe changes from to:  [tensor([[[74940.9062]]]), tensor([[[57805.5000]]])]
step into gymStat at 1641198729462
send bwe to appRecv at 1641198729462
sent bwe to appRecv at 1641198729462
wait for recv string at 1641198729462
recved string at 1641198729663
1
wait for recv [self.estimator, stat] at 1641198729663
recved [self.estimator, stat] at 1641198729664
sorted packlist at 1641198729664
packetSeq:  3697
packetSeq:  3698
packetSeq:  3699
packetSeq:  3700
packetSeq:  3701
packetSeq:  3702
packetSeq:  3703
packetSeq:  3704
packetSeq:  3705
packetSeq:  3706
packetSeq:  3707
processed packlist at 1641198729664
receiving_rate:  304000.0
delay:  197.63636363636363
loss_ratio:  0.0
processed state0-2 at 1641198729664
avgFrameBetween:  6
psnrStat:  [[475205, 476189, 475180, 476240, 476080, 477008]]
delayStat:  [[225, 225, 225, 224, 226, 226]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  10
state:  [475983.6666666667] [225.16666666666666] [32]
processed state3-5 at 1641198729664
liner_to_log:  tensor([[[0.7713]]]) tensor([[[0.3400]]])
linear_to_log at 1641198729664
listState:  [0.076, 0.13175757575757574, 0.0, 0.4759836666666667, 0.22516666666666665, 1.0, tensor([[[0.3400]]])]
state_clone_detach at 1641198729664
reward: 0.08214953405829517
state tensor([0.0760, 0.1318, 0.0000, 0.3400], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198729665
state222:  tensor([[0.0760, 0.1318, 0.0000, 0.3400]], device='cuda:0')
policy_old.forwarded at 1641198729667
give action 48============================
log_to_linear:  tensor([[[0.1508]]], device='cuda:0') tensor([[[0.5748]]], device='cuda:0')
log_to_linear action at 1641198729668
bwe changes from to:  [tensor([[[57805.5000]]]), tensor([[[33224.6719]]])]
step into gymStat at 1641198729668
send bwe to appRecv at 1641198729668
sent bwe to appRecv at 1641198729668
wait for recv string at 1641198729668
recved string at 1641198729891
1
wait for recv [self.estimator, stat] at 1641198729891
recved [self.estimator, stat] at 1641198729892
sorted packlist at 1641198729892
packetSeq:  3708
packetSeq:  3709
packetSeq:  3710
packetSeq:  3711
packetSeq:  3712
packetSeq:  3713
packetSeq:  3714
packetSeq:  3715
packetSeq:  3716
processed packlist at 1641198729892
receiving_rate:  296200.0
delay:  198.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198729892
avgFrameBetween:  6
psnrStat:  [[476774, 476541, 475200, 476591, 474740, 474932]]
delayStat:  [[226, 226, 254, 255, 255, 255]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  10
state:  [475796.3333333333] [245.16666666666666] [32]
processed state3-5 at 1641198729892
liner_to_log:  tensor([[[0.5748]]]) tensor([[[0.1508]]])
linear_to_log at 1641198729892
listState:  [0.07405, 0.13214814814814815, 0.0, 0.4757963333333333, 0.24516666666666664, 1.0, tensor([[[0.1508]]])]
state_clone_detach at 1641198729892
reward: 0.07247013026197047
state tensor([0.0741, 0.1321, 0.0000, 0.1508], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198729893
state222:  tensor([[0.0741, 0.1321, 0.0000, 0.1508]], device='cuda:0')
policy_old.forwarded at 1641198729895
give action 49============================
log_to_linear:  tensor([[[0.4804]]], device='cuda:0') tensor([[[0.9694]]], device='cuda:0')
log_to_linear action at 1641198729895
bwe changes from to:  [tensor([[[33224.6719]]]), tensor([[[32206.7246]]])]
step into gymStat at 1641198729896
send bwe to appRecv at 1641198729896
sent bwe to appRecv at 1641198729896
wait for recv string at 1641198729896
recved string at 1641198730125
1
wait for recv [self.estimator, stat] at 1641198730125
recved [self.estimator, stat] at 1641198730126
sorted packlist at 1641198730126
packetSeq:  3717
packetSeq:  3718
packetSeq:  3719
packetSeq:  3720
packetSeq:  3721
packetSeq:  3722
packetSeq:  3723
packetSeq:  3724
packetSeq:  3725
packetSeq:  3726
processed packlist at 1641198730126
receiving_rate:  277240.0
delay:  197.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198730126
avgFrameBetween:  6
psnrStat:  [[475132, 476014, 476123, 477100, 478566, 479049, 478561]]
delayStat:  [[255, 254, 256, 256, 256, 256, 255]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  10
state:  [477220.71428571426] [255.42857142857142] [32]
processed state3-5 at 1641198730126
liner_to_log:  tensor([[[0.9694]]]) tensor([[[0.4804]]])
linear_to_log at 1641198730126
listState:  [0.06931, 0.13177777777777777, 0.0, 0.4772207142857143, 0.2554285714285714, 1.0, tensor([[[0.4804]]])]
state_clone_detach at 1641198730126
reward: 0.05238362765587773
state tensor([0.0693, 0.1318, 0.0000, 0.4804], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198730127
state222:  tensor([[0.0693, 0.1318, 0.0000, 0.4804]], device='cuda:0')
policy_old.forwarded at 1641198730129
give action 50============================
log_to_linear:  tensor([[[0.6206]]], device='cuda:0') tensor([[[1.2042]]], device='cuda:0')
log_to_linear action at 1641198730130
bwe changes from to:  [tensor([[[32206.7246]]]), tensor([[[38784.3945]]])]
step into gymStat at 1641198730130
send bwe to appRecv at 1641198730130
sent bwe to appRecv at 1641198730130
wait for recv string at 1641198730130
recved string at 1641198730332
1
wait for recv [self.estimator, stat] at 1641198730332
recved [self.estimator, stat] at 1641198730332
sorted packlist at 1641198730332
packetSeq:  3727
packetSeq:  3728
packetSeq:  3729
packetSeq:  3730
packetSeq:  3731
packetSeq:  3732
packetSeq:  3733
packetSeq:  3734
packetSeq:  3735
processed packlist at 1641198730332
receiving_rate:  260920.00000000003
delay:  196.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198730332
avgFrameBetween:  6
psnrStat:  [[478566, 478086, 478088, 477455, 477901, 478308]]
delayStat:  [[256, 256, 256, 256, 255, 256]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  9
state:  [478067.3333333333] [255.83333333333334] [32]
processed state3-5 at 1641198730332
liner_to_log:  tensor([[[1.2042]]]) tensor([[[0.6206]]])
linear_to_log at 1641198730333
listState:  [0.06523000000000001, 0.13125925925925927, 0.0, 0.4780673333333333, 0.25583333333333336, 1.0, tensor([[[0.6206]]])]
state_clone_detach at 1641198730333
reward: 0.035073484809620026
state tensor([0.0652, 0.1313, 0.0000, 0.6206], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198730333
state222:  tensor([[0.0652, 0.1313, 0.0000, 0.6206]], device='cuda:0')
policy_old.forwarded at 1641198730335
give action 51============================
log_to_linear:  tensor([[[0.3326]]], device='cuda:0') tensor([[[0.7620]]], device='cuda:0')
log_to_linear action at 1641198730336
bwe changes from to:  [tensor([[[38784.3945]]]), tensor([[[29552.6387]]])]
step into gymStat at 1641198730336
send bwe to appRecv at 1641198730336
sent bwe to appRecv at 1641198730336
wait for recv string at 1641198730336
recved string at 1641198730558
1
wait for recv [self.estimator, stat] at 1641198730558
recved [self.estimator, stat] at 1641198730558
sorted packlist at 1641198730558
packetSeq:  3736
packetSeq:  3737
packetSeq:  3738
packetSeq:  3739
packetSeq:  3740
packetSeq:  3741
packetSeq:  3742
packetSeq:  3743
packetSeq:  3744
packetSeq:  3745
packetSeq:  3746
processed packlist at 1641198730558
receiving_rate:  310360.0
delay:  197.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198730558
avgFrameBetween:  6
psnrStat:  [[479415, 479627, 480097, 480430, 481045, 480732, 481368]]
delayStat:  [[256, 255, 256, 255, 256, 257, 256]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  9
state:  [480387.71428571426] [255.85714285714286] [32]
processed state3-5 at 1641198730558
liner_to_log:  tensor([[[0.7620]]]) tensor([[[0.3326]]])
linear_to_log at 1641198730558
listState:  [0.07759, 0.1316969696969697, 0.0, 0.48038771428571425, 0.25585714285714284, 1.0, tensor([[[0.3326]]])]
state_clone_detach at 1641198730559
reward: 0.08918020078849093
state tensor([0.0776, 0.1317, 0.0000, 0.3326], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198730559
state222:  tensor([[0.0776, 0.1317, 0.0000, 0.3326]], device='cuda:0')
policy_old.forwarded at 1641198730561
give action 52============================
log_to_linear:  tensor([[[0.5839]]], device='cuda:0') tensor([[[1.1394]]], device='cuda:0')
log_to_linear action at 1641198730562
bwe changes from to:  [tensor([[[29552.6387]]]), tensor([[[33672.0117]]])]
step into gymStat at 1641198730562
send bwe to appRecv at 1641198730562
sent bwe to appRecv at 1641198730562
wait for recv string at 1641198730562
recved string at 1641198730761
1
wait for recv [self.estimator, stat] at 1641198730761
recved [self.estimator, stat] at 1641198730762
sorted packlist at 1641198730762
packetSeq:  3747
packetSeq:  3749
packetSeq:  3750
packetSeq:  3751
packetSeq:  3752
packetSeq:  3753
packetSeq:  3754
packetSeq:  3755
packetSeq:  3756
processed packlist at 1641198730762
receiving_rate:  263760.0
delay:  197.22222222222223
loss_ratio:  0.1
processed state0-2 at 1641198730762
avgFrameBetween:  6
psnrStat:  [[481117, 481290, 482555, 483405, 483917, 485197]]
delayStat:  [[256, 256, 256, 257, 256, 273]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  8
state:  [482913.5] [259.0] [32]
processed state3-5 at 1641198730762
liner_to_log:  tensor([[[1.1394]]]) tensor([[[0.5839]]])
linear_to_log at 1641198730762
listState:  [0.06594, 0.13148148148148148, 0.1, 0.4829135, 0.259, 1.0, tensor([[[0.5839]]])]
state_clone_detach at 1641198730762
reward: -0.96226725695707
state tensor([0.0659, 0.1315, 0.1000, 0.5839], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198730763
state222:  tensor([[0.0659, 0.1315, 0.1000, 0.5839]], device='cuda:0')
policy_old.forwarded at 1641198730765
give action 53============================
log_to_linear:  tensor([[[0.4928]]], device='cuda:0') tensor([[[0.9887]]], device='cuda:0')
log_to_linear action at 1641198730766
bwe changes from to:  [tensor([[[33672.0117]]]), tensor([[[33290.2344]]])]
step into gymStat at 1641198730766
send bwe to appRecv at 1641198730766
sent bwe to appRecv at 1641198730766
wait for recv string at 1641198730766
recved string at 1641198730993
1
wait for recv [self.estimator, stat] at 1641198730993
recved [self.estimator, stat] at 1641198730993
sorted packlist at 1641198730993
packetSeq:  3757
packetSeq:  3758
packetSeq:  3759
packetSeq:  3760
packetSeq:  3761
packetSeq:  3762
packetSeq:  3763
packetSeq:  3764
packetSeq:  3765
packetSeq:  3766
processed packlist at 1641198730993
receiving_rate:  278120.0
delay:  198.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198730993
avgFrameBetween:  6
psnrStat:  [[485527, 485843, 485445, 485826, 483923, 483643, 483793]]
delayStat:  [[273, 273, 273, 272, 274, 273, 273]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  8
state:  [484857.14285714284] [273.0] [32]
processed state3-5 at 1641198730993
liner_to_log:  tensor([[[0.9887]]]) tensor([[[0.4928]]])
linear_to_log at 1641198730993
listState:  [0.06953, 0.13222222222222224, 0.0, 0.4848571428571428, 0.273, 1.0, tensor([[[0.4928]]])]
state_clone_detach at 1641198730994
reward: 0.05205090098258558
state tensor([0.0695, 0.1322, 0.0000, 0.4928], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198730994
state222:  tensor([[0.0695, 0.1322, 0.0000, 0.4928]], device='cuda:0')
policy_old.forwarded at 1641198730996
give action 54============================
log_to_linear:  tensor([[[0.3314]]], device='cuda:0') tensor([[[0.7605]]], device='cuda:0')
log_to_linear action at 1641198730997
bwe changes from to:  [tensor([[[33290.2344]]]), tensor([[[25316.7656]]])]
step into gymStat at 1641198730997
send bwe to appRecv at 1641198730997
sent bwe to appRecv at 1641198730997
wait for recv string at 1641198730997
recved string at 1641198731196
1
wait for recv [self.estimator, stat] at 1641198731196
recved [self.estimator, stat] at 1641198731196
sorted packlist at 1641198731196
packetSeq:  3767
packetSeq:  3768
packetSeq:  3769
packetSeq:  3770
packetSeq:  3771
packetSeq:  3772
packetSeq:  3773
packetSeq:  3774
packetSeq:  3775
processed packlist at 1641198731196
receiving_rate:  257480.00000000003
delay:  197.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198731196
avgFrameBetween:  6
psnrStat:  [[483496, 483920, 483706, 483430, 484008, 484326]]
delayStat:  [[273, 272, 274, 273, 273, 273]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [483814.3333333333] [273.0] [32]
processed state3-5 at 1641198731196
liner_to_log:  tensor([[[0.7605]]]) tensor([[[0.3314]]])
linear_to_log at 1641198731197
listState:  [0.06437000000000001, 0.13140740740740742, 0.0, 0.4838143333333333, 0.273, 1.0, tensor([[[0.3314]]])]
state_clone_detach at 1641198731197
reward: 0.030575607823868423
state tensor([0.0644, 0.1314, 0.0000, 0.3314], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198731198
state222:  tensor([[0.0644, 0.1314, 0.0000, 0.3314]], device='cuda:0')
policy_old.forwarded at 1641198731199
give action 55============================
log_to_linear:  tensor([[[0.6069]]], device='cuda:0') tensor([[[1.1797]]], device='cuda:0')
log_to_linear action at 1641198731200
bwe changes from to:  [tensor([[[25316.7656]]]), tensor([[[29865.0273]]])]
step into gymStat at 1641198731200
send bwe to appRecv at 1641198731200
sent bwe to appRecv at 1641198731200
wait for recv string at 1641198731200
recved string at 1641198731398
1
wait for recv [self.estimator, stat] at 1641198731398
recved [self.estimator, stat] at 1641198731398
sorted packlist at 1641198731398
packetSeq:  3776
packetSeq:  3777
packetSeq:  3778
packetSeq:  3779
packetSeq:  3780
packetSeq:  3781
packetSeq:  3782
packetSeq:  3783
packetSeq:  3784
packetSeq:  3785
packetSeq:  3786
packetSeq:  3787
processed packlist at 1641198731398
receiving_rate:  297040.0
delay:  196.83333333333334
loss_ratio:  0.0
processed state0-2 at 1641198731398
avgFrameBetween:  6
psnrStat:  [[484954, 484612, 486094, 486277, 486084, 487850]]
delayStat:  [[273, 273, 273, 272, 274, 274]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [485978.5] [273.1666666666667] [32]
processed state3-5 at 1641198731398
liner_to_log:  tensor([[[1.1797]]]) tensor([[[0.6069]]])
linear_to_log at 1641198731399
listState:  [0.07426, 0.13122222222222224, 0.0, 0.4859785, 0.27316666666666667, 1.0, tensor([[[0.6069]]])]
state_clone_detach at 1641198731399
reward: 0.0761699381505801
state tensor([0.0743, 0.1312, 0.0000, 0.6069], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198731399
state222:  tensor([[0.0743, 0.1312, 0.0000, 0.6069]], device='cuda:0')
policy_old.forwarded at 1641198731401
give action 56============================
log_to_linear:  tensor([[[0.9298]]], device='cuda:0') tensor([[[1.8367]]], device='cuda:0')
log_to_linear action at 1641198731402
bwe changes from to:  [tensor([[[29865.0273]]]), tensor([[[54852.2969]]])]
step into gymStat at 1641198731402
send bwe to appRecv at 1641198731402
sent bwe to appRecv at 1641198731402
wait for recv string at 1641198731402
recved string at 1641198731622
1
wait for recv [self.estimator, stat] at 1641198731622
recved [self.estimator, stat] at 1641198731622
sorted packlist at 1641198731622
packetSeq:  3788
packetSeq:  3789
packetSeq:  3790
packetSeq:  3791
packetSeq:  3792
packetSeq:  3793
packetSeq:  3794
packetSeq:  3795
packetSeq:  3796
packetSeq:  3797
packetSeq:  3798
packetSeq:  3799
processed packlist at 1641198731622
receiving_rate:  314400.0
delay:  197.16666666666666
loss_ratio:  0.0
processed state0-2 at 1641198731622
avgFrameBetween:  6
psnrStat:  [[487159, 487026, 487458, 487321, 486641, 485952, 484372]]
delayStat:  [[274, 274, 273, 274, 274, 274, 274]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  7
state:  [486561.28571428574] [273.85714285714283] [32]
processed state3-5 at 1641198731622
liner_to_log:  tensor([[[1.8367]]]) tensor([[[0.9298]]])
linear_to_log at 1641198731623
listState:  [0.0786, 0.13144444444444445, 0.0, 0.48656128571428575, 0.27385714285714285, 1.0, tensor([[[0.9298]]])]
state_clone_detach at 1641198731623
reward: 0.0942480479365142
state tensor([0.0786, 0.1314, 0.0000, 0.9298], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198731624
state222:  tensor([[0.0786, 0.1314, 0.0000, 0.9298]], device='cuda:0')
policy_old.forwarded at 1641198731625
give action 57============================
log_to_linear:  tensor([[[0.1745]]], device='cuda:0') tensor([[[0.5943]]], device='cuda:0')
log_to_linear action at 1641198731626
bwe changes from to:  [tensor([[[54852.2969]]]), tensor([[[32599.1426]]])]
step into gymStat at 1641198731626
send bwe to appRecv at 1641198731626
sent bwe to appRecv at 1641198731626
wait for recv string at 1641198731627
recved string at 1641198731825
1
wait for recv [self.estimator, stat] at 1641198731825
recved [self.estimator, stat] at 1641198731826
sorted packlist at 1641198731826
packetSeq:  3800
packetSeq:  3801
packetSeq:  3802
packetSeq:  3803
packetSeq:  3804
packetSeq:  3805
packetSeq:  3806
packetSeq:  3807
packetSeq:  3808
packetSeq:  3809
processed packlist at 1641198731826
receiving_rate:  293800.0
delay:  197.5
loss_ratio:  0.0
processed state0-2 at 1641198731826
avgFrameBetween:  6
psnrStat:  [[484039, 483127, 483307, 482885, 482997, 483320]]
delayStat:  [[273, 274, 274, 274, 274, 273]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [483279.1666666667] [273.6666666666667] [32]
processed state3-5 at 1641198731826
liner_to_log:  tensor([[[0.5943]]]) tensor([[[0.1745]]])
linear_to_log at 1641198731826
listState:  [0.07345, 0.13166666666666665, 0.0, 0.4832791666666667, 0.27366666666666667, 1.0, tensor([[[0.1745]]])]
state_clone_detach at 1641198731826
reward: 0.07127236860167435
state tensor([0.0734, 0.1317, 0.0000, 0.1745], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198731827
state222:  tensor([[0.0734, 0.1317, 0.0000, 0.1745]], device='cuda:0')
policy_old.forwarded at 1641198731829
give action 58============================
log_to_linear:  tensor([[[0.7630]]], device='cuda:0') tensor([[[1.4770]]], device='cuda:0')
log_to_linear action at 1641198731830
bwe changes from to:  [tensor([[[32599.1426]]]), tensor([[[48148.1250]]])]
step into gymStat at 1641198731830
send bwe to appRecv at 1641198731830
sent bwe to appRecv at 1641198731830
wait for recv string at 1641198731830
recved string at 1641198732027
1
wait for recv [self.estimator, stat] at 1641198732027
recved [self.estimator, stat] at 1641198732027
sorted packlist at 1641198732027
packetSeq:  3810
packetSeq:  3811
packetSeq:  3812
packetSeq:  3813
packetSeq:  3814
packetSeq:  3815
packetSeq:  3816
packetSeq:  3817
processed packlist at 1641198732027
receiving_rate:  256279.99999999997
delay:  197.875
loss_ratio:  0.0
processed state0-2 at 1641198732027
avgFrameBetween:  6
psnrStat:  [[482433, 481393, 481050, 478751, 479705, 478527]]
delayStat:  [[274, 274, 274, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [480309.8333333333] [274.5] [32]
processed state3-5 at 1641198732027
liner_to_log:  tensor([[[1.4770]]]) tensor([[[0.7630]]])
linear_to_log at 1641198732027
listState:  [0.06406999999999999, 0.13191666666666665, 0.0, 0.4803098333333333, 0.2745, 1.0, tensor([[[0.7630]]])]
state_clone_detach at 1641198732028
reward: 0.02762737483720068
state tensor([0.0641, 0.1319, 0.0000, 0.7630], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198732028
state222:  tensor([[0.0641, 0.1319, 0.0000, 0.7630]], device='cuda:0')
policy_old.forwarded at 1641198732030
give action 59============================
log_to_linear:  tensor([[[0.4204]]], device='cuda:0') tensor([[[0.8798]]], device='cuda:0')
log_to_linear action at 1641198732031
bwe changes from to:  [tensor([[[48148.1250]]]), tensor([[[42359.9922]]])]
step into gymStat at 1641198732031
send bwe to appRecv at 1641198732031
sent bwe to appRecv at 1641198732031
wait for recv string at 1641198732031
recved string at 1641198732231
1
wait for recv [self.estimator, stat] at 1641198732231
recved [self.estimator, stat] at 1641198732231
sorted packlist at 1641198732231
packetSeq:  3818
packetSeq:  3819
packetSeq:  3820
packetSeq:  3821
packetSeq:  3822
packetSeq:  3823
packetSeq:  3824
packetSeq:  3825
packetSeq:  3826
processed packlist at 1641198732231
receiving_rate:  273360.0
delay:  197.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198732231
avgFrameBetween:  6
psnrStat:  [[479809, 479472, 479524, 481840, 481768, 483082]]
delayStat:  [[275, 274, 275, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [480915.8333333333] [274.8333333333333] [32]
processed state3-5 at 1641198732231
liner_to_log:  tensor([[[0.8798]]]) tensor([[[0.4204]]])
linear_to_log at 1641198732232
listState:  [0.06834, 0.13170370370370368, 0.0, 0.4809158333333333, 0.2748333333333333, 1.0, tensor([[[0.4204]]])]
state_clone_detach at 1641198732232
reward: 0.04817407644558541
state tensor([0.0683, 0.1317, 0.0000, 0.4204], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198732233
state222:  tensor([[0.0683, 0.1317, 0.0000, 0.4204]], device='cuda:0')
policy_old.forwarded at 1641198732235
give action 60============================
log_to_linear:  tensor([[[0.1834]]], device='cuda:0') tensor([[[0.6020]]], device='cuda:0')
log_to_linear action at 1641198732235
bwe changes from to:  [tensor([[[42359.9922]]]), tensor([[[25498.9863]]])]
step into gymStat at 1641198732236
send bwe to appRecv at 1641198732236
sent bwe to appRecv at 1641198732236
wait for recv string at 1641198732236
recved string at 1641198732459
1
wait for recv [self.estimator, stat] at 1641198732459
recved [self.estimator, stat] at 1641198732459
sorted packlist at 1641198732459
packetSeq:  3827
packetSeq:  3828
packetSeq:  3829
packetSeq:  3830
packetSeq:  3831
packetSeq:  3832
packetSeq:  3833
packetSeq:  3834
packetSeq:  3835
processed packlist at 1641198732459
receiving_rate:  265320.0
delay:  198.875
loss_ratio:  0.0
processed state0-2 at 1641198732460
avgFrameBetween:  6
psnrStat:  [[483904, 484827, 484952, 485124, 484256, 484348, 483064]]
delayStat:  [[275, 275, 276, 274, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  6
state:  [483064.0] [275.0] [16]
processed state3-5 at 1641198732460
liner_to_log:  tensor([[[0.6020]]]) tensor([[[0.1834]]])
linear_to_log at 1641198732460
listState:  [0.06633, 0.13258333333333333, 0.0, 0.483064, 0.275, 0.5, tensor([[[0.1834]]])]
state_clone_detach at 1641198732460
reward: 0.03624628943664654
state tensor([0.0663, 0.1326, 0.0000, 0.1834], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198732461
state222:  tensor([[0.0663, 0.1326, 0.0000, 0.1834]], device='cuda:0')
policy_old.forwarded at 1641198732463
give action 61============================
log_to_linear:  tensor([[[0.4517]]], device='cuda:0') tensor([[[0.9257]]], device='cuda:0')
log_to_linear action at 1641198732463
bwe changes from to:  [tensor([[[25498.9863]]]), tensor([[[23603.6348]]])]
step into gymStat at 1641198732464
send bwe to appRecv at 1641198732464
sent bwe to appRecv at 1641198732464
wait for recv string at 1641198732464
recved string at 1641198732687
1
wait for recv [self.estimator, stat] at 1641198732687
recved [self.estimator, stat] at 1641198732687
sorted packlist at 1641198732687
packetSeq:  3836
packetSeq:  3837
packetSeq:  3838
packetSeq:  3839
packetSeq:  3840
packetSeq:  3841
packetSeq:  3842
packetSeq:  3843
packetSeq:  3844
packetSeq:  3845
packetSeq:  3846
processed packlist at 1641198732687
receiving_rate:  307440.0
delay:  195.3
loss_ratio:  0.0
processed state0-2 at 1641198732687
avgFrameBetween:  6
psnrStat:  [[483744, 483148, 481680, 483200, 481406, 482068]]
delayStat:  [[275, 274, 275, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  5
state:  [482068.0] [275.0] [16]
processed state3-5 at 1641198732687
liner_to_log:  tensor([[[0.9257]]]) tensor([[[0.4517]]])
linear_to_log at 1641198732688
listState:  [0.07686, 0.1302, 0.0, 0.482068, 0.275, 0.5, tensor([[[0.4517]]])]
state_clone_detach at 1641198732688
reward: 0.090536373817102
state tensor([0.0769, 0.1302, 0.0000, 0.4517], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198732688
state222:  tensor([[0.0769, 0.1302, 0.0000, 0.4517]], device='cuda:0')
policy_old.forwarded at 1641198732690
give action 62============================
log_to_linear:  tensor([[[0.5339]]], device='cuda:0') tensor([[[1.0548]]], device='cuda:0')
log_to_linear action at 1641198732691
bwe changes from to:  [tensor([[[23603.6348]]]), tensor([[[24896.7441]]])]
step into gymStat at 1641198732691
send bwe to appRecv at 1641198732691
sent bwe to appRecv at 1641198732691
wait for recv string at 1641198732691
recved string at 1641198732889
1
wait for recv [self.estimator, stat] at 1641198732889
recved [self.estimator, stat] at 1641198732889
sorted packlist at 1641198732889
packetSeq:  3847
packetSeq:  3848
packetSeq:  3849
packetSeq:  3850
packetSeq:  3851
packetSeq:  3852
packetSeq:  3853
packetSeq:  3854
processed packlist at 1641198732889
receiving_rate:  258240.0
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198732889
avgFrameBetween:  6
psnrStat:  [[482406, 483548, 483766, 483007, 483826, 483687, 483944]]
delayStat:  [[275, 275, 275, 275, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  5
state:  [483815.5] [275.0] [8]
processed state3-5 at 1641198732890
liner_to_log:  tensor([[[1.0548]]]) tensor([[[0.5339]]])
linear_to_log at 1641198732890
listState:  [0.06456, 0.1285, 0.0, 0.4838155, 0.275, 0.25, tensor([[[0.5339]]])]
state_clone_detach at 1641198732890
reward: 0.04019571545761885
state tensor([0.0646, 0.1285, 0.0000, 0.5339], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198732891
state222:  tensor([[0.0646, 0.1285, 0.0000, 0.5339]], device='cuda:0')
policy_old.forwarded at 1641198732893
give action 63============================
log_to_linear:  tensor([[[0.1724]]], device='cuda:0') tensor([[[0.5925]]], device='cuda:0')
log_to_linear action at 1641198732894
bwe changes from to:  [tensor([[[24896.7441]]]), tensor([[[14751.7607]]])]
step into gymStat at 1641198732894
send bwe to appRecv at 1641198732894
sent bwe to appRecv at 1641198732894
wait for recv string at 1641198732894
recved string at 1641198733099
1
wait for recv [self.estimator, stat] at 1641198733099
recved [self.estimator, stat] at 1641198733100
sorted packlist at 1641198733100
packetSeq:  3855
packetSeq:  3856
packetSeq:  3857
packetSeq:  3858
packetSeq:  3859
packetSeq:  3860
packetSeq:  3861
packetSeq:  3862
packetSeq:  3863
packetSeq:  3864
processed packlist at 1641198733100
receiving_rate:  285400.0
delay:  195.4
loss_ratio:  0.0
processed state0-2 at 1641198733100
avgFrameBetween:  6
psnrStat:  [[484872, 484767, 485259, 485011, 484761, 484390]]
delayStat:  [[275, 274, 275, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  4
state:  [484575.5] [275.0] [8]
processed state3-5 at 1641198733100
liner_to_log:  tensor([[[0.5925]]]) tensor([[[0.1724]]])
linear_to_log at 1641198733100
listState:  [0.07135, 0.13026666666666667, 0.0, 0.4845755, 0.275, 0.25, tensor([[[0.1724]]])]
state_clone_detach at 1641198733100
reward: 0.06613192793440337
state tensor([0.0714, 0.1303, 0.0000, 0.1724], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198733101
state222:  processed allFrame at 1641198729024
send 'asking for bwe' at 1641198729024
sent 'asking for bwe' at 1641198729024
send [estimator, stat] at 1641198729024
sent [estimator, stat] at 1641198729024
pc wait for bwe at 1641198729025
pc got bwe at 1641198729029
bandwidth:  300000
pc flushed at 1641198729029
Bwe Sent: 5 at 1641198729029
got request at 1641198729232
processed allFrame at 1641198729232
send 'asking for bwe' at 1641198729232
sent 'asking for bwe' at 1641198729232
send [estimator, stat] at 1641198729232
sent [estimator, stat] at 1641198729232
pc wait for bwe at 1641198729232
pc got bwe at 1641198729236
bandwidth:  300000
pc flushed at 1641198729237
Bwe Sent: 5 at 1641198729237
got request at 1641198729457
processed allFrame at 1641198729457
send 'asking for bwe' at 1641198729457
sent 'asking for bwe' at 1641198729457
send [estimator, stat] at 1641198729457
sent [estimator, stat] at 1641198729458
pc wait for bwe at 1641198729458
pc got bwe at 1641198729462
bandwidth:  300000
pc flushed at 1641198729462
Bwe Sent: 5 at 1641198729462
got request at 1641198729663
processed allFrame at 1641198729663
send 'asking for bwe' at 1641198729663
sent 'asking for bwe' at 1641198729663
send [estimator, stat] at 1641198729663
sent [estimator, stat] at 1641198729663
pc wait for bwe at 1641198729664
pc got bwe at 1641198729668
bandwidth:  300000
pc flushed at 1641198729668
Bwe Sent: 5 at 1641198729668
got request at 1641198729891
processed allFrame at 1641198729891
send 'asking for bwe' at 1641198729891
sent 'asking for bwe' at 1641198729891
send [estimator, stat] at 1641198729891
sent [estimator, stat] at 1641198729891
pc wait for bwe at 1641198729892
pc got bwe at 1641198729896
bandwidth:  300000
pc flushed at 1641198729896
Bwe Sent: 5 at 1641198729896
got request at 1641198730125
processed allFrame at 1641198730125
send 'asking for bwe' at 1641198730125
sent 'asking for bwe' at 1641198730125
send [estimator, stat] at 1641198730125
sent [estimator, stat] at 1641198730125
pc wait for bwe at 1641198730126
pc got bwe at 1641198730130
bandwidth:  300000
pc flushed at 1641198730130
Bwe Sent: 5 at 1641198730130
got request at 1641198730332
processed allFrame at 1641198730332
send 'asking for bwe' at 1641198730332
sent 'asking for bwe' at 1641198730332
send [estimator, stat] at 1641198730332
sent [estimator, stat] at 1641198730332
pc wait for bwe at 1641198730332
pc got bwe at 1641198730336
bandwidth:  300000
pc flushed at 1641198730336
Bwe Sent: 4 at 1641198730336
got request at 1641198730558
processed allFrame at 1641198730558
send 'asking for bwe' at 1641198730558
sent 'asking for bwe' at 1641198730558
send [estimator, stat] at 1641198730558
sent [estimator, stat] at 1641198730558
pc wait for bwe at 1641198730558
pc got bwe at 1641198730562
bandwidth:  300000
pc flushed at 1641198730562
Bwe Sent: 4 at 1641198730562
got request at 1641198730761
processed allFrame at 1641198730761
send 'asking for bwe' at 1641198730761
sent 'asking for bwe' at 1641198730761
send [estimator, stat] at 1641198730761
sent [estimator, stat] at 1641198730761
pc wait for bwe at 1641198730762
pc got bwe at 1641198730766
bandwidth:  300000
pc flushed at 1641198730766
Bwe Sent: 5 at 1641198730766
got request at 1641198730993
processed allFrame at 1641198730993
send 'asking for bwe' at 1641198730993
sent 'asking for bwe' at 1641198730993
send [estimator, stat] at 1641198730993
sent [estimator, stat] at 1641198730993
pc wait for bwe at 1641198730993
pc got bwe at 1641198730997
bandwidth:  300000
pc flushed at 1641198730997
Bwe Sent: 4 at 1641198730997
got request at 1641198731196
processed allFrame at 1641198731196
send 'asking for bwe' at 1641198731196
sent 'asking for bwe' at 1641198731196
send [estimator, stat] at 1641198731196
sent [estimator, stat] at 1641198731196
pc wait for bwe at 1641198731196
pc got bwe at 1641198731200
bandwidth:  300000
pc flushed at 1641198731201
Bwe Sent: 5 at 1641198731201
got request at 1641198731398
processed allFrame at 1641198731398
send 'asking for bwe' at 1641198731398
sent 'asking for bwe' at 1641198731398
send [estimator, stat] at 1641198731398
sent [estimator, stat] at 1641198731398
pc wait for bwe at 1641198731398
pc got bwe at 1641198731402
bandwidth:  300000
pc flushed at 1641198731402
Bwe Sent: 4 at 1641198731402
got request at 1641198731622
processed allFrame at 1641198731622
send 'asking for bwe' at 1641198731622
sent 'asking for bwe' at 1641198731622
send [estimator, stat] at 1641198731622
sent [estimator, stat] at 1641198731622
pc wait for bwe at 1641198731622
pc got bwe at 1641198731627
bandwidth:  300000
pc flushed at 1641198731627
Bwe Sent: 5 at 1641198731627
got request at 1641198731825
processed allFrame at 1641198731825
send 'asking for bwe' at 1641198731825
sent 'asking for bwe' at 1641198731825
send [estimator, stat] at 1641198731825
sent [estimator, stat] at 1641198731825
pc wait for bwe at 1641198731826
pc got bwe at 1641198731830
bandwidth:  300000
pc flushed at 1641198731830
Bwe Sent: 5 at 1641198731830
got request at 1641198732027
processed allFrame at 1641198732027
send 'asking for bwe' at 1641198732027
sent 'asking for bwe' at 1641198732027
send [estimator, stat] at 1641198732027
sent [estimator, stat] at 1641198732027
pc wait for bwe at 1641198732027
pc got bwe at 1641198732031
bandwidth:  300000
pc flushed at 1641198732031
Bwe Sent: 4 at 1641198732031
got request at 1641198732231
processed allFrame at 1641198732231
send 'asking for bwe' at 1641198732231
sent 'asking for bwe' at 1641198732231
send [estimator, stat] at 1641198732231
sent [estimator, stat] at 1641198732231
pc wait for bwe at 1641198732231
pc got bwe at 1641198732236
bandwidth:  300000
pc flushed at 1641198732236
Bwe Sent: 5 at 1641198732236
got request at 1641198732459
processed allFrame at 1641198732459
send 'asking for bwe' at 1641198732459
sent 'asking for bwe' at 1641198732459
send [estimator, stat] at 1641198732459
sent [estimator, stat] at 1641198732459
pc wait for bwe at 1641198732459
pc got bwe at 1641198732464
bandwidth:  300000
pc flushed at 1641198732464
Bwe Sent: 5 at 1641198732464
got request at 1641198732687
processed allFrame at 1641198732687
send 'asking for bwe' at 1641198732687
sent 'asking for bwe' at 1641198732687
send [estimator, stat] at 1641198732687
sent [estimator, stat] at 1641198732687
pc wait for bwe at 1641198732687
pc got bwe at 1641198732691
bandwidth:  300000
pc flushed at 1641198732691
Bwe Sent: 4 at 1641198732691
got request at 1641198732889
processed allFrame at 1641198732889
send 'asking for bwe' at 1641198732889
sent 'asking for bwe' at 1641198732889
send [estimator, stat] at 1641198732889
sent [estimator, stat] at 1641198732889
pc wait for bwe at 1641198732889
pc got bwe at 1641198732894
bandwidth:  300000
pc flushed at 1641198732894
Bwe Sent: 5 at 1641198732894
got request at 1641198733099
processed allFrame at 1641198733099
send 'asking for bwe' at 1641198733099
sent 'asking for bwe' at 1641198733099
send [estimator, stat] at 1641198733099
sent [estimator, stat] at 1641198733100
pc wait for bwe at 1641198733100
pc got bwe at 1641198733104
bandwidth:  300000
pc flushed at 1641198733104
Bwe Sent: 5 at 1641198733104
got request at 1641198733319
processed allFrame at 1641198733320
send 'asking for bwe' at 1641198733320
sent 'asking for bwe' at 1641198733320
send [estimator, stat] at 1641198733320
sent [estimator, stat] at 1641198733320
pc wait for bwe at 1641198733320
pc got bwe at 1641198733324
bandwidth:  300000
pc flushed at 1641198733324
Bwe Sent: 5 at 1641198733324
got request at 1641198733905
processed allFrame at 1641198733905
send 'asking for bwe' at 1641198733905
sent 'asking for bwe' at 1641198733905
send [estimator, stat] at 1641198733905
sent [estimator, stat] at 1641198733905
pc wait for bwe at 1641198733905
pc got bwe at 1641198733909
bandwidth:  300000
pc flushed at 1641198733909
Bwe Sent: 4 at 1641198733909
got request at 1641198734150
processed allFrame at 1641198734150
send 'asking for bwe' at 1641198734150
sent 'asking for bwe' at 1641198734150
send [estimator, stat] at 1641198734150
sent [estimator, stat] at 1641198734150
pc wait for bwe at 1641198734150
pc got bwe at 1641198734154
bandwidth:  tensor([[0.0714, 0.1303, 0.0000, 0.1724]], device='cuda:0')
policy_old.forwarded at 1641198733103
give action 64============================
log_to_linear:  tensor([[[0.5067]]], device='cuda:0') tensor([[[1.0106]]], device='cuda:0')
log_to_linear action at 1641198733104
bwe changes from to:  [tensor([[[14751.7607]]]), tensor([[[14908.0215]]])]
step into gymStat at 1641198733104
send bwe to appRecv at 1641198733104
sent bwe to appRecv at 1641198733104
wait for recv string at 1641198733104
recved string at 1641198733320
1
wait for recv [self.estimator, stat] at 1641198733320
recved [self.estimator, stat] at 1641198733320
sorted packlist at 1641198733320
packetSeq:  3865
packetSeq:  3866
packetSeq:  3867
packetSeq:  3868
packetSeq:  3869
packetSeq:  3870
packetSeq:  3871
packetSeq:  3872
packetSeq:  3873
packetSeq:  3874
processed packlist at 1641198733320
receiving_rate:  293920.0
delay:  194.6
loss_ratio:  0.0
processed state0-2 at 1641198733320
avgFrameBetween:  6
psnrStat:  [[483975, 483696, 483402, 483620, 483165, 482955]]
delayStat:  [[274, 275, 276, 276, 276, 274]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  4
state:  [483060.0] [275.0] [8]
processed state3-5 at 1641198733320
liner_to_log:  tensor([[[1.0106]]]) tensor([[[0.5067]]])
linear_to_log at 1641198733320
listState:  [0.07348, 0.12973333333333334, 0.0, 0.48306, 0.275, 0.25, tensor([[[0.5067]]])]
state_clone_detach at 1641198733321
reward: 0.0772047556135746
state tensor([0.0735, 0.1297, 0.0000, 0.5067], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198733321
state222:  tensor([[0.0735, 0.1297, 0.0000, 0.5067]], device='cuda:0')
policy_old.forwarded at 1641198733323
give action 65============================
log_to_linear:  tensor([[[0.3231]]], device='cuda:0') tensor([[[0.7503]]], device='cuda:0')
log_to_linear action at 1641198733324
bwe changes from to:  [tensor([[[14908.0215]]]), tensor([[[11185.0986]]])]
step into gymStat at 1641198733324
send bwe to appRecv at 1641198733324
sent bwe to appRecv at 1641198733324
wait for recv string at 1641198733324
recved string at 1641198733905
1
wait for recv [self.estimator, stat] at 1641198733905
recved [self.estimator, stat] at 1641198733905
sorted packlist at 1641198733905
packetSeq:  3875
packetSeq:  3876
packetSeq:  3877
packetSeq:  3878
packetSeq:  3879
packetSeq:  3880
packetSeq:  3881
packetSeq:  3882
packetSeq:  3883
processed packlist at 1641198733905
receiving_rate:  27080.0
delay:  579.0
loss_ratio:  0.0
processed state0-2 at 1641198733905
avgFrameBetween:  6
psnrStat:  [[483170, 480892, 482649, 480495, 480029, 480339, 479957, 479630, 480523, 479479, 479458, 480031, 480454, 479749]]
delayStat:  [[276, 275, 275, 275, 275, 275, 275, 275, 275, 275, 274, 274, 274, 274]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  4
state:  [479964.9] [274.6] [0]
processed state3-5 at 1641198733905
liner_to_log:  tensor([[[0.7503]]]) tensor([[[0.3231]]])
linear_to_log at 1641198733906
listState:  [0.00677, 0.386, 0.0, 0.4799649, 0.2746, 0.0, tensor([[[0.3231]]])]
state_clone_detach at 1641198733906
reward: -1.0962927868247052
state tensor([0.0068, 0.3860, 0.0000, 0.3231], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198733906
state222:  tensor([[0.0068, 0.3860, 0.0000, 0.3231]], device='cuda:0')
policy_old.forwarded at 1641198733908
give action 66============================
log_to_linear:  tensor([[[0.2117]]], device='cuda:0') tensor([[[0.6281]]], device='cuda:0')
log_to_linear action at 1641198733909
bwe changes from to:  [tensor([[[11185.0986]]]), tensor([[[7025.2075]]])]
step into gymStat at 1641198733909
send bwe to appRecv at 1641198733909
sent bwe to appRecv at 1641198733909
wait for recv string at 1641198733909
recved string at 1641198734150
1
wait for recv [self.estimator, stat] at 1641198734150
recved [self.estimator, stat] at 1641198734150
sorted packlist at 1641198734150
packetSeq:  3884
packetSeq:  3885
packetSeq:  3886
packetSeq:  3887
packetSeq:  3888
packetSeq:  3889
packetSeq:  3890
packetSeq:  3891
packetSeq:  3892
processed packlist at 1641198734150
receiving_rate:  63880.0
delay:  543.0
loss_ratio:  0.0
processed state0-2 at 1641198734150
avgFrameBetween:  6
psnrStat:  [[481641, 480135, 480120, 479497, 479795, 479350]]
delayStat:  [[399, 374, 352, 326, 303, 292]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [480089.6666666667] [341.0] [0]
processed state3-5 at 1641198734150
liner_to_log:  tensor([[[0.6281]]]) tensor([[[0.2117]]])
linear_to_log at 1641198734151
listState:  [0.01597, 0.362, 0.0, 0.4800896666666667, 0.341, 0.0, tensor([[[0.2117]]])]
state_clone_detach at 1641198734151
reward: -0.9494701023617844
state tensor([0.0160, 0.3620, 0.0000, 0.2117], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198734151
state222:  tensor([[0.0160, 0.3620, 0.0000, 0.2117]], device='cuda:0')
policy_old.forwarded at 1641198734153
give action 67============================
log_to_linear:  tensor([[[0.3207]]], device='cuda:0') tensor([[[0.7474]]], device='cuda:0')
log_to_linear action at 1641198734154
bwe changes from to:  [tensor([[[7025.2075]]]), tensor([[[5250.4263]]])]
step into gymStat at 1641198734154
send bwe to appRecv at 1641198734154
sent bwe to appRecv at 1641198734154
wait for recv string at 1641198734154
recved string at 1641198734507
1
wait for recv [self.estimator, stat] at 1641198734507
recved [self.estimator, stat] at 1641198734507
sorted packlist at 1641198734507
packetSeq:  3893
packetSeq:  3894
packetSeq:  3895
packetSeq:  3896
packetSeq:  3897
packetSeq:  3898
packetSeq:  3899
packetSeq:  3900
packetSeq:  3901
packetSeq:  3902
packetSeq:  3903
packetSeq:  3904
packetSeq:  3905
packetSeq:  3906
packetSeq:  3907
packetSeq:  3908
processed packlist at 1641198734507
receiving_rate:  40400.0
delay:  611.0
loss_ratio:  0.0
processed state0-2 at 1641198734507
avgFrameBetween:  6
psnrStat:  [[481159, 481048, 482700, 483995, 484797, 485542, 485516, 486164, 486240, 486639, 487365]]
delayStat:  [[444, 419, 398, 372, 349, 326, 314, 315, 323, 322, 321]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [484651.36363636365] [354.8181818181818] [0]
processed state3-5 at 1641198734507
liner_to_log:  tensor([[[0.7474]]]) tensor([[[0.3207]]])
linear_to_log at 1641198734507
listState:  [0.0101, 0.4073333333333333, 0.0, 0.48465136363636363, 0.3548181818181818, 0.0, tensor([[[0.3207]]])]
state_clone_detach at 1641198734508
reward: -1.1321209809469632
state tensor([0.0101, 0.4073, 0.0000, 0.3207], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198734508
state222:  tensor([[0.0101, 0.4073, 0.0000, 0.3207]], device='cuda:0')
policy_old.forwarded at 1641198734510
give action 68============================
log_to_linear:  tensor([[[0.5421]]], device='cuda:0') tensor([[[1.0683]]], device='cuda:0')
log_to_linear action at 1641198734511
bwe changes from to:  [tensor([[[5250.4263]]]), tensor([[[5609.1919]]])]
step into gymStat at 1641198734511
send bwe to appRecv at 1641198734511
sent bwe to appRecv at 1641198734511
wait for recv string at 1641198734511
recved string at 1641198734717
1
wait for recv [self.estimator, stat] at 1641198734717
recved [self.estimator, stat] at 1641198734718
sorted packlist at 1641198734718
packetSeq:  3909
packetSeq:  3910
packetSeq:  3911
packetSeq:  3912
packetSeq:  3913
packetSeq:  3914
processed packlist at 1641198734718
receiving_rate:  115080.0
delay:  590.5
loss_ratio:  0.0
processed state0-2 at 1641198734718
avgFrameBetween:  6
psnrStat:  [[487600, 487721, 487689, 487668]]
delayStat:  [[434, 406, 384, 361]]
skipStat:  [[1, 1, 1, 1]]
skipCount:  0
state:  [487669.5] [396.25] [2]
processed state3-5 at 1641198734718
liner_to_log:  tensor([[[1.0683]]]) tensor([[[0.5421]]])
linear_to_log at 1641198734718
listState:  [0.02877, 0.39366666666666666, 0.0, 0.4876695, 0.39625, 0.0625, tensor([[[0.5421]]])]
state_clone_detach at 1641198734718
reward: -0.953806646128142
state tensor([0.0288, 0.3937, 0.0000, 0.5421], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198734719
state222:  tensor([[0.0288, 0.3937, 0.0000, 0.5421]], device='cuda:0')
policy_old.forwarded at 1641198734721
give action 69============================
log_to_linear:  tensor([[[0.7296]]], device='cuda:0') tensor([[[1.4101]]], device='cuda:0')
log_to_linear action at 1641198734722
bwe changes from to:  [tensor([[[5609.1919]]]), tensor([[[7909.3984]]])]
step into gymStat at 1641198734722
send bwe to appRecv at 1641198734722
sent bwe to appRecv at 1641198734722
wait for recv string at 1641198734722
recved string at 1641198734923
1
wait for recv [self.estimator, stat] at 1641198734923
recved [self.estimator, stat] at 1641198734923
sorted packlist at 1641198734923
packetSeq:  3915
packetSeq:  3916
packetSeq:  3917
packetSeq:  3918
packetSeq:  3919
packetSeq:  3920
packetSeq:  3921
packetSeq:  3922
packetSeq:  3923
packetSeq:  3924
packetSeq:  3925
packetSeq:  3926
packetSeq:  3927
packetSeq:  3928
packetSeq:  3929
packetSeq:  3930
packetSeq:  3931
packetSeq:  3932
packetSeq:  3933
packetSeq:  3934
packetSeq:  3935
packetSeq:  3936
packetSeq:  3937
packetSeq:  3938
packetSeq:  3939
packetSeq:  3940
packetSeq:  3941
packetSeq:  3942
packetSeq:  3943
packetSeq:  3944
packetSeq:  3945
processed packlist at 1641198734923
receiving_rate:  867440.0
delay:  487.1666666666667
loss_ratio:  0.0
processed state0-2 at 1641198734923
avgFrameBetween:  6
psnrStat:  [[487012, 487043, 486349, 486558, 485871, 485611, 484471, 484689, 484753, 485360, 485918]]
delayStat:  [[512, 488, 464, 444, 422, 400, 375, 354, 347, 350, 356]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  2
state:  [485508.8888888889] [390.22222222222223] [0]
processed state3-5 at 1641198734923
liner_to_log:  tensor([[[1.4101]]]) tensor([[[0.7296]]])
linear_to_log at 1641198734924
listState:  [0.21686, 0.3247777777777778, 0.0, 0.4855088888888889, 0.39022222222222225, 0.0, tensor([[[0.7296]]])]
state_clone_detach at 1641198734924
reward: -0.07593250378800454
state tensor([0.2169, 0.3248, 0.0000, 0.7296], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198734925
state222:  tensor([[0.2169, 0.3248, 0.0000, 0.7296]], device='cuda:0')
policy_old.forwarded at 1641198734926
give action 70============================
log_to_linear:  tensor([[[0.0714]]], device='cuda:0') tensor([[[0.5229]]], device='cuda:0')
log_to_linear action at 1641198734927
bwe changes from to:  [tensor([[[7909.3984]]]), tensor([[[4135.7632]]])]
step into gymStat at 1641198734927
send bwe to appRecv at 1641198734927
sent bwe to appRecv at 1641198734928
wait for recv string at 1641198734928
recved string at 1641198735153
1
wait for recv [self.estimator, stat] at 1641198735153
recved [self.estimator, stat] at 1641198735153
sorted packlist at 1641198735153
packetSeq:  3946
packetSeq:  3947
packetSeq:  3948
packetSeq:  3949
packetSeq:  3950
packetSeq:  3951
packetSeq:  3952
packetSeq:  3953
packetSeq:  3954
packetSeq:  3955
processed packlist at 1641198735154
receiving_rate:  218640.0
delay:  196.0
loss_ratio:  0.0
processed state0-2 at 1641198735154
avgFrameBetween:  6
psnrStat:  [[487084, 487322, 488177, 488177, 493028, 495139, 495978]]
delayStat:  [[358, 360, 359, 355, 357, 356, 356]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [490700.71428571426] [357.2857142857143] [0]
processed state3-5 at 1641198735154
liner_to_log:  tensor([[[0.5229]]]) tensor([[[0.0714]]])
linear_to_log at 1641198735154
listState:  [0.05466, 0.13066666666666665, 0.0, 0.49070071428571427, 0.35728571428571426, 0.0, tensor([[[0.0714]]])]
state_clone_detach at 1641198735154
reward: -0.014975620869693895
state tensor([0.0547, 0.1307, 0.0000, 0.0714], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198735155
state222:  tensor([[0.0547, 0.1307, 0.0000, 0.0714]], device='cuda:0')
policy_old.forwarded at 1641198735157
give action 71============================
log_to_linear:  tensor([[[0.5427]]], device='cuda:0') tensor([[[1.0693]]], device='cuda:0')
log_to_linear action at 1641198735157
bwe changes from to:  [tensor([[[4135.7632]]]), tensor([[[4422.2808]]])]
step into gymStat at 1641198735158
send bwe to appRecv at 1641198735158
sent bwe to appRecv at 1641198735158
wait for recv string at 1641198735158
recved string at 1641198735385
1
wait for recv [self.estimator, stat] at 1641198735385
recved [self.estimator, stat] at 1641198735386
sorted packlist at 1641198735386
packetSeq:  3956
packetSeq:  3957
packetSeq:  3958
packetSeq:  3959
packetSeq:  3960
packetSeq:  3961
packetSeq:  3962
packetSeq:  3963
processed packlist at 1641198735386
receiving_rate:  266760.0
delay:  192.625
loss_ratio:  0.0
processed state0-2 at 1641198735386
avgFrameBetween:  6
psnrStat:  [[498433, 499109, 501339, 503497, 506754, 507970, 510881]]
delayStat:  [[356, 356, 354, 355, 354, 353, 353]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [503997.5714285714] [354.42857142857144] [0]
processed state3-5 at 1641198735386
liner_to_log:  tensor([[[1.0693]]]) tensor([[[0.5427]]])
linear_to_log at 1641198735386
listState:  [0.06669, 0.12841666666666668, 0.0, 0.5039975714285714, 0.3544285714285714, 0.0, tensor([[[0.5427]]])]
state_clone_detach at 1641198735386
reward: 0.05042057933791244
state tensor([0.0667, 0.1284, 0.0000, 0.5427], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198735387
state222:  tensor([[0.0667, 0.1284, 0.0000, 0.5427]], device='cuda:0')
policy_old.forwarded at 1641198735389
give action 72============================
log_to_linear:  tensor([[[0.3482]]], device='cuda:0') tensor([[[0.7818]]], device='cuda:0')
log_to_linear action at 1641198735390
bwe changes from to:  [tensor([[[4422.2808]]]), tensor([[[3457.1785]]])]
step into gymStat at 1641198735390
send bwe to appRecv at 1641198735390
sent bwe to appRecv at 1641198735390
wait for recv string at 1641198735390
recved string at 1641198735591
1
wait for recv [self.estimator, stat] at 1641198735591
recved [self.estimator, stat] at 1641198735591
sorted packlist at 1641198735591
packetSeq:  3964
packetSeq:  3965
packetSeq:  3966
packetSeq:  3967
packetSeq:  3968
packetSeq:  3969
packetSeq:  3970
packetSeq:  3971
processed packlist at 1641198735591
receiving_rate:  258839.99999999997
delay:  193.0
loss_ratio:  0.0
processed state0-2 at 1641198735591
avgFrameBetween:  6
psnrStat:  [[512797, 515639, 517292, 518770, 520737, 522393]]
delayStat:  [[352, 351, 352, 350, 350, 349]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [517938.0] [350.6666666666667] [0]
processed state3-5 at 1641198735591
liner_to_log:  tensor([[[0.7818]]]) tensor([[[0.3482]]])
linear_to_log at 1641198735592
listState:  [0.06470999999999999, 0.12866666666666668, 0.0, 0.517938, 0.3506666666666667, 0.0, tensor([[[0.3482]]])]
state_clone_detach at 1641198735592
reward: 0.0404036244554582
state tensor([0.0647, 0.1287, 0.0000, 0.3482], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198735592
state222:  tensor([[0.0647, 0.1287, 0.0000, 0.3482]], device='cuda:0')
policy_old.forwarded at 1641198735594
give action 73============================
log_to_linear:  tensor([[[0.1872]]], device='cuda:0') tensor([[[0.6054]]], device='cuda:0')
log_to_linear action at 1641198735595
bwe changes from to:  [tensor([[[3457.1785]]]), tensor([[[2092.8201]]])]
step into gymStat at 1641198735595
send bwe to appRecv at 1641198735595
sent bwe to appRecv at 1641198735595
wait for recv string at 1641198735595
recved string at 1641198735824
1
wait for recv [self.estimator, stat] at 1641198735824
recved [self.estimator, stat] at 1641198735824
sorted packlist at 1641198735824
packetSeq:  3972
packetSeq:  3973
packetSeq:  3974
packetSeq:  3975
packetSeq:  3976
packetSeq:  3977
packetSeq:  3978
packetSeq:  3979
packetSeq:  3980
processed packlist at 1641198735824
receiving_rate:  256040.00000000003
delay:  192.85714285714286
loss_ratio:  0.0
processed state0-2 at 1641198735824
avgFrameBetween:  6
psnrStat:  [[525198, 526707, 528066, 529433, 529327, 530546, 532593]]
delayStat:  [[349, 350, 348, 348, 347, 348, 347]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [528838.5714285715] [348.14285714285717] [0]
processed state3-5 at 1641198735824
liner_to_log:  tensor([[[0.6054]]]) tensor([[[0.1872]]])
linear_to_log at 1641198735824
listState:  [0.06401000000000001, 0.1285714285714286, 0.0, 0.5288385714285715, 0.34814285714285714, 0.0, tensor([[[0.1872]]])]
state_clone_detach at 1641198735825
reward: 0.03737859409624772
state tensor([0.0640, 0.1286, 0.0000, 0.1872], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198735825
state222:  tensor([[0.0640, 0.1286, 0.0000, 0.1872]], device='cuda:0')
policy_old.forwarded at 1641198735827
give action 74============================
log_to_linear:  tensor([[[0.7259]]], device='cuda:0') tensor([[[1.4028]]], device='cuda:0')
log_to_linear action at 1641198735828
bwe changes from to:  [tensor([[[2092.8201]]]), tensor([[[2935.7908]]])]
step into gymStat at 1641198735828
send bwe to appRecv at 1641198735828
sent bwe to appRecv at 1641198735828
wait for recv string at 1641198735828
recved string at 1641198736053
1
wait for recv [self.estimator, stat] at 1641198736053
recved [self.estimator, stat] at 1641198736053
sorted packlist at 1641198736053
packetSeq:  3981
packetSeq:  3982
packetSeq:  3983
packetSeq:  3984
packetSeq:  3985
packetSeq:  3986
packetSeq:  3987
processed packlist at 1641198736053
receiving_rate:  263600.0
delay:  193.57142857142858
loss_ratio:  0.0
processed state0-2 at 1641198736053
avgFrameBetween:  6
psnrStat:  [[532816, 532506, 533802, 532875, 533924, 533983, 534212]]
delayStat:  [[346, 346, 344, 345, 344, 344, 344]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [533445.4285714285] [344.7142857142857] [0]
processed state3-5 at 1641198736053
liner_to_log:  tensor([[[1.4028]]]) tensor([[[0.7259]]])
linear_to_log at 1641198736054
listState:  [0.0659, 0.12904761904761905, 0.0, 0.5334454285714285, 0.3447142857142857, 0.0, tensor([[[0.7259]]])]
state_clone_detach at 1641198736054
reward: 0.04484744356524589
state tensor([0.0659, 0.1290, 0.0000, 0.7259], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198736054
state222:  tensor([[0.0659, 0.1290, 0.0000, 0.7259]], device='cuda:0')
policy_old.forwarded at 1641198736056
give action 75============================
log_to_linear:  tensor([[[0.5236]]], device='cuda:0') tensor([[[1.0380]]], device='cuda:0')
log_to_linear action at 1641198736057
bwe changes from to:  [tensor([[[2935.7908]]]), tensor([[[3047.2451]]])]
step into gymStat at 1641198736057
send bwe to appRecv at 1641198736057
sent bwe to appRecv at 1641198736057
wait for recv string at 1641198736057
recved string at 1641198736254
1
wait for recv [self.estimator, stat] at 1641198736254
recved [self.estimator, stat] at 1641198736254
sorted packlist at 1641198736254
packetSeq:  3988
packetSeq:  3989
packetSeq:  3990
packetSeq:  3991
packetSeq:  3992
packetSeq:  3993
packetSeq:  3994
packetSeq:  3995
processed packlist at 1641198736254
receiving_rate:  252760.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198736255
avgFrameBetween:  6
psnrStat:  [[534921, 535500, 536114, 536636, 536821, 537261]]
delayStat:  [[343, 343, 342, 342, 342, 342]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [536208.8333333334] [342.3333333333333] [0]
processed state3-5 at 1641198736255
liner_to_log:  tensor([[[1.0380]]]) tensor([[[0.5236]]])
linear_to_log at 1641198736255
listState:  [0.06319, 0.12816666666666668, 0.0, 0.5362088333333334, 0.3423333333333333, 0.0, tensor([[[0.5236]]])]
state_clone_detach at 1641198736255
reward: 0.034691200716634174
state tensor([0.0632, 0.1282, 0.0000, 0.5236], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198736256
state222:  tensor([[0.0632, 0.1282, 0.0000, 0.5236]], device='cuda:0')
policy_old.forwarded at 1641198736258
give action 76============================
log_to_linear:  tensor([[[0.5523]]], device='cuda:0') tensor([[[1.0853]]], device='cuda:0')
log_to_linear action at 1641198736259
bwe changes from to:  [tensor([[[3047.2451]]]), tensor([[[3307.2837]]])]
step into gymStat at 1641198736259
send bwe to appRecv at 1641198736259
sent bwe to appRecv at 1641198736259
wait for recv string at 1641198736259
recved string at 1641198736455
1
wait for recv [self.estimator, stat] at 1641198736455
recved [self.estimator, stat] at 1641198736455
sorted packlist at 1641198736455
packetSeq:  3996
packetSeq:  3997
packetSeq:  3998
packetSeq:  3999
packetSeq:  4000
packetSeq:  4001
packetSeq:  4002
packetSeq:  4003
packetSeq:  4004
packetSeq:  4005
processed packlist at 1641198736455
receiving_rate:  275400.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198736455
avgFrameBetween:  6
psnrStat:  [[538000, 537923, 539401, 540402, 542770, 543616, 545515]]
delayStat:  [[341, 341, 341, 341, 340, 339, 340]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [541089.5714285715] [340.42857142857144] [0]
processed state3-5 at 1641198736455
liner_to_log:  tensor([[[1.0853]]]) tensor([[[0.5523]]])
linear_to_log at 1641198736456
listState:  [0.06885, 0.12806666666666666, 0.0, 0.5410895714285715, 0.34042857142857147, 0.0, tensor([[[0.5523]]])]
state_clone_detach at 1641198736456
reward: 0.06141937563351585
state tensor([0.0689, 0.1281, 0.0000, 0.5523], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198736457
state222:  tensor([[0.0689, 0.1281, 0.0000, 0.5523]], device='cuda:0')
policy_old.forwarded at 1641198736458
give action 77============================
log_to_linear:  tensor([[[0.5002]]], device='cuda:0') tensor([[[1.0004]]], device='cuda:0')
log_to_linear action at 1641198736459
bwe changes from to:  [tensor([[[3307.2837]]]), tensor([[[3308.5693]]])]
step into gymStat at 1641198736460
send bwe to appRecv at 1641198736460
sent bwe to appRecv at 1641198736460
wait for recv string at 1641198736460
recved string at 1641198736658
1
wait for recv [self.estimator, stat] at 1641198736658
recved [self.estimator, stat] at 1641198736658
sorted packlist at 1641198736658
packetSeq:  4006
packetSeq:  4007
packetSeq:  4008
packetSeq:  4009
packetSeq:  4010
packetSeq:  4011
packetSeq:  4012
packetSeq:  4013
packetSeq:  4014
packetSeq:  4015
processed packlist at 1641198736658
receiving_rate:  277240.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198736658
avgFrameBetween:  6
psnrStat:  [[545987, 546830, 547538, 547735, 547793, 548244]]
delayStat:  [[340, 338, 339, 338, 337, 338]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547354.5] [338.3333333333333] [0]
processed state3-5 at 1641198736658
liner_to_log:  tensor([[[1.0004]]]) tensor([[[0.5002]]])
linear_to_log at 1641198736659
listState:  [0.06931, 0.12806666666666666, 0.0, 0.5473545, 0.3383333333333333, 0.0, tensor([[[0.5002]]])]
state_clone_detach at 1641198736659
reward: 0.06351696098921106
state tensor([0.0693, 0.1281, 0.0000, 0.5002], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198736660
state222:  tensor([[0.0693, 0.1281, 0.0000, 0.5002]], device='cuda:0')
policy_old.forwarded at 1641198736661
give action 78============================
log_to_linear:  tensor([[[0.5451]]], device='cuda:0') tensor([[[1.0733]]], device='cuda:0')
log_to_linear action at 1641198736662
bwe changes from to:  [tensor([[[3308.5693]]]), tensor([[[3550.9622]]])]
step into gymStat at 1641198736662
send bwe to appRecv at 1641198736662
sent bwe to appRecv at 1641198736662
wait for recv string at 1641198736662
recved string at 1641198736888
1
wait for recv [self.estimator, stat] at 1641198736888
recved [self.estimator, stat] at 1641198736889
sorted packlist at 1641198736889
packetSeq:  4016
packetSeq:  4017
packetSeq:  4018
packetSeq:  4019
packetSeq:  4020
packetSeq:  4021
packetSeq:  4022
packetSeq:  4023
packetSeq:  4024
processed packlist at 1641198736889
receiving_rate:  275080.0
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198736889
avgFrameBetween:  6
psnrStat:  [[548429, 549971, 550754, 551282, 551598, 550940]]
delayStat:  [[337, 358, 358, 357, 357, 357]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550495.6666666666] [354.0] [0]
processed state3-5 at 1641198736889
liner_to_log:  tensor([[[1.0733]]]) tensor([[[0.5451]]])
linear_to_log at 1641198736889
listState:  [0.06877, 0.1285, 0.0, 0.5504956666666666, 0.354, 0.0, tensor([[[0.5451]]])]
state_clone_detach at 1641198736889
reward: 0.05975382844377608
state tensor([0.0688, 0.1285, 0.0000, 0.5451], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198736890
state222:  tensor([[0.0688, 0.1285, 0.0000, 0.5451]], device='cuda:0')
policy_old.forwarded at 1641198736892
give action 79============================
log_to_linear:  tensor([[[0.3745]]], device='cuda:0') tensor([[[0.8162]]], device='cuda:0')
log_to_linear action at 1641198736893
bwe changes from to:  [tensor([[[3550.9622]]]), tensor([[[2898.2810]]])]
step into gymStat at 1641198736893
send bwe to appRecv at 1641198736893
sent bwe to appRecv at 1641198736893
wait for recv string at 1641198736893
recved string at 1641198737120
1
wait for recv [self.estimator, stat] at 1641198737120
recved [self.estimator, stat] at 1641198737120
sorted packlist at 1641198737120
packetSeq:  4025
packetSeq:  4026
packetSeq:  4027
packetSeq:  4028
packetSeq:  4029
packetSeq:  4030
packetSeq:  4031
packetSeq:  4032
packetSeq:  4033
packetSeq:  4034
processed packlist at 1641198737120
receiving_rate:  279640.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198737120
avgFrameBetween:  6
psnrStat:  [[551032, 550851, 551418, 551031, 550868, 551995, 551993]]
delayStat:  [[356, 356, 356, 355, 355, 355, 354]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551312.5714285715] [355.2857142857143] [0]
processed state3-5 at 1641198737120
liner_to_log:  tensor([[[0.8162]]]) tensor([[[0.3745]]])
linear_to_log at 1641198737121
listState:  [0.06991, 0.12814814814814815, 0.0, 0.5513125714285715, 0.35528571428571426, 0.0, tensor([[[0.3745]]])]
state_clone_detach at 1641198737121
reward: 0.06599752312114132
state tensor([0.0699, 0.1281, 0.0000, 0.3745], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198737121
state222:  tensor([[0.0699, 0.1281, 0.0000, 0.3745]], device='cuda:0')
policy_old.forwarded at 1641198737123
give action 80============================
log_to_linear:  tensor([[[0.4587]]], device='cuda:0') tensor([[[0.9362]]], device='cuda:0')
log_to_linear action at 1641198737124
bwe changes from to:  [tensor([[[2898.2810]]]), tensor([[[2713.2522]]])]
step into gymStat at 1641198737124
send bwe to appRecv at 1641198737124
sent bwe to appRecv at 1641198737124
wait for recv string at 1641198737124
recved string at 1641198737323
1
wait for recv [self.estimator, stat] at 1641198737323
recved [self.estimator, stat] at 1641198737323
sorted packlist at 1641198737323
packetSeq:  4035
packetSeq:  4036
packetSeq:  4037
packetSeq:  4038
packetSeq:  4039
packetSeq:  4040
packetSeq:  4041
packetSeq:  4042
packetSeq:  4043
packetSeq:  4044
packetSeq:  4045
processed packlist at 1641198737323
receiving_rate:  298120.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198737323
avgFrameBetween:  6
psnrStat:  [[552165, 552858, 552510, 552507, 552224, 551585]]
delayStat:  [[354, 354, 354, 353, 354, 352]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552308.1666666666] [353.5] [0]
processed state3-5 at 1641198737323
liner_to_log:  tensor([[[0.9362]]]) tensor([[[0.4587]]])
linear_to_log at 1641198737323
listState:  [0.07453, 0.12793939393939394, 0.0, 0.5523081666666666, 0.3535, 0.0, tensor([[[0.4587]]])]
state_clone_detach at 1641198737323
reward: 0.08720181211127037
state tensor([0.0745, 0.1279, 0.0000, 0.4587], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198737324
state222:  tensor([[0.0745, 0.1279, 0.0000, 0.4587]], device='cuda:0')
policy_old.forwarded at 1641198737326
give action 81============================
log_to_linear:  tensor([[[0.4904]]], device='cuda:0') tensor([[[0.9848]]], device='cuda:0')
log_to_linear action at 1641198737327
bwe changes from to:  [tensor([[[2713.2522]]]), tensor([[[2672.1333]]])]
step into gymStat at 1641198737327
send bwe to appRecv at 1641198737327
sent bwe to appRecv at 1641198737327
wait for recv string at 1641198737327
recved string at 1641198737525
1
wait for recv [self.estimator, stat] at 1641198737525
recved [self.estimator, stat] at 1641198737526
sorted packlist at 1641198737526
packetSeq:  4046
packetSeq:  4047
packetSeq:  4048
packetSeq:  4049
packetSeq:  4050
packetSeq:  4051
packetSeq:  4052
packetSeq:  4053
packetSeq:  4054
packetSeq:  4055
processed packlist at 1641198737526
receiving_rate:  267720.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198737526
avgFrameBetween:  6
psnrStat:  [[552050, 551564, 551819, 551638, 552087, 552355]]
delayStat:  [[352, 352, 351, 351, 351, 350]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551918.8333333334] [351.1666666666667] [0]
processed state3-5 at 1641198737526
liner_to_log:  tensor([[[0.9848]]]) tensor([[[0.4904]]])
linear_to_log at 1641198737526
listState:  [0.06693, 0.12793333333333334, 0.0, 0.5519188333333334, 0.3511666666666667, 0.0, tensor([[[0.4904]]])]
state_clone_detach at 1641198737526
reward: 0.052984182627194454
state tensor([0.0669, 0.1279, 0.0000, 0.4904], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198737527
state222:  tensor([[0.0669, 0.1279, 0.0000, 0.4904]], device='cuda:0')
policy_old.forwarded at 1641198737529
give action 82============================
log_to_linear:  tensor([[[0.3844]]], device='cuda:0') tensor([[[0.8296]]], device='cuda:0')
log_to_linear action at 1641198737530
bwe changes from to:  [tensor([[[2672.1333]]]), tensor([[[2216.8145]]])]
step into gymStat at 1641198737530
send bwe to appRecv at 1641198737530
sent bwe to appRecv at 1641198737530
wait for recv string at 1641198737530
recved string at 1641198737729
1
wait for recv [self.estimator, stat] at 1641198737729
recved [self.estimator, stat] at 1641198737729
sorted packlist at 1641198737729
packetSeq:  4056
packetSeq:  4057
packetSeq:  4058
packetSeq:  4059
packetSeq:  4060
packetSeq:  4061
packetSeq:  4062
packetSeq:  4063
packetSeq:  4064
packetSeq:  4065
processed packlist at 1641198737729
receiving_rate:  269640.0
delay:  194.8
loss_ratio:  0.0
processed state0-2 at 1641198737729
avgFrameBetween:  6
psnrStat:  [[551800, 552415, 551849, 552125, 553618, 553915]]
delayStat:  [[350, 351, 350, 349, 350, 349]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552620.3333333334] [349.8333333333333] [0]
processed state3-5 at 1641198737729
liner_to_log:  tensor([[[0.8296]]]) tensor([[[0.3844]]])
linear_to_log at 1641198737730
listState:  [0.06741, 0.12986666666666669, 0.0, 0.5526203333333334, 0.34983333333333333, 0.0, tensor([[[0.3844]]])]
state_clone_detach at 1641198737730
reward: 0.04940520775917134
state tensor([0.0674, 0.1299, 0.0000, 0.3844], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198737731
state222:  tensor([[0.0674, 0.1299, 0.0000, 0.3844]], device='cuda:0')
policy_old.forwarded at 1641198737732
give action 83============================
log_to_linear:  tensor([[[0.6012]]], device='cuda:0') tensor([[[1.1697]]], device='cuda:0')
log_to_linear action at 1641198737733
bwe changes from to:  [tensor([[[2216.8145]]]), tensor([[[2592.9512]]])]
step into gymStat at 1641198737733
send bwe to appRecv at 1641198737733
sent bwe to appRecv at 1641198737733
wait for recv string at 1641198737733
recved string at 1641198737960
1
wait for recv [self.estimator, stat] at 1641198737960
recved [self.estimator, stat] at 1641198737960
sorted packlist at 1641198737960
packetSeq:  4066
packetSeq:  4067
packetSeq:  4068
packetSeq:  4069
packetSeq:  4070
packetSeq:  4071
packetSeq:  4072
packetSeq:  4073
packetSeq:  4074
packetSeq:  4075
packetSeq:  4076
processed packlist at 1641198737960
receiving_rate:  330440.0
delay:  197.72727272727272
loss_ratio:  0.0
processed state0-2 at 1641198737960
avgFrameBetween:  6
psnrStat:  [[555157, 554824, 556171, 555655, 555878, 555614, 555846]]
delayStat:  [[364, 364, 363, 364, 363, 362, 363]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555592.1428571428] [363.2857142857143] [0]
processed state3-5 at 1641198737960
liner_to_log:  tensor([[[1.1697]]]) tensor([[[0.6012]]])
linear_to_log at 1641198737961
listState:  [0.08261, 0.1318181818181818, 0.0, 0.5555921428571429, 0.36328571428571427, 0.0, tensor([[[0.6012]]])]
state_clone_detach at 1641198737961
reward: 0.10994076580184203
state tensor([0.0826, 0.1318, 0.0000, 0.6012], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198737961
state222:  tensor([[0.0826, 0.1318, 0.0000, 0.6012]], device='cuda:0')
policy_old.forwarded at 1641198737963
give action 84============================
log_to_linear:  tensor([[[0.8250]]], device='cuda:0') tensor([[[1.6058]]], device='cuda:0')
log_to_linear action at 1641198737964
bwe changes from to:  [tensor([[[2592.9512]]]), tensor([[[4163.8652]]])]
step into gymStat at 1641198737964
send bwe to appRecv at 1641198737964
sent bwe to appRecv at 1641198737964
wait for recv string at 1641198737964
recved string at 1641198738162
1
wait for recv [self.estimator, stat] at 1641198738162
recved [self.estimator, stat] at 1641198738162
sorted packlist at 1641198738162
packetSeq:  4077
packetSeq:  4078
packetSeq:  4079
packetSeq:  4080
packetSeq:  4081
packetSeq:  4082
packetSeq:  4083
packetSeq:  4084
packetSeq:  4085
packetSeq:  4086
processed packlist at 1641198738162
receiving_rate:  283160.0
delay:  198.5
loss_ratio:  0.0
processed state0-2 at 1641198738162
avgFrameBetween:  6
psnrStat:  [[555306, 556462, 556990, 557883, 557527, 558527]]
delayStat:  [[362, 362, 363, 361, 360, 361]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557115.8333333334] [361.5] [0]
processed state3-5 at 1641198738162
liner_to_log:  tensor([[[1.6058]]]) tensor([[[0.8250]]])
linear_to_log at 1641198738163
listState:  [0.07079, 0.13233333333333333, 0.0, 0.5571158333333334, 0.3615, 0.0, tensor([[[0.8250]]])]
state_clone_detach at 1641198738163
reward: 0.057416389324142614
state tensor([0.0708, 0.1323, 0.0000, 0.8250], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198738163
state222:  tensor([[0.0708, 0.1323, 0.0000, 0.8250]], device='cuda:0')
policy_old.forwarded at 1641198738165
give action 85============================
log_to_linear:  tensor([[[0.1865]]], device='cuda:0') tensor([[[0.6048]]], device='cuda:0')
log_to_linear action at 1641198738166
bwe changes from to:  [tensor([[[4163.8652]]]), tensor([[[2518.1677]]])]
step into gymStat at 1641198738166
send bwe to appRecv at 1641198738166
sent bwe to appRecv at 1641198738166
wait for recv string at 1641198738166
recved string at 1641198738390
1
wait for recv [self.estimator, stat] at 1641198738390
recved [self.estimator, stat] at 1641198738390
sorted packlist at 1641198738390
packetSeq:  4087
packetSeq:  4088
packetSeq:  4089
packetSeq:  4090
packetSeq:  4091
packetSeq:  4092
packetSeq:  4093
packetSeq:  4094
packetSeq:  4095
packetSeq:  4096
packetSeq:  4097
processed packlist at 1641198738390
receiving_rate:  312080.0
delay:  198.4
loss_ratio:  0.0
processed state0-2 at 1641198738390
avgFrameBetween:  6
psnrStat:  [[557967, 558835, 559059, 557981, 557236, 556954, 555426]]
delayStat:  [[361, 360, 360, 361, 361, 359, 360]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557636.8571428572] [360.2857142857143] [0]
processed state3-5 at 1641198738390
liner_to_log:  tensor([[[0.6048]]]) tensor([[[0.1865]]])
linear_to_log at 1641198738391
listState:  [0.07802, 0.13226666666666667, 0.0, 0.5576368571428572, 0.36028571428571426, 0.0, tensor([[[0.1865]]])]
state_clone_detach at 1641198738391
reward: 0.0893099623931623
state tensor([0.0780, 0.1323, 0.0000, 0.1865], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198738391
state222:  tensor([[0.0780, 0.1323, 0.0000, 0.1865]], device='cuda:0')
policy_old.forwarded at 1641198738393
give action 86============================
log_to_linear:  tensor([[[0.3587]]], device='cuda:0') tensor([[[0.7953]]], device='cuda:0')
log_to_linear action at 1641198738394
bwe changes from to:  [tensor([[[2518.1677]]]), tensor([[[2002.7472]]])]
step into gymStat at 1641198738394
send bwe to appRecv at 1641198738394
sent bwe to appRecv at 1641198738394
wait for recv string at 1641198738394
recved string at 1641198738597
1
wait for recv [self.estimator, stat] at 1641198738597
recved [self.estimator, stat] at 1641198738597
sorted packlist at 1641198738597
packetSeq:  4098
packetSeq:  4099
packetSeq:  4100
packetSeq:  4101
packetSeq:  4102
packetSeq:  4103
packetSeq:  4104
packetSeq:  4105
packetSeq:  4106
packetSeq:  4107
processed packlist at 1641198738597
receiving_rate:  264800.0
delay:  196.7
loss_ratio:  0.0
processed state0-2 at 1641198738597
avgFrameBetween:  6
psnrStat:  [[555341, 555438, 555872, 554560, 554648, 553289]]
delayStat:  [[360, 359, 359, 359, 358, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554858.0] [358.8333333333333] [0]
processed state3-5 at 1641198738597
liner_to_log:  tensor([[[0.7953]]]) tensor([[[0.3587]]])
linear_to_log at 1641198738598
listState:  [0.0662, 0.13113333333333332, 0.0, 0.554858, 0.35883333333333334, 0.0, tensor([[[0.3587]]])]
state_clone_detach at 1641198738598
reward: 0.03999053471583508
state tensor([0.0662, 0.1311, 0.0000, 0.3587], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198738598
state222:  tensor([[0.0662, 0.1311, 0.0000, 0.3587]], device='cuda:0')
policy_old.forwarded at 1641198738600
give action 87============================
log_to_linear:  tensor([[[0.6691]]], device='cuda:0') tensor([[[1.2934]]], device='cuda:0')
log_to_linear action at 1641198738601
bwe changes from to:  [tensor([[[2002.7472]]]), tensor([[[2590.3499]]])]
step into gymStat at 1641198738601
send bwe to appRecv at 1641198738601
sent bwe to appRecv at 1641198738601
wait for recv string at 1641198738601
recved string at 1641198738827
1
wait for recv [self.estimator, stat] at 1641198738827
recved [self.estimator, stat] at 1641198738827
sorted packlist at 1641198738827
packetSeq:  4108
packetSeq:  4109
packetSeq:  4110
packetSeq:  4111
packetSeq:  4112
packetSeq:  4113
packetSeq:  4114
packetSeq:  4115
packetSeq:  4116
packetSeq:  4117
processed packlist at 1641198738827
receiving_rate:  366480.0
delay:  200.8
loss_ratio:  0.0
processed state0-2 at 1641198738827
avgFrameBetween:  6
psnrStat:  [[552953, 552124, 552079, 550961, 550712, 549666, 550549]]
delayStat:  [[359, 357, 357, 358, 357, 357, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551292.0] [357.57142857142856] [0]
processed state3-5 at 1641198738827
liner_to_log:  tensor([[[1.2934]]]) tensor([[[0.6691]]])
linear_to_log at 1641198738827
listState:  [0.09162, 0.13386666666666666, 0.0, 0.551292, 0.35757142857142854, 0.0, tensor([[[0.6691]]])]
state_clone_detach at 1641198738828
reward: 0.13993897780936043
state tensor([0.0916, 0.1339, 0.0000, 0.6691], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198738828
state222:  tensor([[0.0916, 0.1339, 0.0000, 0.6691]], device='cuda:0')
policy_old.forwarded at 1641198738830
give action 88============================
log_to_linear:  tensor([[[0.6568]]], device='cuda:0') tensor([[[1.2704]]], device='cuda:0')
log_to_linear action at 1641198738831
bwe changes from to:  [tensor([[[2590.3499]]]), tensor([[[3290.9006]]])]
step into gymStat at 1641198738831
send bwe to appRecv at 1641198738831
sent bwe to appRecv at 1641198738831
wait for recv string at 1641198738831
recved string at 1641198739057
1
wait for recv [self.estimator, stat] at 1641198739057
recved [self.estimator, stat] at 1641198739057
sorted packlist at 1641198739057
packetSeq:  4118
packetSeq:  4119
packetSeq:  300000
pc flushed at 1641198734154
Bwe Sent: 4 at 1641198734154
got request at 1641198734506
processed allFrame at 1641198734507
send 'asking for bwe' at 1641198734507
sent 'asking for bwe' at 1641198734507
send [estimator, stat] at 1641198734507
sent [estimator, stat] at 1641198734507
pc wait for bwe at 1641198734507
pc got bwe at 1641198734511
bandwidth:  300000
pc flushed at 1641198734511
Bwe Sent: 5 at 1641198734511
got request at 1641198734717
processed allFrame at 1641198734717
send 'asking for bwe' at 1641198734717
sent 'asking for bwe' at 1641198734717
send [estimator, stat] at 1641198734717
sent [estimator, stat] at 1641198734717
pc wait for bwe at 1641198734718
pc got bwe at 1641198734722
bandwidth:  300000
pc flushed at 1641198734722
Bwe Sent: 5 at 1641198734722
got request at 1641198734923
processed allFrame at 1641198734923
send 'asking for bwe' at 1641198734923
sent 'asking for bwe' at 1641198734923
send [estimator, stat] at 1641198734923
sent [estimator, stat] at 1641198734923
pc wait for bwe at 1641198734923
pc got bwe at 1641198734928
bandwidth:  300000
pc flushed at 1641198734928
Bwe Sent: 5 at 1641198734928
got request at 1641198735153
processed allFrame at 1641198735153
send 'asking for bwe' at 1641198735153
sent 'asking for bwe' at 1641198735153
send [estimator, stat] at 1641198735153
sent [estimator, stat] at 1641198735153
pc wait for bwe at 1641198735153
pc got bwe at 1641198735158
bandwidth:  300000
pc flushed at 1641198735158
Bwe Sent: 5 at 1641198735158
got request at 1641198735385
processed allFrame at 1641198735385
send 'asking for bwe' at 1641198735385
sent 'asking for bwe' at 1641198735385
send [estimator, stat] at 1641198735385
sent [estimator, stat] at 1641198735386
pc wait for bwe at 1641198735386
pc got bwe at 1641198735390
bandwidth:  300000
pc flushed at 1641198735390
Bwe Sent: 5 at 1641198735390
got request at 1641198735591
processed allFrame at 1641198735591
send 'asking for bwe' at 1641198735591
sent 'asking for bwe' at 1641198735591
send [estimator, stat] at 1641198735591
sent [estimator, stat] at 1641198735591
pc wait for bwe at 1641198735591
pc got bwe at 1641198735595
bandwidth:  300000
pc flushed at 1641198735595
Bwe Sent: 4 at 1641198735595
got request at 1641198735824
processed allFrame at 1641198735824
send 'asking for bwe' at 1641198735824
sent 'asking for bwe' at 1641198735824
send [estimator, stat] at 1641198735824
sent [estimator, stat] at 1641198735824
pc wait for bwe at 1641198735824
pc got bwe at 1641198735828
bandwidth:  300000
pc flushed at 1641198735828
Bwe Sent: 4 at 1641198735828
got request at 1641198736053
processed allFrame at 1641198736053
send 'asking for bwe' at 1641198736053
sent 'asking for bwe' at 1641198736053
send [estimator, stat] at 1641198736053
sent [estimator, stat] at 1641198736053
pc wait for bwe at 1641198736053
pc got bwe at 1641198736058
bandwidth:  300000
pc flushed at 1641198736058
Bwe Sent: 5 at 1641198736058
got request at 1641198736254
processed allFrame at 1641198736254
send 'asking for bwe' at 1641198736254
sent 'asking for bwe' at 1641198736254
send [estimator, stat] at 1641198736254
sent [estimator, stat] at 1641198736254
pc wait for bwe at 1641198736254
pc got bwe at 1641198736259
bandwidth:  300000
pc flushed at 1641198736259
Bwe Sent: 5 at 1641198736259
got request at 1641198736455
processed allFrame at 1641198736455
send 'asking for bwe' at 1641198736455
sent 'asking for bwe' at 1641198736455
send [estimator, stat] at 1641198736455
sent [estimator, stat] at 1641198736455
pc wait for bwe at 1641198736455
pc got bwe at 1641198736460
bandwidth:  300000
pc flushed at 1641198736460
Bwe Sent: 5 at 1641198736460
got request at 1641198736658
processed allFrame at 1641198736658
send 'asking for bwe' at 1641198736658
sent 'asking for bwe' at 1641198736658
send [estimator, stat] at 1641198736658
sent [estimator, stat] at 1641198736658
pc wait for bwe at 1641198736658
pc got bwe at 1641198736663
bandwidth:  300000
pc flushed at 1641198736663
Bwe Sent: 5 at 1641198736663
got request at 1641198736888
processed allFrame at 1641198736888
send 'asking for bwe' at 1641198736888
sent 'asking for bwe' at 1641198736888
send [estimator, stat] at 1641198736888
sent [estimator, stat] at 1641198736889
pc wait for bwe at 1641198736889
pc got bwe at 1641198736893
bandwidth:  300000
pc flushed at 1641198736893
Bwe Sent: 5 at 1641198736893
got request at 1641198737120
processed allFrame at 1641198737120
send 'asking for bwe' at 1641198737120
sent 'asking for bwe' at 1641198737120
send [estimator, stat] at 1641198737120
sent [estimator, stat] at 1641198737120
pc wait for bwe at 1641198737120
pc got bwe at 1641198737124
bandwidth:  300000
pc flushed at 1641198737124
Bwe Sent: 4 at 1641198737124
got request at 1641198737322
processed allFrame at 1641198737322
send 'asking for bwe' at 1641198737322
sent 'asking for bwe' at 1641198737322
send [estimator, stat] at 1641198737322
sent [estimator, stat] at 1641198737323
pc wait for bwe at 1641198737323
pc got bwe at 1641198737327
bandwidth:  300000
pc flushed at 1641198737327
Bwe Sent: 5 at 1641198737327
got request at 1641198737525
processed allFrame at 1641198737525
send 'asking for bwe' at 1641198737525
sent 'asking for bwe' at 1641198737525
send [estimator, stat] at 1641198737525
sent [estimator, stat] at 1641198737525
pc wait for bwe at 1641198737525
pc got bwe at 1641198737530
bandwidth:  300000
pc flushed at 1641198737530
Bwe Sent: 5 at 1641198737530
got request at 1641198737729
processed allFrame at 1641198737729
send 'asking for bwe' at 1641198737729
sent 'asking for bwe' at 1641198737729
send [estimator, stat] at 1641198737729
sent [estimator, stat] at 1641198737729
pc wait for bwe at 1641198737729
pc got bwe at 1641198737733
bandwidth:  300000
pc flushed at 1641198737734
Bwe Sent: 5 at 1641198737734
got request at 1641198737960
processed allFrame at 1641198737960
send 'asking for bwe' at 1641198737960
sent 'asking for bwe' at 1641198737960
send [estimator, stat] at 1641198737960
sent [estimator, stat] at 1641198737960
pc wait for bwe at 1641198737960
pc got bwe at 1641198737964
bandwidth:  300000
pc flushed at 1641198737964
Bwe Sent: 4 at 1641198737964
got request at 1641198738162
processed allFrame at 1641198738162
send 'asking for bwe' at 1641198738162
sent 'asking for bwe' at 1641198738162
send [estimator, stat] at 1641198738162
sent [estimator, stat] at 1641198738162
pc wait for bwe at 1641198738162
pc got bwe at 1641198738166
bandwidth:  300000
pc flushed at 1641198738166
Bwe Sent: 4 at 1641198738166
got request at 1641198738390
processed allFrame at 1641198738390
send 'asking for bwe' at 1641198738390
sent 'asking for bwe' at 1641198738390
send [estimator, stat] at 1641198738390
sent [estimator, stat] at 1641198738390
pc wait for bwe at 1641198738390
pc got bwe at 1641198738394
bandwidth:  300000
pc flushed at 1641198738394
Bwe Sent: 4 at 1641198738394
got request at 1641198738597
processed allFrame at 1641198738597
send 'asking for bwe' at 1641198738597
sent 'asking for bwe' at 1641198738597
send [estimator, stat] at 1641198738597
sent [estimator, stat] at 1641198738597
pc wait for bwe at 1641198738597
pc got bwe at 1641198738601
bandwidth:  300000
pc flushed at 1641198738601
Bwe Sent: 4 at 1641198738601
got request at 1641198738826
processed allFrame at 1641198738827
send 'asking for bwe' at 1641198738827
sent 'asking for bwe' at 1641198738827
send [estimator, stat] at 1641198738827
sent [estimator, stat] at 1641198738827
pc wait for bwe at 1641198738827
pc got bwe at 1641198738831
bandwidth:  300000
pc flushed at 1641198738831
Bwe Sent: 5 at 1641198738831
got request at 1641198739057
processed allFrame at 1641198739057
send 'asking for bwe' at 1641198739057
sent 'asking for bwe' at 1641198739057
send [estimator, stat] at 1641198739057
sent [estimator, stat] at 1641198739057
pc wait for bwe at 1641198739057
pc got bwe at 1641198739061
bandwidth:  300000
pc flushed at 1641198739061
Bwe Sent: 4 at 1641198739061
got request at 1641198739261
processed allFrame at 1641198739261
send 'asking for bwe' at 1641198739261
sent 'asking for bwe' at 1641198739261
send [estimator, stat] at 1641198739261
4120
packetSeq:  4121
packetSeq:  4122
packetSeq:  4123
packetSeq:  4124
packetSeq:  4125
packetSeq:  4126
packetSeq:  4127
processed packlist at 1641198739057
receiving_rate:  294040.0
delay:  197.4
loss_ratio:  0.0
processed state0-2 at 1641198739057
avgFrameBetween:  6
psnrStat:  [[550836, 551766, 552035, 552167, 551597, 552219, 548415]]
delayStat:  [[356, 356, 357, 356, 356, 355, 355]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551290.7142857143] [355.85714285714283] [0]
processed state3-5 at 1641198739057
liner_to_log:  tensor([[[1.2704]]]) tensor([[[0.6568]]])
linear_to_log at 1641198739057
listState:  [0.07351, 0.1316, 0.0, 0.5512907142857143, 0.3558571428571428, 0.0, tensor([[[0.6568]]])]
state_clone_detach at 1641198739058
reward: 0.07173711342138384
state tensor([0.0735, 0.1316, 0.0000, 0.6568], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198739058
state222:  tensor([[0.0735, 0.1316, 0.0000, 0.6568]], device='cuda:0')
policy_old.forwarded at 1641198739060
give action 89============================
log_to_linear:  tensor([[[0.3339]]], device='cuda:0') tensor([[[0.7637]]], device='cuda:0')
log_to_linear action at 1641198739061
bwe changes from to:  [tensor([[[3290.9006]]]), tensor([[[2513.1365]]])]
step into gymStat at 1641198739061
send bwe to appRecv at 1641198739061
sent bwe to appRecv at 1641198739061
wait for recv string at 1641198739061
recved string at 1641198739261
1
wait for recv [self.estimator, stat] at 1641198739261
recved [self.estimator, stat] at 1641198739261
sorted packlist at 1641198739261
packetSeq:  4128
packetSeq:  4129
packetSeq:  4130
packetSeq:  4131
packetSeq:  4132
packetSeq:  4133
packetSeq:  4134
packetSeq:  4135
packetSeq:  4136
packetSeq:  4137
processed packlist at 1641198739261
receiving_rate:  284320.0
delay:  197.3
loss_ratio:  0.0
processed state0-2 at 1641198739262
avgFrameBetween:  6
psnrStat:  [[546989, 548411, 546640, 548414, 548513, 548782]]
delayStat:  [[355, 355, 355, 354, 354, 354]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547958.1666666666] [354.5] [0]
processed state3-5 at 1641198739262
liner_to_log:  tensor([[[0.7637]]]) tensor([[[0.3339]]])
linear_to_log at 1641198739262
listState:  [0.07108, 0.13153333333333334, 0.0, 0.5479581666666666, 0.3545, 0.0, tensor([[[0.3339]]])]
state_clone_detach at 1641198739262
reward: 0.06112039553308041
state tensor([0.0711, 0.1315, 0.0000, 0.3339], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198739263
state222:  tensor([[0.0711, 0.1315, 0.0000, 0.3339]], device='cuda:0')
policy_old.forwarded at 1641198739264
give action 90============================
log_to_linear:  tensor([[[0.6868]]], device='cuda:0') tensor([[[1.3269]]], device='cuda:0')
log_to_linear action at 1641198739265
bwe changes from to:  [tensor([[[2513.1365]]]), tensor([[[3334.6711]]])]
step into gymStat at 1641198739266
send bwe to appRecv at 1641198739266
sent bwe to appRecv at 1641198739266
wait for recv string at 1641198739266
recved string at 1641198739465
1
wait for recv [self.estimator, stat] at 1641198739465
recved [self.estimator, stat] at 1641198739465
sorted packlist at 1641198739465
packetSeq:  4138
packetSeq:  4139
packetSeq:  4140
packetSeq:  4141
packetSeq:  4142
packetSeq:  4143
packetSeq:  4144
packetSeq:  4145
processed packlist at 1641198739465
receiving_rate:  248040.0
delay:  197.25
loss_ratio:  0.0
processed state0-2 at 1641198739465
avgFrameBetween:  6
psnrStat:  [[547993, 548317, 548348, 548290, 547382, 548566]]
delayStat:  [[355, 354, 353, 354, 353, 353]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548149.3333333334] [353.6666666666667] [0]
processed state3-5 at 1641198739465
liner_to_log:  tensor([[[1.3269]]]) tensor([[[0.6868]]])
linear_to_log at 1641198739465
listState:  [0.06201, 0.1315, 0.0, 0.5481493333333334, 0.3536666666666667, 0.0, tensor([[[0.6868]]])]
state_clone_detach at 1641198739466
reward: 0.0190316863331792
state tensor([0.0620, 0.1315, 0.0000, 0.6868], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198739466
state222:  tensor([[0.0620, 0.1315, 0.0000, 0.6868]], device='cuda:0')
policy_old.forwarded at 1641198739468
give action 91============================
log_to_linear:  tensor([[[0.4679]]], device='cuda:0') tensor([[[0.9501]]], device='cuda:0')
log_to_linear action at 1641198739469
bwe changes from to:  [tensor([[[3334.6711]]]), tensor([[[3168.3359]]])]
step into gymStat at 1641198739469
send bwe to appRecv at 1641198739469
sent bwe to appRecv at 1641198739469
wait for recv string at 1641198739469
recved string at 1641198739692
1
wait for recv [self.estimator, stat] at 1641198739692
recved [self.estimator, stat] at 1641198739692
sorted packlist at 1641198739692
packetSeq:  4146
packetSeq:  4147
packetSeq:  4148
packetSeq:  4149
packetSeq:  4150
packetSeq:  4151
packetSeq:  4152
packetSeq:  4153
packetSeq:  4154
processed packlist at 1641198739692
receiving_rate:  252760.0
delay:  197.25
loss_ratio:  0.0
processed state0-2 at 1641198739692
avgFrameBetween:  6
psnrStat:  [[548305, 548414, 546538, 548303, 547572, 547536, 547282]]
delayStat:  [[354, 354, 352, 352, 352, 352, 352]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547707.1428571428] [352.57142857142856] [0]
processed state3-5 at 1641198739692
liner_to_log:  tensor([[[0.9501]]]) tensor([[[0.4679]]])
linear_to_log at 1641198739693
listState:  [0.06319, 0.1315, 0.0, 0.5477071428571428, 0.35257142857142854, 0.0, tensor([[[0.4679]]])]
state_clone_detach at 1641198739693
reward: 0.024691200716634165
state tensor([0.0632, 0.1315, 0.0000, 0.4679], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198739693
state222:  tensor([[0.0632, 0.1315, 0.0000, 0.4679]], device='cuda:0')
policy_old.forwarded at 1641198739695
give action 92============================
log_to_linear:  tensor([[[0.4214]]], device='cuda:0') tensor([[[0.8813]]], device='cuda:0')
log_to_linear action at 1641198739696
bwe changes from to:  [tensor([[[3168.3359]]]), tensor([[[2792.2427]]])]
step into gymStat at 1641198739696
send bwe to appRecv at 1641198739696
sent bwe to appRecv at 1641198739696
wait for recv string at 1641198739696
recved string at 1641198739898
1
wait for recv [self.estimator, stat] at 1641198739898
recved [self.estimator, stat] at 1641198739898
sorted packlist at 1641198739898
packetSeq:  4155
packetSeq:  4156
packetSeq:  4157
packetSeq:  4158
packetSeq:  4159
packetSeq:  4160
packetSeq:  4161
packetSeq:  4162
processed packlist at 1641198739898
receiving_rate:  255800.0
delay:  198.125
loss_ratio:  0.0
processed state0-2 at 1641198739898
avgFrameBetween:  6
psnrStat:  [[546449, 546529, 547455, 547733, 548897, 550591]]
delayStat:  [[351, 373, 374, 373, 373, 372]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547942.3333333334] [369.3333333333333] [0]
processed state3-5 at 1641198739898
liner_to_log:  tensor([[[0.8813]]]) tensor([[[0.4214]]])
linear_to_log at 1641198739899
listState:  [0.06395, 0.13208333333333333, 0.0, 0.5479423333333334, 0.3693333333333333, 0.0, tensor([[[0.4214]]])]
state_clone_detach at 1641198739899
reward: 0.026558249824171043
state tensor([0.0640, 0.1321, 0.0000, 0.4214], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198739899
state222:  tensor([[0.0640, 0.1321, 0.0000, 0.4214]], device='cuda:0')
policy_old.forwarded at 1641198739901
give action 93============================
log_to_linear:  tensor([[[0.3844]]], device='cuda:0') tensor([[[0.8296]]], device='cuda:0')
log_to_linear action at 1641198739902
bwe changes from to:  [tensor([[[2792.2427]]]), tensor([[[2316.5615]]])]
step into gymStat at 1641198739902
send bwe to appRecv at 1641198739902
sent bwe to appRecv at 1641198739902
wait for recv string at 1641198739902
recved string at 1641198740121
1
wait for recv [self.estimator, stat] at 1641198740121
recved [self.estimator, stat] at 1641198740121
sorted packlist at 1641198740121
packetSeq:  4163
packetSeq:  4164
packetSeq:  4165
packetSeq:  4166
packetSeq:  4167
packetSeq:  4168
packetSeq:  4169
packetSeq:  4170
packetSeq:  4171
packetSeq:  4172
processed packlist at 1641198740121
receiving_rate:  301880.0
delay:  197.8
loss_ratio:  0.0
processed state0-2 at 1641198740121
avgFrameBetween:  6
psnrStat:  [[550832, 550288, 550071, 551244, 552603, 551350, 553981]]
delayStat:  [[372, 372, 373, 372, 372, 373, 371]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551481.2857142857] [372.14285714285717] [0]
processed state3-5 at 1641198740121
liner_to_log:  tensor([[[0.8296]]]) tensor([[[0.3844]]])
linear_to_log at 1641198740121
listState:  [0.07547, 0.1318666666666667, 0.0, 0.5514812857142857, 0.37214285714285716, 0.0, tensor([[[0.3844]]])]
state_clone_detach at 1641198740122
reward: 0.07952182808655167
state tensor([0.0755, 0.1319, 0.0000, 0.3844], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198740122
state222:  tensor([[0.0755, 0.1319, 0.0000, 0.3844]], device='cuda:0')
policy_old.forwarded at 1641198740124
give action 94============================
log_to_linear:  tensor([[[0.2629]]], device='cuda:0') tensor([[[0.6805]]], device='cuda:0')
log_to_linear action at 1641198740125
bwe changes from to:  [tensor([[[2316.5615]]]), tensor([[[1576.3286]]])]
step into gymStat at 1641198740125
send bwe to appRecv at 1641198740125
sent bwe to appRecv at 1641198740125
wait for recv string at 1641198740125
recved string at 1641198740326
1
wait for recv [self.estimator, stat] at 1641198740326
recved [self.estimator, stat] at 1641198740326
sorted packlist at 1641198740326
packetSeq:  4173
packetSeq:  4174
packetSeq:  4175
packetSeq:  4176
packetSeq:  4177
packetSeq:  4178
packetSeq:  4179
packetSeq:  4180
packetSeq:  4181
packetSeq:  4182
packetSeq:  4183
processed packlist at 1641198740326
receiving_rate:  292240.0
delay:  196.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198740326
avgFrameBetween:  6
psnrStat:  [[555100, 555888, 555219, 555679, 555274, 555446]]
delayStat:  [[371, 372, 372, 372, 372, 370]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555434.3333333334] [371.5] [0]
processed state3-5 at 1641198740326
liner_to_log:  tensor([[[0.6805]]]) tensor([[[0.2629]]])
linear_to_log at 1641198740327
listState:  [0.07306, 0.1312121212121212, 0.0, 0.5554343333333334, 0.3715, 0.0, tensor([[[0.2629]]])]
state_clone_detach at 1641198740327
reward: 0.07091231036153328
state tensor([0.0731, 0.1312, 0.0000, 0.2629], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198740328
state222:  tensor([[0.0731, 0.1312, 0.0000, 0.2629]], device='cuda:0')
policy_old.forwarded at 1641198740329
give action 95============================
log_to_linear:  tensor([[[0.7094]]], device='cuda:0') tensor([[[1.3705]]], device='cuda:0')
log_to_linear action at 1641198740330
bwe changes from to:  [tensor([[[1576.3286]]]), tensor([[[2160.3093]]])]
step into gymStat at 1641198740330
send bwe to appRecv at 1641198740330
sent bwe to appRecv at 1641198740331
wait for recv string at 1641198740331
recved string at 1641198740556
1
wait for recv [self.estimator, stat] at 1641198740556
recved [self.estimator, stat] at 1641198740556
sorted packlist at 1641198740556
packetSeq:  4184
packetSeq:  4185
packetSeq:  4186
packetSeq:  4187
packetSeq:  4188
packetSeq:  4189
packetSeq:  4190
packetSeq:  4191
packetSeq:  4192
processed packlist at 1641198740556
receiving_rate:  276640.0
delay:  197.625
loss_ratio:  0.0
processed state0-2 at 1641198740556
avgFrameBetween:  6
psnrStat:  [[553620, 552805, 552641, 552668, 552465, 550203, 549974]]
delayStat:  [[370, 371, 370, 370, 370, 369, 369]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552053.7142857143] [369.85714285714283] [0]
processed state3-5 at 1641198740556
liner_to_log:  tensor([[[1.3705]]]) tensor([[[0.7094]]])
linear_to_log at 1641198740557
listState:  [0.06916, 0.13175, 0.0, 0.5520537142857144, 0.36985714285714283, 0.0, tensor([[[0.7094]]])]
state_clone_detach at 1641198740557
reward: 0.051783771178424864
state tensor([0.0692, 0.1318, 0.0000, 0.7094], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198740557
state222:  tensor([[0.0692, 0.1318, 0.0000, 0.7094]], device='cuda:0')
policy_old.forwarded at 1641198740559
give action 96============================
log_to_linear:  tensor([[[0.5189]]], device='cuda:0') tensor([[[1.0303]]], device='cuda:0')
log_to_linear action at 1641198740560
bwe changes from to:  [tensor([[[2160.3093]]]), tensor([[[2225.7158]]])]
step into gymStat at 1641198740560
send bwe to appRecv at 1641198740560
sent bwe to appRecv at 1641198740560
wait for recv string at 1641198740560
recved string at 1641198740760
1
wait for recv [self.estimator, stat] at 1641198740760
recved [self.estimator, stat] at 1641198740760
sorted packlist at 1641198740760
packetSeq:  4193
packetSeq:  4194
packetSeq:  4195
packetSeq:  4196
packetSeq:  4197
packetSeq:  4198
packetSeq:  4199
packetSeq:  4200
processed packlist at 1641198740760
receiving_rate:  278120.0
delay:  198.25
loss_ratio:  0.0
processed state0-2 at 1641198740760
avgFrameBetween:  6
psnrStat:  [[550607, 550359, 549354, 548977, 549092, 548883]]
delayStat:  [[369, 369, 369, 369, 368, 368]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549545.3333333334] [368.6666666666667] [0]
processed state3-5 at 1641198740760
liner_to_log:  tensor([[[1.0303]]]) tensor([[[0.5189]]])
linear_to_log at 1641198740761
listState:  [0.06953, 0.13216666666666665, 0.0, 0.5495453333333333, 0.3686666666666667, 0.0, tensor([[[0.5189]]])]
state_clone_detach at 1641198740761
reward: 0.05221756764925223
state tensor([0.0695, 0.1322, 0.0000, 0.5189], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198740761
state222:  tensor([[0.0695, 0.1322, 0.0000, 0.5189]], device='cuda:0')
policy_old.forwarded at 1641198740763
give action 97============================
log_to_linear:  tensor([[[0.4757]]], device='cuda:0') tensor([[[0.9620]]], device='cuda:0')
log_to_linear action at 1641198740764
bwe changes from to:  [tensor([[[2225.7158]]]), tensor([[[2141.1060]]])]
step into gymStat at 1641198740764
send bwe to appRecv at 1641198740764
sent bwe to appRecv at 1641198740764
wait for recv string at 1641198740764
recved string at 1641198740966
1
wait for recv [self.estimator, stat] at 1641198740966
recved [self.estimator, stat] at 1641198740966
sorted packlist at 1641198740966
packetSeq:  4201
packetSeq:  4202
packetSeq:  4203
packetSeq:  4204
packetSeq:  4205
packetSeq:  4206
packetSeq:  4207
packetSeq:  4208
packetSeq:  4209
processed packlist at 1641198740966
receiving_rate:  261440.0
delay:  197.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198740966
avgFrameBetween:  6
psnrStat:  [[549762, 548483, 548540, 548525, 549135, 549078]]
delayStat:  [[368, 368, 368, 368, 368, 367]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548920.5] [367.8333333333333] [0]
processed state3-5 at 1641198740966
liner_to_log:  tensor([[[0.9620]]]) tensor([[[0.4757]]])
linear_to_log at 1641198740966
listState:  [0.06536, 0.13148148148148148, 0.0, 0.5489205, 0.3678333333333333, 0.0, tensor([[[0.4757]]])]
state_clone_detach at 1641198740966
reward: 0.035017170713290524
state tensor([0.0654, 0.1315, 0.0000, 0.4757], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198740967
state222:  tensor([[0.0654, 0.1315, 0.0000, 0.4757]], device='cuda:0')
policy_old.forwarded at 1641198740969
give action 98============================
log_to_linear:  tensor([[[0.6352]]], device='cuda:0') tensor([[[1.2307]]], device='cuda:0')
log_to_linear action at 1641198740970
bwe changes from to:  [tensor([[[2141.1060]]]), tensor([[[2635.0842]]])]
step into gymStat at 1641198740970
send bwe to appRecv at 1641198740970
sent bwe to appRecv at 1641198740970
wait for recv string at 1641198740970
recved string at 1641198741196
1
wait for recv [self.estimator, stat] at 1641198741196
recved [self.estimator, stat] at 1641198741196
sorted packlist at 1641198741196
packetSeq:  4210
packetSeq:  4211
packetSeq:  4212
packetSeq:  4213
packetSeq:  4214
packetSeq:  4215
packetSeq:  4216
packetSeq:  4217
processed packlist at 1641198741196
receiving_rate:  257480.00000000003
delay:  199.28571428571428
loss_ratio:  0.0
processed state0-2 at 1641198741196
avgFrameBetween:  6
psnrStat:  [[549174, 548936, 548195, 549276, 547955, 548664, 548955]]
delayStat:  [[366, 367, 367, 368, 367, 367, 367]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548736.4285714285] [367.0] [0]
processed state3-5 at 1641198741196
liner_to_log:  tensor([[[1.2307]]]) tensor([[[0.6352]]])
linear_to_log at 1641198741197
listState:  [0.06437000000000001, 0.13285714285714284, 0.0, 0.5487364285714286, 0.367, 0.0, tensor([[[0.6352]]])]
state_clone_detach at 1641198741197
reward: 0.026226401474662098
state tensor([0.0644, 0.1329, 0.0000, 0.6352], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198741198
state222:  tensor([[0.0644, 0.1329, 0.0000, 0.6352]], device='cuda:0')
policy_old.forwarded at 1641198741199
give action 99============================
log_to_linear:  tensor([[[0.5216]]], device='cuda:0') tensor([[[1.0346]]], device='cuda:0')
log_to_linear action at 1641198741200
bwe changes from to:  [tensor([[[2635.0842]]]), tensor([[[2726.2429]]])]
step into gymStat at 1641198741200
send bwe to appRecv at 1641198741200
sent bwe to appRecv at 1641198741201
wait for recv string at 1641198741201
recved string at 1641198741425
1
wait for recv [self.estimator, stat] at 1641198741425
recved [self.estimator, stat] at 1641198741425
sorted packlist at 1641198741425
packetSeq:  4218
packetSeq:  4219
packetSeq:  4220
packetSeq:  4221
packetSeq:  4222
packetSeq:  4223
packetSeq:  4224
packetSeq:  4225
packetSeq:  4226
packetSeq:  4227
processed packlist at 1641198741425
receiving_rate:  272640.0
delay:  197.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198741425
avgFrameBetween:  6
psnrStat:  [[547545, 549125, 548312, 549190, 549553, 549941, 551392]]
delayStat:  [[366, 366, 367, 366, 366, 367, 366]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549294.0] [366.2857142857143] [0]
processed state3-5 at 1641198741425
liner_to_log:  tensor([[[1.0346]]]) tensor([[[0.5216]]])
linear_to_log at 1641198741425
listState:  [0.06816, 0.13170370370370368, 0.0, 0.549294, 0.36628571428571427, 0.0, tensor([[[0.5216]]])]
state_clone_detach at 1641198741425
reward: 0.04734807249103795
state tensor([0.0682, 0.1317, 0.0000, 0.5216], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198741426
state222:  tensor([[0.0682, 0.1317, 0.0000, 0.5216]], device='cuda:0')
policy_old.forwarded at 1641198741428
give action 100============================
log_to_linear:  tensor([[[0.4212]]], device='cuda:0') tensor([[[0.8810]]], device='cuda:0')
log_to_linear action at 1641198741429
bwe changes from to:  [tensor([[[2726.2429]]]), tensor([[[2401.8269]]])]
step into gymStat at 1641198741429
send bwe to appRecv at 1641198741429
sent bwe to appRecv at 1641198741429
wait for recv string at 1641198741429
recved string at 1641198741629
1
wait for recv [self.estimator, stat] at 1641198741629
recved [self.estimator, stat] at 1641198741629
sorted packlist at 1641198741629
packetSeq:  4228
packetSeq:  4229
packetSeq:  4230
packetSeq:  4231
packetSeq:  4232
packetSeq:  4233
packetSeq:  4234
packetSeq:  4235
packetSeq:  4236
packetSeq:  4237
packetSeq:  4238
packetSeq:  4239
packetSeq:  4240
processed packlist at 1641198741629
receiving_rate:  308120.0
delay:  196.30769230769232
loss_ratio:  0.0
processed state0-2 at 1641198741629
avgFrameBetween:  6
psnrStat:  [[551582, 552464, 552847, 553517, 553154, 554320]]
delayStat:  [[366, 366, 365, 365, 366, 365]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552980.6666666666] [365.5] [0]
processed state3-5 at 1641198741629
liner_to_log:  tensor([[[0.8810]]]) tensor([[[0.4212]]])
linear_to_log at 1641198741629
listState:  [0.07703, 0.13087179487179487, 0.0, 0.5529806666666667, 0.3655, 0.0, tensor([[[0.4212]]])]
state_clone_detach at 1641198741630
reward: 0.08925246048167101
state tensor([0.0770, 0.1309, 0.0000, 0.4212], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198741630
state222:  tensor([[0.0770, 0.1309, 0.0000, 0.4212]], device='cuda:0')
policy_old.forwarded at 1641198741632
give action 101============================
log_to_linear:  tensor([[[0.3302]]], device='cuda:0') tensor([[[0.7591]]], device='cuda:0')
log_to_linear action at 1641198741633
bwe changes from to:  [tensor([[[2401.8269]]]), tensor([[[1823.2135]]])]
step into gymStat at 1641198741633
send bwe to appRecv at 1641198741633
sent bwe to appRecv at 1641198741633
wait for recv string at 1641198741633
recved string at 1641198741860
1
wait for recv [self.estimator, stat] at 1641198741860
recved [self.estimator, stat] at 1641198741860
sorted packlist at 1641198741860
packetSeq:  4241
packetSeq:  4242
packetSeq:  4243
packetSeq:  4244
packetSeq:  4245
packetSeq:  4246
packetSeq:  4247
packetSeq:  4248
packetSeq:  4249
packetSeq:  4250
processed packlist at 1641198741860
receiving_rate:  288320.0
delay:  197.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198741860
avgFrameBetween:  6
psnrStat:  [[555069, 555556, 554595, 554399, 554711, 553047, 552404]]
delayStat:  [[365, 365, 364, 364, 364, 364, 364]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554254.4285714285] [364.2857142857143] [0]
processed state3-5 at 1641198741860
liner_to_log:  tensor([[[0.7591]]]) tensor([[[0.3302]]])
linear_to_log at 1641198741860
listState:  [0.07208, 0.13192592592592592, 0.0, 0.5542544285714285, 0.36428571428571427, 0.0, tensor([[[0.3302]]])]
state_clone_detach at 1641198741861
reward: 0.06441757883291188
state tensor([0.0721, 0.1319, 0.0000, 0.3302], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198741861
state222:  tensor([[0.0721, 0.1319, 0.0000, 0.3302]], device='cuda:0')
policy_old.forwarded at 1641198741863
give action 102============================
log_to_linear:  tensor([[[0.4506]]], device='cuda:0') tensor([[[0.9240]]], device='cuda:0')
log_to_linear action at 1641198741864
bwe changes from to:  [tensor([[[1823.2135]]]), tensor([[[1684.6953]]])]
step into gymStat at 1641198741864
send bwe to appRecv at 1641198741864
sent bwe to appRecv at 1641198741864
wait for recv string at 1641198741864
recved string at 1641198742065
1
wait for recv [self.estimator, stat] at 1641198742065
recved [self.estimator, stat] at 1641198742066
sorted packlist at 1641198742066
packetSeq:  4251
packetSeq:  4252
packetSeq:  4253
packetSeq:  4254
packetSeq:  4255
packetSeq:  4256
packetSeq:  4257
packetSeq:  4258
packetSeq:  4259
packetSeq:  4260
packetSeq:  4261
processed packlist at 1641198742066
receiving_rate:  275960.0
delay:  196.36363636363637
loss_ratio:  0.0
processed state0-2 at 1641198742066
avgFrameBetween:  6
psnrStat:  [[551129, 552011, 550618, 550190, 549582, 548847]]
delayStat:  [[364, 364, 363, 363, 363, 363]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550396.1666666666] [363.3333333333333] [0]
processed state3-5 at 1641198742066
liner_to_log:  tensor([[[0.9240]]]) tensor([[[0.4506]]])
linear_to_log at 1641198742066
listState:  [0.06899, 0.13090909090909092, 0.0, 0.5503961666666666, 0.36333333333333334, 0.0, tensor([[[0.4506]]])]
state_clone_detach at 1641198742066
reward: 0.05353127508661021
state tensor([0.0690, 0.1309, 0.0000, 0.4506], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198742067
state222:  tensor([[0.0690, 0.1309, 0.0000, 0.4506]], device='cuda:0')
policy_old.forwarded at 1641198742069
give action 103============================
log_to_linear:  tensor([[[0.0663]]], device='cuda:0') tensor([[[0.5203]]], device='cuda:0')
log_to_linear action at 1641198742070
bwe changes from to:  [tensor([[[1684.6953]]]), tensor([[[876.6274]]])]
step into gymStat at 1641198742070
send bwe to appRecv at 1641198742070
sent bwe to appRecv at 1641198742070
wait for recv string at 1641198742070
recved string at 1641198742290
1
wait for recv [self.estimator, stat] at 1641198742290
recved [self.estimator, stat] at 1641198742290
sorted packlist at 1641198742290
packetSeq:  4262
packetSeq:  4263
packetSeq:  4264
packetSeq:  4265
packetSeq:  4266
packetSeq:  4267
packetSeq:  4268
packetSeq:  4269
processed packlist at 1641198742290
receiving_rate:  276360.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198742291
avgFrameBetween:  6
psnrStat:  [[547252, 548389, 549410, 548075, 548016, 547904, 547989]]
delayStat:  [[363, 363, 363, 363, 362, 362, 362]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548147.8571428572] [362.57142857142856] [0]
processed state3-5 at 1641198742291
liner_to_log:  tensor([[[0.5203]]]) tensor([[[0.0663]]])
linear_to_log at 1641198742291
listState:  [0.06909, 0.132, 0.0, 0.5481478571428572, 0.36257142857142854, 0.0, tensor([[[0.0663]]])]
state_clone_detach at 1641198742291
reward: 0.050714682851815907
state tensor([0.0691, 0.1320, 0.0000, 0.0663], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198742292
state222:  tensor([[0.0691, 0.1320, 0.0000, 0.0663]], device='cuda:0')
policy_old.forwarded at 1641198742294
give action 104============================
log_to_linear:  tensor([[[0.2699]]], device='cuda:0') tensor([[[0.6881]]], device='cuda:0')
log_to_linear action at 1641198742294
bwe changes from to:  [tensor([[[876.6274]]]), tensor([[[603.2453]]])]
step into gymStat at 1641198742295
send bwe to appRecv at 1641198742295
sent bwe to appRecv at 1641198742295
wait for recv string at 1641198742295
recved string at 1641198742558
1
wait for recv [self.estimator, stat] at 1641198742558
recved [self.estimator, stat] at 1641198742558
sorted packlist at 1641198742558
packetSeq:  4270
packetSeq:  4271
packetSeq:  4272
packetSeq:  4273
packetSeq:  4274
packetSeq:  4275
packetSeq:  4276
packetSeq:  4277
packetSeq:  4278
packetSeq:  4279
processed packlist at 1641198742558
receiving_rate:  287560.0
delay:  197.4
loss_ratio:  0.0
processed state0-2 at 1641198742558
avgFrameBetween:  6
psnrStat:  [[548334, 546349, 545891, 547626]]
delayStat:  [[362, 362, 362, 362]]
skipStat:  [[1, 1, 1, 1]]
skipCount:  0
state:  [547050.0] [362.0] [2]
processed state3-5 at 1641198742558
liner_to_log:  tensor([[[0.6881]]]) tensor([[[0.2699]]])
linear_to_log at 1641198742559
listState:  [0.07189, 0.1316, 0.0, 0.54705, 0.362, 0.0625, tensor([[[0.2699]]])]
state_clone_detach at 1641198742559
reward: 0.06454767828997021
state tensor([0.0719, 0.1316, 0.0000, 0.2699], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198742560
state222:  tensor([[0.0719, 0.1316, 0.0000, 0.2699]], device='cuda:0')
policy_old.forwarded at 1641198742561
give action 105============================
log_to_linear:  tensor([[[0.1519]]], device='cuda:0') tensor([[[0.5757]]], device='cuda:0')
log_to_linear action at 1641198742562
bwe changes from to:  [tensor([[[603.2453]]]), tensor([[[347.2591]]])]
step into gymStat at 1641198742563
send bwe to appRecv at 1641198742563
sent bwe to appRecv at 1641198742563
wait for recv string at 1641198742563
recved string at 1641198742788
1
wait for recv [self.estimator, stat] at 1641198742788
recved [self.estimator, stat] at 1641198742788
sorted packlist at 1641198742788
packetSeq:  4280
packetSeq:  4281
packetSeq:  4282
packetSeq:  4283
packetSeq:  4284
packetSeq:  4285
packetSeq:  4286
packetSeq:  4287
packetSeq:  4288
packetSeq:  4289
packetSeq:  4290
packetSeq:  4291
processed packlist at 1641198742788
receiving_rate:  272080.0
delay:  195.25
loss_ratio:  0.0
processed state0-2 at 1641198742788
avgFrameBetween:  6
psnrStat:  [[547621, 547644, 549040, 549571, 550658, 550273, 550833, 551460, 551521, 552671, 552946]]
delayStat:  [[362, 362, 361, 362, 361, 364, 362, 362, 363, 362, 363]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  2
state:  [550997.0] [362.22222222222223] [0]
processed state3-5 at 1641198742788
liner_to_log:  tensor([[[0.5757]]]) tensor([[[0.1519]]])
linear_to_log at 1641198742789
listState:  [0.06802, 0.13016666666666668, 0.0, 0.550997, 0.3622222222222222, 0.0, tensor([[[0.1519]]])]
state_clone_detach at 1641198742789
reward: 0.051315948989658355
state tensor([0.0680, 0.1302, 0.0000, 0.1519], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198742790
state222:  tensor([[0.0680, 0.1302, 0.0000, 0.1519]], device='cuda:0')
policy_old.forwarded at 1641198742791
give action 106============================
log_to_linear:  tensor([[[0.3036]]], device='cuda:0') tensor([[[0.7268]]], device='cuda:0')
log_to_linear action at 1641198742792
bwe changes from to:  [tensor([[[347.2591]]]), tensor([[[252.3880]]])]
step into gymStat at 1641198742792
send bwe to appRecv at 1641198742792
sent bwe to appRecv at 1641198742793
wait for recv string at 1641198742793
recved string at 1641198742990
1
wait for recv [self.estimator, stat] at 1641198742990
recved [self.estimator, stat] at 1641198742990
sorted packlist at 1641198742990
packetSeq:  4292
packetSeq:  4293
packetSeq:  4294
packetSeq:  4295
packetSeq:  4296
packetSeq:  4297
packetSeq:  4298
packetSeq:  4299
processed packlist at 1641198742990
receiving_rate:  254400.0
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198742990
avgFrameBetween:  6
psnrStat:  [[551036, 550381, 550845, 551236, 551571, 551947]]
delayStat:  [[362, 362, 362, 362, 360, 361]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551169.3333333334] [361.5] [0]
processed state3-5 at 1641198742990
liner_to_log:  tensor([[[0.7268]]]) tensor([[[0.3036]]])
linear_to_log at 1641198742991
listState:  [0.0636, 0.12833333333333333, 0.0, 0.5511693333333334, 0.3615, 0.0, tensor([[[0.3036]]])]
state_clone_detach at 1641198742991
reward: 0.03614521173711077
state tensor([0.0636, 0.1283, 0.0000, 0.3036], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198742991
state222:  tensor([[0.0636, 0.1283, 0.0000, 0.3036]], device='cuda:0')
policy_old.forwarded at 1641198742993
give action 107============================
log_to_linear:  tensor([[[0.3328]]], device='cuda:0') tensor([[[0.7623]]], device='cuda:0')
log_to_linear action at 1641198742994
bwe changes from to:  [tensor([[[252.3880]]]), tensor([[[192.3871]]])]
step into gymStat at 1641198742994
send bwe to appRecv at 1641198742994
sent bwe to appRecv at 1641198742994
wait for recv string at 1641198742994
recved string at 1641198743192
1
wait for recv [self.estimator, stat] at 1641198743192
recved [self.estimator, stat] at 1641198743192
sorted packlist at 1641198743192
packetSeq:  4300
packetSeq:  4301
packetSeq:  4302
packetSeq:  4303
packetSeq:  4304
packetSeq:  4305
packetSeq:  4306
packetSeq:  4307
packetSeq:  4308
packetSeq:  4309
processed packlist at 1641198743192
receiving_rate:  269000.0
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198743192
avgFrameBetween:  6
psnrStat:  [[551281, 551914, 552442, 551854, 552261, 553057]]
delayStat:  [[361, 361, 361, 360, 360, 361]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552134.8333333334] [360.6666666666667] [0]
processed state3-5 at 1641198743192
liner_to_log:  tensor([[[0.7623]]]) tensor([[[0.3328]]])
linear_to_log at 1641198743193
listState:  [0.06725, 0.12833333333333333, 0.0, 0.5521348333333334, 0.3606666666666667, 0.0, tensor([[[0.3328]]])]
state_clone_detach at 1641198743193
reward: 0.053265779182185335
state tensor([0.0672, 0.1283, 0.0000, 0.3328], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198743193
state222:  tensor([[0.0672, 0.1283, 0.0000, 0.3328]], device='cuda:0')
policy_old.forwarded at 1641198743195
give action 108============================
log_to_linear:  tensor([[[0.3633]]], device='cuda:0') tensor([[[0.8014]]], device='cuda:0')
log_to_linear action at 1641198743196
bwe changes from to:  [tensor([[[192.3871]]]), tensor([[[154.1810]]])]
step into gymStat at 1641198743196
send bwe to appRecv at 1641198743196
sent [estimator, stat] at 1641198739261
pc wait for bwe at 1641198739261
pc got bwe at 1641198739266
bandwidth:  300000
pc flushed at 1641198739266
Bwe Sent: 5 at 1641198739266
got request at 1641198739464
processed allFrame at 1641198739465
send 'asking for bwe' at 1641198739465
sent 'asking for bwe' at 1641198739465
send [estimator, stat] at 1641198739465
sent [estimator, stat] at 1641198739465
pc wait for bwe at 1641198739465
pc got bwe at 1641198739469
bandwidth:  300000
pc flushed at 1641198739469
Bwe Sent: 5 at 1641198739469
got request at 1641198739692
processed allFrame at 1641198739692
send 'asking for bwe' at 1641198739692
sent 'asking for bwe' at 1641198739692
send [estimator, stat] at 1641198739692
sent [estimator, stat] at 1641198739692
pc wait for bwe at 1641198739692
pc got bwe at 1641198739696
bandwidth:  300000
pc flushed at 1641198739696
Bwe Sent: 4 at 1641198739696
got request at 1641198739898
processed allFrame at 1641198739898
send 'asking for bwe' at 1641198739898
sent 'asking for bwe' at 1641198739898
send [estimator, stat] at 1641198739898
sent [estimator, stat] at 1641198739898
pc wait for bwe at 1641198739898
pc got bwe at 1641198739902
bandwidth:  300000
pc flushed at 1641198739902
Bwe Sent: 4 at 1641198739902
got request at 1641198740121
processed allFrame at 1641198740121
send 'asking for bwe' at 1641198740121
sent 'asking for bwe' at 1641198740121
send [estimator, stat] at 1641198740121
sent [estimator, stat] at 1641198740121
pc wait for bwe at 1641198740121
pc got bwe at 1641198740125
bandwidth:  300000
pc flushed at 1641198740125
Bwe Sent: 4 at 1641198740125
got request at 1641198740326
processed allFrame at 1641198740326
send 'asking for bwe' at 1641198740326
sent 'asking for bwe' at 1641198740326
send [estimator, stat] at 1641198740326
sent [estimator, stat] at 1641198740326
pc wait for bwe at 1641198740326
pc got bwe at 1641198740331
bandwidth:  300000
pc flushed at 1641198740331
Bwe Sent: 5 at 1641198740331
got request at 1641198740556
processed allFrame at 1641198740556
send 'asking for bwe' at 1641198740556
sent 'asking for bwe' at 1641198740556
send [estimator, stat] at 1641198740556
sent [estimator, stat] at 1641198740556
pc wait for bwe at 1641198740556
pc got bwe at 1641198740560
bandwidth:  300000
pc flushed at 1641198740560
Bwe Sent: 4 at 1641198740560
got request at 1641198740760
processed allFrame at 1641198740760
send 'asking for bwe' at 1641198740760
sent 'asking for bwe' at 1641198740760
send [estimator, stat] at 1641198740760
sent [estimator, stat] at 1641198740760
pc wait for bwe at 1641198740760
pc got bwe at 1641198740764
bandwidth:  300000
pc flushed at 1641198740764
Bwe Sent: 4 at 1641198740764
got request at 1641198740965
processed allFrame at 1641198740965
send 'asking for bwe' at 1641198740965
sent 'asking for bwe' at 1641198740966
send [estimator, stat] at 1641198740966
sent [estimator, stat] at 1641198740966
pc wait for bwe at 1641198740966
pc got bwe at 1641198740970
bandwidth:  300000
pc flushed at 1641198740970
Bwe Sent: 5 at 1641198740970
got request at 1641198741196
processed allFrame at 1641198741196
send 'asking for bwe' at 1641198741196
sent 'asking for bwe' at 1641198741196
send [estimator, stat] at 1641198741196
sent [estimator, stat] at 1641198741196
pc wait for bwe at 1641198741196
pc got bwe at 1641198741201
bandwidth:  300000
pc flushed at 1641198741201
Bwe Sent: 5 at 1641198741201
got request at 1641198741424
processed allFrame at 1641198741424
send 'asking for bwe' at 1641198741424
sent 'asking for bwe' at 1641198741424
send [estimator, stat] at 1641198741424
sent [estimator, stat] at 1641198741425
pc wait for bwe at 1641198741425
pc got bwe at 1641198741429
bandwidth:  300000
pc flushed at 1641198741429
Bwe Sent: 5 at 1641198741429
got request at 1641198741628
processed allFrame at 1641198741629
send 'asking for bwe' at 1641198741629
sent 'asking for bwe' at 1641198741629
send [estimator, stat] at 1641198741629
sent [estimator, stat] at 1641198741629
pc wait for bwe at 1641198741629
pc got bwe at 1641198741633
bandwidth:  300000
pc flushed at 1641198741633
Bwe Sent: 5 at 1641198741633
got request at 1641198741859
processed allFrame at 1641198741860
send 'asking for bwe' at 1641198741860
sent 'asking for bwe' at 1641198741860
send [estimator, stat] at 1641198741860
sent [estimator, stat] at 1641198741860
pc wait for bwe at 1641198741860
pc got bwe at 1641198741864
bandwidth:  300000
pc flushed at 1641198741864
Bwe Sent: 5 at 1641198741864
got request at 1641198742065
processed allFrame at 1641198742065
send 'asking for bwe' at 1641198742065
sent 'asking for bwe' at 1641198742065
send [estimator, stat] at 1641198742065
sent [estimator, stat] at 1641198742065
pc wait for bwe at 1641198742065
pc got bwe at 1641198742070
bandwidth:  300000
pc flushed at 1641198742070
Bwe Sent: 5 at 1641198742070
got request at 1641198742290
processed allFrame at 1641198742290
send 'asking for bwe' at 1641198742290
sent 'asking for bwe' at 1641198742290
send [estimator, stat] at 1641198742290
sent [estimator, stat] at 1641198742290
pc wait for bwe at 1641198742290
pc got bwe at 1641198742295
bandwidth:  300000
pc flushed at 1641198742295
Bwe Sent: 5 at 1641198742295
got request at 1641198742558
processed allFrame at 1641198742558
send 'asking for bwe' at 1641198742558
sent 'asking for bwe' at 1641198742558
send [estimator, stat] at 1641198742558
sent [estimator, stat] at 1641198742558
pc wait for bwe at 1641198742558
pc got bwe at 1641198742563
bandwidth:  300000
pc flushed at 1641198742577
Bwe Sent: 19 at 1641198742577
got request at 1641198742788
processed allFrame at 1641198742788
send 'asking for bwe' at 1641198742788
sent 'asking for bwe' at 1641198742788
send [estimator, stat] at 1641198742788
sent [estimator, stat] at 1641198742788
pc wait for bwe at 1641198742788
pc got bwe at 1641198742793
bandwidth:  300000
pc flushed at 1641198742793
Bwe Sent: 5 at 1641198742793
got request at 1641198742990
processed allFrame at 1641198742990
send 'asking for bwe' at 1641198742990
sent 'asking for bwe' at 1641198742990
send [estimator, stat] at 1641198742990
sent [estimator, stat] at 1641198742990
pc wait for bwe at 1641198742990
pc got bwe at 1641198742994
bandwidth:  300000
pc flushed at 1641198742994
Bwe Sent: 4 at 1641198742994
got request at 1641198743192
processed allFrame at 1641198743192
send 'asking for bwe' at 1641198743192
sent 'asking for bwe' at 1641198743192
send [estimator, stat] at 1641198743192
sent [estimator, stat] at 1641198743192
pc wait for bwe at 1641198743192
pc got bwe at 1641198743196
bandwidth:  300000
pc flushed at 1641198743196
Bwe Sent: 4 at 1641198743196
got request at 1641198743422
processed allFrame at 1641198743422
send 'asking for bwe' at 1641198743422
sent 'asking for bwe' at 1641198743422
send [estimator, stat] at 1641198743422
sent [estimator, stat] at 1641198743422
pc wait for bwe at 1641198743422
pc got bwe at 1641198743426
bandwidth:  300000
pc flushed at 1641198743426
Bwe Sent: 4 at 1641198743426
got request at 1641198743622
processed allFrame at 1641198743622
send 'asking for bwe' at 1641198743622
sent 'asking for bwe' at 1641198743622
send [estimator, stat] at 1641198743622
sent [estimator, stat] at 1641198743622
pc wait for bwe at 1641198743622
pc got bwe at 1641198743627
bandwidth:  300000
pc flushed at 1641198743627
Bwe Sent: 5 at 1641198743627
got request at 1641198743854
processed allFrame at 1641198743854
send 'asking for bwe' at 1641198743854
sent 'asking for bwe' at 1641198743854
send [estimator, stat] at 1641198743854
sent [estimator, stat] at 1641198743855
pc wait for bwe at 1641198743855
pc got bwe at 1641198743859
bandwidth:  300000
pc flushed at 1641198743859
Bwe Sent: 5 at 1641198743859
got request at 1641198744058
processed allFrame at 1641198744058
send 'asking for bwe' at 1641198744058
sent 'asking for bwe' at 1641198744058
send [estimator, stat] at 1641198744058
sent [estimator, stat] at 1641198744058
pc wait for bwe at 1641198744059
pc got bwe at 1641198744063
bandwidth:  300000
pc flushed at 1641198744063
Bwe Sent: 5 at 1641198744063
got request at 1641198744402
processed allFrame at 1641198744402
sent bwe to appRecv at 1641198743196
wait for recv string at 1641198743196
recved string at 1641198743422
1
wait for recv [self.estimator, stat] at 1641198743422
recved [self.estimator, stat] at 1641198743422
sorted packlist at 1641198743422
packetSeq:  4310
packetSeq:  4311
packetSeq:  4312
packetSeq:  4313
packetSeq:  4314
packetSeq:  4315
packetSeq:  4316
packetSeq:  4317
packetSeq:  4318
packetSeq:  4319
processed packlist at 1641198743422
receiving_rate:  266280.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198743422
avgFrameBetween:  6
psnrStat:  [[553176, 553928, 553265, 554456, 554335, 554068, 555151]]
delayStat:  [[360, 361, 361, 360, 359, 359, 359]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554054.1428571428] [359.85714285714283] [0]
processed state3-5 at 1641198743422
liner_to_log:  tensor([[[0.8014]]]) tensor([[[0.3633]]])
linear_to_log at 1641198743422
listState:  [0.06657, 0.12814814814814815, 0.0, 0.5540541428571428, 0.3598571428571428, 0.0, tensor([[[0.3633]]])]
state_clone_detach at 1641198743423
reward: 0.05066855721839991
state tensor([0.0666, 0.1281, 0.0000, 0.3633], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198743423
state222:  tensor([[0.0666, 0.1281, 0.0000, 0.3633]], device='cuda:0')
policy_old.forwarded at 1641198743425
give action 109============================
log_to_linear:  tensor([[[0.5771]]], device='cuda:0') tensor([[[1.1276]]], device='cuda:0')
log_to_linear action at 1641198743426
bwe changes from to:  [tensor([[[154.1810]]]), tensor([[[173.8470]]])]
step into gymStat at 1641198743426
send bwe to appRecv at 1641198743426
sent bwe to appRecv at 1641198743426
wait for recv string at 1641198743426
recved string at 1641198743622
1
wait for recv [self.estimator, stat] at 1641198743622
recved [self.estimator, stat] at 1641198743622
sorted packlist at 1641198743622
packetSeq:  4320
packetSeq:  4321
packetSeq:  4322
packetSeq:  4323
packetSeq:  4324
packetSeq:  4325
packetSeq:  4326
packetSeq:  4327
processed packlist at 1641198743622
receiving_rate:  230640.0
delay:  208.75
loss_ratio:  0.0
processed state0-2 at 1641198743623
avgFrameBetween:  6
psnrStat:  [[555719, 555912, 555972, 556640, 556368, 556782]]
delayStat:  [[360, 359, 359, 359, 359, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556232.1666666666] [359.0] [0]
processed state3-5 at 1641198743623
liner_to_log:  tensor([[[1.1276]]]) tensor([[[0.5771]]])
linear_to_log at 1641198743623
listState:  [0.05766, 0.13916666666666666, 0.0, 0.5562321666666666, 0.359, 0.0, tensor([[[0.5771]]])]
state_clone_detach at 1641198743623
reward: -0.025305421235918668
state tensor([0.0577, 0.1392, 0.0000, 0.5771], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198743624
state222:  tensor([[0.0577, 0.1392, 0.0000, 0.5771]], device='cuda:0')
policy_old.forwarded at 1641198743626
give action 110============================
log_to_linear:  tensor([[[0.6692]]], device='cuda:0') tensor([[[1.2937]]], device='cuda:0')
log_to_linear action at 1641198743626
bwe changes from to:  [tensor([[[173.8470]]]), tensor([[[224.8992]]])]
step into gymStat at 1641198743627
send bwe to appRecv at 1641198743627
sent bwe to appRecv at 1641198743627
wait for recv string at 1641198743627
recved string at 1641198743854
1
wait for recv [self.estimator, stat] at 1641198743854
recved [self.estimator, stat] at 1641198743855
sorted packlist at 1641198743855
packetSeq:  4328
packetSeq:  4329
packetSeq:  4330
packetSeq:  4331
packetSeq:  4332
packetSeq:  4333
packetSeq:  4334
packetSeq:  4335
packetSeq:  4336
packetSeq:  4337
packetSeq:  4338
processed packlist at 1641198743855
receiving_rate:  330240.0
delay:  194.2
loss_ratio:  0.0
processed state0-2 at 1641198743855
avgFrameBetween:  6
psnrStat:  [[556408, 556741, 556487, 556841, 556608, 556896, 556259]]
delayStat:  [[360, 359, 358, 359, 358, 358, 359]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556605.7142857143] [358.7142857142857] [0]
processed state3-5 at 1641198743855
liner_to_log:  tensor([[[1.2937]]]) tensor([[[0.6692]]])
linear_to_log at 1641198743855
listState:  [0.08256, 0.12946666666666665, 0.0, 0.5566057142857144, 0.3587142857142857, 0.0, tensor([[[0.6692]]])]
state_clone_detach at 1641198743855
reward: 0.11678853591574634
state tensor([0.0826, 0.1295, 0.0000, 0.6692], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198743856
state222:  tensor([[0.0826, 0.1295, 0.0000, 0.6692]], device='cuda:0')
policy_old.forwarded at 1641198743858
give action 111============================
log_to_linear:  tensor([[[0.5708]]], device='cuda:0') tensor([[[1.1167]]], device='cuda:0')
log_to_linear action at 1641198743859
bwe changes from to:  [tensor([[[224.8992]]]), tensor([[[251.1431]]])]
step into gymStat at 1641198743859
send bwe to appRecv at 1641198743859
sent bwe to appRecv at 1641198743859
wait for recv string at 1641198743859
recved string at 1641198744058
1
wait for recv [self.estimator, stat] at 1641198744058
recved [self.estimator, stat] at 1641198744059
sorted packlist at 1641198744059
packetSeq:  4339
packetSeq:  4340
packetSeq:  4341
packetSeq:  4342
packetSeq:  4343
packetSeq:  4344
packetSeq:  4345
packetSeq:  4346
packetSeq:  4347
packetSeq:  4348
processed packlist at 1641198744059
receiving_rate:  269800.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198744059
avgFrameBetween:  6
psnrStat:  [[556393, 556057, 556421, 555204, 556539, 556200]]
delayStat:  [[358, 357, 359, 357, 358, 357]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556135.6666666666] [357.6666666666667] [0]
processed state3-5 at 1641198744059
liner_to_log:  tensor([[[1.1167]]]) tensor([[[0.5708]]])
linear_to_log at 1641198744059
listState:  [0.06745, 0.12806666666666666, 0.0, 0.5561356666666666, 0.3576666666666667, 0.0, tensor([[[0.5708]]])]
state_clone_detach at 1641198744060
reward: 0.05498992260708613
state tensor([0.0675, 0.1281, 0.0000, 0.5708], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198744060
state222:  tensor([[0.0675, 0.1281, 0.0000, 0.5708]], device='cuda:0')
policy_old.forwarded at 1641198744062
give action 112============================
log_to_linear:  tensor([[[0.3394]]], device='cuda:0') tensor([[[0.7706]]], device='cuda:0')
log_to_linear action at 1641198744063
bwe changes from to:  [tensor([[[251.1431]]]), tensor([[[193.5399]]])]
step into gymStat at 1641198744063
send bwe to appRecv at 1641198744063
sent bwe to appRecv at 1641198744063
wait for recv string at 1641198744063
recved string at 1641198744402
1
wait for recv [self.estimator, stat] at 1641198744402
recved [self.estimator, stat] at 1641198744402
sorted packlist at 1641198744402
packetSeq:  4349
packetSeq:  4350
packetSeq:  4351
processed packlist at 1641198744402
receiving_rate:  25680.0
delay:  473.0
loss_ratio:  0.0
processed state0-2 at 1641198744402
avgFrameBetween:  6
psnrStat:  [[556896, 556536, 556365, 555786, 555608, 555977, 555693, 554845, 554372, 554393]]
delayStat:  [[357, 358, 357, 357, 357, 357, 358, 357, 357, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555647.1] [357.3] [0]
processed state3-5 at 1641198744402
liner_to_log:  tensor([[[0.7706]]]) tensor([[[0.3394]]])
linear_to_log at 1641198744403
listState:  [0.00642, 0.31533333333333335, 0.0, 0.5556471, 0.3573, 0.0, tensor([[[0.3394]]])]
state_clone_detach at 1641198744403
reward: -0.8873320870495762
state tensor([0.0064, 0.3153, 0.0000, 0.3394], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198744404
state222:  tensor([[0.0064, 0.3153, 0.0000, 0.3394]], device='cuda:0')
policy_old.forwarded at 1641198744406
give action 113============================
log_to_linear:  tensor([[[0.2648]]], device='cuda:0') tensor([[[0.6826]]], device='cuda:0')
log_to_linear action at 1641198744406
bwe changes from to:  [tensor([[[193.5399]]]), tensor([[[132.1073]]])]
step into gymStat at 1641198744407
send bwe to appRecv at 1641198744407
sent bwe to appRecv at 1641198744407
wait for recv string at 1641198744407
recved string at 1641198744808
1
wait for recv [self.estimator, stat] at 1641198744808
recved [self.estimator, stat] at 1641198744808
sorted packlist at 1641198744808
packetSeq:  4352
packetSeq:  4353
packetSeq:  4354
packetSeq:  4355
packetSeq:  4356
packetSeq:  4357
packetSeq:  4358
packetSeq:  4359
packetSeq:  4360
packetSeq:  4361
packetSeq:  4362
packetSeq:  4363
packetSeq:  4364
packetSeq:  4365
packetSeq:  4366
packetSeq:  4367
packetSeq:  4368
processed packlist at 1641198744808
receiving_rate:  26320.0
delay:  549.0
loss_ratio:  0.0
processed state0-2 at 1641198744808
avgFrameBetween:  6
psnrStat:  [[554804, 554510, 554374, 553900, 554225, 554290, 554215, 555479, 554176, 553773, 553731, 553684]]
delayStat:  [[356, 356, 361, 364, 364, 365, 365, 365, 365, 365, 365, 365]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554263.4166666666] [363.0] [0]
processed state3-5 at 1641198744808
liner_to_log:  tensor([[[0.6826]]]) tensor([[[0.2648]]])
linear_to_log at 1641198744809
listState:  [0.00658, 0.366, 0.0, 0.5542634166666667, 0.363, 0.0, tensor([[[0.2648]]])]
state_clone_detach at 1641198744809
reward: -1.0379407820783269
state tensor([0.0066, 0.3660, 0.0000, 0.2648], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198744810
state222:  tensor([[0.0066, 0.3660, 0.0000, 0.2648]], device='cuda:0')
policy_old.forwarded at 1641198744812
give action 114============================
log_to_linear:  tensor([[[0.2898]]], device='cuda:0') tensor([[[0.7106]]], device='cuda:0')
log_to_linear action at 1641198744813
bwe changes from to:  [tensor([[[132.1073]]]), tensor([[[93.8754]]])]
step into gymStat at 1641198744813
send bwe to appRecv at 1641198744813
sent bwe to appRecv at 1641198744813
wait for recv string at 1641198744813
recved string at 1641198745022
1
wait for recv [self.estimator, stat] at 1641198745022
recved [self.estimator, stat] at 1641198745022
sorted packlist at 1641198745022
packetSeq:  4369
packetSeq:  4370
packetSeq:  4371
packetSeq:  4372
packetSeq:  4373
packetSeq:  4374
packetSeq:  4375
packetSeq:  4376
packetSeq:  4377
packetSeq:  4378
packetSeq:  4379
packetSeq:  4380
packetSeq:  4381
packetSeq:  4382
packetSeq:  4383
packetSeq:  4384
packetSeq:  4385
packetSeq:  4386
packetSeq:  4387
packetSeq:  4388
packetSeq:  4389
packetSeq:  4390
packetSeq:  4391
packetSeq:  4392
packetSeq:  4393
packetSeq:  4394
processed packlist at 1641198745022
receiving_rate:  725920.0
delay:  345.0833333333333
loss_ratio:  0.0
processed state0-2 at 1641198745022
avgFrameBetween:  6
psnrStat:  [[553051, 552281, 551795, 550197, 549534, 549652]]
delayStat:  [[369, 368, 369, 374, 378, 378]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551085.0] [372.6666666666667] [0]
processed state3-5 at 1641198745022
liner_to_log:  tensor([[[0.7106]]]) tensor([[[0.2898]]])
linear_to_log at 1641198745023
listState:  [0.18148, 0.23005555555555554, 0.0, 0.551085, 0.3726666666666667, 0.0, tensor([[[0.2898]]])]
state_clone_detach at 1641198745023
reward: 0.12698515212027484
state tensor([0.1815, 0.2301, 0.0000, 0.2898], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198745023
state222:  tensor([[0.1815, 0.2301, 0.0000, 0.2898]], device='cuda:0')
policy_old.forwarded at 1641198745025
give action 115============================
log_to_linear:  tensor([[[-0.0286]]], device='cuda:0') tensor([[[nan]]], device='cuda:0')
log_to_linear action at 1641198745026
bwe changes from to:  [tensor([[[93.8754]]]), tensor([[[46.9377]]])]
step into gymStat at 1641198745026
send bwe to appRecv at 1641198745026
sent bwe to appRecv at 1641198745026
wait for recv string at 1641198745026
recved string at 1641198745222
1
wait for recv [self.estimator, stat] at 1641198745222
recved [self.estimator, stat] at 1641198745222
sorted packlist at 1641198745222
packetSeq:  4395
packetSeq:  4396
packetSeq:  4397
packetSeq:  4398
packetSeq:  4399
packetSeq:  4400
packetSeq:  4401
packetSeq:  4402
packetSeq:  4403
packetSeq:  4404
processed packlist at 1641198745222
receiving_rate:  281920.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198745223
avgFrameBetween:  6
psnrStat:  [[548458, 547994, 548964, 548675, 548858, 550570]]
delayStat:  [[376, 378, 377, 377, 377, 377]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548919.8333333334] [377.0] [0]
processed state3-5 at 1641198745223
liner_to_log:  tensor([[[0.5000]]]) tensor([[[0.]]])
linear_to_log at 1641198745223
listState:  [0.07048, 0.128, 0.0, 0.5489198333333334, 0.377, 0.0, tensor([[[0.]]])]
state_clone_detach at 1641198745223
reward: 0.06901931053917176
state tensor([0.0705, 0.1280, 0.0000, 0.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198745224
state222:  tensor([[0.0705, 0.1280, 0.0000, 0.0000]], device='cuda:0')
policy_old.forwarded at 1641198745225
give action 116============================
log_to_linear:  tensor([[[0.6333]]], device='cuda:0') tensor([[[1.2272]]], device='cuda:0')
log_to_linear action at 1641198745226
bwe changes from to:  [tensor([[[46.9377]]]), tensor([[[57.6032]]])]
step into gymStat at 1641198745227
send bwe to appRecv at 1641198745227
sent bwe to appRecv at 1641198745227
wait for recv string at 1641198745227
recved string at 1641198745451
1
wait for recv [self.estimator, stat] at 1641198745451
recved [self.estimator, stat] at 1641198745451
sorted packlist at 1641198745451
packetSeq:  4405
packetSeq:  4406
packetSeq:  4407
packetSeq:  4408
packetSeq:  4409
packetSeq:  4410
packetSeq:  4411
packetSeq:  4412
processed packlist at 1641198745451
receiving_rate:  289880.0
delay:  193.25
loss_ratio:  0.0
processed state0-2 at 1641198745452
avgFrameBetween:  6
psnrStat:  [[550844, 552440, 552351, 553342, 552523, 552956, 553052]]
delayStat:  [[376, 377, 376, 376, 377, 375, 375]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552501.1428571428] [376.0] [0]
processed state3-5 at 1641198745452
liner_to_log:  tensor([[[1.2272]]]) tensor([[[0.6333]]])
linear_to_log at 1641198745452
listState:  [0.07247, 0.12883333333333333, 0.0, 0.5525011428571428, 0.376, 0.0, tensor([[[0.6333]]])]
state_clone_detach at 1641198745452
reward: 0.07543158486730084
state tensor([0.0725, 0.1288, 0.0000, 0.6333], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198745453
state222:  tensor([[0.0725, 0.1288, 0.0000, 0.6333]], device='cuda:0')
policy_old.forwarded at 1641198745455
give action 117============================
log_to_linear:  tensor([[[0.3840]]], device='cuda:0') tensor([[[0.8291]]], device='cuda:0')
log_to_linear action at 1641198745455
bwe changes from to:  [tensor([[[57.6032]]]), tensor([[[47.7593]]])]
step into gymStat at 1641198745456
send bwe to appRecv at 1641198745456
sent bwe to appRecv at 1641198745456
wait for recv string at 1641198745456
recved string at 1641198745654
1
wait for recv [self.estimator, stat] at 1641198745654
recved [self.estimator, stat] at 1641198745654
sorted packlist at 1641198745654
packetSeq:  4413
packetSeq:  4414
packetSeq:  4415
packetSeq:  4416
packetSeq:  4417
packetSeq:  4418
packetSeq:  4419
packetSeq:  4420
packetSeq:  4421
packetSeq:  4422
packetSeq:  4423
packetSeq:  4424
processed packlist at 1641198745654
receiving_rate:  285520.0
delay:  191.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198745654
avgFrameBetween:  6
psnrStat:  [[553278, 552496, 552062, 552542, 552438, 551155]]
delayStat:  [[376, 375, 375, 376, 375, 374]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552328.5] [375.1666666666667] [0]
processed state3-5 at 1641198745654
liner_to_log:  tensor([[[0.8291]]]) tensor([[[0.3840]]])
linear_to_log at 1641198745654
listState:  [0.07138, 0.12755555555555556, 0.0, 0.5523285, 0.3751666666666667, 0.0, tensor([[[0.3840]]])]
state_clone_detach at 1641198745655
reward: 0.07439972509379528
state tensor([0.0714, 0.1276, 0.0000, 0.3840], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198745655
state222:  tensor([[0.0714, 0.1276, 0.0000, 0.3840]], device='cuda:0')
policy_old.forwarded at 1641198745657
give action 118============================
log_to_linear:  tensor([[[0.5357]]], device='cuda:0') tensor([[[1.0578]]], device='cuda:0')
log_to_linear action at 1641198745658
bwe changes from to:  [tensor([[[47.7593]]]), tensor([[[50.5195]]])]
step into gymStat at 1641198745658
send bwe to appRecv at 1641198745658
sent bwe to appRecv at 1641198745658
wait for recv string at 1641198745658
recved string at 1641198745857
1
wait for recv [self.estimator, stat] at 1641198745857
recved [self.estimator, stat] at 1641198745857
sorted packlist at 1641198745857
packetSeq:  4425
packetSeq:  4426
packetSeq:  4427
packetSeq:  4428
packetSeq:  4429
packetSeq:  4430
packetSeq:  4431
packetSeq:  4432
packetSeq:  4433
packetSeq:  4434
packetSeq:  4435
processed packlist at 1641198745857
receiving_rate:  296200.0
delay:  192.1818181818182
loss_ratio:  0.0
processed state0-2 at 1641198745857
avgFrameBetween:  6
psnrStat:  [[550764, 549970, 548761, 548920, 548421, 548632, 547939]]
delayStat:  [[375, 374, 374, 375, 374, 373, 374]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549058.1428571428] [374.14285714285717] [0]
processed state3-5 at 1641198745857
liner_to_log:  tensor([[[1.0578]]]) tensor([[[0.5357]]])
linear_to_log at 1641198745858
listState:  [0.07405, 0.12812121212121214, 0.0, 0.5490581428571428, 0.37414285714285717, 0.0, tensor([[[0.5357]]])]
state_clone_detach at 1641198745858
reward: 0.08455093834277855
state tensor([0.0741, 0.1281, 0.0000, 0.5357], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198745859
state222:  tensor([[0.0741, 0.1281, 0.0000, 0.5357]], device='cuda:0')
policy_old.forwarded at 1641198745861
give action 119============================
log_to_linear:  tensor([[[0.4694]]], device='cuda:0') tensor([[[0.9524]]], device='cuda:0')
log_to_linear action at 1641198745862
bwe changes from to:  [tensor([[[50.5195]]]), tensor([[[48.1165]]])]
step into gymStat at 1641198745863
send bwe to appRecv at 1641198745863
sent bwe to appRecv at 1641198745863
wait for recv string at 1641198745863
recved string at 1641198746087
1
wait for recv [self.estimator, stat] at 1641198746087
recved [self.estimator, stat] at 1641198746087
sorted packlist at 1641198746087
packetSeq:  4436
packetSeq:  4437
packetSeq:  4438
packetSeq:  4439
packetSeq:  4440
packetSeq:  4441
packetSeq:  4442
packetSeq:  4443
processed packlist at 1641198746087
receiving_rate:  282640.0
delay:  192.875
loss_ratio:  0.0
processed state0-2 at 1641198746087
avgFrameBetween:  6
psnrStat:  [[547921, 547845, 546661, 546702, 546666, 545544, 546640]]
delayStat:  [[373, 374, 373, 373, 372, 373, 372]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [546854.1428571428] [372.85714285714283] [0]
processed state3-5 at 1641198746087
liner_to_log:  tensor([[[0.9524]]]) tensor([[[0.4694]]])
linear_to_log at 1641198746088
listState:  [0.07066, 0.12858333333333333, 0.0, 0.5468541428571428, 0.37285714285714283, 0.0, tensor([[[0.4694]]])]
state_clone_detach at 1641198746088
reward: 0.06808091357868101
state tensor([0.0707, 0.1286, 0.0000, 0.4694], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198746089
state222:  tensor([[0.0707, 0.1286, 0.0000, 0.4694]], device='cuda:0')
policy_old.forwarded at 1641198746091
give action 120============================
log_to_linear:  tensor([[[0.4088]]], device='cuda:0') tensor([[[0.8634]]], device='cuda:0')
log_to_linear action at 1641198746092
bwe changes from to:  [tensor([[[48.1165]]]), tensor([[[41.5438]]])]
step into gymStat at 1641198746093
send bwe to appRecv at 1641198746093
sent bwe to appRecv at 1641198746093
wait for recv string at 1641198746093
recved string at 1641198746319
1
wait for recv [self.estimator, stat] at 1641198746319
recved [self.estimator, stat] at 1641198746319
sorted packlist at 1641198746319
packetSeq:  4444
packetSeq:  4445
packetSeq:  4446
packetSeq:  4447
packetSeq:  4448
packetSeq:  4449
packetSeq:  4450
packetSeq:  4451
packetSeq:  4452
processed packlist at 1641198746319
receiving_rate:  281200.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198746319
avgFrameBetween:  6
psnrStat:  [[544996, 544725, 544703, 544712, 545210, 544226]]
delayStat:  [[372, 372, 372, 372, 371, 371]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [544762.0] [371.6666666666667] [0]
processed state3-5 at 1641198746319
liner_to_log:  tensor([[[0.8634]]]) tensor([[[0.4088]]])
linear_to_log at 1641198746319
listState:  [0.0703, 0.12822222222222224, 0.0, 0.544762, 0.3716666666666667, 0.0, tensor([[[0.4088]]])]
state_clone_detach at 1641198746320
reward: 0.06753994151331988
state tensor([0.0703, 0.1282, 0.0000, 0.4088], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198746320
state222:  tensor([[0.0703, 0.1282, 0.0000, 0.4088]], device='cuda:0')
policy_old.forwarded at 1641198746322
give action 121============================
log_to_linear:  tensor([[[0.4358]]], device='cuda:0') tensor([[[0.9022]]], device='cuda:0')
log_to_linear action at 1641198746323
bwe changes from to:  [tensor([[[41.5438]]]), tensor([[[37.4808]]])]
step into gymStat at 1641198746323
send bwe to appRecv at 1641198746323
sent bwe to appRecv at 1641198746323
wait for recv string at 1641198746323
recved string at 1641198746552
1
wait for recv [self.estimator, stat] at 1641198746552
recved [self.estimator, stat] at 1641198746552
sorted packlist at 1641198746552
packetSeq:  4453
packetSeq:  4454
packetSeq:  4455
packetSeq:  4456
packetSeq:  4457
packetSeq:  4458
packetSeq:  4459
packetSeq:  4460
packetSeq:  4461
packetSeq:  4462
packetSeq:  4463
processed packlist at 1641198746552
receiving_rate:  247400.0
delay:  191.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198746552
avgFrameBetween:  6
psnrStat:  [[543426, 544336, 544736, 546132, 546506, 546602, 547261, 546835]]
delayStat:  [[371, 371, 371, 371, 370, 370, 371, 370]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [545729.25] [370.625] [0]
processed state3-5 at 1641198746552
liner_to_log:  tensor([[[0.9022]]]) tensor([[[0.4358]]])
linear_to_log at 1641198746553
listState:  [0.06185, 0.12792592592592592, 0.0, 0.54572925, 0.370625, 0.0, tensor([[[0.4358]]])]
state_clone_detach at 1641198746553
reward: 0.02898239032645694
state tensor([0.0619, 0.1279, 0.0000, 0.4358], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198746554
state222:  tensor([[0.0619, 0.1279, 0.0000, 0.4358]], device='cuda:0')
policy_old.forwarded at 1641198746555
give action 122============================
log_to_linear:  tensor([[[0.4307]]], device='cuda:0') tensor([[[0.8947]]], device='cuda:0')
log_to_linear action at 1641198746556
bwe changes from to:  [tensor([[[37.4808]]]), tensor([[[33.5334]]])]
step into gymStat at 1641198746557
send bwe to appRecv at 1641198746557
sent bwe to appRecv at 1641198746557
wait for recv string at 1641198746557
recved string at 1641198746753
1
wait for recv [self.estimator, stat] at 1641198746753
recved [self.estimator, stat] at 1641198746753
sorted packlist at 1641198746753
packetSeq:  4464
packetSeq:  4465
packetSeq:  4466
packetSeq:  4467
packetSeq:  4468
packetSeq:  4469
packetSeq:  4470
packetSeq:  4471
packetSeq:  4472
processed packlist at 1641198746753
receiving_rate:  277040.0
delay:  192.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198746753
avgFrameBetween:  6
psnrStat:  [[547522, 546990, 547151, 546911, 547921, 549223]]
delayStat:  [[370, 370, 371, 369, 369, 370]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547619.6666666666] [369.8333333333333] [0]
processed state3-5 at 1641198746754
liner_to_log:  tensor([[[0.8947]]]) tensor([[[0.4307]]])
linear_to_log at 1641198746754
listState:  [0.06926, 0.12807407407407406, 0.0, 0.5476196666666666, 0.3698333333333333, 0.0, tensor([[[0.4307]]])]
state_clone_detach at 1641198746754
reward: 0.06326709525399515
state tensor([0.0693, 0.1281, 0.0000, 0.4307], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198746755
state222:  tensor([[0.0693, 0.1281, 0.0000, 0.4307]], device='cuda:0')
policy_old.forwarded at 1641198746757
give action 123============================
log_to_linear:  tensor([[[0.6895]]], device='cuda:0') tensor([[[1.3320]]], device='cuda:0')
log_to_linear action at 1641198746757
bwe changes from to:  [tensor([[[33.5334]]]), tensor([[[44.6671]]])]
step into gymStat at 1641198746758
send bwe to appRecv at 1641198746758
sent bwe to appRecv at 1641198746758
wait for recv string at 1641198746758
recved string at 1641198746958
1
wait for recv [self.estimator, stat] at 1641198746958
recved [self.estimator, stat] at 1641198746958
sorted packlist at 1641198746958
packetSeq:  4473
packetSeq:  4474
packetSeq:  4475
packetSeq:  4476
packetSeq:  4477
packetSeq:  4478
packetSeq:  4479
packetSeq:  4480
packetSeq:  4481
packetSeq:  4482
packetSeq:  4483
processed packlist at 1641198746958
receiving_rate:  277480.0
delay:  191.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198746958
avgFrameBetween:  6
psnrStat:  [[548875, 549124, 549024, 548600, 549308, 549704]]
delayStat:  [[369, 369, 370, 369, 368, 368]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549105.8333333334] [368.8333333333333] [0]
processed state3-5 at 1641198746958
liner_to_log:  tensor([[[1.3320]]]) tensor([[[0.6895]]])
linear_to_log at 1641198746959
listState:  [0.06937, 0.1276969696969697, 0.0, 0.5491058333333334, 0.3688333333333333, 0.0, tensor([[[0.6895]]])]
state_clone_detach at 1641198746959
reward: 0.06489911015544203
state tensor([0.0694, 0.1277, 0.0000, 0.6895], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198746959
state222:  tensor([[0.0694, 0.1277, 0.0000, 0.6895]], device='cuda:0')
policy_old.forwarded at 1641198746961
give action 124============================
log_to_linear:  tensor([[[0.4277]]], device='cuda:0') tensor([[[0.8903]]], device='cuda:0')
log_to_linear action at 1641198746962
bwe changes from to:  [tensor([[[44.6671]]]), tensor([[[39.7667]]])]
step into gymStat at 1641198746962
send bwe to appRecv at 1641198746962
sent bwe to appRecv at 1641198746962
wait for recv string at 1641198746962
recved string at 1641198747186
1
wait for recv [self.estimator, stat] at 1641198747186
recved [self.estimator, stat] at 1641198747186
sorted packlist at 1641198747186
packetSeq:  4484
packetSeq:  4485
packetSeq:  4486
packetSeq:  4487
packetSeq:  4488
packetSeq:  4489
packetSeq:  4490
packetSeq:  4491
packetSeq:  4492
packetSeq:  4493
processed packlist at 1641198747186
receiving_rate:  260880.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198747187
avgFrameBetween:  6
psnrStat:  [[550382, 549012, 549072, 548501, 550259, 549532, 548681]]
delayStat:  [[368, 368, 368, 368, 368, 367, 367]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549348.4285714285] [367.7142857142857] [0]
processed state3-5 at 1641198747187
liner_to_log:  tensor([[[0.8903]]]) tensor([[[0.4277]]])
linear_to_log at 1641198747187
listState:  [0.06522, 0.1282962962962963, 0.0, 0.5493484285714285, 0.3677142857142857, 0.0, tensor([[[0.4277]]])]
state_clone_detach at 1641198747187
reward: 0.04391539777364639
state tensor([0.0652, 0.1283, 0.0000, 0.4277], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198747188
state222:  tensor([[0.0652, 0.1283, 0.0000, 0.4277]], device='cuda:0')
policy_old.forwarded at 1641198747190
give action 125============================
log_to_linear:  tensor([[[0.7286]]], device='cuda:0') tensor([[[1.4082]]], device='cuda:0')
log_to_linear action at 1641198747190
bwe changes from to:  [tensor([[[39.7667]]]), tensor([[[56.0000]]])]
step into gymStat at 1641198747191
send bwe to appRecv at 1641198747191
sent bwe to appRecv at 1641198747191
wait for recv string at 1641198747191
recved string at 1641198747422
1
wait for recv [self.estimator, stat] at 1641198747422
recved [self.estimator, stat] at 1641198747422
sorted packlist at 1641198747422
packetSeq:  4494
packetSeq:  4495
packetSeq:  4496
packetSeq:  4497
packetSeq:  4498
packetSeq:  4499
packetSeq:  4500
packetSeq:  4501
packetSeq:  4502
packetSeq:  4503
packetSeq:  4504
packetSeq:  4505
processed packlist at 1641198747422
receiving_rate:  262320.0
delay:  192.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198747422
avgFrameBetween:  6
psnrStat:  [[548692, 549972, 548916, 549058, 549350, 549347, 549128]]
delayStat:  [[367, 367, 368, 367, 367, 368, 367]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549209.0] [367.2857142857143] [0]
processed state3-5 at 1641198747422
liner_to_log:  tensor([[[1.4082]]]) tensor([[[0.7286]]])
linear_to_log at 1641198747423
listState:  [0.06558, 0.12807407407407406, 0.0, 0.549209, 0.36728571428571427, 0.0, tensor([[[0.7286]]])]
state_clone_detach at 1641198747423
reward: 0.046270884945439794
state tensor([0.0656, 0.1281, 0.0000, 0.7286], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198747423
state222:  tensor([[0.0656, 0.1281, 0.0000, 0.7286]], device='cuda:0')
policy_old.forwarded at 1641198747425
give action 126============================
log_to_linear:  tensor([[[0.3825]]], device='cuda:0') tensor([[[0.8271]]], device='cuda:0')
log_to_linear action at 1641198747426
bwe changes from to:  [tensor([[[56.0000]]]), tensor([[[46.3155]]])]
step into gymStat at 1641198747426
send bwe to appRecv at 1641198747426
sent bwe to appRecv at 1641198747426
wait for recv string at 1641198747426
recved string at 1641198747657
1
wait for recv [self.estimator, stat] at 1641198747657
recved [self.estimator, stat] at 1641198747657
sorted packlist at 1641198747657
packetSeq:  4506
packetSeq:  4507
packetSeq:  4508
packetSeq:  4509
packetSeq:  4510
packetSeq:  4511
packetSeq:  4512
packetSeq:  4513
packetSeq:  4514
packetSeq:  4515
processed packlist at 1641198747658
receiving_rate:  252360.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198747658
avgFrameBetween:  6
psnrStat:  [[550287, 550426, 549456, 549389, 549376, 548580, 548600]]
delayStat:  [[366, 366, 366, 366, 367, 366, 365]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549444.8571428572] [366.0] [0]
processed state3-5 at 1641198747658
liner_to_log:  tensor([[[0.8271]]]) tensor([[[0.3825]]])
linear_to_log at 1641198747658
listState:  [0.06309, 0.12822222222222224, 0.0, 0.5494448571428572, 0.366, 0.0, tensor([[[0.3825]]])]
state_clone_detach at 1641198747658
reward: 0.034046979105940955
state tensor([0.0631, 0.1282, 0.0000, 0.3825], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198747659
state222:  tensor([[0.0631, 0.1282, 0.0000, 0.3825]], device='cuda:0')
policy_old.forwarded at 1641198747660
give action 127============================
log_to_linear:  tensor([[[0.3176]]], device='cuda:0') tensor([[[0.7435]]], device='cuda:0')
log_to_linear action at 1641198747661
bwe changes from to:  [tensor([[[46.3155]]]), tensor([[[34.4376]]])]
step into gymStat at 1641198747662
send bwe to appRecv at 1641198747662
sent bwe to appRecv at 1641198747662
wait for recv string at 1641198747662
recved string at 1641198747863
1
wait for recv [self.estimator, stat] at 1641198747863
recved [self.estimator, stat] at 1641198747864
sorted packlist at 1641198747864
packetSeq:  4516
packetSeq:  4517
packetSeq:  4518
packetSeq:  4519
packetSeq:  4520
packetSeq:  4521
packetSeq:  4522
packetSeq:  4523
packetSeq:  4524
packetSeq:  4525
packetSeq:  4526
processed packlist at 1641198747864
receiving_rate:  301080.0
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198747864
avgFrameBetween:  6
psnrStat:  [[546889, 547524, 546209, 548240, 546993, 547042]]
delayStat:  [[366, 365, 365, 366, 365, 365]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547149.5] [365.3333333333333] [0]
processed state3-5 at 1641198747864
liner_to_log:  tensor([[[0.7435]]]) tensor([[[0.3176]]])
linear_to_log at 1641198747864
listState:  [0.07527, 0.13133333333333333, 0.0, 0.5471495, 0.3653333333333333, 0.0, tensor([[[0.3176]]])]
state_clone_detach at 1641198747864
reward: 0.08025144278849111
state tensor([0.0753, 0.1313, 0.0000, 0.3176], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198747865
state222:  tensor([[0.0753, 0.1313, 0.0000, 0.3176]], device='cuda:0')
policy_old.forwarded at 1641198747867
give action 128============================
log_to_linear:  tensor([[[0.2233]]], device='cuda:0') tensor([[[0.6393]]], device='cuda:0')
log_to_linear action at 1641198747868
bwe changes from to:  [tensor([[[34.4376]]]), tensor([[[22.0174]]])]
step into gymStat at 1641198747868
send bwe to appRecv at 1641198747868
sent bwe to appRecv at 1641198747868
wait for recv string at 1641198747868
recved string at 1641198748091
1
wait for recv [self.estimator, stat] at 1641198748091
recved [self.estimator, stat] at 1641198748091
sorted packlist at 1641198748091
packetSeq:  4527
packetSeq:  4528
packetSeq:  4529
packetSeq:  4530
packetSeq:  4531
packetSeq:  4532
packetSeq:  4533
packetSeq:  4534
packetSeq:  4535
packetSeq:  4536
processed packlist at 1641198748091
receiving_rate:  277000.0
delay:  197.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198748091
avgFrameBetween:  6
psnrStat:  [[546742, 548408, 547957, 547180, 547346, 547232, 545831]]
delayStat:  [[366, 365, 365, 365, 364, 365, 364]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547242.2857142857] [364.85714285714283] [0]
processed state3-5 at 1641198748091
liner_to_log:  tensor([[[0.6393]]]) tensor([[[0.2233]]])
linear_to_log at 1641198748092
listState:  [0.06925, 0.13177777777777777, 0.0, 0.5472422857142857, 0.3648571428571428, 0.0, tensor([[[0.2233]]])]
state_clone_detach at 1641198748092
reward: 0.05211044507413626
state tensor([0.0693, 0.1318, 0.0000, 0.2233], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198748093
state222:  tensor([[0.0693, 0.1318, 0.0000, 0.2233]], device='cuda:0')
policy_old.forwarded at 1641198748094
give action 129============================
log_to_linear:  tensor([[[0.8588]]], device='cuda:0') tensor([[[1.6784]]], device='cuda:0')
log_to_linear action at 1641198748095
bwe changes from to:  [tensor([[[22.0174]]]), tensor([[[36.9536]]])]
step into gymStat at 1641198748095
send bwe to appRecv at 1641198748095
sent bwe to appRecv at 1641198748095
wait for recv string at 1641198748095
recved string at 1641198748296
1
wait for recv [self.estimator, stat] at 1641198748296
recved [self.estimator, stat] at 1641198748296
sorted packlist at 1641198748296
packetSeq:  4537
packetSeq:  4538
packetSeq:  4539
packetSeq:  4540
packetSeq:  4541
packetSeq:  4542
packetSeq:  4543
packetSeq:  4544
processed packlist at 1641198748296
receiving_rate:  251560.0
delay:  199.25
loss_ratio:  0.0
processed state0-2 at 1641198748296
avgFrameBetween:  6
psnrStat:  [[545625, 545641, 546135, 545453, 545111, 544144]]
delayStat:  [[364, 364, 365, 364, 362, 364]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [545351.5] [363.8333333333333] [0]
processed state3-5 at 1641198748296
liner_to_log:  tensor([[[1.6784]]]) tensor([[[0.8588]]])
linear_to_log at 1641198748297
listState:  [0.06289, 0.13283333333333333, 0.0, 0.5453515, 0.36383333333333334, 0.0, tensor([[[0.8588]]])]
state_clone_detach at 1641198748297
reward: 0.019257394076054368
state tensor([0.0629, 0.1328, 0.0000, 0.8588], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198748297
state222:  tensor([[0.0629, 0.1328, 0.0000, 0.8588]], device='cuda:0')
policy_old.forwarded at 1641198748299
give action 130============================
log_to_linear:  tensor([[[0.5396]]], device='cuda:0') tensor([[[1.0641]]], device='cuda:0')
log_to_linear action at 1641198748300
bwe changes from to:  [tensor([[[36.9536]]]), tensor([[[39.3237]]])]
step into gymStat at 1641198748300
send bwe to appRecv at 1641198748300
sent bwe to appRecv at 1641198748300
wait for recv string at 1641198748300
recved string at 1641198748499
1
wait for recv [self.estimator, stat] at 1641198748499
recved [self.estimator, stat] at 1641198748499
sorted packlist at 1641198748499
packetSeq:  4545
packetSeq:  4546
packetSeq:  4547
packetSeq:  4548
packetSeq:  4549
packetSeq:  4550
packetSeq:  4551
packetSeq:  4552
processed packlist at 1641198748499
receiving_rate:  266800.0
delay:  198.625
loss_ratio:  0.0
processed state0-2 at 1641198748499
avgFrameBetween:  6
psnrStat:  [[544409, 543045, 541960, 541853, 542640, 543425]]
delayStat:  [[364, 363, 363, 363, 363, 363]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [542888.6666666666] [363.1666666666667] [0]
processed state3-5 at 1641198748499
liner_to_log:  tensor([[[1.0641]]]) tensor([[[0.5396]]])
linear_to_log at 1641198748500
listState:  [0.0667, 0.13241666666666665, 0.0, 0.5428886666666666, 0.3631666666666667, 0.0, tensor([[[0.5396]]])]
state_clone_detach at 1641198748500
reward: 0.0384670207636873
state tensor([0.0667, 0.1324, 0.0000, 0.5396], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198748500
state222:  tensor([[0.0667, 0.1324, 0.0000, 0.5396]], device='cuda:0')
policy_old.forwarded at 1641198748502
give action 131============================
log_to_linear:  tensor([[[0.0662]]], device='cuda:0') tensor([[[0.5203]]], device='cuda:0')
log_to_linear action at 1641198748503
bwe changes from to:  [tensor([[[39.3237]]]), tensor([[[20.4588]]])]
step into gymStat at 1641198748503
send bwe to appRecv at 1641198748503
sent bwe to appRecv at 1641198748503
wait for recv string at 1641198748503
recved string at 1641198748727
1
wait for recv [self.estimator, stat] at 1641198748727
recved [self.estimator, stat] at 1641198748727
sorted packlist at 1641198748727
packetSeq:  4553
packetSeq:  4554
packetSeq:  4555
packetSeq:  4556
packetSeq:  4557
packetSeq:  4558
packetSeq:  4559
packetSeq:  4560
packetSeq:  4561
packetSeq:  4562
packetSeq:  4563
packetSeq:  4564
processed packlist at 1641198748727
receiving_rate:  304040.0
delay:  197.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198748727
avgFrameBetween:  6
psnrStat:  [[543293, 543779, 544329, 545059, 545004, 545728, 546408]]
delayStat:  [[363, 363, 364, 363, 362, 362, 362]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [544800.0] [362.7142857142857] [0]
processed state3-5 at 1641198748727
liner_to_log:  tensor([[[0.5203]]]) tensor([[[0.0662]]])
linear_to_log at 1641198748727
listState:  [0.07601, 0.1316969696969697, 0.0, 0.5448, 0.3627142857142857, 0.0, tensor([[[0.0662]]])]
state_clone_detach at 1641198748728
reward: 0.08237467197588177
state tensor([0.0760, 0.1317, 0.0000, 0.0662], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198748728
state222:  tensor([[0.0760, 0.1317, 0.0000, 0.0662]], device='cuda:0')
policy_old.forwarded at 1641198748730
give action 132============================
log_to_linear:  tensor([[[0.5572]]], device='cuda:0') tensor([[[1.0936]]], device='cuda:0')
log_to_linear action at 1641198748731
bwe changes from to:  [tensor([[[20.4588]]]), tensor([[[22.3733]]])]
step into gymStat at 1641198748731
send bwe to appRecv at 1641198748731
sent bwe to appRecv at 1641198748731
wait for recv string at 1641198748731
recved string at 1641198748929
1
wait for recv [self.estimator, stat] at 1641198748929
recved [self.estimator, stat] at 1641198748929
sorted packlist at 1641198748929
packetSeq:  4565
packetSeq:  4566
packetSeq:  4567
packetSeq:  4568
packetSeq:  4569
packetSeq:  4570
packetSeq:  4571
packetSeq:  4572
packetSeq:  4573
processed packlist at 1641198748929
receiving_rate:  257519.99999999997
delay:  197.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198748929
avgFrameBetween:  6
psnrStat:  [[547075, 547743, 546291, 548321, 548362, 549268]]
delayStat:  [[362, 362, 362, 362, 362, 362]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547843.3333333334] [362.0] [0]
processed state3-5 at 1641198748929
send 'asking for bwe' at 1641198744402
sent 'asking for bwe' at 1641198744402
send [estimator, stat] at 1641198744402
sent [estimator, stat] at 1641198744402
pc wait for bwe at 1641198744402
pc got bwe at 1641198744407
bandwidth:  300000
pc flushed at 1641198744407
Bwe Sent: 5 at 1641198744407
got request at 1641198744808
processed allFrame at 1641198744808
send 'asking for bwe' at 1641198744808
sent 'asking for bwe' at 1641198744808
send [estimator, stat] at 1641198744808
sent [estimator, stat] at 1641198744808
pc wait for bwe at 1641198744808
pc got bwe at 1641198744813
bandwidth:  300000
pc flushed at 1641198744813
Bwe Sent: 5 at 1641198744813
got request at 1641198745022
processed allFrame at 1641198745022
send 'asking for bwe' at 1641198745022
sent 'asking for bwe' at 1641198745022
send [estimator, stat] at 1641198745022
sent [estimator, stat] at 1641198745022
pc wait for bwe at 1641198745022
pc got bwe at 1641198745026
bandwidth:  300000
pc flushed at 1641198745026
Bwe Sent: 4 at 1641198745026
got request at 1641198745222
processed allFrame at 1641198745222
send 'asking for bwe' at 1641198745222
sent 'asking for bwe' at 1641198745222
send [estimator, stat] at 1641198745222
sent [estimator, stat] at 1641198745222
pc wait for bwe at 1641198745222
pc got bwe at 1641198745227
bandwidth:  300000
pc flushed at 1641198745227
Bwe Sent: 5 at 1641198745227
got request at 1641198745451
processed allFrame at 1641198745451
send 'asking for bwe' at 1641198745451
sent 'asking for bwe' at 1641198745451
send [estimator, stat] at 1641198745451
sent [estimator, stat] at 1641198745451
pc wait for bwe at 1641198745451
pc got bwe at 1641198745456
bandwidth:  300000
pc flushed at 1641198745456
Bwe Sent: 5 at 1641198745456
got request at 1641198745653
processed allFrame at 1641198745653
send 'asking for bwe' at 1641198745653
sent 'asking for bwe' at 1641198745654
send [estimator, stat] at 1641198745654
sent [estimator, stat] at 1641198745654
pc wait for bwe at 1641198745654
pc got bwe at 1641198745658
bandwidth:  300000
pc flushed at 1641198745658
Bwe Sent: 5 at 1641198745658
got request at 1641198745856
processed allFrame at 1641198745856
send 'asking for bwe' at 1641198745856
sent 'asking for bwe' at 1641198745857
send [estimator, stat] at 1641198745857
sent [estimator, stat] at 1641198745857
pc wait for bwe at 1641198745857
pc got bwe at 1641198745863
bandwidth:  300000
pc flushed at 1641198745863
Bwe Sent: 7 at 1641198745863
got request at 1641198746087
processed allFrame at 1641198746087
send 'asking for bwe' at 1641198746087
sent 'asking for bwe' at 1641198746087
send [estimator, stat] at 1641198746087
sent [estimator, stat] at 1641198746087
pc wait for bwe at 1641198746087
pc got bwe at 1641198746093
bandwidth:  300000
pc flushed at 1641198746093
Bwe Sent: 6 at 1641198746093
got request at 1641198746318
processed allFrame at 1641198746319
send 'asking for bwe' at 1641198746319
sent 'asking for bwe' at 1641198746319
send [estimator, stat] at 1641198746319
sent [estimator, stat] at 1641198746319
pc wait for bwe at 1641198746319
pc got bwe at 1641198746323
bandwidth:  300000
pc flushed at 1641198746323
Bwe Sent: 4 at 1641198746323
got request at 1641198746552
processed allFrame at 1641198746552
send 'asking for bwe' at 1641198746552
sent 'asking for bwe' at 1641198746552
send [estimator, stat] at 1641198746552
sent [estimator, stat] at 1641198746552
pc wait for bwe at 1641198746552
pc got bwe at 1641198746557
bandwidth:  300000
pc flushed at 1641198746557
Bwe Sent: 5 at 1641198746557
got request at 1641198746753
processed allFrame at 1641198746753
send 'asking for bwe' at 1641198746753
sent 'asking for bwe' at 1641198746753
send [estimator, stat] at 1641198746753
sent [estimator, stat] at 1641198746753
pc wait for bwe at 1641198746753
pc got bwe at 1641198746758
bandwidth:  300000
pc flushed at 1641198746758
Bwe Sent: 5 at 1641198746758
got request at 1641198746958
processed allFrame at 1641198746958
send 'asking for bwe' at 1641198746958
sent 'asking for bwe' at 1641198746958
send [estimator, stat] at 1641198746958
sent [estimator, stat] at 1641198746958
pc wait for bwe at 1641198746958
pc got bwe at 1641198746962
bandwidth:  300000
pc flushed at 1641198746962
Bwe Sent: 4 at 1641198746962
got request at 1641198747186
processed allFrame at 1641198747186
send 'asking for bwe' at 1641198747186
sent 'asking for bwe' at 1641198747186
send [estimator, stat] at 1641198747186
sent [estimator, stat] at 1641198747186
pc wait for bwe at 1641198747186
pc got bwe at 1641198747191
bandwidth:  300000
pc flushed at 1641198747191
Bwe Sent: 5 at 1641198747191
got request at 1641198747422
processed allFrame at 1641198747422
send 'asking for bwe' at 1641198747422
sent 'asking for bwe' at 1641198747422
send [estimator, stat] at 1641198747422
sent [estimator, stat] at 1641198747422
pc wait for bwe at 1641198747422
pc got bwe at 1641198747426
bandwidth:  300000
pc flushed at 1641198747426
Bwe Sent: 4 at 1641198747426
got request at 1641198747657
processed allFrame at 1641198747657
send 'asking for bwe' at 1641198747657
sent 'asking for bwe' at 1641198747657
send [estimator, stat] at 1641198747657
sent [estimator, stat] at 1641198747657
pc wait for bwe at 1641198747657
pc got bwe at 1641198747662
bandwidth:  300000
pc flushed at 1641198747662
Bwe Sent: 5 at 1641198747662
got request at 1641198747863
processed allFrame at 1641198747863
send 'asking for bwe' at 1641198747863
sent 'asking for bwe' at 1641198747863
send [estimator, stat] at 1641198747863
sent [estimator, stat] at 1641198747863
pc wait for bwe at 1641198747864
pc got bwe at 1641198747868
bandwidth:  300000
pc flushed at 1641198747868
Bwe Sent: 5 at 1641198747868
got request at 1641198748091
processed allFrame at 1641198748091
send 'asking for bwe' at 1641198748091
sent 'asking for bwe' at 1641198748091
send [estimator, stat] at 1641198748091
sent [estimator, stat] at 1641198748091
pc wait for bwe at 1641198748091
pc got bwe at 1641198748095
bandwidth:  300000
pc flushed at 1641198748095
Bwe Sent: 4 at 1641198748095
got request at 1641198748296
processed allFrame at 1641198748296
send 'asking for bwe' at 1641198748296
sent 'asking for bwe' at 1641198748296
send [estimator, stat] at 1641198748296
sent [estimator, stat] at 1641198748296
pc wait for bwe at 1641198748296
pc got bwe at 1641198748300
bandwidth:  300000
pc flushed at 1641198748300
Bwe Sent: 4 at 1641198748300
got request at 1641198748499
processed allFrame at 1641198748499
send 'asking for bwe' at 1641198748499
sent 'asking for bwe' at 1641198748499
send [estimator, stat] at 1641198748499
sent [estimator, stat] at 1641198748499
pc wait for bwe at 1641198748499
pc got bwe at 1641198748503
bandwidth:  300000
pc flushed at 1641198748503
Bwe Sent: 4 at 1641198748503
got request at 1641198748726
processed allFrame at 1641198748727
send 'asking for bwe' at 1641198748727
sent 'asking for bwe' at 1641198748727
send [estimator, stat] at 1641198748727
sent [estimator, stat] at 1641198748727
pc wait for bwe at 1641198748727
pc got bwe at 1641198748731
bandwidth:  300000
pc flushed at 1641198748731
Bwe Sent: 5 at 1641198748731
got request at 1641198748929
processed allFrame at 1641198748929
send 'asking for bwe' at 1641198748929
sent 'asking for bwe' at 1641198748929
send [estimator, stat] at 1641198748929
sent [estimator, stat] at 1641198748929
pc wait for bwe at 1641198748929
pc got bwe at 1641198748933
bandwidth:  300000
pc flushed at 1641198748933
Bwe Sent: 4 at 1641198748933
got request at 1641198749132
processed allFrame at 1641198749132
send 'asking for bwe' at 1641198749132
sent 'asking for bwe' at 1641198749132
send [estimator, stat] at 1641198749132
sent [estimator, stat] at 1641198749132
pc wait for bwe at 1641198749132
pc got bwe at 1641198749136
bandwidth:  300000
pc flushed at 1641198749136
Bwe Sent: 4 at 1641198749136
got request at 1641198749360
processed allFrame at 1641198749360
send 'asking for bwe' at 1641198749360
sent 'asking for bwe' at 1641198749360
send [estimator, stat] at 1641198749360
sent [estimator, stat] at 1641198749360
pc wait for bwe at 1641198749360
pc got bwe at 1641198749364
bandwidth:  300000
pc flushed at 1641198749364
liner_to_log:  tensor([[[1.0936]]]) tensor([[[0.5572]]])
linear_to_log at 1641198748930
listState:  [0.06437999999999999, 0.13185185185185186, 0.0, 0.5478433333333333, 0.362, 0.0, tensor([[[0.5572]]])]
state_clone_detach at 1641198748930
reward: 0.029289565130953654
state tensor([0.0644, 0.1319, 0.0000, 0.5572], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198748930
state222:  tensor([[0.0644, 0.1319, 0.0000, 0.5572]], device='cuda:0')
policy_old.forwarded at 1641198748932
give action 133============================
log_to_linear:  tensor([[[1.0053]]], device='cuda:0') tensor([[[2.0126]]], device='cuda:0')
log_to_linear action at 1641198748933
bwe changes from to:  [tensor([[[22.3733]]]), tensor([[[44.7466]]])]
step into gymStat at 1641198748933
send bwe to appRecv at 1641198748933
sent bwe to appRecv at 1641198748933
wait for recv string at 1641198748933
recved string at 1641198749132
1
wait for recv [self.estimator, stat] at 1641198749132
recved [self.estimator, stat] at 1641198749132
sorted packlist at 1641198749132
packetSeq:  4574
packetSeq:  4575
packetSeq:  4576
packetSeq:  4577
packetSeq:  4578
packetSeq:  4579
packetSeq:  4580
packetSeq:  4581
packetSeq:  4582
processed packlist at 1641198749132
receiving_rate:  264720.0
delay:  198.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198749132
avgFrameBetween:  6
psnrStat:  [[547390, 548413, 546458, 546017, 546198, 546660]]
delayStat:  [[362, 362, 362, 362, 362, 362]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [546856.0] [362.0] [0]
processed state3-5 at 1641198749132
liner_to_log:  tensor([[[2.]]]) tensor([[[1.]]])
linear_to_log at 1641198749132
listState:  [0.06618, 0.13214814814814815, 0.0, 0.546856, 0.362, 0.0, tensor([[[1.]]])]
state_clone_detach at 1641198749133
reward: 0.03685284293263752
state tensor([0.0662, 0.1321, 0.0000, 1.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198749133
state222:  tensor([[0.0662, 0.1321, 0.0000, 1.0000]], device='cuda:0')
policy_old.forwarded at 1641198749135
give action 134============================
log_to_linear:  tensor([[[0.5329]]], device='cuda:0') tensor([[[1.0532]]], device='cuda:0')
log_to_linear action at 1641198749136
bwe changes from to:  [tensor([[[44.7466]]]), tensor([[[47.1270]]])]
step into gymStat at 1641198749136
send bwe to appRecv at 1641198749136
sent bwe to appRecv at 1641198749136
wait for recv string at 1641198749136
recved string at 1641198749360
1
wait for recv [self.estimator, stat] at 1641198749360
recved [self.estimator, stat] at 1641198749360
sorted packlist at 1641198749360
packetSeq:  4583
packetSeq:  4584
packetSeq:  4585
packetSeq:  4586
packetSeq:  4587
packetSeq:  4588
packetSeq:  4589
packetSeq:  4590
packetSeq:  4591
packetSeq:  4592
processed packlist at 1641198749360
receiving_rate:  277880.0
delay:  198.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198749360
avgFrameBetween:  6
psnrStat:  [[546957, 546198, 546703, 547383, 547164, 546804, 547544]]
delayStat:  [[361, 361, 361, 361, 361, 361, 361]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [546964.7142857143] [361.0] [0]
processed state3-5 at 1641198749360
liner_to_log:  tensor([[[1.0532]]]) tensor([[[0.5329]]])
linear_to_log at 1641198749361
listState:  [0.06947, 0.13214814814814815, 0.0, 0.5469647142857144, 0.361, 0.0, tensor([[[0.5329]]])]
state_clone_detach at 1641198749361
reward: 0.052000395927384924
state tensor([0.0695, 0.1321, 0.0000, 0.5329], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198749361
state222:  tensor([[0.0695, 0.1321, 0.0000, 0.5329]], device='cuda:0')
policy_old.forwarded at 1641198749363
give action 135============================
log_to_linear:  tensor([[[0.7633]]], device='cuda:0') tensor([[[1.4776]]], device='cuda:0')
log_to_linear action at 1641198749364
bwe changes from to:  [tensor([[[47.1270]]]), tensor([[[69.6348]]])]
step into gymStat at 1641198749364
send bwe to appRecv at 1641198749364
sent bwe to appRecv at 1641198749364
wait for recv string at 1641198749364
recved string at 1641198749562
1
wait for recv [self.estimator, stat] at 1641198749562
recved [self.estimator, stat] at 1641198749562
sorted packlist at 1641198749562
packetSeq:  4593
packetSeq:  4594
packetSeq:  4595
packetSeq:  4596
packetSeq:  4597
packetSeq:  4598
packetSeq:  4599
packetSeq:  4600
packetSeq:  4601
packetSeq:  4602
processed packlist at 1641198749562
receiving_rate:  259959.99999999997
delay:  196.8
loss_ratio:  0.0
processed state0-2 at 1641198749563
avgFrameBetween:  6
psnrStat:  [[546884, 547153, 547296, 547235, 548689, 547448]]
delayStat:  [[361, 361, 361, 362, 360, 360]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547450.8333333334] [360.8333333333333] [0]
processed state3-5 at 1641198749563
liner_to_log:  tensor([[[1.4776]]]) tensor([[[0.7633]]])
linear_to_log at 1641198749563
listState:  [0.06498999999999999, 0.1312, 0.0, 0.5474508333333333, 0.36083333333333334, 0.0, tensor([[[0.7633]]])]
state_clone_detach at 1641198749563
reward: 0.03412282404460498
state tensor([0.0650, 0.1312, 0.0000, 0.7633], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198749564
state222:  tensor([[0.0650, 0.1312, 0.0000, 0.7633]], device='cuda:0')
policy_old.forwarded at 1641198749565
give action 136============================
log_to_linear:  tensor([[[0.4947]]], device='cuda:0') tensor([[[0.9916]]], device='cuda:0')
log_to_linear action at 1641198749566
bwe changes from to:  [tensor([[[69.6348]]]), tensor([[[69.0502]]])]
step into gymStat at 1641198749567
send bwe to appRecv at 1641198749567
sent bwe to appRecv at 1641198749567
wait for recv string at 1641198749567
recved string at 1641198749763
1
wait for recv [self.estimator, stat] at 1641198749763
recved [self.estimator, stat] at 1641198749763
sorted packlist at 1641198749763
packetSeq:  4603
packetSeq:  4604
packetSeq:  4605
packetSeq:  4606
packetSeq:  4607
packetSeq:  4608
packetSeq:  4609
packetSeq:  4610
packetSeq:  4611
processed packlist at 1641198749763
receiving_rate:  272200.0
delay:  197.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198749764
avgFrameBetween:  6
psnrStat:  [[548362, 549653, 550256, 548643, 549376, 549556]]
delayStat:  [[362, 360, 360, 361, 360, 360]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549307.6666666666] [360.5] [0]
processed state3-5 at 1641198749764
liner_to_log:  tensor([[[0.9916]]]) tensor([[[0.4947]]])
linear_to_log at 1641198749764
listState:  [0.06805, 0.13155555555555556, 0.0, 0.5493076666666666, 0.3605, 0.0, tensor([[[0.4947]]])]
state_clone_detach at 1641198749764
reward: 0.04728717637500257
state tensor([0.0680, 0.1316, 0.0000, 0.4947], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198749765
state222:  tensor([[0.0680, 0.1316, 0.0000, 0.4947]], device='cuda:0')
policy_old.forwarded at 1641198749766
give action 137============================
log_to_linear:  tensor([[[0.5668]]], device='cuda:0') tensor([[[1.1100]]], device='cuda:0')
log_to_linear action at 1641198749767
bwe changes from to:  [tensor([[[69.0502]]]), tensor([[[76.6460]]])]
step into gymStat at 1641198749768
send bwe to appRecv at 1641198749768
sent bwe to appRecv at 1641198749768
wait for recv string at 1641198749768
recved string at 1641198749990
1
wait for recv [self.estimator, stat] at 1641198749990
recved [self.estimator, stat] at 1641198749990
sorted packlist at 1641198749990
packetSeq:  4612
packetSeq:  4613
packetSeq:  4614
packetSeq:  4615
packetSeq:  4616
packetSeq:  4617
packetSeq:  4618
packetSeq:  4619
processed packlist at 1641198749990
receiving_rate:  282280.0
delay:  197.5
loss_ratio:  0.0
processed state0-2 at 1641198749991
avgFrameBetween:  6
psnrStat:  [[551054, 550126, 550248, 550237, 550551, 549900, 549886]]
delayStat:  [[361, 360, 360, 361, 359, 359, 360]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550286.0] [360.0] [0]
processed state3-5 at 1641198749991
liner_to_log:  tensor([[[1.1100]]]) tensor([[[0.5668]]])
linear_to_log at 1641198749991
listState:  [0.07057, 0.13166666666666665, 0.0, 0.550286, 0.36, 0.0, tensor([[[0.5668]]])]
state_clone_detach at 1641198749991
reward: 0.0584252492879771
state tensor([0.0706, 0.1317, 0.0000, 0.5668], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198749992
state222:  tensor([[0.0706, 0.1317, 0.0000, 0.5668]], device='cuda:0')
policy_old.forwarded at 1641198749994
give action 138============================
log_to_linear:  tensor([[[0.6170]]], device='cuda:0') tensor([[[1.1978]]], device='cuda:0')
log_to_linear action at 1641198749994
bwe changes from to:  [tensor([[[76.6460]]]), tensor([[[91.8087]]])]
step into gymStat at 1641198749995
send bwe to appRecv at 1641198749995
sent bwe to appRecv at 1641198749995
wait for recv string at 1641198749995
recved string at 1641198750222
1
wait for recv [self.estimator, stat] at 1641198750222
recved [self.estimator, stat] at 1641198750222
sorted packlist at 1641198750222
packetSeq:  4620
packetSeq:  4621
packetSeq:  4622
packetSeq:  4623
packetSeq:  4624
packetSeq:  4625
packetSeq:  4626
packetSeq:  4627
processed packlist at 1641198750222
receiving_rate:  260240.0
delay:  199.14285714285714
loss_ratio:  0.0
processed state0-2 at 1641198750222
avgFrameBetween:  6
psnrStat:  [[550083, 550823, 549445, 550709, 550906, 551628, 551453]]
delayStat:  [[359, 359, 360, 359, 359, 360, 359]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550721.0] [359.2857142857143] [0]
processed state3-5 at 1641198750222
liner_to_log:  tensor([[[1.1978]]]) tensor([[[0.6170]]])
linear_to_log at 1641198750222
listState:  [0.06506, 0.13276190476190475, 0.0, 0.550721, 0.35928571428571426, 0.0, tensor([[[0.6170]]])]
state_clone_detach at 1641198750223
reward: 0.02976645695398633
state tensor([0.0651, 0.1328, 0.0000, 0.6170], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198750223
state222:  tensor([[0.0651, 0.1328, 0.0000, 0.6170]], device='cuda:0')
policy_old.forwarded at 1641198750225
give action 139============================
log_to_linear:  tensor([[[0.4601]]], device='cuda:0') tensor([[[0.9382]]], device='cuda:0')
log_to_linear action at 1641198750226
bwe changes from to:  [tensor([[[91.8087]]]), tensor([[[86.1378]]])]
step into gymStat at 1641198750226
send bwe to appRecv at 1641198750226
sent bwe to appRecv at 1641198750226
wait for recv string at 1641198750226
recved string at 1641198750424
1
wait for recv [self.estimator, stat] at 1641198750424
recved [self.estimator, stat] at 1641198750424
sorted packlist at 1641198750424
packetSeq:  4628
packetSeq:  4629
packetSeq:  4630
packetSeq:  4631
packetSeq:  4632
packetSeq:  4633
packetSeq:  4634
packetSeq:  4635
packetSeq:  4636
packetSeq:  4637
processed packlist at 1641198750424
receiving_rate:  289840.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198750424
avgFrameBetween:  6
psnrStat:  [[551415, 551057, 551730, 551876, 551472, 551546]]
delayStat:  [[359, 360, 359, 358, 359, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551516.0] [358.8333333333333] [0]
processed state3-5 at 1641198750424
liner_to_log:  tensor([[[0.9382]]]) tensor([[[0.4601]]])
linear_to_log at 1641198750425
listState:  [0.07246, 0.132, 0.0, 0.551516, 0.35883333333333334, 0.0, tensor([[[0.4601]]])]
state_clone_detach at 1641198750425
reward: 0.06588712889721421
state tensor([0.0725, 0.1320, 0.0000, 0.4601], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198750425
state222:  tensor([[0.0725, 0.1320, 0.0000, 0.4601]], device='cuda:0')
policy_old.forwarded at 1641198750427
give action 140============================
log_to_linear:  tensor([[[0.3832]]], device='cuda:0') tensor([[[0.8280]]], device='cuda:0')
log_to_linear action at 1641198750428
bwe changes from to:  [tensor([[[86.1378]]]), tensor([[[71.3215]]])]
step into gymStat at 1641198750428
send bwe to appRecv at 1641198750428
sent bwe to appRecv at 1641198750428
wait for recv string at 1641198750428
recved string at 1641198750633
1
wait for recv [self.estimator, stat] at 1641198750633
recved [self.estimator, stat] at 1641198750633
sorted packlist at 1641198750633
packetSeq:  4638
packetSeq:  4639
packetSeq:  4640
packetSeq:  4641
packetSeq:  4642
packetSeq:  4643
packetSeq:  4644
packetSeq:  4645
packetSeq:  4646
packetSeq:  4647
processed packlist at 1641198750633
receiving_rate:  274080.0
delay:  197.3
loss_ratio:  0.0
processed state0-2 at 1641198750633
avgFrameBetween:  6
psnrStat:  [[553039, 553170, 553592, 554079, 554517, 554314]]
delayStat:  [[358, 359, 358, 358, 360, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553785.1666666666] [358.5] [0]
processed state3-5 at 1641198750633
liner_to_log:  tensor([[[0.8280]]]) tensor([[[0.3832]]])
linear_to_log at 1641198750633
listState:  [0.06852, 0.13153333333333334, 0.0, 0.5537851666666667, 0.3585, 0.0, tensor([[[0.3832]]])]
state_clone_detach at 1641198750634
reward: 0.04951005593682467
state tensor([0.0685, 0.1315, 0.0000, 0.3832], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198750634
state222:  tensor([[0.0685, 0.1315, 0.0000, 0.3832]], device='cuda:0')
policy_old.forwarded at 1641198750636
give action 141============================
log_to_linear:  tensor([[[0.1347]]], device='cuda:0') tensor([[[0.5626]]], device='cuda:0')
log_to_linear action at 1641198750637
bwe changes from to:  [tensor([[[71.3215]]]), tensor([[[40.1233]]])]
step into gymStat at 1641198750637
send bwe to appRecv at 1641198750637
sent bwe to appRecv at 1641198750637
wait for recv string at 1641198750637
recved string at 1641198750859
1
wait for recv [self.estimator, stat] at 1641198750859
recved [self.estimator, stat] at 1641198750859
sorted packlist at 1641198750859
packetSeq:  4648
packetSeq:  4649
packetSeq:  4650
packetSeq:  4651
packetSeq:  4652
packetSeq:  4653
packetSeq:  4654
packetSeq:  4655
packetSeq:  4656
packetSeq:  4657
processed packlist at 1641198750859
receiving_rate:  293440.0
delay:  198.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198750859
avgFrameBetween:  6
psnrStat:  [[555163, 555178, 555569, 556477, 557300, 558383, 558492]]
delayStat:  [[358, 359, 359, 358, 359, 357, 357]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556651.7142857143] [358.14285714285717] [0]
processed state3-5 at 1641198750859
liner_to_log:  tensor([[[0.5626]]]) tensor([[[0.1347]]])
linear_to_log at 1641198750860
listState:  [0.07336, 0.1322962962962963, 0.0, 0.5566517142857144, 0.35814285714285715, 0.0, tensor([[[0.1347]]])]
state_clone_detach at 1641198750860
reward: 0.06898614332359104
state tensor([0.0734, 0.1323, 0.0000, 0.1347], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198750860
state222:  tensor([[0.0734, 0.1323, 0.0000, 0.1347]], device='cuda:0')
policy_old.forwarded at 1641198750862
give action 142============================
log_to_linear:  tensor([[[0.4808]]], device='cuda:0') tensor([[[0.9699]]], device='cuda:0')
log_to_linear action at 1641198750863
bwe changes from to:  [tensor([[[40.1233]]]), tensor([[[38.9156]]])]
step into gymStat at 1641198750863
send bwe to appRecv at 1641198750863
sent bwe to appRecv at 1641198750863
wait for recv string at 1641198750863
recved string at 1641198751059
1
wait for recv [self.estimator, stat] at 1641198751059
recved [self.estimator, stat] at 1641198751059
sorted packlist at 1641198751059
packetSeq:  4658
packetSeq:  4659
packetSeq:  4660
packetSeq:  4661
packetSeq:  4662
packetSeq:  4663
packetSeq:  4664
packetSeq:  4665
packetSeq:  4666
processed packlist at 1641198751059
receiving_rate:  251920.0
delay:  197.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198751059
avgFrameBetween:  6
psnrStat:  [[559032, 559118, 559571, 559183, 558410, 558655]]
delayStat:  [[358, 357, 357, 358, 357, 357]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558994.8333333334] [357.3333333333333] [0]
processed state3-5 at 1641198751059
liner_to_log:  tensor([[[0.9699]]]) tensor([[[0.4808]]])
linear_to_log at 1641198751060
listState:  [0.06298, 0.13162962962962962, 0.0, 0.5589948333333333, 0.35733333333333334, 0.0, tensor([[[0.4808]]])]
state_clone_detach at 1641198751060
reward: 0.023299007059183974
state tensor([0.0630, 0.1316, 0.0000, 0.4808], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198751061
state222:  tensor([[0.0630, 0.1316, 0.0000, 0.4808]], device='cuda:0')
policy_old.forwarded at 1641198751062
give action 143============================
log_to_linear:  tensor([[[0.5327]]], device='cuda:0') tensor([[[1.0528]]], device='cuda:0')
log_to_linear action at 1641198751063
bwe changes from to:  [tensor([[[38.9156]]]), tensor([[[40.9685]]])]
step into gymStat at 1641198751063
send bwe to appRecv at 1641198751063
sent bwe to appRecv at 1641198751063
wait for recv string at 1641198751063
recved string at 1641198751263
1
wait for recv [self.estimator, stat] at 1641198751263
recved [self.estimator, stat] at 1641198751263
sorted packlist at 1641198751263
packetSeq:  4667
packetSeq:  4668
packetSeq:  4669
packetSeq:  4670
packetSeq:  4671
packetSeq:  4672
packetSeq:  4673
packetSeq:  4674
packetSeq:  4675
packetSeq:  4676
packetSeq:  4677
processed packlist at 1641198751263
receiving_rate:  288880.0
delay:  196.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198751263
avgFrameBetween:  6
psnrStat:  [[558824, 559329, 558994, 559135, 558557, 558691]]
delayStat:  [[358, 357, 358, 357, 357, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558921.6666666666] [357.5] [0]
processed state3-5 at 1641198751263
liner_to_log:  tensor([[[1.0528]]]) tensor([[[0.5327]]])
linear_to_log at 1641198751264
listState:  [0.07222, 0.1312121212121212, 0.0, 0.5589216666666666, 0.3575, 0.0, tensor([[[0.5327]]])]
state_clone_detach at 1641198751264
reward: 0.06718283261463287
state tensor([0.0722, 0.1312, 0.0000, 0.5327], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198751264
state222:  tensor([[0.0722, 0.1312, 0.0000, 0.5327]], device='cuda:0')
policy_old.forwarded at 1641198751266
give action 144============================
log_to_linear:  tensor([[[0.5147]]], device='cuda:0') tensor([[[1.0234]]], device='cuda:0')
log_to_linear action at 1641198751267
bwe changes from to:  [tensor([[[40.9685]]]), tensor([[[41.9283]]])]
step into gymStat at 1641198751267
send bwe to appRecv at 1641198751267
sent bwe to appRecv at 1641198751267
wait for recv string at 1641198751267
recved string at 1641198751463
1
wait for recv [self.estimator, stat] at 1641198751463
recved [self.estimator, stat] at 1641198751464
sorted packlist at 1641198751464
packetSeq:  4678
packetSeq:  4679
packetSeq:  4680
packetSeq:  4681
packetSeq:  4682
packetSeq:  4683
packetSeq:  4684
packetSeq:  4685
packetSeq:  4686
packetSeq:  4687
processed packlist at 1641198751464
receiving_rate:  284760.0
delay:  198.3
loss_ratio:  0.0
processed state0-2 at 1641198751464
avgFrameBetween:  6
psnrStat:  [[558695, 559250, 559548, 560148, 560779, 561517]]
delayStat:  [[357, 357, 357, 357, 356, 358]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559989.5] [357.0] [0]
processed state3-5 at 1641198751464
liner_to_log:  tensor([[[1.0234]]]) tensor([[[0.5147]]])
linear_to_log at 1641198751464
listState:  [0.07119, 0.1322, 0.0, 0.5599895, 0.357, 0.0, tensor([[[0.5147]]])]
state_clone_detach at 1641198751464
reward: 0.05961427815160519
state tensor([0.0712, 0.1322, 0.0000, 0.5147], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198751465
state222:  tensor([[0.0712, 0.1322, 0.0000, 0.5147]], device='cuda:0')
policy_old.forwarded at 1641198751467
give action 145============================
log_to_linear:  tensor([[[0.0373]]], device='cuda:0') tensor([[[0.5082]]], device='cuda:0')
log_to_linear action at 1641198751467
bwe changes from to:  [tensor([[[41.9283]]]), tensor([[[21.3064]]])]
step into gymStat at 1641198751468
send bwe to appRecv at 1641198751468
sent bwe to appRecv at 1641198751468
wait for recv string at 1641198751468
recved string at 1641198751665
1
wait for recv [self.estimator, stat] at 1641198751665
recved [self.estimator, stat] at 1641198751666
sorted packlist at 1641198751666
packetSeq:  4688
packetSeq:  4689
packetSeq:  4690
packetSeq:  4691
packetSeq:  4692
packetSeq:  4693
packetSeq:  4694
packetSeq:  4695
packetSeq:  4696
packetSeq:  4697
processed packlist at 1641198751666
receiving_rate:  287280.0
delay:  197.4
loss_ratio:  0.0
processed state0-2 at 1641198751666
avgFrameBetween:  6
psnrStat:  [[562348, 561527, 561322, 561201, 561527, 560197]]
delayStat:  [[356, 356, 357, 356, 356, 357]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561353.6666666666] [356.3333333333333] [0]
processed state3-5 at 1641198751666
liner_to_log:  tensor([[[0.5082]]]) tensor([[[0.0373]]])
linear_to_log at 1641198751666
listState:  [0.07182, 0.1316, 0.0, 0.5613536666666666, 0.35633333333333334, 0.0, tensor([[[0.0373]]])]
state_clone_detach at 1641198751666
reward: 0.06423507361615771
state tensor([0.0718, 0.1316, 0.0000, 0.0373], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198751667
state222:  tensor([[0.0718, 0.1316, 0.0000, 0.0373]], device='cuda:0')
policy_old.forwarded at 1641198751669
give action 146============================
log_to_linear:  tensor([[[0.3219]]], device='cuda:0') tensor([[[0.7488]]], device='cuda:0')
log_to_linear action at 1641198751669
bwe changes from to:  [tensor([[[21.3064]]]), tensor([[[15.9549]]])]
step into gymStat at 1641198751670
send bwe to appRecv at 1641198751670
sent bwe to appRecv at 1641198751670
wait for recv string at 1641198751670
recved string at 1641198751896
1
wait for recv [self.estimator, stat] at 1641198751896
recved [self.estimator, stat] at 1641198751896
sorted packlist at 1641198751896
packetSeq:  4698
packetSeq:  4699
packetSeq:  4700
packetSeq:  4701
packetSeq:  4702
packetSeq:  4703
packetSeq:  4704
packetSeq:  4705
packetSeq:  4706
packetSeq:  4707
processed packlist at 1641198751896
receiving_rate:  265960.0
delay:  197.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198751896
avgFrameBetween:  6
psnrStat:  [[561383, 559164, 559373, 558645, 558766, 558247, 557854]]
delayStat:  [[356, 356, 357, 356, 356, 357, 355]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559061.7142857143] [356.14285714285717] [0]
processed state3-5 at 1641198751896
liner_to_log:  tensor([[[0.7488]]]) tensor([[[0.3219]]])
linear_to_log at 1641198751897
listState:  [0.06649, 0.13185185185185186, 0.0, 0.5590617142857143, 0.35614285714285715, 0.0, tensor([[[0.3219]]])]
state_clone_detach at 1641198751897
reward: 0.03918543958525772
state tensor([0.0665, 0.1319, 0.0000, 0.3219], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198751897
state222:  tensor([[0.0665, 0.1319, 0.0000, 0.3219]], device='cuda:0')
policy_old.forwarded at 1641198751899
give action 147============================
log_to_linear:  tensor([[[0.6842]]], device='cuda:0') tensor([[[1.3220]]], device='cuda:0')
log_to_linear action at 1641198751900
bwe changes from to:  [tensor([[[15.9549]]]), tensor([[[21.0928]]])]
step into gymStat at 1641198751900
send bwe to appRecv at 1641198751900
sent bwe to appRecv at 1641198751900
wait for recv string at 1641198751900
recved string at 1641198752122
1
wait for recv [self.estimator, stat] at 1641198752122
recved [self.estimator, stat] at 1641198752122
sorted packlist at 1641198752122
packetSeq:  4708
packetSeq:  4709
packetSeq:  4710
packetSeq:  4711
packetSeq:  4712
packetSeq:  4713
packetSeq:  4714
packetSeq:  4715
processed packlist at 1641198752123
receiving_rate:  279920.0
delay:  198.875
loss_ratio:  0.0
processed state0-2 at 1641198752123
avgFrameBetween:  6
psnrStat:  [[558360, 557116, 556275, 556490, 554369, 555599, 554285]]
delayStat:  [[355, 356, 355, 355, 357, 355, 355]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556070.5714285715] [355.42857142857144] [0]
processed state3-5 at 1641198752123
liner_to_log:  tensor([[[1.3220]]]) tensor([[[0.6842]]])
linear_to_log at 1641198752123
listState:  [0.06998, 0.13258333333333333, 0.0, 0.5560705714285715, 0.3554285714285714, 0.0, tensor([[[0.6842]]])]
state_clone_detach at 1641198752123
reward: 0.053009080336756165
state tensor([0.0700, 0.1326, 0.0000, 0.6842], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198752124
state222:  tensor([[0.0700, 0.1326, 0.0000, 0.6842]], device='cuda:0')
policy_old.forwarded at 1641198752126
give action 148============================
log_to_linear:  tensor([[[0.4688]]], device='cuda:0') tensor([[[0.9514]]], device='cuda:0')
log_to_linear action at 1641198752126
bwe changes from to:  [tensor([[[21.0928]]]), tensor([[[20.0678]]])]
step into gymStat at 1641198752127
send bwe to appRecv at 1641198752127
sent bwe to appRecv at 1641198752127
wait for recv string at 1641198752127
recved string at 1641198752325
1
wait for recv [self.estimator, stat] at 1641198752325
recved [self.estimator, stat] at 1641198752325
sorted packlist at 1641198752325
packetSeq:  4716
packetSeq:  4717
packetSeq:  4718
packetSeq:  4719
packetSeq:  4720
packetSeq:  4721
packetSeq:  4722
packetSeq:  4723
packetSeq:  4724
packetSeq:  4725
processed packlist at 1641198752325
receiving_rate:  305680.0
delay:  198.1
loss_ratio:  0.0
processed state0-2 at 1641198752325
avgFrameBetween:  6
psnrStat:  [[554940, 554483, 554732, 555110, 555897, 556405]]
delayStat:  [[355, 355, 355, 356, 355, 355]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555261.1666666666] [355.1666666666667] [0]
processed state3-5 at 1641198752325
liner_to_log:  tensor([[[0.9514]]]) tensor([[[0.4688]]])
linear_to_log at 1641198752325
listState:  [0.07642, 0.13206666666666667, 0.0, 0.5552611666666666, 0.3551666666666667, 0.0, tensor([[[0.4688]]])]
state_clone_detach at 1641198752326
reward: 0.08303900268011044
state tensor([0.0764, 0.1321, 0.0000, 0.4688], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198752326
state222:  tensor([[0.0764, 0.1321, 0.0000, 0.4688]], device='cuda:0')
policy_old.forwarded at 1641198752328
give action 149============================
log_to_linear:  tensor([[[0.9013]]], device='cuda:0') tensor([[[1.7723]]], device='cuda:0')
log_to_linear action at 1641198752329
bwe changes from to:  [tensor([[[20.0678]]]), tensor([[[35.5665]]])]
step into gymStat at 1641198752329
send bwe to appRecv at 1641198752329
sent bwe to appRecv at 1641198752329
wait for recv string at 1641198752329
recved string at 1641198752529
1
wait for recv [self.estimator, stat] at 1641198752529
recved [self.estimator, stat] at 1641198752530
sorted packlist at 1641198752530
packetSeq:  4726
packetSeq:  4727
packetSeq:  4728
packetSeq:  4729
packetSeq:  4730
packetSeq:  4731
packetSeq:  4732
processed packlist at 1641198752530
receiving_rate:  245600.0
delay:  198.14285714285714
loss_ratio:  0.0
processed state0-2 at 1641198752530
avgFrameBetween:  6
psnrStat:  [[556229, 556382, 557015, 557337, 558060, 557590]]
delayStat:  [[356, 355, 355, 355, 354, 353]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557102.1666666666] [354.6666666666667] [0]
processed state3-5 at 1641198752530
liner_to_log:  tensor([[[1.7723]]]) tensor([[[0.9013]]])
linear_to_log at 1641198752530
listState:  [0.0614, 0.1320952380952381, 0.0, 0.5571021666666667, 0.3546666666666667, 0.0, tensor([[[0.9013]]])]
state_clone_detach at 1641198752530
reward: 0.014299224849930292
state tensor([0.0614, 0.1321, 0.0000, 0.9013], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198752531
state222:  tensor([[0.0614, 0.1321, 0.0000, 0.9013]], device='cuda:0')
policy_old.forwarded at 1641198752533
give action 150============================
log_to_linear:  tensor([[[0.4941]]], device='cuda:0') tensor([[[0.9907]]], device='cuda:0')
log_to_linear action at 1641198752533
bwe changes from to:  [tensor([[[35.5665]]]), tensor([[[35.2351]]])]
step into gymStat at 1641198752534
send bwe to appRecv at 1641198752534
sent bwe to appRecv at 1641198752534
wait for recv string at 1641198752534
recved string at 1641198752754
1
wait for recv [self.estimator, stat] at 1641198752754
recved [self.estimator, stat] at 1641198752755
sorted packlist at 1641198752755
packetSeq:  4733
packetSeq:  4734
packetSeq:  4735
packetSeq:  4736
packetSeq:  4737
packetSeq:  4738
packetSeq:  4739
packetSeq:  4740
packetSeq:  4741
packetSeq:  4742
packetSeq:  4743
processed packlist at 1641198752755
receiving_rate:  307480.0
delay:  194.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198752755
avgFrameBetween:  6
psnrStat:  [[557532, 558291, 556959, 557473, 555948, 556037, 556859]]
delayStat:  [[354, 354, 354, 354, 354, 354, 354]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557014.1428571428] [354.0] [0]
processed state3-5 at 1641198752755
liner_to_log:  tensor([[[0.9907]]]) tensor([[[0.4941]]])
linear_to_log at 1641198752755
listState:  [0.07687, 0.1296969696969697, 0.0, 0.5570141428571428, 0.354, 0.0, tensor([[[0.4941]]])]
state_clone_detach at 1641198752755
reward: 0.09208851714252836
state tensor([0.0769, 0.1297, 0.0000, 0.4941], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198752756
state222:  tensor([[0.0769, 0.1297, 0.0000, 0.4941]], device='cuda:0')
policy_old.forwarded at 1641198752758
give action 151============================
log_to_linear:  tensor([[[0.2622]]], device='cuda:0') tensor([[[0.6797]]], device='cuda:0')
log_to_linear action at 1641198752758
bwe changes from to:  [tensor([[[35.2351]]]), tensor([[[23.9500]]])]
step into gymStat at 1641198752759
send bwe to appRecv at 1641198752759
sent bwe to appRecv at 1641198752759
wait for recv string at 1641198752759
recved string at 1641198752956
1
wait for recv [self.estimator, stat] at 1641198752956
recved [self.estimator, stat] at 1641198752956
sorted packlist at 1641198752956
packetSeq:  4744
packetSeq:  4745
packetSeq:  4746
packetSeq:  4747
packetSeq:  4748
packetSeq:  4749
packetSeq:  4750
packetSeq:  4751
packetSeq:  4752
packetSeq:  4753
processed packlist at 1641198752957
receiving_rate:  289040.0
delay:  192.2
loss_ratio:  0.0
processed state0-2 at 1641198752957
avgFrameBetween:  6
psnrStat:  [[556932, 557630, 558326, 559165, 560430, 560591]]
delayStat:  [[355, 355, 354, 354, 355, 353]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558845.6666666666] [354.3333333333333] [0]
processed state3-5 at 1641198752957
liner_to_log:  tensor([[[0.6797]]]) tensor([[[0.2622]]])
linear_to_log at 1641198752957
listState:  [0.07226, 0.12813333333333332, 0.0, 0.5588456666666666, 0.35433333333333333, 0.0, tensor([[[0.2622]]])]
state_clone_detach at 1641198752957
reward: 0.07659731707759776
state tensor([0.0723, 0.1281, 0.0000, 0.2622], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198752958
state222:  tensor([[0.0723, 0.1281, 0.0000, 0.2622]], device='cuda:0')
policy_old.forwarded at 1641198752960
give action 152============================
log_to_linear:  tensor([[[0.3475]]], device='cuda:0') tensor([[[0.7808]]], device='cuda:0')
log_to_linear action at 1641198752960
bwe changes from to:  [tensor([[[23.9500]]]), tensor([[[18.7009]]])]
step into gymStat at 1641198752961
send bwe to appRecv at 1641198752961
sent bwe to appRecv at 1641198752961
wait for recv string at 1641198752961
recved string at 1641198753187
1
wait for recv [self.estimator, stat] at 1641198753187
recved [self.estimator, stat] at 1641198753188
sorted packlist at 1641198753188
packetSeq:  4754
packetSeq:  4755
packetSeq:  4756
packetSeq:  4757
packetSeq:  4758
packetSeq:  4759
packetSeq:  4760
packetSeq:  4761
packetSeq:  4762
processed packlist at 1641198753188
receiving_rate:  309400.0
delay:  192.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198753188
avgFrameBetween:  6
psnrStat:  [[561710, 561602, 561798, 562234, 561706, 561551, 561828]]
delayStat:  [[353, 353, 353, 353, 355, 353, 354]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  Bwe Sent: 4 at 1641198749364
got request at 1641198749562
processed allFrame at 1641198749562
send 'asking for bwe' at 1641198749562
sent 'asking for bwe' at 1641198749562
send [estimator, stat] at 1641198749562
sent [estimator, stat] at 1641198749562
pc wait for bwe at 1641198749562
pc got bwe at 1641198749567
bandwidth:  300000
pc flushed at 1641198749567
Bwe Sent: 5 at 1641198749567
got request at 1641198749763
processed allFrame at 1641198749763
send 'asking for bwe' at 1641198749763
sent 'asking for bwe' at 1641198749763
send [estimator, stat] at 1641198749763
sent [estimator, stat] at 1641198749763
pc wait for bwe at 1641198749763
pc got bwe at 1641198749768
bandwidth:  300000
pc flushed at 1641198749768
Bwe Sent: 5 at 1641198749768
got request at 1641198749990
processed allFrame at 1641198749990
send 'asking for bwe' at 1641198749990
sent 'asking for bwe' at 1641198749990
send [estimator, stat] at 1641198749990
sent [estimator, stat] at 1641198749990
pc wait for bwe at 1641198749990
pc got bwe at 1641198749995
bandwidth:  300000
pc flushed at 1641198749995
Bwe Sent: 5 at 1641198749995
got request at 1641198750221
processed allFrame at 1641198750222
send 'asking for bwe' at 1641198750222
sent 'asking for bwe' at 1641198750222
send [estimator, stat] at 1641198750222
sent [estimator, stat] at 1641198750222
pc wait for bwe at 1641198750222
pc got bwe at 1641198750226
bandwidth:  300000
pc flushed at 1641198750226
Bwe Sent: 5 at 1641198750226
got request at 1641198750424
processed allFrame at 1641198750424
send 'asking for bwe' at 1641198750424
sent 'asking for bwe' at 1641198750424
send [estimator, stat] at 1641198750424
sent [estimator, stat] at 1641198750424
pc wait for bwe at 1641198750424
pc got bwe at 1641198750428
bandwidth:  300000
pc flushed at 1641198750428
Bwe Sent: 4 at 1641198750428
got request at 1641198750633
processed allFrame at 1641198750633
send 'asking for bwe' at 1641198750633
sent 'asking for bwe' at 1641198750633
send [estimator, stat] at 1641198750633
sent [estimator, stat] at 1641198750633
pc wait for bwe at 1641198750633
pc got bwe at 1641198750637
bandwidth:  300000
pc flushed at 1641198750637
Bwe Sent: 4 at 1641198750637
got request at 1641198750859
processed allFrame at 1641198750859
send 'asking for bwe' at 1641198750859
sent 'asking for bwe' at 1641198750859
send [estimator, stat] at 1641198750859
sent [estimator, stat] at 1641198750859
pc wait for bwe at 1641198750859
pc got bwe at 1641198750863
bandwidth:  300000
pc flushed at 1641198750863
Bwe Sent: 4 at 1641198750863
got request at 1641198751059
processed allFrame at 1641198751059
send 'asking for bwe' at 1641198751059
sent 'asking for bwe' at 1641198751059
send [estimator, stat] at 1641198751059
sent [estimator, stat] at 1641198751059
pc wait for bwe at 1641198751059
pc got bwe at 1641198751063
bandwidth:  300000
pc flushed at 1641198751063
Bwe Sent: 4 at 1641198751063
got request at 1641198751263
processed allFrame at 1641198751263
send 'asking for bwe' at 1641198751263
sent 'asking for bwe' at 1641198751263
send [estimator, stat] at 1641198751263
sent [estimator, stat] at 1641198751263
pc wait for bwe at 1641198751263
pc got bwe at 1641198751267
bandwidth:  300000
pc flushed at 1641198751267
Bwe Sent: 4 at 1641198751267
got request at 1641198751463
processed allFrame at 1641198751463
send 'asking for bwe' at 1641198751463
sent 'asking for bwe' at 1641198751463
send [estimator, stat] at 1641198751463
sent [estimator, stat] at 1641198751463
pc wait for bwe at 1641198751464
pc got bwe at 1641198751468
bandwidth:  300000
pc flushed at 1641198751468
Bwe Sent: 5 at 1641198751468
got request at 1641198751665
processed allFrame at 1641198751665
send 'asking for bwe' at 1641198751665
sent 'asking for bwe' at 1641198751665
send [estimator, stat] at 1641198751665
sent [estimator, stat] at 1641198751665
pc wait for bwe at 1641198751665
pc got bwe at 1641198751670
bandwidth:  300000
pc flushed at 1641198751670
Bwe Sent: 5 at 1641198751670
got request at 1641198751896
processed allFrame at 1641198751896
send 'asking for bwe' at 1641198751896
sent 'asking for bwe' at 1641198751896
send [estimator, stat] at 1641198751896
sent [estimator, stat] at 1641198751896
pc wait for bwe at 1641198751896
pc got bwe at 1641198751900
bandwidth:  300000
pc flushed at 1641198751900
Bwe Sent: 4 at 1641198751900
got request at 1641198752122
processed allFrame at 1641198752122
send 'asking for bwe' at 1641198752122
sent 'asking for bwe' at 1641198752122
send [estimator, stat] at 1641198752122
sent [estimator, stat] at 1641198752122
pc wait for bwe at 1641198752122
pc got bwe at 1641198752127
bandwidth:  300000
pc flushed at 1641198752127
Bwe Sent: 5 at 1641198752127
got request at 1641198752325
processed allFrame at 1641198752325
send 'asking for bwe' at 1641198752325
sent 'asking for bwe' at 1641198752325
send [estimator, stat] at 1641198752325
sent [estimator, stat] at 1641198752325
pc wait for bwe at 1641198752325
pc got bwe at 1641198752329
bandwidth:  300000
pc flushed at 1641198752329
Bwe Sent: 4 at 1641198752329
got request at 1641198752529
processed allFrame at 1641198752529
send 'asking for bwe' at 1641198752529
sent 'asking for bwe' at 1641198752529
send [estimator, stat] at 1641198752529
sent [estimator, stat] at 1641198752529
pc wait for bwe at 1641198752530
pc got bwe at 1641198752534
bandwidth:  300000
pc flushed at 1641198752534
Bwe Sent: 5 at 1641198752534
got request at 1641198752754
processed allFrame at 1641198752754
send 'asking for bwe' at 1641198752754
sent 'asking for bwe' at 1641198752754
send [estimator, stat] at 1641198752754
sent [estimator, stat] at 1641198752754
pc wait for bwe at 1641198752755
pc got bwe at 1641198752759
bandwidth:  300000
pc flushed at 1641198752759
Bwe Sent: 5 at 1641198752759
got request at 1641198752956
processed allFrame at 1641198752956
send 'asking for bwe' at 1641198752956
sent 'asking for bwe' at 1641198752956
send [estimator, stat] at 1641198752956
sent [estimator, stat] at 1641198752956
pc wait for bwe at 1641198752956
pc got bwe at 1641198752961
bandwidth:  300000
pc flushed at 1641198752961
Bwe Sent: 5 at 1641198752961
got request at 1641198753187
processed allFrame at 1641198753187
send 'asking for bwe' at 1641198753187
sent 'asking for bwe' at 1641198753187
send [estimator, stat] at 1641198753187
sent [estimator, stat] at 1641198753188
pc wait for bwe at 1641198753188
pc got bwe at 1641198753192
bandwidth:  300000
pc flushed at 1641198753192
Bwe Sent: 5 at 1641198753192
got request at 1641198753389
processed allFrame at 1641198753389
send 'asking for bwe' at 1641198753389
sent 'asking for bwe' at 1641198753389
send [estimator, stat] at 1641198753389
sent [estimator, stat] at 1641198753389
pc wait for bwe at 1641198753389
pc got bwe at 1641198753393
bandwidth:  300000
pc flushed at 1641198753393
Bwe Sent: 4 at 1641198753393
got request at 1641198753591
processed allFrame at 1641198753592
send 'asking for bwe' at 1641198753592
sent 'asking for bwe' at 1641198753592
send [estimator, stat] at 1641198753592
sent [estimator, stat] at 1641198753592
pc wait for bwe at 1641198753592
pc got bwe at 1641198753596
bandwidth:  300000
pc flushed at 1641198753596
Bwe Sent: 5 at 1641198753596
got request at 1641198753820
processed allFrame at 1641198753820
send 'asking for bwe' at 1641198753820
sent 'asking for bwe' at 1641198753820
send [estimator, stat] at 1641198753820
sent [estimator, stat] at 1641198753820
pc wait for bwe at 1641198753820
pc got bwe at 1641198753825
bandwidth:  300000
pc flushed at 1641198753825
Bwe Sent: 5 at 1641198753825
got request at 1641198754054
processed allFrame at 1641198754054
send 'asking for bwe' at 1641198754054
sent 'asking for bwe' at 1641198754054
send [estimator, stat] at 1641198754054
sent [estimator, stat] at 1641198754055
pc wait for bwe at 1641198754055
pc got bwe at 1641198754059
bandwidth:  300000
pc flushed at 1641198754059
Bwe Sent: 5 at 1641198754059
got request at 1641198754285
processed allFrame at 1641198754285
send 'asking for bwe' at 1641198754285
sent 'asking for bwe' at 1641198754285
send [estimator, stat] at 1641198754285
sent [estimator, stat] at 1641198754286
[561775.5714285715] [353.42857142857144] [0]
processed state3-5 at 1641198753188
liner_to_log:  tensor([[[0.7808]]]) tensor([[[0.3475]]])
linear_to_log at 1641198753188
listState:  [0.07735, 0.1285185185185185, 0.0, 0.5617755714285715, 0.3534285714285714, 0.0, tensor([[[0.3475]]])]
state_clone_detach at 1641198753188
reward: 0.09768676218781219
state tensor([0.0773, 0.1285, 0.0000, 0.3475], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198753189
state222:  tensor([[0.0773, 0.1285, 0.0000, 0.3475]], device='cuda:0')
policy_old.forwarded at 1641198753191
give action 153============================
log_to_linear:  tensor([[[0.4179]]], device='cuda:0') tensor([[[0.8762]]], device='cuda:0')
log_to_linear action at 1641198753191
bwe changes from to:  [tensor([[[18.7009]]]), tensor([[[16.3865]]])]
step into gymStat at 1641198753192
send bwe to appRecv at 1641198753192
sent bwe to appRecv at 1641198753192
wait for recv string at 1641198753192
recved string at 1641198753389
1
wait for recv [self.estimator, stat] at 1641198753389
recved [self.estimator, stat] at 1641198753389
sorted packlist at 1641198753389
packetSeq:  4763
packetSeq:  4764
packetSeq:  4765
packetSeq:  4766
packetSeq:  4767
packetSeq:  4768
packetSeq:  4769
packetSeq:  4770
packetSeq:  4771
packetSeq:  4772
packetSeq:  4773
processed packlist at 1641198753389
receiving_rate:  281840.0
delay:  191.72727272727272
loss_ratio:  0.0
processed state0-2 at 1641198753389
avgFrameBetween:  6
psnrStat:  [[560538, 560452, 560084, 559298, 558779, 558531]]
delayStat:  [[354, 353, 353, 353, 353, 353]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559613.6666666666] [353.1666666666667] [0]
processed state3-5 at 1641198753389
liner_to_log:  tensor([[[0.8762]]]) tensor([[[0.4179]]])
linear_to_log at 1641198753390
listState:  [0.07046, 0.1278181818181818, 0.0, 0.5596136666666667, 0.3531666666666667, 0.0, tensor([[[0.4179]]])]
state_clone_detach at 1641198753390
reward: 0.06947451916432384
state tensor([0.0705, 0.1278, 0.0000, 0.4179], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198753390
state222:  tensor([[0.0705, 0.1278, 0.0000, 0.4179]], device='cuda:0')
policy_old.forwarded at 1641198753392
give action 154============================
log_to_linear:  tensor([[[0.9137]]], device='cuda:0') tensor([[[1.8001]]], device='cuda:0')
log_to_linear action at 1641198753393
bwe changes from to:  [tensor([[[16.3865]]]), tensor([[[29.4981]]])]
step into gymStat at 1641198753393
send bwe to appRecv at 1641198753393
sent bwe to appRecv at 1641198753393
wait for recv string at 1641198753393
recved string at 1641198753592
1
wait for recv [self.estimator, stat] at 1641198753592
recved [self.estimator, stat] at 1641198753592
sorted packlist at 1641198753592
packetSeq:  4774
packetSeq:  4775
packetSeq:  4776
packetSeq:  4777
packetSeq:  4778
packetSeq:  4779
packetSeq:  4780
packetSeq:  4781
packetSeq:  4782
processed packlist at 1641198753592
receiving_rate:  265120.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198753592
avgFrameBetween:  6
psnrStat:  [[557609, 557171, 556910, 556556, 556794, 556986]]
delayStat:  [[352, 352, 352, 353, 352, 352]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557004.3333333334] [352.1666666666667] [0]
processed state3-5 at 1641198753592
liner_to_log:  tensor([[[1.8001]]]) tensor([[[0.9137]]])
linear_to_log at 1641198753592
listState:  [0.06628, 0.12814814814814815, 0.0, 0.5570043333333333, 0.3521666666666667, 0.0, tensor([[[0.9137]]])]
state_clone_detach at 1641198753593
reward: 0.0493189347761645
state tensor([0.0663, 0.1281, 0.0000, 0.9137], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198753593
state222:  tensor([[0.0663, 0.1281, 0.0000, 0.9137]], device='cuda:0')
policy_old.forwarded at 1641198753595
give action 155============================
log_to_linear:  tensor([[[0.6066]]], device='cuda:0') tensor([[[1.1792]]], device='cuda:0')
log_to_linear action at 1641198753596
bwe changes from to:  [tensor([[[29.4981]]]), tensor([[[34.7844]]])]
step into gymStat at 1641198753596
send bwe to appRecv at 1641198753596
sent bwe to appRecv at 1641198753596
wait for recv string at 1641198753596
recved string at 1641198753820
1
wait for recv [self.estimator, stat] at 1641198753820
recved [self.estimator, stat] at 1641198753820
sorted packlist at 1641198753820
packetSeq:  4783
packetSeq:  4784
packetSeq:  4785
packetSeq:  4786
packetSeq:  4787
packetSeq:  4788
packetSeq:  4789
packetSeq:  4790
packetSeq:  4791
packetSeq:  4792
processed packlist at 1641198753820
receiving_rate:  282840.0
delay:  192.2
loss_ratio:  0.0
processed state0-2 at 1641198753820
avgFrameBetween:  6
psnrStat:  [[556146, 556410, 556334, 555731, 556316, 556505, 556321]]
delayStat:  [[353, 352, 352, 353, 352, 352, 352]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556251.8571428572] [352.2857142857143] [0]
processed state3-5 at 1641198753820
liner_to_log:  tensor([[[1.1792]]]) tensor([[[0.6066]]])
linear_to_log at 1641198753821
listState:  [0.07071, 0.12813333333333332, 0.0, 0.5562518571428572, 0.35228571428571426, 0.0, tensor([[[0.6066]]])]
state_clone_detach at 1641198753821
reward: 0.06965616417260939
state tensor([0.0707, 0.1281, 0.0000, 0.6066], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198753822
state222:  tensor([[0.0707, 0.1281, 0.0000, 0.6066]], device='cuda:0')
policy_old.forwarded at 1641198753823
give action 156============================
log_to_linear:  tensor([[[0.6707]]], device='cuda:0') tensor([[[1.2964]]], device='cuda:0')
log_to_linear action at 1641198753824
bwe changes from to:  [tensor([[[34.7844]]]), tensor([[[45.0957]]])]
step into gymStat at 1641198753824
send bwe to appRecv at 1641198753824
sent bwe to appRecv at 1641198753825
wait for recv string at 1641198753825
recved string at 1641198754054
1
wait for recv [self.estimator, stat] at 1641198754054
recved [self.estimator, stat] at 1641198754055
sorted packlist at 1641198754055
packetSeq:  4793
packetSeq:  4794
packetSeq:  4795
packetSeq:  4796
packetSeq:  4797
packetSeq:  4798
packetSeq:  4799
packetSeq:  4800
packetSeq:  4801
packetSeq:  4802
packetSeq:  4803
processed packlist at 1641198754055
receiving_rate:  311440.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198754055
avgFrameBetween:  6
psnrStat:  [[556410, 556176, 557285, 557184, 556937, 556726, 556399]]
delayStat:  [[351, 351, 351, 351, 352, 351, 351]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556731.0] [351.14285714285717] [0]
processed state3-5 at 1641198754055
liner_to_log:  tensor([[[1.2964]]]) tensor([[[0.6707]]])
linear_to_log at 1641198754055
listState:  [0.07786, 0.12793939393939394, 0.0, 0.556731, 0.35114285714285715, 0.0, tensor([[[0.6707]]])]
state_clone_detach at 1641198754055
reward: 0.10160821456541325
state tensor([0.0779, 0.1279, 0.0000, 0.6707], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198754056
state222:  tensor([[0.0779, 0.1279, 0.0000, 0.6707]], device='cuda:0')
policy_old.forwarded at 1641198754058
give action 157============================
log_to_linear:  tensor([[[0.3549]]], device='cuda:0') tensor([[[0.7905]]], device='cuda:0')
log_to_linear action at 1641198754058
bwe changes from to:  [tensor([[[45.0957]]]), tensor([[[35.6469]]])]
step into gymStat at 1641198754059
send bwe to appRecv at 1641198754059
sent bwe to appRecv at 1641198754059
wait for recv string at 1641198754059
recved string at 1641198754286
1
wait for recv [self.estimator, stat] at 1641198754286
recved [self.estimator, stat] at 1641198754286
sorted packlist at 1641198754286
packetSeq:  4804
packetSeq:  4805
packetSeq:  4806
packetSeq:  4807
packetSeq:  4808
packetSeq:  4809
packetSeq:  4810
packetSeq:  4811
packetSeq:  4812
packetSeq:  4813
packetSeq:  4814
processed packlist at 1641198754286
receiving_rate:  303760.0
delay:  191.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198754286
avgFrameBetween:  6
psnrStat:  [[557670, 557645, 558592, 559362, 560387, 560871, 560332]]
delayStat:  [[352, 351, 352, 352, 351, 351, 352]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559265.5714285715] [351.57142857142856] [0]
processed state3-5 at 1641198754286
liner_to_log:  tensor([[[0.7905]]]) tensor([[[0.3549]]])
linear_to_log at 1641198754286
listState:  [0.07594, 0.12787878787878787, 0.0, 0.5592655714285715, 0.35157142857142853, 0.0, tensor([[[0.3549]]])]
state_clone_detach at 1641198754286
reward: 0.09352591357850903
state tensor([0.0759, 0.1279, 0.0000, 0.3549], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198754287
state222:  tensor([[0.0759, 0.1279, 0.0000, 0.3549]], device='cuda:0')
policy_old.forwarded at 1641198754289
give action 158============================
log_to_linear:  tensor([[[0.4066]]], device='cuda:0') tensor([[[0.8603]]], device='cuda:0')
log_to_linear action at 1641198754289
bwe changes from to:  [tensor([[[35.6469]]]), tensor([[[30.6685]]])]
step into gymStat at 1641198754290
send bwe to appRecv at 1641198754290
sent bwe to appRecv at 1641198754290
wait for recv string at 1641198754290
recved string at 1641198754520
1
wait for recv [self.estimator, stat] at 1641198754520
recved [self.estimator, stat] at 1641198754520
sorted packlist at 1641198754520
packetSeq:  4815
packetSeq:  4816
packetSeq:  4817
packetSeq:  4818
packetSeq:  4819
packetSeq:  4820
packetSeq:  4821
packetSeq:  4822
packetSeq:  4823
packetSeq:  4824
packetSeq:  4825
processed packlist at 1641198754520
receiving_rate:  276520.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198754520
avgFrameBetween:  6
psnrStat:  [[561051, 561275, 561784, 562270, 562474, 561649, 562022]]
delayStat:  [[351, 350, 351, 350, 350, 352, 350]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561789.2857142857] [350.57142857142856] [0]
processed state3-5 at 1641198754520
liner_to_log:  tensor([[[0.8603]]]) tensor([[[0.4066]]])
linear_to_log at 1641198754521
listState:  [0.06913, 0.1282962962962963, 0.0, 0.5617892857142857, 0.35057142857142853, 0.0, tensor([[[0.4066]]])]
state_clone_detach at 1641198754521
reward: 0.062008150928087424
state tensor([0.0691, 0.1283, 0.0000, 0.4066], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198754521
state222:  tensor([[0.0691, 0.1283, 0.0000, 0.4066]], device='cuda:0')
policy_old.forwarded at 1641198754523
give action 159============================
log_to_linear:  tensor([[[0.3004]]], device='cuda:0') tensor([[[0.7230]]], device='cuda:0')
log_to_linear action at 1641198754524
bwe changes from to:  [tensor([[[30.6685]]]), tensor([[[22.1736]]])]
step into gymStat at 1641198754524
send bwe to appRecv at 1641198754524
sent bwe to appRecv at 1641198754524
wait for recv string at 1641198754524
recved string at 1641198754722
1
wait for recv [self.estimator, stat] at 1641198754722
recved [self.estimator, stat] at 1641198754722
sorted packlist at 1641198754722
packetSeq:  4826
packetSeq:  4827
packetSeq:  4828
packetSeq:  4829
packetSeq:  4830
packetSeq:  4831
packetSeq:  4832
packetSeq:  4833
packetSeq:  4834
processed packlist at 1641198754722
receiving_rate:  286960.0
delay:  192.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198754722
avgFrameBetween:  6
psnrStat:  [[561363, 561685, 561943, 562825, 562183, 562128, 560780]]
delayStat:  [[350, 351, 350, 350, 350, 350, 336]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561843.8571428572] [348.14285714285717] [0]
processed state3-5 at 1641198754722
liner_to_log:  tensor([[[0.7230]]]) tensor([[[0.3004]]])
linear_to_log at 1641198754722
listState:  [0.07174, 0.12837037037037036, 0.0, 0.5618438571428571, 0.34814285714285714, 0.0, tensor([[[0.3004]]])]
state_clone_detach at 1641198754723
reward: 0.07356650047481306
state tensor([0.0717, 0.1284, 0.0000, 0.3004], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198754723
state222:  tensor([[0.0717, 0.1284, 0.0000, 0.3004]], device='cuda:0')
policy_old.forwarded at 1641198754725
give action 160============================
log_to_linear:  tensor([[[0.2282]]], device='cuda:0') tensor([[[0.6443]]], device='cuda:0')
log_to_linear action at 1641198754726
bwe changes from to:  [tensor([[[22.1736]]]), tensor([[[14.2854]]])]
step into gymStat at 1641198754726
send bwe to appRecv at 1641198754726
sent bwe to appRecv at 1641198754726
wait for recv string at 1641198754726
recved string at 1641198754924
1
wait for recv [self.estimator, stat] at 1641198754924
recved [self.estimator, stat] at 1641198754924
sorted packlist at 1641198754924
packetSeq:  4835
packetSeq:  4836
packetSeq:  4837
packetSeq:  4838
packetSeq:  4839
packetSeq:  4840
packetSeq:  4841
packetSeq:  4842
packetSeq:  4843
processed packlist at 1641198754924
receiving_rate:  253400.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198754925
avgFrameBetween:  6
psnrStat:  [[561481, 561010, 560890, 560483, 561164, 561873]]
delayStat:  [[335, 336, 335, 335, 334, 335]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561150.1666666666] [335.0] [0]
processed state3-5 at 1641198754925
liner_to_log:  tensor([[[0.6443]]]) tensor([[[0.2282]]])
linear_to_log at 1641198754925
listState:  [0.06335, 0.128, 0.0, 0.5611501666666666, 0.335, 0.0, tensor([[[0.2282]]])]
state_clone_detach at 1641198754925
reward: 0.03595449890338154
state tensor([0.0633, 0.1280, 0.0000, 0.2282], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198754926
state222:  tensor([[0.0633, 0.1280, 0.0000, 0.2282]], device='cuda:0')
policy_old.forwarded at 1641198754928
give action 161============================
log_to_linear:  tensor([[[0.4397]]], device='cuda:0') tensor([[[0.9079]]], device='cuda:0')
log_to_linear action at 1641198754928
bwe changes from to:  [tensor([[[14.2854]]]), tensor([[[12.9697]]])]
step into gymStat at 1641198754929
send bwe to appRecv at 1641198754929
sent bwe to appRecv at 1641198754929
wait for recv string at 1641198754929
recved string at 1641198755155
1
wait for recv [self.estimator, stat] at 1641198755155
recved [self.estimator, stat] at 1641198755155
sorted packlist at 1641198755155
packetSeq:  4844
packetSeq:  4845
packetSeq:  4846
packetSeq:  4847
packetSeq:  4848
packetSeq:  4849
packetSeq:  4850
packetSeq:  4851
packetSeq:  4852
packetSeq:  4853
processed packlist at 1641198755155
receiving_rate:  296280.0
delay:  192.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198755155
avgFrameBetween:  6
psnrStat:  [[561375, 562395, 562073, 562971, 563043, 562347, 562183]]
delayStat:  [[335, 334, 336, 334, 334, 335, 334]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [562341.0] [334.57142857142856] [0]
processed state3-5 at 1641198755155
liner_to_log:  tensor([[[0.9079]]]) tensor([[[0.4397]]])
linear_to_log at 1641198755156
listState:  [0.07407, 0.1285185185185185, 0.0, 0.562341, 0.3345714285714286, 0.0, tensor([[[0.4397]]])]
state_clone_detach at 1641198755156
reward: 0.08344689261323085
state tensor([0.0741, 0.1285, 0.0000, 0.4397], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198755157
state222:  tensor([[0.0741, 0.1285, 0.0000, 0.4397]], device='cuda:0')
policy_old.forwarded at 1641198755158
give action 162============================
log_to_linear:  tensor([[[0.4885]]], device='cuda:0') tensor([[[0.9820]]], device='cuda:0')
log_to_linear action at 1641198755159
bwe changes from to:  [tensor([[[12.9697]]]), tensor([[[12.7357]]])]
step into gymStat at 1641198755159
send bwe to appRecv at 1641198755159
sent bwe to appRecv at 1641198755159
wait for recv string at 1641198755159
recved string at 1641198755386
1
wait for recv [self.estimator, stat] at 1641198755386
recved [self.estimator, stat] at 1641198755386
sorted packlist at 1641198755386
packetSeq:  4854
packetSeq:  4855
packetSeq:  4856
packetSeq:  4857
packetSeq:  4858
packetSeq:  4859
packetSeq:  4860
packetSeq:  4861
packetSeq:  4862
processed packlist at 1641198755386
receiving_rate:  256839.99999999997
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198755386
avgFrameBetween:  6
psnrStat:  [[560810, 561493, 561468, 561234, 561522, 560984, 561356]]
delayStat:  [[334, 335, 334, 334, 334, 334, 333]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561266.7142857143] [334.0] [0]
processed state3-5 at 1641198755386
liner_to_log:  tensor([[[0.9820]]]) tensor([[[0.4885]]])
linear_to_log at 1641198755387
listState:  [0.06420999999999999, 0.12833333333333333, 0.0, 0.5612667142857143, 0.334, 0.0, tensor([[[0.4885]]])]
state_clone_detach at 1641198755387
reward: 0.03904067244107662
state tensor([0.0642, 0.1283, 0.0000, 0.4885], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198755387
state222:  tensor([[0.0642, 0.1283, 0.0000, 0.4885]], device='cuda:0')
policy_old.forwarded at 1641198755389
give action 163============================
log_to_linear:  tensor([[[0.0743]]], device='cuda:0') tensor([[[0.5244]]], device='cuda:0')
log_to_linear action at 1641198755390
bwe changes from to:  [tensor([[[12.7357]]]), tensor([[[6.6784]]])]
step into gymStat at 1641198755390
send bwe to appRecv at 1641198755390
sent bwe to appRecv at 1641198755390
wait for recv string at 1641198755390
recved string at 1641198755586
1
wait for recv [self.estimator, stat] at 1641198755586
recved [self.estimator, stat] at 1641198755587
sorted packlist at 1641198755587
packetSeq:  4863
packetSeq:  4864
packetSeq:  4865
packetSeq:  4866
packetSeq:  4867
packetSeq:  4868
packetSeq:  4869
packetSeq:  4870
packetSeq:  4871
packetSeq:  4872
packetSeq:  4873
packetSeq:  4874
processed packlist at 1641198755587
receiving_rate:  317240.0
delay:  191.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198755587
avgFrameBetween:  6
psnrStat:  [[561007, 561557, 560291, 560211, 560545, 560222]]
delayStat:  [[334, 333, 333, 334, 333, 333]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560638.8333333334] [333.3333333333333] [0]
processed state3-5 at 1641198755587
liner_to_log:  tensor([[[0.5244]]]) tensor([[[0.0743]]])
linear_to_log at 1641198755587
listState:  [0.07931, 0.12777777777777777, 0.0, 0.5606388333333334, 0.3333333333333333, 0.0, tensor([[[0.0743]]])]
state_clone_detach at 1641198755587
reward: 0.10825961270520595
state tensor([0.0793, 0.1278, 0.0000, 0.0743], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198755588
state222:  tensor([[0.0793, 0.1278, 0.0000, 0.0743]], device='cuda:0')
policy_old.forwarded at 1641198755590
give action 164============================
log_to_linear:  tensor([[[0.5072]]], device='cuda:0') tensor([[[1.0114]]], device='cuda:0')
log_to_linear action at 1641198755590
bwe changes from to:  [tensor([[[6.6784]]]), tensor([[[6.7547]]])]
step into gymStat at 1641198755591
send bwe to appRecv at 1641198755591
sent bwe to appRecv at 1641198755591
wait for recv string at 1641198755591
recved string at 1641198755790
1
wait for recv [self.estimator, stat] at 1641198755790
recved [self.estimator, stat] at 1641198755791
sorted packlist at 1641198755791
packetSeq:  4875
packetSeq:  4876
packetSeq:  4877
packetSeq:  4878
packetSeq:  4879
packetSeq:  4880
packetSeq:  4881
packetSeq:  4882
packetSeq:  4883
processed packlist at 1641198755791
receiving_rate:  281360.0
delay:  192.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198755791
avgFrameBetween:  6
psnrStat:  [[559963, 559417, 558171, 556885, 556336, 555812]]
delayStat:  [[334, 333, 333, 334, 333, 333]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557764.0] [333.3333333333333] [0]
processed state3-5 at 1641198755791
liner_to_log:  tensor([[[1.0114]]]) tensor([[[0.5072]]])
linear_to_log at 1641198755791
listState:  [0.07034, 0.1285925925925926, 0.0, 0.557764, 0.3333333333333333, 0.0, tensor([[[0.5072]]])]
state_clone_detach at 1641198755791
reward: 0.06660952608193627
state tensor([0.0703, 0.1286, 0.0000, 0.5072], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198755792
state222:  tensor([[0.0703, 0.1286, 0.0000, 0.5072]], device='cuda:0')
policy_old.forwarded at 1641198755794
give action 165============================
log_to_linear:  tensor([[[0.3435]]], device='cuda:0') tensor([[[0.7757]]], device='cuda:0')
log_to_linear action at 1641198755794
bwe changes from to:  [tensor([[[6.7547]]]), tensor([[[5.2398]]])]
step into gymStat at 1641198755795
send bwe to appRecv at 1641198755795
sent bwe to appRecv at 1641198755795
wait for recv string at 1641198755795
recved string at 1641198756021
1
wait for recv [self.estimator, stat] at 1641198756021
recved [self.estimator, stat] at 1641198756022
sorted packlist at 1641198756022
packetSeq:  4884
packetSeq:  4885
packetSeq:  4886
packetSeq:  4887
packetSeq:  4888
packetSeq:  4889
packetSeq:  4890
packetSeq:  4891
processed packlist at 1641198756022
receiving_rate:  236280.0
delay:  192.42857142857142
loss_ratio:  0.0
processed state0-2 at 1641198756022
avgFrameBetween:  6
psnrStat:  [[555002, 553819, 552489, 550594, 550905, 552058, 550492]]
delayStat:  [[334, 333, 333, 333, 333, 332, 332]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552194.1428571428] [332.85714285714283] [0]
processed state3-5 at 1641198756022
liner_to_log:  tensor([[[0.7757]]]) tensor([[[0.3435]]])
linear_to_log at 1641198756022
listState:  [0.05907, 0.12828571428571428, 0.0, 0.5521941428571429, 0.33285714285714285, 0.0, tensor([[[0.3435]]])]
state_clone_detach at 1641198756022
reward: 0.014337060238065247
state tensor([0.0591, 0.1283, 0.0000, 0.3435], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198756023
state222:  tensor([[0.0591, 0.1283, 0.0000, 0.3435]], device='cuda:0')
policy_old.forwarded at 1641198756025
give action 166============================
log_to_linear:  tensor([[[0.6355]]], device='cuda:0') tensor([[[1.2312]]], device='cuda:0')
log_to_linear action at 1641198756025
bwe changes from to:  [tensor([[[5.2398]]]), tensor([[[6.4514]]])]
step into gymStat at 1641198756026
send bwe to appRecv at 1641198756026
sent bwe to appRecv at 1641198756026
wait for recv string at 1641198756026
recved string at 1641198756253
1
wait for recv [self.estimator, stat] at 1641198756253
recved [self.estimator, stat] at 1641198756254
sorted packlist at 1641198756254
packetSeq:  4892
packetSeq:  4893
packetSeq:  4894
packetSeq:  4895
packetSeq:  4896
packetSeq:  4897
packetSeq:  4898
packetSeq:  4899
processed packlist at 1641198756254
receiving_rate:  215800.0
delay:  191.85714285714286
loss_ratio:  0.0
processed state0-2 at 1641198756254
avgFrameBetween:  6
psnrStat:  [[550180, 549456, 549530, 548882, 549151, 548912, 549703]]
delayStat:  [[333, 332, 333, 333, 332, 333, 332]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549402.0] [332.57142857142856] [0]
processed state3-5 at 1641198756254
liner_to_log:  tensor([[[1.2312]]]) tensor([[[0.6355]]])
linear_to_log at 1641198756254
listState:  [0.05395, 0.1279047619047619, 0.0, 0.549402, 0.3325714285714286, 0.0, tensor([[[0.6355]]])]
state_clone_detach at 1641198756254
reward: -0.010337021961489223
state tensor([0.0540, 0.1279, 0.0000, 0.6355], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198756255
state222:  tensor([[0.0540, 0.1279, 0.0000, 0.6355]], device='cuda:0')
policy_old.forwarded at 1641198756257
give action 167============================
log_to_linear:  tensor([[[0.3481]]], device='cuda:0') tensor([[[0.7817]]], device='cuda:0')
log_to_linear action at 1641198756257
bwe changes from to:  [tensor([[[6.4514]]]), tensor([[[5.0431]]])]
step into gymStat at 1641198756258
send bwe to appRecv at 1641198756258
sent bwe to appRecv at 1641198756258
wait for recv string at 1641198756258
recved string at 1641198756456
1
wait for recv [self.estimator, stat] at 1641198756456
recved [self.estimator, stat] at 1641198756457
sorted packlist at 1641198756457
packetSeq:  4900
packetSeq:  4901
packetSeq:  4902
packetSeq:  4903
packetSeq:  4904
packetSeq:  4905
packetSeq:  4906
packetSeq:  4907
packetSeq:  4908
packetSeq:  4909
packetSeq:  4910
processed packlist at 1641198756457
receiving_rate:  306080.0
delay:  191.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198756457
avgFrameBetween:  6
psnrStat:  [[549646, 550670, 552295, 553316, 554638, 555740]]
delayStat:  [[332, 333, 332, 332, 333, 332]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552717.5] [332.3333333333333] [0]
processed state3-5 at 1641198756457
liner_to_log:  tensor([[[0.7817]]]) tensor([[[0.3481]]])
linear_to_log at 1641198756457
listState:  [0.07652, 0.12787878787878787, 0.0, 0.5527175, 0.3323333333333333, 0.0, tensor([[[0.3481]]])]
state_clone_detach at 1641198756457
reward: 0.0960343869216268
state tensor([0.0765, 0.1279, 0.0000, 0.3481], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198756458
state222:  tensor([[0.0765, 0.1279, 0.0000, 0.3481]], device='cuda:0')
policy_old.forwarded at 1641198756460
give action 168============================
log_to_linear:  tensor([[[0.4126]]], device='cuda:0') tensor([[[0.8687]]], device='cuda:0')
log_to_linear action at 1641198756460
bwe changes from to:  [tensor([[[5.0431]]]), tensor([[[4.3811]]])]
step into gymStat at 1641198756461
send bwe to appRecv at 1641198756461
sent bwe to appRecv at 1641198756461
wait for recv string at 1641198756461
recved string at 1641198756685
1
wait for recv [self.estimator, stat] at 1641198756685
recved [self.estimator, stat] at 1641198756685
sorted packlist at 1641198756685
packetSeq:  4911
packetSeq:  4912
packetSeq:  4913
packetSeq:  4914
packetSeq:  4915
packetSeq:  4916
packetSeq:  4917
packetSeq:  4918
packetSeq:  4919
packetSeq:  4920
packetSeq:  4921
processed packlist at 1641198756685
receiving_rate:  297080.0
delay:  192.0909090909091
loss_ratio:  0.0
processed state0-2 at 1641198756685
avgFrameBetween:  6
psnrStat:  [[556077, 556950, 556835, 557228, 557020, 556169, 556824]]
delayStat:  [[332, 332, 333, 332, 331, 331, 331]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556729.0] [331.7142857142857] [0]
processed state3-5 at 1641198756685
liner_to_log:  tensor([[[0.8687]]]) tensor([[[0.4126]]])
linear_to_log at 1641198756686
listState:  [0.07427, 0.12806060606060607, 0.0, 0.556729, 0.33171428571428574, 0.0, tensor([[[0.4126]]])]
state_clone_detach at 1641198756686
reward: 0.08569865750884414
state tensor([0.0743, 0.1281, 0.0000, 0.4126], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198756686
state222:  tensor([[0.0743, 0.1281, 0.0000, 0.4126]], device='cuda:0')
policy_old.forwarded at 1641198756688
give action 169============================
log_to_linear:  tensor([[[0.4702]]], device='cuda:0') tensor([[[0.9536]]], device='cuda:0')
log_to_linear action at 1641198756689
bwe changes from to:  [tensor([[[4.3811]]]), tensor([[[4.1779]]])]
step into gymStat at 1641198756689
send bwe to appRecv at 1641198756689
sent bwe to appRecv at 1641198756689
wait for recv string at 1641198756689
recved string at 1641198756888
1
wait for recv [self.estimator, stat] at 1641198756888
recved [self.estimator, stat] at 1641198756888
sorted packlist at 1641198756888
packetSeq:  4922
packetSeq:  4923
packetSeq:  4924
packetSeq:  4925
packetSeq:  4926
packetSeq:  4927
packetSeq:  4928
packetSeq:  4929
packetSeq:  4930
packetSeq:  4931
processed packlist at 1641198756888
receiving_rate:  281840.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198756889
avgFrameBetween:  6
psnrStat:  [[558585, 559090, 559442, 559222, 558462, 558044]]
delayStat:  [[331, 332, 331, 332, 332, 331]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558807.5] [331.5] [0]
processed state3-5 at 1641198756889
liner_to_log:  tensor([[[0.9536]]]) tensor([[[0.4702]]])
linear_to_log at 1641198756889
listState:  [0.07046, 0.12806666666666666, 0.0, 0.5588075, 0.3315, 0.0, tensor([[[0.4702]]])]
state_clone_detach at 1641198756889
reward: 0.0687290646188693
state tensor([0.0705, 0.1281, 0.0000, 0.4702], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198756890
state222:  tensor([[0.0705, 0.1281, 0.0000, 0.4702]], device='cuda:0')
policy_old.forwarded at 1641198756891
give action 170============================
log_to_linear:  tensor([[[0.2921]]], device='cuda:0') tensor([[[0.7133]]], device='cuda:0')
log_to_linear action at 1641198756892
bwe changes from to:  [tensor([[[4.1779]]]), tensor([[[2.9802]]])]
step into gymStat at 1641198756893
send bwe to appRecv at 1641198756893
sent bwe to appRecv at 1641198756893
wait for recv string at 1641198756893
recved string at 1641198757091
1
wait for recv [self.estimator, stat] at 1641198757091
recved [self.estimator, stat] at 1641198757092
sorted packlist at 1641198757092
packetSeq:  4932
packetSeq:  4933
packetSeq:  4934
packetSeq:  4935
packetSeq:  4936
packetSeq:  4937
packetSeq:  4938
packetSeq:  4939
packetSeq:  4940
packetSeq:  4941
processed packlist at 1641198757092
receiving_rate:  267280.0
delay:  191.8
loss_ratio:  0.0
processed state0-2 at 1641198757092
avgFrameBetween:  6
psnrStat:  [[557884, 558449, 557946, 556967, 556906, 556627]]
delayStat:  [[331, 331, 331, 332, 331, 331]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557463.1666666666] [331.1666666666667] [0]
processed state3-5 at 1641198757092
liner_to_log:  tensor([[[0.7133]]]) tensor([[[0.2921]]])
linear_to_log at 1641198757092
listState:  [0.06682, 0.12786666666666668, 0.0, 0.5574631666666666, 0.33116666666666666, 0.0, tensor([[[0.2921]]])]
state_clone_detach at 1641198757092
reward: 0.052674037668925866
state tensor([0.0668, 0.1279, 0.0000, 0.2921], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198757093
state222:  tensor([[0.0668, 0.1279, 0.0000, 0.2921]], device='cuda:0')
policy_old.forwarded at 1641198757095
give action 171============================
log_to_linear:  tensor([[[0.3468]]], device='cuda:0') tensor([[[0.7800]]], device='cuda:0')
log_to_linear action at 1641198757095
bwe changes from to:  [tensor([[[2.9802]]]), tensor([[[2.3246]]])]
step into gymStat at 1641198757096
send bwe to appRecv at 1641198757096
sent bwe to appRecv at 1641198757096
wait for recv string at 1641198757096
recved string at 1641198757321
1
wait for recv [self.estimator, stat] at 1641198757321
recved [self.estimator, stat] at 1641198757321
sorted packlist at 1641198757321
packetSeq:  4942
packetSeq:  4943
packetSeq:  4944
packetSeq:  4945
packetSeq:  4946
packetSeq:  4947
packetSeq:  4948
packetSeq:  4949
packetSeq:  4950
processed packlist at 1641198757321
receiving_rate:  245360.0
delay:  192.85714285714286
loss_ratio:  0.0
processed state0-2 at 1641198757321
avgFrameBetween:  6
psnrStat:  [[557302, 557571, 557839, 557200, 556084, 556120, 555524]]
delayStat:  [[331, 332, 332, 331, 330, 330, 330]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556805.7142857143] [330.85714285714283] [0]
processed state3-5 at 1641198757321
liner_to_log:  tensor([[[0.7800]]]) tensor([[[0.3468]]])
linear_to_log at 1641198757322
listState:  [0.06134, 0.1285714285714286, 0.0, 0.5568057142857143, 0.33085714285714285, 0.0, tensor([[[0.3468]]])]
state_clone_detach at 1641198757322
reward: 0.02458002614515098
state tensor([0.0613, 0.1286, 0.0000, 0.3468], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198757323
state222:  tensor([[0.0613, 0.1286, 0.0000, 0.3468]], device='cuda:0')
policy_old.forwarded at 1641198757324
give action 172============================
log_to_linear:  tensor([[[0.5592]]], device='cuda:0') tensor([[[1.0970]]], device='cuda:0')
log_to_linear action at 1641198757325
bwe changes from to:  [tensor([[[2.3246]]]), tensor([[[2.5501]]])]
step into gymStat at 1641198757325
send bwe to appRecv at 1641198757325
sent bwe to appRecv at 1641198757325
wait for recv string at 1641198757325
recved string at 1641198757552
1
wait for recv [self.estimator, stat] at 1641198757552
recved [self.estimator, stat] at 1641198757552
sorted packlist at 1641198757552
packetSeq:  4951
packetSeq:  4952
packetSeq:  4953
packetSeq:  4954
packetSeq:  4955
packetSeq:  4956
packetSeq:  4957
packetSeq:  4958
packetSeq:  4959
processed packlist at 1641198757552
receiving_rate:  282640.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198757552
avgFrameBetween:  6
psnrStat:  [[556248, 554785, 555435, 555150, 554445, 554161, 555238]]
delayStat:  [[330, 330, 330, 330, 330, 330, 330]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555066.0] [330.0] [0]
processed state3-5 at 1641198757552
liner_to_log:  tensor([[[1.0970]]]) tensor([[[0.5592]]])
linear_to_log at 1641198757553
listState:  [0.07066, 0.12822222222222224, 0.0, 0.555066, 0.33, 0.0, tensor([[[0.5592]]])]
state_clone_detach at 1641198757553
reward: 0.06916424691201428
state tensor([0.0707, 0.1282, 0.0000, 0.5592], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198757553
state222:  tensor([[0.0707, 0.1282, 0.0000, 0.5592]], device='cuda:0')
policy_old.forwarded at 1641198757555
give action 173============================
log_to_linear:  tensor([[[0.4209]]], device='cuda:0') tensor([[[0.8805]]], device='cuda:0')
log_to_linear action at 1641198757556
bwe changes from to:  [tensor([[[2.5501]]]), tensor([[[2.2454]]])]
step into gymStat at 1641198757556
send bwe to appRecv at 1641198757556
sent bwe to appRecv at 1641198757556
wait for recv string at 1641198757556
recved string at 1641198757759
1
wait for recv [self.estimator, stat] at 1641198757759
recved [self.estimator, stat] at 1641198757759
sorted packlist at 1641198757759
packetSeq:  4960
packetSeq:  4961
packetSeq:  4962
packetSeq:  4963
packetSeq:  4964
packetSeq:  4965
packetSeq:  4966
packetSeq:  4967
packetSeq:  4968
packetSeq:  4969
processed packlist at 1641198757759
receiving_rate:  261000.0
delay:  194.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198757759
avgFrameBetween:  6
psnrStat:  [[554241, 554043, 555346, 556246, 557048, 557395]]
delayStat:  [[330, 330, 331, 330, 330, 315]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555719.8333333334] [327.6666666666667] [0]
processed state3-5 at 1641198757759
liner_to_log:  tensor([[[0.8805]]]) tensor([[[0.4209]]])
linear_to_log at 1641198757760
listState:  [0.06525, 0.12977777777777777, 0.0, 0.5557198333333334, 0.32766666666666666, 0.0, tensor([[[0.4209]]])]
state_clone_detach at 1641198757760
reward: 0.03961187007154149
state tensor([0.0653, 0.1298, 0.0000, 0.4209], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198757761
state222:  tensor([[0.0653, 0.1298, 0.0000, 0.4209]], device='cuda:0')
policy_old.forwarded at 1641198757762
give action 174============================
log_to_linear:  tensor([[[0.4607]]], device='cuda:0') tensor([[[0.9392]]], device='cuda:0')
log_to_linear action at 1641198757763
bwe changes from to:  [tensor([[[2.2454]]]), tensor([[[2.1088]]])]
step into gymStat at 1641198757764
send bwe to appRecv at 1641198757764
sent bwe to appRecv at 1641198757764
wait for recv string at 1641198757764
recved string at 1641198757987
1
wait for recv [self.estimator, stat] at 1641198757987
recved [self.estimator, stat] at 1641198757987
sorted packlist at 1641198757987
packetSeq:  4970
packetSeq:  4971
packetSeq:  4972
packetSeq:  4973
packetSeq:  4974
packetSeq:  4975
packetSeq:  4976
packetSeq:  4977
packetSeq:  4978
packetSeq:  4979
processed packlist at 1641198757987
receiving_rate:  312000.0
delay:  197.6
loss_ratio:  0.0
processed state0-2 at 1641198757987
avgFrameBetween:  6
psnrStat:  [[558394, 558367, 558756, 558850, 560457, 558990, 560078]]
delayStat:  [[315, 315, 315, 314, 315, 316, 315]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559127.4285714285] [315.0] [0]
processed state3-5 at 1641198757987
liner_to_log:  tensor([[[0.9392]]]) tensor([[[0.4607]]])
linear_to_log at 1641198757988
listState:  [0.078, 0.13173333333333334, 0.0, 0.5591274285714285, 0.315, 0.0, tensor([[[0.4607]]])]
state_clone_detach at 1641198757988
reward: 0.09082455921886684
state tensor([0.0780, 0.1317, 0.0000, 0.4607], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198757988
state222:  tensor([[0.0780, 0.1317, 0.0000, 0.4607]], device='cuda:0')
policy_old.forwarded at 1641198757990
give action 175============================
log_to_linear:  tensor([[[0.4530]]], device='cuda:0') tensor([[[0.9276]]], device='cuda:0')
log_to_linear action at 1641198757991
bwe changes from to:  [tensor([[[2.1088]]]), tensor([[[1.9561]]])]
step into gymStat at 1641198757991
send bwe to appRecv at 1641198757991
sent bwe to appRecv at 1641198757991
wait for recv string at 1641198757991
recved string at 1641198758189
1
wait for recv [self.estimator, stat] at 1641198758189
recved [self.estimator, stat] at 1641198758189
sorted packlist at 1641198758189
packetSeq:  4980
packetSeq:  4981
packetSeq:  4982
packetSeq:  4983
packetSeq:  4984
packetSeq:  4985
packetSeq:  4986
packetSeq:  4987
packetSeq:  4988
processed packlist at 1641198758189
receiving_rate:  277440.0
delay:  198.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198758189
avgFrameBetween:  6
psnrStat:  [[560393, 561473, 561489, 562071, 562066, 561686]]
delayStat:  [[314, 314, 314, 315, 314, 314]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561529.6666666666] [314.1666666666667] [0]
processed state3-5 at 1641198758190
liner_to_log:  tensor([[[0.9276]]]) tensor([[[0.4530]]])
linear_to_log at 1641198758190
listState:  [0.06936, 0.1325185185185185, 0.0, 0.5615296666666666, 0.3141666666666667, 0.0, tensor([[[0.4530]]])]
state_clone_detach at 1641198758190
reward: 0.0503889626101231
state tensor([0.0694, 0.1325, 0.0000, 0.4530], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198758191
state222:  tensor([[0.0694, 0.1325, 0.0000, 0.4530]], device='cuda:0')
policy_old.forwarded at 1641198758192
give action 176============================
log_to_linear:  tensor([[[0.5580]]], device='cuda:0') tensor([[[1.0951]]], device='cuda:0')
log_to_linear action at 1641198758193
bwe changes from to:  [tensor([[[1.9561]]]), tensor([[[2.1420]]])]
step into gymStat at 1641198758193
send bwe to appRecv at 1641198758193
sent bwe to appRecv at 1641198758193
wait for recv string at 1641198758194
recved string at 1641198758392
1
wait for recv [self.estimator, stat] at 1641198758392
recved [self.estimator, stat] at 1641198758392
sorted packlist at 1641198758392
packetSeq:  4989
packetSeq:  4990
packetSeq:  4991
packetSeq:  4992
packetSeq:  4993
packetSeq:  4994
packetSeq:  4995
packetSeq:  4996
packetSeq:  4997
packetSeq:  4998
processed packlist at 1641198758392
receiving_rate:  297000.0
delay:  197.8
loss_ratio:  0.0
processed state0-2 at 1641198758392
avgFrameBetween:  6
psnrStat:  [[561610, 561159, 559984, 558319, 557967, 556726]]
delayStat:  [[316, 314, 314, 315, 314, 314]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559294.1666666666] [314.5] [0]
processed state3-5 at 1641198758392
liner_to_log:  tensor([[[1.0951]]]) tensor([[[0.5580]]])
linear_to_log at 1641198758393
listState:  [0.07425, 0.1318666666666667, 0.0, 0.5592941666666666, 0.3145, 0.0, tensor([[[0.5580]]])]
state_clone_detach at 1641198758393
reward: 0.07419273073584076
state tensor([0.0742, 0.1319, 0.0000, 0.5580], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198758394
state222:  tensor([[0.0742, 0.1319, 0.0000, 0.5580]], device='cuda:0')
policy_old.forwarded at 1641198758395
give action 177============================
log_to_linear:  tensor([[[0.4851]]], device='cuda:0') tensor([[[0.9767]]], device='cuda:0')
log_to_linear action at 1641198758396
bwe changes from to:  [tensor([[[2.1420]]]), tensor([[[2.0920]]])]
step into gymStat at 1641198758397
send bwe to appRecv at 1641198758397
sent bwe to appRecv at 1641198758397
wait for recv string at 1641198758397
recved string at 1641198758597
1
wait for recv [self.estimator, stat] at 1641198758597
recved [self.estimator, stat] at 1641198758597
sorted packlist at 1641198758597
packetSeq:  4999
packetSeq:  5000
packetSeq:  pc wait for bwe at 1641198754286
pc got bwe at 1641198754290
bandwidth:  300000
pc flushed at 1641198754290
Bwe Sent: 5 at 1641198754290
got request at 1641198754520
processed allFrame at 1641198754520
send 'asking for bwe' at 1641198754520
sent 'asking for bwe' at 1641198754520
send [estimator, stat] at 1641198754520
sent [estimator, stat] at 1641198754520
pc wait for bwe at 1641198754520
pc got bwe at 1641198754524
bandwidth:  300000
pc flushed at 1641198754524
Bwe Sent: 4 at 1641198754524
got request at 1641198754722
processed allFrame at 1641198754722
send 'asking for bwe' at 1641198754722
sent 'asking for bwe' at 1641198754722
send [estimator, stat] at 1641198754722
sent [estimator, stat] at 1641198754722
pc wait for bwe at 1641198754722
pc got bwe at 1641198754726
bandwidth:  300000
pc flushed at 1641198754726
Bwe Sent: 4 at 1641198754726
got request at 1641198754924
processed allFrame at 1641198754924
send 'asking for bwe' at 1641198754924
sent 'asking for bwe' at 1641198754924
send [estimator, stat] at 1641198754924
sent [estimator, stat] at 1641198754924
pc wait for bwe at 1641198754924
pc got bwe at 1641198754929
bandwidth:  300000
pc flushed at 1641198754929
Bwe Sent: 5 at 1641198754929
got request at 1641198755155
processed allFrame at 1641198755155
send 'asking for bwe' at 1641198755155
sent 'asking for bwe' at 1641198755155
send [estimator, stat] at 1641198755155
sent [estimator, stat] at 1641198755155
pc wait for bwe at 1641198755155
pc got bwe at 1641198755159
bandwidth:  300000
pc flushed at 1641198755159
Bwe Sent: 4 at 1641198755159
got request at 1641198755386
processed allFrame at 1641198755386
send 'asking for bwe' at 1641198755386
sent 'asking for bwe' at 1641198755386
send [estimator, stat] at 1641198755386
sent [estimator, stat] at 1641198755386
pc wait for bwe at 1641198755386
pc got bwe at 1641198755390
bandwidth:  300000
pc flushed at 1641198755390
Bwe Sent: 4 at 1641198755390
got request at 1641198755586
processed allFrame at 1641198755586
send 'asking for bwe' at 1641198755586
sent 'asking for bwe' at 1641198755586
send [estimator, stat] at 1641198755586
sent [estimator, stat] at 1641198755587
pc wait for bwe at 1641198755587
pc got bwe at 1641198755591
bandwidth:  300000
pc flushed at 1641198755591
Bwe Sent: 5 at 1641198755591
got request at 1641198755790
processed allFrame at 1641198755790
send 'asking for bwe' at 1641198755790
sent 'asking for bwe' at 1641198755790
send [estimator, stat] at 1641198755790
sent [estimator, stat] at 1641198755790
pc wait for bwe at 1641198755791
pc got bwe at 1641198755795
bandwidth:  300000
pc flushed at 1641198755795
Bwe Sent: 5 at 1641198755795
got request at 1641198756021
processed allFrame at 1641198756021
send 'asking for bwe' at 1641198756021
sent 'asking for bwe' at 1641198756021
send [estimator, stat] at 1641198756021
sent [estimator, stat] at 1641198756021
pc wait for bwe at 1641198756022
pc got bwe at 1641198756026
bandwidth:  300000
pc flushed at 1641198756026
Bwe Sent: 5 at 1641198756026
got request at 1641198756253
processed allFrame at 1641198756253
send 'asking for bwe' at 1641198756253
sent 'asking for bwe' at 1641198756253
send [estimator, stat] at 1641198756253
sent [estimator, stat] at 1641198756253
pc wait for bwe at 1641198756254
pc got bwe at 1641198756258
bandwidth:  300000
pc flushed at 1641198756258
Bwe Sent: 5 at 1641198756258
got request at 1641198756456
processed allFrame at 1641198756456
send 'asking for bwe' at 1641198756456
sent 'asking for bwe' at 1641198756456
send [estimator, stat] at 1641198756456
sent [estimator, stat] at 1641198756456
pc wait for bwe at 1641198756457
pc got bwe at 1641198756461
bandwidth:  300000
pc flushed at 1641198756461
Bwe Sent: 5 at 1641198756461
got request at 1641198756685
processed allFrame at 1641198756685
send 'asking for bwe' at 1641198756685
sent 'asking for bwe' at 1641198756685
send [estimator, stat] at 1641198756685
sent [estimator, stat] at 1641198756685
pc wait for bwe at 1641198756685
pc got bwe at 1641198756689
bandwidth:  300000
pc flushed at 1641198756689
Bwe Sent: 4 at 1641198756689
got request at 1641198756888
processed allFrame at 1641198756888
send 'asking for bwe' at 1641198756888
sent 'asking for bwe' at 1641198756888
send [estimator, stat] at 1641198756888
sent [estimator, stat] at 1641198756888
pc wait for bwe at 1641198756888
pc got bwe at 1641198756893
bandwidth:  300000
pc flushed at 1641198756893
Bwe Sent: 5 at 1641198756893
got request at 1641198757091
processed allFrame at 1641198757091
send 'asking for bwe' at 1641198757091
sent 'asking for bwe' at 1641198757091
send [estimator, stat] at 1641198757091
sent [estimator, stat] at 1641198757092
pc wait for bwe at 1641198757092
pc got bwe at 1641198757096
bandwidth:  300000
pc flushed at 1641198757096
Bwe Sent: 5 at 1641198757096
got request at 1641198757321
processed allFrame at 1641198757321
send 'asking for bwe' at 1641198757321
sent 'asking for bwe' at 1641198757321
send [estimator, stat] at 1641198757321
sent [estimator, stat] at 1641198757321
pc wait for bwe at 1641198757321
pc got bwe at 1641198757325
bandwidth:  300000
pc flushed at 1641198757326
Bwe Sent: 5 at 1641198757326
got request at 1641198757552
processed allFrame at 1641198757552
send 'asking for bwe' at 1641198757552
sent 'asking for bwe' at 1641198757552
send [estimator, stat] at 1641198757552
sent [estimator, stat] at 1641198757552
pc wait for bwe at 1641198757552
pc got bwe at 1641198757556
bandwidth:  300000
pc flushed at 1641198757556
Bwe Sent: 4 at 1641198757556
got request at 1641198757759
processed allFrame at 1641198757759
send 'asking for bwe' at 1641198757759
sent 'asking for bwe' at 1641198757759
send [estimator, stat] at 1641198757759
sent [estimator, stat] at 1641198757759
pc wait for bwe at 1641198757759
pc got bwe at 1641198757764
bandwidth:  300000
pc flushed at 1641198757764
Bwe Sent: 5 at 1641198757764
got request at 1641198757987
processed allFrame at 1641198757987
send 'asking for bwe' at 1641198757987
sent 'asking for bwe' at 1641198757987
send [estimator, stat] at 1641198757987
sent [estimator, stat] at 1641198757987
pc wait for bwe at 1641198757987
pc got bwe at 1641198757991
bandwidth:  300000
pc flushed at 1641198757991
Bwe Sent: 4 at 1641198757991
got request at 1641198758189
processed allFrame at 1641198758189
send 'asking for bwe' at 1641198758189
sent 'asking for bwe' at 1641198758189
send [estimator, stat] at 1641198758189
sent [estimator, stat] at 1641198758189
pc wait for bwe at 1641198758189
pc got bwe at 1641198758194
bandwidth:  300000
pc flushed at 1641198758194
Bwe Sent: 5 at 1641198758194
got request at 1641198758392
processed allFrame at 1641198758392
send 'asking for bwe' at 1641198758392
sent 'asking for bwe' at 1641198758392
send [estimator, stat] at 1641198758392
sent [estimator, stat] at 1641198758392
pc wait for bwe at 1641198758392
pc got bwe at 1641198758397
bandwidth:  300000
pc flushed at 1641198758397
Bwe Sent: 5 at 1641198758397
got request at 1641198758597
processed allFrame at 1641198758597
send 'asking for bwe' at 1641198758597
sent 'asking for bwe' at 1641198758597
send [estimator, stat] at 1641198758597
sent [estimator, stat] at 1641198758597
pc wait for bwe at 1641198758597
pc got bwe at 1641198758601
bandwidth:  300000
pc flushed at 1641198758601
Bwe Sent: 4 at 1641198758601
got request at 1641198758797
processed allFrame at 1641198758798
send 'asking for bwe' at 1641198758798
sent 'asking for bwe' at 1641198758798
send [estimator, stat] at 1641198758798
sent [estimator, stat] at 1641198758798
pc wait for bwe at 1641198758798
pc got bwe at 1641198758802
bandwidth:  300000
pc flushed at 1641198758802
Bwe Sent: 5 at 1641198758802
got request at 1641198759024
processed allFrame at 1641198759024
send 'asking for bwe' at 1641198759024
sent 'asking for bwe' at 1641198759024
send [estimator, stat] at 1641198759024
sent [estimator, stat] at 1641198759024
pc wait for bwe at 1641198759025
pc got bwe at 1641198759029
bandwidth:  300000
pc flushed at 1641198759029
Bwe Sent: 5 at 1641198759029
got request at 1641198759226
processed allFrame at 1641198759226
send 'asking for bwe' at 1641198759226
5001
packetSeq:  5002
packetSeq:  5003
packetSeq:  5004
packetSeq:  5005
packetSeq:  5006
processed packlist at 1641198758597
receiving_rate:  261760.0
delay:  198.375
loss_ratio:  0.0
processed state0-2 at 1641198758597
avgFrameBetween:  6
psnrStat:  [[557408, 556976, 556191, 555685, 555490, 554902, 554283]]
delayStat:  [[314, 314, 314, 314, 314, 314, 315]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555847.8571428572] [314.14285714285717] [0]
processed state3-5 at 1641198758597
liner_to_log:  tensor([[[0.9767]]]) tensor([[[0.4851]]])
linear_to_log at 1641198758598
listState:  [0.06544, 0.13225, 0.0, 0.5558478571428571, 0.31414285714285717, 0.0, tensor([[[0.4851]]])]
state_clone_detach at 1641198758598
reward: 0.03308690836778144
state tensor([0.0654, 0.1322, 0.0000, 0.4851], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198758598
state222:  tensor([[0.0654, 0.1322, 0.0000, 0.4851]], device='cuda:0')
policy_old.forwarded at 1641198758600
give action 178============================
log_to_linear:  tensor([[[0.5177]]], device='cuda:0') tensor([[[1.0283]]], device='cuda:0')
log_to_linear action at 1641198758601
bwe changes from to:  [tensor([[[2.0920]]]), tensor([[[2.1512]]])]
step into gymStat at 1641198758601
send bwe to appRecv at 1641198758601
sent bwe to appRecv at 1641198758601
wait for recv string at 1641198758601
recved string at 1641198758798
1
wait for recv [self.estimator, stat] at 1641198758798
recved [self.estimator, stat] at 1641198758798
sorted packlist at 1641198758798
packetSeq:  5007
packetSeq:  5008
packetSeq:  5009
packetSeq:  5010
packetSeq:  5011
packetSeq:  5012
packetSeq:  5013
packetSeq:  5014
packetSeq:  5015
packetSeq:  5016
processed packlist at 1641198758798
receiving_rate:  334680.0
delay:  200.1
loss_ratio:  0.0
processed state0-2 at 1641198758798
avgFrameBetween:  6
psnrStat:  [[552889, 551949, 551940, 551527, 552298, 552341]]
delayStat:  [[314, 314, 315, 314, 313, 315]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552157.3333333334] [314.1666666666667] [0]
processed state3-5 at 1641198758798
liner_to_log:  tensor([[[1.0283]]]) tensor([[[0.5177]]])
linear_to_log at 1641198758798
listState:  [0.08367, 0.1334, 0.0, 0.5521573333333334, 0.3141666666666667, 0.0, tensor([[[0.5177]]])]
state_clone_detach at 1641198758799
reward: 0.10956226269530733
state tensor([0.0837, 0.1334, 0.0000, 0.5177], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198758799
state222:  tensor([[0.0837, 0.1334, 0.0000, 0.5177]], device='cuda:0')
policy_old.forwarded at 1641198758801
give action 179============================
log_to_linear:  tensor([[[0.3711]]], device='cuda:0') tensor([[[0.8118]]], device='cuda:0')
log_to_linear action at 1641198758802
bwe changes from to:  [tensor([[[2.1512]]]), tensor([[[1.7462]]])]
step into gymStat at 1641198758802
send bwe to appRecv at 1641198758802
sent bwe to appRecv at 1641198758802
wait for recv string at 1641198758802
recved string at 1641198759024
1
wait for recv [self.estimator, stat] at 1641198759024
recved [self.estimator, stat] at 1641198759025
sorted packlist at 1641198759025
packetSeq:  5017
packetSeq:  5018
packetSeq:  5019
packetSeq:  5020
packetSeq:  5021
packetSeq:  5022
packetSeq:  5023
packetSeq:  5024
packetSeq:  5025
packetSeq:  5026
processed packlist at 1641198759025
receiving_rate:  298240.0
delay:  197.3
loss_ratio:  0.0
processed state0-2 at 1641198759025
avgFrameBetween:  6
psnrStat:  [[553448, 554026, 554492, 554233, 554889, 550931]]
delayStat:  [[314, 314, 315, 314, 313, 315]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553669.8333333334] [314.1666666666667] [0]
processed state3-5 at 1641198759025
liner_to_log:  tensor([[[0.8118]]]) tensor([[[0.3711]]])
linear_to_log at 1641198759025
listState:  [0.07456, 0.13153333333333334, 0.0, 0.5536698333333334, 0.3141666666666667, 0.0, tensor([[[0.3711]]])]
state_clone_detach at 1641198759025
reward: 0.07655133764919314
state tensor([0.0746, 0.1315, 0.0000, 0.3711], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198759026
state222:  tensor([[0.0746, 0.1315, 0.0000, 0.3711]], device='cuda:0')
policy_old.forwarded at 1641198759028
give action 180============================
log_to_linear:  tensor([[[0.6056]]], device='cuda:0') tensor([[[1.1773]]], device='cuda:0')
log_to_linear action at 1641198759029
bwe changes from to:  [tensor([[[1.7462]]]), tensor([[[2.0559]]])]
step into gymStat at 1641198759029
send bwe to appRecv at 1641198759029
sent bwe to appRecv at 1641198759029
wait for recv string at 1641198759029
recved string at 1641198759226
1
wait for recv [self.estimator, stat] at 1641198759226
recved [self.estimator, stat] at 1641198759226
sorted packlist at 1641198759226
packetSeq:  5027
packetSeq:  5028
packetSeq:  5029
packetSeq:  5030
packetSeq:  5031
packetSeq:  5032
packetSeq:  5033
packetSeq:  5034
packetSeq:  5035
packetSeq:  5036
processed packlist at 1641198759226
receiving_rate:  263840.0
delay:  196.9
loss_ratio:  0.0
processed state0-2 at 1641198759226
avgFrameBetween:  6
psnrStat:  [[549424, 549541, 549611, 550036, 551475, 550763]]
delayStat:  [[314, 314, 314, 314, 314, 314]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550141.6666666666] [314.0] [0]
processed state3-5 at 1641198759226
liner_to_log:  tensor([[[1.1773]]]) tensor([[[0.6056]]])
linear_to_log at 1641198759227
listState:  [0.06596, 0.13126666666666667, 0.0, 0.5501416666666666, 0.314, 0.0, tensor([[[0.6056]]])]
state_clone_detach at 1641198759227
reward: 0.03847060905237443
state tensor([0.0660, 0.1313, 0.0000, 0.6056], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198759227
state222:  tensor([[0.0660, 0.1313, 0.0000, 0.6056]], device='cuda:0')
policy_old.forwarded at 1641198759229
give action 181============================
log_to_linear:  tensor([[[0.0249]]], device='cuda:0') tensor([[[0.5043]]], device='cuda:0')
log_to_linear action at 1641198759230
bwe changes from to:  [tensor([[[2.0559]]]), tensor([[[1.0368]]])]
step into gymStat at 1641198759230
send bwe to appRecv at 1641198759230
sent bwe to appRecv at 1641198759230
wait for recv string at 1641198759230
recved string at 1641198759458
1
wait for recv [self.estimator, stat] at 1641198759458
recved [self.estimator, stat] at 1641198759458
sorted packlist at 1641198759458
packetSeq:  5037
packetSeq:  5038
packetSeq:  5039
packetSeq:  5040
packetSeq:  5041
packetSeq:  5042
packetSeq:  5043
packetSeq:  5044
processed packlist at 1641198759458
receiving_rate:  260560.0
delay:  198.14285714285714
loss_ratio:  0.0
processed state0-2 at 1641198759459
avgFrameBetween:  6
psnrStat:  [[550630, 549929, 550027, 550191, 549953, 549852, 550003]]
delayStat:  [[313, 314, 315, 313, 314, 315, 314]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550083.5714285715] [314.0] [0]
processed state3-5 at 1641198759459
liner_to_log:  tensor([[[0.5043]]]) tensor([[[0.0249]]])
linear_to_log at 1641198759459
listState:  [0.06514, 0.1320952380952381, 0.0, 0.5500835714285714, 0.314, 0.0, tensor([[[0.0249]]])]
state_clone_detach at 1641198759459
reward: 0.032142632514897906
state tensor([0.0651, 0.1321, 0.0000, 0.0249], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198759460
state222:  tensor([[0.0651, 0.1321, 0.0000, 0.0249]], device='cuda:0')
policy_old.forwarded at 1641198759462
give action 182============================
log_to_linear:  tensor([[[0.2445]]], device='cuda:0') tensor([[[0.6608]]], device='cuda:0')
log_to_linear action at 1641198759463
bwe changes from to:  [tensor([[[1.0368]]]), tensor([[[0.6852]]])]
step into gymStat at 1641198759463
send bwe to appRecv at 1641198759463
sent bwe to appRecv at 1641198759463
wait for recv string at 1641198759463
recved string at 1641198759659
1
wait for recv [self.estimator, stat] at 1641198759659
recved [self.estimator, stat] at 1641198759660
sorted packlist at 1641198759660
packetSeq:  5045
packetSeq:  5046
packetSeq:  5047
packetSeq:  5048
packetSeq:  5049
packetSeq:  5050
packetSeq:  5051
processed packlist at 1641198759660
receiving_rate:  248920.0
delay:  198.57142857142858
loss_ratio:  0.0
processed state0-2 at 1641198759660
avgFrameBetween:  6
psnrStat:  [[549934, 549247, 550216, 550395, 550387, 549517]]
delayStat:  [[314, 314, 314, 314, 314, 313]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549949.3333333334] [313.8333333333333] [0]
processed state3-5 at 1641198759660
liner_to_log:  tensor([[[0.6608]]]) tensor([[[0.2445]]])
linear_to_log at 1641198759660
listState:  [0.06223, 0.13238095238095238, 0.0, 0.5499493333333334, 0.3138333333333333, 0.0, tensor([[[0.2445]]])]
state_clone_detach at 1641198759660
reward: 0.01744804949430001
state tensor([0.0622, 0.1324, 0.0000, 0.2445], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198759661
state222:  tensor([[0.0622, 0.1324, 0.0000, 0.2445]], device='cuda:0')
policy_old.forwarded at 1641198759663
give action 183============================
log_to_linear:  tensor([[[0.3723]]], device='cuda:0') tensor([[[0.8133]]], device='cuda:0')
log_to_linear action at 1641198759664
bwe changes from to:  [tensor([[[0.6852]]]), tensor([[[0.5573]]])]
step into gymStat at 1641198759664
send bwe to appRecv at 1641198759664
sent bwe to appRecv at 1641198759664
wait for recv string at 1641198759664
recved string at 1641198759889
1
wait for recv [self.estimator, stat] at 1641198759889
recved [self.estimator, stat] at 1641198759889
sorted packlist at 1641198759889
packetSeq:  5052
packetSeq:  5053
packetSeq:  5054
packetSeq:  5055
packetSeq:  5056
packetSeq:  5057
packetSeq:  5058
packetSeq:  5059
packetSeq:  5060
processed packlist at 1641198759889
receiving_rate:  276200.0
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198759890
avgFrameBetween:  6
psnrStat:  [[548779, 548780, 550312, 549936, 550881, 551693, 552796]]
delayStat:  [[313, 314, 312, 313, 314, 313, 313]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550453.8571428572] [313.14285714285717] [0]
processed state3-5 at 1641198759890
liner_to_log:  tensor([[[0.8133]]]) tensor([[[0.3723]]])
linear_to_log at 1641198759890
listState:  [0.06905, 0.13133333333333333, 0.0, 0.5504538571428571, 0.31314285714285717, 0.0, tensor([[[0.3723]]])]
state_clone_detach at 1641198759890
reward: 0.052532270446367546
state tensor([0.0690, 0.1313, 0.0000, 0.3723], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198759891
state222:  tensor([[0.0690, 0.1313, 0.0000, 0.3723]], device='cuda:0')
policy_old.forwarded at 1641198759892
give action 184============================
log_to_linear:  tensor([[[0.4800]]], device='cuda:0') tensor([[[0.9687]]], device='cuda:0')
log_to_linear action at 1641198759893
bwe changes from to:  [tensor([[[0.5573]]]), tensor([[[0.5398]]])]
step into gymStat at 1641198759894
send bwe to appRecv at 1641198759894
sent bwe to appRecv at 1641198759894
wait for recv string at 1641198759894
recved string at 1641198760122
1
wait for recv [self.estimator, stat] at 1641198760122
recved [self.estimator, stat] at 1641198760122
sorted packlist at 1641198760122
packetSeq:  5061
packetSeq:  5062
packetSeq:  5063
packetSeq:  5064
packetSeq:  5065
packetSeq:  5066
packetSeq:  5067
packetSeq:  5068
packetSeq:  5069
packetSeq:  5070
packetSeq:  5071
packetSeq:  5072
processed packlist at 1641198760122
receiving_rate:  285120.0
delay:  197.2
loss_ratio:  0.0
processed state0-2 at 1641198760122
avgFrameBetween:  6
psnrStat:  [[551394, 551495, 552108, 553730, 552735, 554926, 556444]]
delayStat:  [[314, 313, 314, 314, 313, 312, 314]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553261.7142857143] [313.42857142857144] [0]
processed state3-5 at 1641198760122
liner_to_log:  tensor([[[0.9687]]]) tensor([[[0.4800]]])
linear_to_log at 1641198760123
listState:  [0.07128, 0.13146666666666665, 0.0, 0.5532617142857144, 0.31342857142857145, 0.0, tensor([[[0.4800]]])]
state_clone_detach at 1641198760123
reward: 0.06221806176874378
state tensor([0.0713, 0.1315, 0.0000, 0.4800], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198760123
state222:  tensor([[0.0713, 0.1315, 0.0000, 0.4800]], device='cuda:0')
policy_old.forwarded at 1641198760125
give action 185============================
log_to_linear:  tensor([[[0.4373]]], device='cuda:0') tensor([[[0.9043]]], device='cuda:0')
log_to_linear action at 1641198760126
bwe changes from to:  [tensor([[[0.5398]]]), tensor([[[0.4882]]])]
step into gymStat at 1641198760126
send bwe to appRecv at 1641198760126
sent bwe to appRecv at 1641198760127
wait for recv string at 1641198760127
recved string at 1641198760327
1
wait for recv [self.estimator, stat] at 1641198760327
recved [self.estimator, stat] at 1641198760328
sorted packlist at 1641198760328
packetSeq:  5073
packetSeq:  5074
packetSeq:  5075
packetSeq:  5076
packetSeq:  5077
packetSeq:  5078
packetSeq:  5079
packetSeq:  5080
packetSeq:  5081
packetSeq:  5082
packetSeq:  5083
packetSeq:  5084
packetSeq:  5085
processed packlist at 1641198760328
receiving_rate:  312040.0
delay:  197.23076923076923
loss_ratio:  0.0
processed state0-2 at 1641198760328
avgFrameBetween:  6
psnrStat:  [[557135, 556359, 556890, 556295, 557415, 555084, 553384]]
delayStat:  [[313, 313, 313, 313, 314, 313, 313]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556080.2857142857] [313.14285714285717] [0]
processed state3-5 at 1641198760328
liner_to_log:  tensor([[[0.9043]]]) tensor([[[0.4373]]])
linear_to_log at 1641198760328
listState:  [0.07801, 0.13148717948717947, 0.0, 0.5560802857142857, 0.31314285714285717, 0.0, tensor([[[0.4373]]])]
state_clone_detach at 1641198760328
reward: 0.09160572386399735
state tensor([0.0780, 0.1315, 0.0000, 0.4373], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198760329
state222:  tensor([[0.0780, 0.1315, 0.0000, 0.4373]], device='cuda:0')
policy_old.forwarded at 1641198760331
give action 186============================
log_to_linear:  tensor([[[0.6454]]], device='cuda:0') tensor([[[1.2493]]], device='cuda:0')
log_to_linear action at 1641198760331
bwe changes from to:  [tensor([[[0.4882]]]), tensor([[[0.6098]]])]
step into gymStat at 1641198760332
send bwe to appRecv at 1641198760332
sent bwe to appRecv at 1641198760332
wait for recv string at 1641198760332
recved string at 1641198760555
1
wait for recv [self.estimator, stat] at 1641198760555
recved [self.estimator, stat] at 1641198760555
sorted packlist at 1641198760555
packetSeq:  5086
packetSeq:  5087
packetSeq:  5088
packetSeq:  5089
packetSeq:  5090
packetSeq:  5091
packetSeq:  5092
packetSeq:  5093
processed packlist at 1641198760555
receiving_rate:  277880.0
delay:  197.875
loss_ratio:  0.0
processed state0-2 at 1641198760555
avgFrameBetween:  6
psnrStat:  [[554846, 553724, 553805, 552185, 551441, 552518]]
delayStat:  [[314, 313, 312, 314, 313, 313]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553086.5] [313.1666666666667] [0]
processed state3-5 at 1641198760555
liner_to_log:  tensor([[[1.2493]]]) tensor([[[0.6454]]])
linear_to_log at 1641198760556
listState:  [0.06947, 0.13191666666666665, 0.0, 0.5530865, 0.3131666666666667, 0.0, tensor([[[0.6454]]])]
state_clone_detach at 1641198760556
reward: 0.052694840371829366
state tensor([0.0695, 0.1319, 0.0000, 0.6454], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198760556
state222:  tensor([[0.0695, 0.1319, 0.0000, 0.6454]], device='cuda:0')
policy_old.forwarded at 1641198760558
give action 187============================
log_to_linear:  tensor([[[0.7865]]], device='cuda:0') tensor([[[1.5250]]], device='cuda:0')
log_to_linear action at 1641198760559
bwe changes from to:  [tensor([[[0.6098]]]), tensor([[[0.9300]]])]
step into gymStat at 1641198760559
send bwe to appRecv at 1641198760559
sent bwe to appRecv at 1641198760559
wait for recv string at 1641198760559
recved string at 1641198760760
1
wait for recv [self.estimator, stat] at 1641198760760
recved [self.estimator, stat] at 1641198760760
sorted packlist at 1641198760760
packetSeq:  5094
packetSeq:  5095
packetSeq:  5096
packetSeq:  5097
packetSeq:  5098
packetSeq:  5099
packetSeq:  5100
packetSeq:  5101
packetSeq:  5102
packetSeq:  5103
processed packlist at 1641198760760
receiving_rate:  292600.0
delay:  197.6
loss_ratio:  0.0
processed state0-2 at 1641198760760
avgFrameBetween:  6
psnrStat:  [[551721, 551864, 550289, 551462, 550331, 550578, 551097]]
delayStat:  [[315, 314, 313, 314, 314, 294, 295]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551048.8571428572] [308.42857142857144] [0]
processed state3-5 at 1641198760760
liner_to_log:  tensor([[[1.5250]]]) tensor([[[0.7865]]])
linear_to_log at 1641198760761
listState:  [0.07315, 0.13173333333333334, 0.0, 0.5510488571428571, 0.30842857142857144, 0.0, tensor([[[0.7865]]])]
state_clone_detach at 1641198760761
reward: 0.06974688941755092
state tensor([0.0732, 0.1317, 0.0000, 0.7865], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198760761
state222:  tensor([[0.0732, 0.1317, 0.0000, 0.7865]], device='cuda:0')
policy_old.forwarded at 1641198760763
give action 188============================
log_to_linear:  tensor([[[0.7174]]], device='cuda:0') tensor([[[1.3861]]], device='cuda:0')
log_to_linear action at 1641198760764
bwe changes from to:  [tensor([[[0.9300]]]), tensor([[[1.2891]]])]
step into gymStat at 1641198760764
send bwe to appRecv at 1641198760764
sent bwe to appRecv at 1641198760764
wait for recv string at 1641198760764
recved string at 1641198760962
1
wait for recv [self.estimator, stat] at 1641198760962
recved [self.estimator, stat] at 1641198760962
sorted packlist at 1641198760962
packetSeq:  5104
packetSeq:  5105
packetSeq:  5106
packetSeq:  5107
packetSeq:  5108
packetSeq:  5109
packetSeq:  5110
packetSeq:  5111
packetSeq:  5112
processed packlist at 1641198760962
receiving_rate:  245560.0
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198760962
avgFrameBetween:  6
psnrStat:  [[550691, 550279, 551391, 550219, 551994, 549885]]
delayStat:  [[294, 294, 295, 295, 294, 295]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550743.1666666666] [294.5] [0]
processed state3-5 at 1641198760962
liner_to_log:  tensor([[[1.3861]]]) tensor([[[0.7174]]])
linear_to_log at 1641198760963
listState:  [0.06139, 0.13133333333333333, 0.0, 0.5507431666666667, 0.2945, 0.0, tensor([[[0.7174]]])]
state_clone_detach at 1641198760963
reward: 0.016536511031174983
state tensor([0.0614, 0.1313, 0.0000, 0.7174], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198760964
state222:  tensor([[0.0614, 0.1313, 0.0000, 0.7174]], device='cuda:0')
policy_old.forwarded at 1641198760965
give action 189============================
log_to_linear:  tensor([[[0.4005]]], device='cuda:0') tensor([[[0.8517]]], device='cuda:0')
log_to_linear action at 1641198760966
bwe changes from to:  [tensor([[[1.2891]]]), tensor([[[1.0979]]])]
step into gymStat at 1641198760966
send bwe to appRecv at 1641198760966
sent bwe to appRecv at 1641198760966
wait for recv string at 1641198760966
recved string at 1641198761164
1
wait for recv [self.estimator, stat] at 1641198761164
recved [self.estimator, stat] at 1641198761164
sorted packlist at 1641198761164
packetSeq:  5113
packetSeq:  5114
packetSeq:  5115
packetSeq:  5116
packetSeq:  5117
packetSeq:  5118
packetSeq:  5119
processed packlist at 1641198761164
receiving_rate:  246480.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198761165
avgFrameBetween:  6
psnrStat:  [[549690, 550406, 550231, 549475, 550258, 548943]]
delayStat:  [[294, 294, 295, 294, 294, 294]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549833.8333333334] [294.1666666666667] [0]
processed state3-5 at 1641198761165
liner_to_log:  tensor([[[0.8517]]]) tensor([[[0.4005]]])
linear_to_log at 1641198761165
listState:  [0.06162, 0.132, 0.0, 0.5498338333333334, 0.2941666666666667, 0.0, tensor([[[0.4005]]])]
state_clone_detach at 1641198761165
reward: 0.015649369702694205
state tensor([0.0616, 0.1320, 0.0000, 0.4005], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198761166
state222:  tensor([[0.0616, 0.1320, 0.0000, 0.4005]], device='cuda:0')
policy_old.forwarded at 1641198761167
give action 190============================
log_to_linear:  tensor([[[0.1807]]], device='cuda:0') tensor([[[0.5997]]], device='cuda:0')
log_to_linear action at 1641198761168
bwe changes from to:  [tensor([[[1.0979]]]), tensor([[[0.6584]]])]
step into gymStat at 1641198761169
send bwe to appRecv at 1641198761169
sent bwe to appRecv at 1641198761169
wait for recv string at 1641198761169
recved string at 1641198761392
1
wait for recv [self.estimator, stat] at 1641198761392
recved [self.estimator, stat] at 1641198761392
sorted packlist at 1641198761392
packetSeq:  5120
packetSeq:  5121
packetSeq:  5122
packetSeq:  5123
packetSeq:  5124
packetSeq:  5125
packetSeq:  5126
packetSeq:  5127
packetSeq:  5128
packetSeq:  5129
processed packlist at 1641198761392
receiving_rate:  268360.0
delay:  198.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198761392
avgFrameBetween:  6
psnrStat:  [[549930, 549717, 550193, 549722, 550759, 551060, 553002]]
delayStat:  [[293, 294, 294, 293, 293, 294, 293]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550626.1428571428] [293.42857142857144] [0]
processed state3-5 at 1641198761392
liner_to_log:  tensor([[[0.5997]]]) tensor([[[0.1807]]])
linear_to_log at 1641198761393
listState:  [0.06709, 0.13207407407407407, 0.0, 0.5506261428571428, 0.2934285714285714, 0.0, tensor([[[0.1807]]])]
state_clone_detach at 1641198761393
reward: 0.041303216000758036
state tensor([0.0671, 0.1321, 0.0000, 0.1807], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198761393
state222:  tensor([[0.0671, 0.1321, 0.0000, 0.1807]], device='cuda:0')
policy_old.forwarded at 1641198761395
give action 191============================
log_to_linear:  tensor([[[-0.0410]]], device='cuda:0') tensor([[[nan]]], device='cuda:0')
log_to_linear action at 1641198761396
bwe changes from to:  [tensor([[[0.6584]]]), tensor([[[0.3292]]])]
step into gymStat at 1641198761396
send bwe to appRecv at 1641198761396
sent bwe to appRecv at 1641198761396
wait for recv string at 1641198761396
recved string at 1641198761596
1
wait for recv [self.estimator, stat] at 1641198761596
recved [self.estimator, stat] at 1641198761596
sorted packlist at 1641198761596
packetSeq:  5130
packetSeq:  5131
packetSeq:  5132
packetSeq:  5133
packetSeq:  5134
packetSeq:  5135
packetSeq:  5136
packetSeq:  5137
packetSeq:  5138
packetSeq:  5139
packetSeq:  5140
processed packlist at 1641198761596
receiving_rate:  302440.0
delay:  196.72727272727272
loss_ratio:  0.0
processed state0-2 at 1641198761596
avgFrameBetween:  6
psnrStat:  [[553050, 554768, 553831, 554759, 555331, 555539]]
delayStat:  [[293, 293, 293, 295, 293, 293]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554546.3333333334] [293.3333333333333] [0]
processed state3-5 at 1641198761596
liner_to_log:  tensor([[[0.5000]]]) tensor([[[0.]]])
linear_to_log at 1641198761596
listState:  [0.07561, 0.13115151515151516, 0.0, 0.5545463333333334, 0.29333333333333333, 0.0, tensor([[[0.]]])]
state_clone_detach at 1641198761597
reward: 0.08227580195394346
state tensor([0.0756, 0.1312, 0.0000, 0.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198761597
state222:  tensor([[0.0756, 0.1312, 0.0000, 0.0000]], device='cuda:0')
policy_old.forwarded at 1641198761599
give action 192============================
log_to_linear:  tensor([[[0.6637]]], device='cuda:0') tensor([[[1.2834]]], device='cuda:0')
log_to_linear action at 1641198761600
bwe changes from to:  [tensor([[[0.3292]]]), tensor([[[0.4225]]])]
step into gymStat at 1641198761600
send bwe to appRecv at 1641198761600
sent bwe to appRecv at 1641198761600
wait for recv string at 1641198761600
recved string at 1641198761797
1
wait for recv [self.estimator, stat] at 1641198761797
recved [self.estimator, stat] at 1641198761798
sorted packlist at 1641198761798
packetSeq:  5141
packetSeq:  5142
packetSeq:  5143
packetSeq:  5144
packetSeq:  5145
packetSeq:  5146
packetSeq:  5147
packetSeq:  5148
packetSeq:  5149
packetSeq:  5150
processed packlist at 1641198761798
receiving_rate:  285680.0
delay:  197.6
loss_ratio:  0.0
processed state0-2 at 1641198761798
avgFrameBetween:  6
psnrStat:  [[555874, 555518, 555249, 555223, 554819, 553500]]
delayStat:  [[294, 293, 293, 294, 293, 293]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555030.5] [293.3333333333333] [0]
processed state3-5 at 1641198761798
liner_to_log:  tensor([[[1.2834]]]) tensor([[[0.6637]]])
linear_to_log at 1641198761798
listState:  [0.07142, 0.13173333333333334, 0.0, 0.5550305, 0.29333333333333333, 0.0, tensor([[[0.6637]]])]
state_clone_detach at 1641198761798
reward: 0.062045629999285534
state tensor([0.0714, 0.1317, 0.0000, 0.6637], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198761799
state222:  tensor([[0.0714, 0.1317, 0.0000, 0.6637]], device='cuda:0')
policy_old.forwarded at 1641198761801
give action 193============================
log_to_linear:  tensor([[[0.2341]]], device='cuda:0') tensor([[[0.6502]]], device='cuda:0')
log_to_linear action at 1641198761802
bwe changes from to:  [tensor([[[0.4225]]]), tensor([[[0.2747]]])]
step into gymStat at 1641198761802
send bwe to appRecv at 1641198761802
sent bwe to appRecv at 1641198761802
wait for recv string at 1641198761802
recved string at 1641198762001
1
wait for recv [self.estimator, stat] at 1641198762001
recved [self.estimator, stat] at 1641198762001
sorted packlist at 1641198762001
packetSeq:  5151
packetSeq:  5152
packetSeq:  5153
packetSeq:  5154
packetSeq:  5155
packetSeq:  5156
packetSeq:  5157
packetSeq:  5158
processed packlist at 1641198762001
receiving_rate:  272880.0
delay:  198.375
loss_ratio:  0.0
processed state0-2 at 1641198762001
avgFrameBetween:  6
psnrStat:  [[552560, 553292, 552535, 551919, 551425, 550960]]
delayStat:  [[294, 293, 294, 294, 294, 293]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552115.1666666666] [293.6666666666667] [0]
processed state3-5 at 1641198762001
liner_to_log:  tensor([[[0.6502]]]) tensor([[[0.2341]]])
linear_to_log at 1641198762002
listState:  [0.06822, 0.13225, 0.0, 0.5521151666666666, 0.2936666666666667, 0.0, tensor([[[0.2341]]])]
state_clone_detach at 1641198762002
reward: 0.04598464462160218
state tensor([0.0682, 0.1322, 0.0000, 0.2341], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198762002
state222:  tensor([[0.0682, 0.1322, 0.0000, 0.2341]], device='cuda:0')
policy_old.forwarded at 1641198762004
give action 194============================
log_to_linear:  tensor([[[0.7758]]], device='cuda:0') tensor([[[1.5031]]], device='cuda:0')
log_to_linear action at 1641198762005
bwe changes from to:  [tensor([[[0.2747]]]), tensor([[[0.4129]]])]
step into gymStat at 1641198762005
send bwe to appRecv at 1641198762005
sent bwe to appRecv at 1641198762005
wait for recv string at 1641198762005
recved string at 1641198762225
1
wait for recv [self.estimator, stat] at 1641198762225
recved [self.estimator, stat] at 1641198762225
sorted packlist at 1641198762225
packetSeq:  5159
packetSeq:  5160
packetSeq:  5161
packetSeq:  5162
packetSeq:  5163
packetSeq:  5164
packetSeq:  5165
processed packlist at 1641198762225
receiving_rate:  272880.0
delay:  198.85714285714286
loss_ratio:  0.0
processed state0-2 at 1641198762225
avgFrameBetween:  6
psnrStat:  [[549108, 549657, 551025, 550177, 549008, 548919, 548909]]
delayStat:  [[294, 293, 293, 294, 293, 293, 293]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549543.2857142857] [293.2857142857143] [0]
processed state3-5 at 1641198762226
liner_to_log:  tensor([[[1.5031]]]) tensor([[[0.7758]]])
linear_to_log at 1641198762226
listState:  [0.06822, 0.13257142857142856, 0.0, 0.5495432857142857, 0.29328571428571426, 0.0, tensor([[[0.7758]]])]
state_clone_detach at 1641198762226
reward: 0.04502035890731643
state tensor([0.0682, 0.1326, 0.0000, 0.7758], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198762227
state222:  tensor([[0.0682, 0.1326, 0.0000, 0.7758]], device='cuda:0')
policy_old.forwarded at 1641198762228
give action 195============================
log_to_linear:  tensor([[[0.6410]]], device='cuda:0') tensor([[[1.2413]]], device='cuda:0')
log_to_linear action at 1641198762229
bwe changes from to:  [tensor([[[0.4129]]]), tensor([[[0.5125]]])]
step into gymStat at 1641198762230
send bwe to appRecv at 1641198762230
sent bwe to appRecv at 1641198762230
wait for recv string at 1641198762230
recved string at 1641198762456
1
wait for recv [self.estimator, stat] at 1641198762456
recved [self.estimator, stat] at 1641198762456
sorted packlist at 1641198762456
packetSeq:  5166
packetSeq:  5167
packetSeq:  5168
packetSeq:  5169
packetSeq:  5170
packetSeq:  5171
packetSeq:  5172
packetSeq:  5173
packetSeq:  5174
packetSeq:  5175
packetSeq:  5176
processed packlist at 1641198762456
receiving_rate:  302480.0
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198762456
avgFrameBetween:  6
psnrStat:  [[548713, 548466, 548368, 548277, 548311, 549487, 550095]]
delayStat:  [[293, 293, 293, 293, 295, 293, 294]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548816.7142857143] [293.42857142857144] [0]
processed state3-5 at 1641198762456
liner_to_log:  tensor([[[1.2413]]]) tensor([[[0.6410]]])
linear_to_log at 1641198762456
listState:  [0.07562, 0.13133333333333333, 0.0, 0.5488167142857143, 0.2934285714285714, 0.0, tensor([[[0.6410]]])]
state_clone_detach at 1641198762457
reward: 0.0817737894676615
state tensor([0.0756, 0.1313, 0.0000, 0.6410], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198762457
state222:  tensor([[0.0756, 0.1313, 0.0000, 0.6410]], device='cuda:0')
policy_old.forwarded at 1641198762459
give action 196============================
log_to_linear:  tensor([[[0.6729]]], device='cuda:0') tensor([[[1.3007]]], device='cuda:0')
log_to_linear action at 1641198762460
bwe changes from to:  [tensor([[[0.5125]]]), tensor([[[0.6666]]])]
step into gymStat at 1641198762460
send bwe to appRecv at 1641198762460
sent bwe to appRecv at 1641198762460
wait for recv string at 1641198762460
recved string at 1641198762658
1
wait for recv [self.estimator, stat] at 1641198762658
recved [self.estimator, stat] at 1641198762658
sorted packlist at 1641198762658
packetSeq:  5177
packetSeq:  5178
packetSeq:  5179
packetSeq:  5180
packetSeq:  5181
packetSeq:  5182
packetSeq:  5183
packetSeq:  5184
packetSeq:  5185
processed packlist at 1641198762658
receiving_rate:  255320.0
delay:  196.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198762658
avgFrameBetween:  6
psnrStat:  [[550869, 551047, 551337, 552112, 552085, 552445]]
delayStat:  [[294, 293, 293, 293, 293, 293]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551649.1666666666] [293.1666666666667] [0]
processed state3-5 at 1641198762658
liner_to_log:  tensor([[[1.3007]]]) tensor([[[0.6729]]])
linear_to_log at 1641198762658
listState:  [0.06383, 0.13118518518518518, 0.0, 0.5516491666666666, 0.2931666666666667, 0.0, tensor([[[0.6729]]])]
state_clone_detach at 1641198762659
reward: 0.028683028904198604
state tensor([0.0638, 0.1312, 0.0000, 0.6729], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198762659
state222:  tensor([[0.0638, 0.1312, 0.0000, 0.6729]], device='cuda:0')
policy_old.forwarded at 1641198762661
give action 197============================
log_to_linear:  tensor([[[0.2345]]], device='cuda:0') tensor([[[0.6506]]], device='cuda:0')
log_to_linear action at 1641198762662
bwe changes from to:  [tensor([[[0.6666]]]), tensor([[[0.4337]]])]
step into gymStat at 1641198762662
send bwe to appRecv at 1641198762662
sent bwe to appRecv at 1641198762662
wait for recv string at 1641198762662
recved string at 1641198762884
1
wait for recv [self.estimator, stat] at 1641198762884
recved [self.estimator, stat] at 1641198762884
sorted packlist at 1641198762884
packetSeq:  5186
packetSeq:  5187
packetSeq:  5188
packetSeq:  5189
packetSeq:  5190
packetSeq:  5191
packetSeq:  5192
packetSeq:  5193
packetSeq:  5194
packetSeq:  5195
packetSeq:  5196
packetSeq:  5197
processed packlist at 1641198762884
receiving_rate:  272120.0
delay:  191.8
loss_ratio:  0.0
processed state0-2 at 1641198762884
avgFrameBetween:  6
psnrStat:  [[553281, 553045, 552722, 552952, 552213, 552314, 552771]]
delayStat:  [[293, 293, 278, 277, 276, 278, 276]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552756.8571428572] [281.57142857142856] [0]
processed state3-5 at 1641198762884
liner_to_log:  tensor([[[0.6506]]]) tensor([[[0.2345]]])
linear_to_log at 1641198762885
listState:  [0.06803, 0.12786666666666668, 0.0, 0.5527568571428572, 0.28157142857142853, 0.0, tensor([[[0.2345]]])]
state_clone_detach at 1641198762885
reward: 0.05826191719500545
state tensor([0.0680, 0.1279, 0.0000, 0.2345], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198762886
state222:  tensor([[0.0680, 0.1279, 0.0000, 0.2345]], device='cuda:0')
policy_old.forwarded at 1641198762887
give action 198============================
log_to_linear:  tensor([[[0.5977]]], device='cuda:0') tensor([[[1.1635]]], device='cuda:0')
log_to_linear action at 1641198762888
bwe changes from to:  [tensor([[[0.4337]]]), tensor([[[0.5046]]])]
step into gymStat at 1641198762888
send bwe to appRecv at 1641198762888
sent bwe to appRecv at 1641198762888
wait for recv string at 1641198762888
recved string at 1641198763087
1
wait for recv [self.estimator, stat] at 1641198763087
recved [self.estimator, stat] at 1641198763087
sorted packlist at 1641198763087
packetSeq:  5198
packetSeq:  5199
packetSeq:  5200
packetSeq:  5201
packetSeq:  5202
packetSeq:  5203
packetSeq:  5204
packetSeq:  5205
processed packlist at 1641198763087
receiving_rate:  255840.0
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198763087
avgFrameBetween:  6
psnrStat:  [[552154, 552804, 553341, 553095, 553526, 553851]]
delayStat:  [[276, 276, 275, 276, 277, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553128.5] [275.8333333333333] [0]
processed state3-5 at 1641198763087
liner_to_log:  tensor([[[1.1635]]]) tensor([[[0.5977]]])
linear_to_log at 1641198763087
listState:  [0.06396, 0.1285, 0.0, 0.5531285, 0.2758333333333333, 0.0, tensor([[[0.5977]]])]
state_clone_detach at 1641198763088
reward: 0.03735569753286688
state tensor([0.0640, 0.1285, 0.0000, 0.5977], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198763088
state222:  tensor([[0.0640, 0.1285, 0.0000, 0.5977]], device='cuda:0')
policy_old.forwarded at 1641198763090
give action 199============================
log_to_linear:  tensor([[[0.1697]]], device='cuda:0') tensor([[[0.5902]]], device='cuda:0')
log_to_linear action at 1641198763091
bwe changes from to:  [tensor([[[0.5046]]]), tensor([[[0.2978]]])]
step into gymStat at 1641198763091
send bwe to appRecv at 1641198763091
sent bwe to appRecv at 1641198763091
wait for recv string at 1641198763091
recved string at 1641198763323
1
wait for recv [self.estimator, stat] at 1641198763323
recved [self.estimator, stat] at 1641198763324
sorted packlist at 1641198763324
packetSeq:  5206
packetSeq:  5207
packetSeq:  5208
packetSeq:  5209
packetSeq:  5210
packetSeq:  5211
packetSeq:  5212
packetSeq:  5213
packetSeq:  5214
packetSeq:  5215
processed packlist at 1641198763324
receiving_rate:  238640.0
delay:  192.875
loss_ratio:  0.0
processed state0-2 at 1641198763324
avgFrameBetween:  6
psnrStat:  [[554415, 554714, 555040, 554848, 555415, 555845, 555681]]
delayStat:  [[276, 277, 276, 276, 277, 275, 276]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555136.8571428572] [276.14285714285717] [0]
processed state3-5 at 1641198763324
liner_to_log:  tensor([[[0.5902]]]) tensor([[[0.1697]]])
linear_to_log at 1641198763324
listState:  [0.05966, 0.12858333333333333, 0.0, 0.5551368571428572, 0.2761428571428572, 0.0, tensor([[[0.1697]]])]
state_clone_detach at 1641198763324
reward: 0.016349062500812694
state tensor([0.0597, 0.1286, 0.0000, 0.1697], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198763325
state222:  tensor([[0.0597, 0.1286, 0.0000, 0.1697]], device='cuda:0')
policy_old.forwarded at 1641198763327
give action 200============================
log_to_linear:  tensor([[[0.4483]]], device='cuda:0') tensor([[[0.9205]]], device='cuda:0')
log_to_linear action at 1641198763328
bwe changes from to:  [tensor([[[0.2978]]]), tensor([[[0.2742]]])]
step into gymStat at 1641198763328
send bwe to appRecv at 1641198763328
sent bwe to appRecv at 1641198763328
wait for recv string at 1641198763328
recved string at 1641198763555
1
wait for recv [self.estimator, stat] at 1641198763555
recved [self.estimator, stat] at 1641198763555
sorted packlist at 1641198763555
packetSeq:  5216
packetSeq:  5217
packetSeq:  5218
packetSeq:  5219
packetSeq:  5220
packetSeq:  5221
packetSeq:  5222
packetSeq:  5223
packetSeq:  5224
packetSeq:  5225
packetSeq:  5226
processed packlist at 1641198763555
receiving_rate:  282360.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198763555
avgFrameBetween:  6
psnrStat:  [[557211, 556786, 557758, 557239, 557782, 557435, 557517]]
delayStat:  [[277, 276, 276, 277, 276, 276, 277]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557389.7142857143] [276.42857142857144] [0]
processed state3-5 at 1641198763555
liner_to_log:  tensor([[[0.9205]]]) tensor([[[0.4483]]])
linear_to_log at 1641198763556
listState:  [0.07059, 0.12814814814814815, 0.0, 0.5573897142857143, 0.27642857142857147, 0.0, tensor([[[0.4483]]])]
state_clone_detach at 1641198763556
reward: 0.06907097616922075
state tensor([0.0706, 0.1281, 0.0000, 0.4483], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198763556
state222:  tensor([[0.0706, 0.1281, 0.0000, 0.4483]], device='cuda:0')
policy_old.forwarded at 1641198763558
give action 201============================
log_to_linear:  tensor([[[0.3590]]], device='cuda:0') tensor([[[0.7958]]], device='cuda:0')
log_to_linear action at 1641198763559
bwe changes from to:  [tensor([[[0.2742]]]), tensor([[[0.2182]]])]
step into gymStat at 1641198763559
send bwe to appRecv at 1641198763559
sent bwe to appRecv at 1641198763559
wait for recv string at 1641198763559
recved string at 1641198763786
1
wait for recv [self.estimator, stat] at 1641198763786
recved [self.estimator, stat] at 1641198763786
sorted packlist at 1641198763786
packetSeq:  5227
packetSeq:  5228
packetSeq:  5229
packetSeq:  5230
packetSeq:  5231
packetSeq:  5232
packetSeq:  5233
packetSeq:  5234
packetSeq:  5235
processed packlist at 1641198763786
receiving_rate:  250840.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198763786
avgFrameBetween:  6
psnrStat:  [[557442, 557620, 557133, 558384, 557988, 558643, 558626]]
delayStat:  [[276, 276, 277, 276, 276, 275, 276]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557976.5714285715] [276.0] [0]
processed state3-5 at 1641198763786
liner_to_log:  tensor([[[0.7958]]]) tensor([[[0.3590]]])
linear_to_log at 1641198763786
listState:  [0.06271, 0.12816666666666668, 0.0, 0.5579765714285715, 0.276, 0.0, tensor([[[0.3590]]])]
state_clone_detach at 1641198763787
reward: 0.03239546256310022
state tensor([0.0627, 0.1282, 0.0000, 0.3590], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198763787
state222:  tensor([[0.0627, 0.1282, 0.0000, 0.3590]], device='cuda:0')
policy_old.forwarded at 1641198763789
give action 202============================
log_to_linear:  tensor([[[0.5238]]], device='cuda:0') tensor([[[1.0382]]], device='cuda:0')
log_to_linear action at 1641198763790
bwe changes from to:  [tensor([[[0.2182]]]), tensor([[[0.2265]]])]
step into gymStat at 1641198763790
sent 'asking for bwe' at 1641198759226
send [estimator, stat] at 1641198759226
sent [estimator, stat] at 1641198759226
pc wait for bwe at 1641198759226
pc got bwe at 1641198759230
bandwidth:  300000
pc flushed at 1641198759230
Bwe Sent: 4 at 1641198759230
got request at 1641198759458
processed allFrame at 1641198759458
send 'asking for bwe' at 1641198759458
sent 'asking for bwe' at 1641198759458
send [estimator, stat] at 1641198759458
sent [estimator, stat] at 1641198759458
pc wait for bwe at 1641198759458
pc got bwe at 1641198759463
bandwidth:  300000
pc flushed at 1641198759463
Bwe Sent: 5 at 1641198759463
got request at 1641198759659
processed allFrame at 1641198759659
send 'asking for bwe' at 1641198759659
sent 'asking for bwe' at 1641198759659
send [estimator, stat] at 1641198759659
sent [estimator, stat] at 1641198759659
pc wait for bwe at 1641198759660
pc got bwe at 1641198759664
bandwidth:  300000
pc flushed at 1641198759664
Bwe Sent: 5 at 1641198759664
got request at 1641198759889
processed allFrame at 1641198759889
send 'asking for bwe' at 1641198759889
sent 'asking for bwe' at 1641198759889
send [estimator, stat] at 1641198759889
sent [estimator, stat] at 1641198759889
pc wait for bwe at 1641198759889
pc got bwe at 1641198759894
bandwidth:  300000
pc flushed at 1641198759894
Bwe Sent: 5 at 1641198759894
got request at 1641198760122
processed allFrame at 1641198760122
send 'asking for bwe' at 1641198760122
sent 'asking for bwe' at 1641198760122
send [estimator, stat] at 1641198760122
sent [estimator, stat] at 1641198760122
pc wait for bwe at 1641198760122
pc got bwe at 1641198760126
bandwidth:  300000
pc flushed at 1641198760126
Bwe Sent: 4 at 1641198760126
got request at 1641198760327
processed allFrame at 1641198760327
send 'asking for bwe' at 1641198760327
sent 'asking for bwe' at 1641198760327
send [estimator, stat] at 1641198760327
sent [estimator, stat] at 1641198760327
pc wait for bwe at 1641198760328
pc got bwe at 1641198760332
bandwidth:  300000
pc flushed at 1641198760332
Bwe Sent: 5 at 1641198760332
got request at 1641198760555
processed allFrame at 1641198760555
send 'asking for bwe' at 1641198760555
sent 'asking for bwe' at 1641198760555
send [estimator, stat] at 1641198760555
sent [estimator, stat] at 1641198760555
pc wait for bwe at 1641198760555
pc got bwe at 1641198760559
bandwidth:  300000
pc flushed at 1641198760560
Bwe Sent: 5 at 1641198760560
got request at 1641198760760
processed allFrame at 1641198760760
send 'asking for bwe' at 1641198760760
sent 'asking for bwe' at 1641198760760
send [estimator, stat] at 1641198760760
sent [estimator, stat] at 1641198760760
pc wait for bwe at 1641198760760
pc got bwe at 1641198760764
bandwidth:  300000
pc flushed at 1641198760764
Bwe Sent: 4 at 1641198760764
got request at 1641198760962
processed allFrame at 1641198760962
send 'asking for bwe' at 1641198760962
sent 'asking for bwe' at 1641198760962
send [estimator, stat] at 1641198760962
sent [estimator, stat] at 1641198760962
pc wait for bwe at 1641198760962
pc got bwe at 1641198760966
bandwidth:  300000
pc flushed at 1641198760966
Bwe Sent: 4 at 1641198760966
got request at 1641198761164
processed allFrame at 1641198761164
send 'asking for bwe' at 1641198761164
sent 'asking for bwe' at 1641198761164
send [estimator, stat] at 1641198761164
sent [estimator, stat] at 1641198761164
pc wait for bwe at 1641198761164
pc got bwe at 1641198761169
bandwidth:  300000
pc flushed at 1641198761169
Bwe Sent: 5 at 1641198761169
got request at 1641198761392
processed allFrame at 1641198761392
send 'asking for bwe' at 1641198761392
sent 'asking for bwe' at 1641198761392
send [estimator, stat] at 1641198761392
sent [estimator, stat] at 1641198761392
pc wait for bwe at 1641198761392
pc got bwe at 1641198761396
bandwidth:  300000
pc flushed at 1641198761396
Bwe Sent: 4 at 1641198761396
got request at 1641198761596
processed allFrame at 1641198761596
send 'asking for bwe' at 1641198761596
sent 'asking for bwe' at 1641198761596
send [estimator, stat] at 1641198761596
sent [estimator, stat] at 1641198761596
pc wait for bwe at 1641198761596
pc got bwe at 1641198761600
bandwidth:  300000
pc flushed at 1641198761600
Bwe Sent: 4 at 1641198761600
got request at 1641198761797
processed allFrame at 1641198761797
send 'asking for bwe' at 1641198761797
sent 'asking for bwe' at 1641198761797
send [estimator, stat] at 1641198761797
sent [estimator, stat] at 1641198761797
pc wait for bwe at 1641198761797
pc got bwe at 1641198761802
bandwidth:  300000
pc flushed at 1641198761802
Bwe Sent: 5 at 1641198761802
got request at 1641198762001
processed allFrame at 1641198762001
send 'asking for bwe' at 1641198762001
sent 'asking for bwe' at 1641198762001
send [estimator, stat] at 1641198762001
sent [estimator, stat] at 1641198762001
pc wait for bwe at 1641198762001
pc got bwe at 1641198762005
bandwidth:  300000
pc flushed at 1641198762005
Bwe Sent: 4 at 1641198762005
got request at 1641198762225
processed allFrame at 1641198762225
send 'asking for bwe' at 1641198762225
sent 'asking for bwe' at 1641198762225
send [estimator, stat] at 1641198762225
sent [estimator, stat] at 1641198762225
pc wait for bwe at 1641198762225
pc got bwe at 1641198762230
bandwidth:  300000
pc flushed at 1641198762230
Bwe Sent: 5 at 1641198762230
got request at 1641198762455
processed allFrame at 1641198762456
send 'asking for bwe' at 1641198762456
sent 'asking for bwe' at 1641198762456
send [estimator, stat] at 1641198762456
sent [estimator, stat] at 1641198762456
pc wait for bwe at 1641198762456
pc got bwe at 1641198762460
bandwidth:  300000
pc flushed at 1641198762460
Bwe Sent: 5 at 1641198762460
got request at 1641198762658
processed allFrame at 1641198762658
send 'asking for bwe' at 1641198762658
sent 'asking for bwe' at 1641198762658
send [estimator, stat] at 1641198762658
sent [estimator, stat] at 1641198762658
pc wait for bwe at 1641198762658
pc got bwe at 1641198762662
bandwidth:  300000
pc flushed at 1641198762662
Bwe Sent: 4 at 1641198762662
got request at 1641198762884
processed allFrame at 1641198762884
send 'asking for bwe' at 1641198762884
sent 'asking for bwe' at 1641198762884
send [estimator, stat] at 1641198762884
sent [estimator, stat] at 1641198762884
pc wait for bwe at 1641198762884
pc got bwe at 1641198762889
bandwidth:  300000
pc flushed at 1641198762889
Bwe Sent: 5 at 1641198762889
got request at 1641198763086
processed allFrame at 1641198763087
send 'asking for bwe' at 1641198763087
sent 'asking for bwe' at 1641198763087
send [estimator, stat] at 1641198763087
sent [estimator, stat] at 1641198763087
pc wait for bwe at 1641198763087
pc got bwe at 1641198763091
bandwidth:  300000
pc flushed at 1641198763091
Bwe Sent: 5 at 1641198763091
got request at 1641198763323
processed allFrame at 1641198763323
send 'asking for bwe' at 1641198763323
sent 'asking for bwe' at 1641198763323
send [estimator, stat] at 1641198763323
sent [estimator, stat] at 1641198763323
pc wait for bwe at 1641198763324
pc got bwe at 1641198763328
bandwidth:  300000
pc flushed at 1641198763328
Bwe Sent: 5 at 1641198763328
got request at 1641198763555
processed allFrame at 1641198763555
send 'asking for bwe' at 1641198763555
sent 'asking for bwe' at 1641198763555
send [estimator, stat] at 1641198763555
sent [estimator, stat] at 1641198763555
pc wait for bwe at 1641198763555
pc got bwe at 1641198763559
bandwidth:  300000
pc flushed at 1641198763559
Bwe Sent: 4 at 1641198763559
got request at 1641198763785
processed allFrame at 1641198763786
send 'asking for bwe' at 1641198763786
sent 'asking for bwe' at 1641198763786
send [estimator, stat] at 1641198763786
sent [estimator, stat] at 1641198763786
pc wait for bwe at 1641198763786
pc got bwe at 1641198763790
bandwidth:  300000
pc flushed at 1641198763790
Bwe Sent: 5 at 1641198763790
got request at 1641198764021
processed allFrame at 1641198764021
send 'asking for bwe' at 1641198764021
sent 'asking for bwe' at 1641198764021
send [estimator, stat] at 1641198764021
sent [estimator, stat] at 1641198764021
pc wait for bwe at 1641198764021
pc got bwe at 1641198764026
bandwidth:  300000
pc flushed at 1641198764026
Bwe Sent: 5 at 1641198764026
send bwe to appRecv at 1641198763790
sent bwe to appRecv at 1641198763790
wait for recv string at 1641198763790
recved string at 1641198764021
1
wait for recv [self.estimator, stat] at 1641198764021
recved [self.estimator, stat] at 1641198764021
sorted packlist at 1641198764021
packetSeq:  5236
packetSeq:  5237
packetSeq:  5238
packetSeq:  5239
packetSeq:  5240
packetSeq:  5241
packetSeq:  5242
packetSeq:  5243
packetSeq:  5244
packetSeq:  5245
packetSeq:  5246
packetSeq:  5247
processed packlist at 1641198764021
receiving_rate:  312600.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198764021
avgFrameBetween:  6
psnrStat:  [[558701, 558858, 558149, 558095, 557681, 557669, 558453]]
delayStat:  [[275, 276, 276, 275, 275, 276, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558229.4285714285] [275.42857142857144] [0]
processed state3-5 at 1641198764021
liner_to_log:  tensor([[[1.0382]]]) tensor([[[0.5238]]])
linear_to_log at 1641198764022
listState:  [0.07815, 0.12793939393939394, 0.0, 0.5582294285714285, 0.27542857142857147, 0.0, tensor([[[0.5238]]])]
state_clone_detach at 1641198764022
reward: 0.10284660509792415
state tensor([0.0781, 0.1279, 0.0000, 0.5238], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198764023
state222:  tensor([[0.0781, 0.1279, 0.0000, 0.5238]], device='cuda:0')
policy_old.forwarded at 1641198764024
give action 203============================
log_to_linear:  tensor([[[0.6049]]], device='cuda:0') tensor([[[1.1761]]], device='cuda:0')
log_to_linear action at 1641198764025
bwe changes from to:  [tensor([[[0.2265]]]), tensor([[[0.2664]]])]
step into gymStat at 1641198764025
send bwe to appRecv at 1641198764025
sent bwe to appRecv at 1641198764025
wait for recv string at 1641198764025
recved string at 1641198764252
1
wait for recv [self.estimator, stat] at 1641198764252
recved [self.estimator, stat] at 1641198764252
sorted packlist at 1641198764252
packetSeq:  5248
packetSeq:  5249
packetSeq:  5250
packetSeq:  5251
packetSeq:  5252
packetSeq:  5253
packetSeq:  5254
packetSeq:  5255
packetSeq:  5256
packetSeq:  5257
processed packlist at 1641198764252
receiving_rate:  280920.0
delay:  192.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198764252
avgFrameBetween:  6
psnrStat:  [[557677, 558043, 557628, 558227, 558209, 558081, 558587]]
delayStat:  [[274, 275, 275, 275, 275, 276, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558064.5714285715] [275.0] [0]
processed state3-5 at 1641198764252
liner_to_log:  tensor([[[1.1761]]]) tensor([[[0.6049]]])
linear_to_log at 1641198764253
listState:  [0.07023, 0.12837037037037036, 0.0, 0.5580645714285715, 0.275, 0.0, tensor([[[0.6049]]])]
state_clone_detach at 1641198764253
reward: 0.06677914862634204
state tensor([0.0702, 0.1284, 0.0000, 0.6049], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198764253
state222:  tensor([[0.0702, 0.1284, 0.0000, 0.6049]], device='cuda:0')
policy_old.forwarded at 1641198764255
give action 204============================
log_to_linear:  tensor([[[0.6782]]], device='cuda:0') tensor([[[1.3105]]], device='cuda:0')
log_to_linear action at 1641198764256
bwe changes from to:  [tensor([[[0.2664]]]), tensor([[[0.3491]]])]
step into gymStat at 1641198764256
send bwe to appRecv at 1641198764256
sent bwe to appRecv at 1641198764256
wait for recv string at 1641198764256
recved string at 1641198764455
1
wait for recv [self.estimator, stat] at 1641198764455
recved [self.estimator, stat] at 1641198764455
sorted packlist at 1641198764455
packetSeq:  5258
packetSeq:  5259
packetSeq:  5260
packetSeq:  5261
packetSeq:  5262
packetSeq:  5263
packetSeq:  5264
packetSeq:  5265
packetSeq:  5266
packetSeq:  5267
packetSeq:  5268
processed packlist at 1641198764455
receiving_rate:  304120.0
delay:  191.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198764455
avgFrameBetween:  6
psnrStat:  [[556762, 557180, 556940, 556101, 556000, 556011]]
delayStat:  [[275, 275, 274, 276, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556499.0] [275.0] [0]
processed state3-5 at 1641198764455
liner_to_log:  tensor([[[1.3105]]]) tensor([[[0.6782]]])
linear_to_log at 1641198764456
listState:  [0.07603, 0.12787878787878787, 0.0, 0.556499, 0.275, 0.0, tensor([[[0.6782]]])]
state_clone_detach at 1641198764456
reward: 0.09391584752009541
state tensor([0.0760, 0.1279, 0.0000, 0.6782], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198764457
state222:  tensor([[0.0760, 0.1279, 0.0000, 0.6782]], device='cuda:0')
policy_old.forwarded at 1641198764458
give action 205============================
log_to_linear:  tensor([[[0.5594]]], device='cuda:0') tensor([[[1.0974]]], device='cuda:0')
log_to_linear action at 1641198764459
bwe changes from to:  [tensor([[[0.3491]]]), tensor([[[0.3831]]])]
step into gymStat at 1641198764459
send bwe to appRecv at 1641198764459
sent bwe to appRecv at 1641198764459
wait for recv string at 1641198764459
recved string at 1641198764658
1
wait for recv [self.estimator, stat] at 1641198764658
recved [self.estimator, stat] at 1641198764658
sorted packlist at 1641198764658
packetSeq:  5269
packetSeq:  5270
packetSeq:  5271
packetSeq:  5272
packetSeq:  5273
packetSeq:  5274
packetSeq:  5275
packetSeq:  5276
packetSeq:  5277
packetSeq:  5278
packetSeq:  5279
processed packlist at 1641198764658
receiving_rate:  280320.0
delay:  191.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198764658
avgFrameBetween:  6
psnrStat:  [[555584, 555328, 556815, 556596, 556132, 555754, 555630]]
delayStat:  [[276, 275, 275, 276, 275, 275, 275]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555977.0] [275.2857142857143] [0]
processed state3-5 at 1641198764658
liner_to_log:  tensor([[[1.0974]]]) tensor([[[0.5594]]])
linear_to_log at 1641198764658
listState:  [0.07008, 0.1276969696969697, 0.0, 0.555977, 0.2752857142857143, 0.0, tensor([[[0.5594]]])]
state_clone_detach at 1641198764659
reward: 0.06812089895162476
state tensor([0.0701, 0.1277, 0.0000, 0.5594], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198764659
state222:  tensor([[0.0701, 0.1277, 0.0000, 0.5594]], device='cuda:0')
policy_old.forwarded at 1641198764661
give action 206============================
log_to_linear:  tensor([[[0.2082]]], device='cuda:0') tensor([[[0.6247]]], device='cuda:0')
log_to_linear action at 1641198764662
bwe changes from to:  [tensor([[[0.3831]]]), tensor([[[0.2393]]])]
step into gymStat at 1641198764662
send bwe to appRecv at 1641198764662
sent bwe to appRecv at 1641198764662
wait for recv string at 1641198764662
recved string at 1641198764885
1
wait for recv [self.estimator, stat] at 1641198764885
recved [self.estimator, stat] at 1641198764885
sorted packlist at 1641198764885
packetSeq:  5280
packetSeq:  5281
packetSeq:  5282
packetSeq:  5283
packetSeq:  5284
packetSeq:  5285
packetSeq:  5286
packetSeq:  5287
packetSeq:  5288
processed packlist at 1641198764885
receiving_rate:  235200.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198764885
avgFrameBetween:  6
psnrStat:  [[555113, 554906, 553475, 553203, 552744, 551998, 551285]]
delayStat:  [[275, 259, 260, 259, 259, 260, 259]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553246.2857142857] [261.57142857142856] [0]
processed state3-5 at 1641198764885
liner_to_log:  tensor([[[0.6247]]]) tensor([[[0.2082]]])
linear_to_log at 1641198764886
listState:  [0.0588, 0.12816666666666668, 0.0, 0.5532462857142857, 0.26157142857142857, 0.0, tensor([[[0.2082]]])]
state_clone_detach at 1641198764886
reward: 0.013360155524589135
state tensor([0.0588, 0.1282, 0.0000, 0.2082], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198764887
state222:  tensor([[0.0588, 0.1282, 0.0000, 0.2082]], device='cuda:0')
policy_old.forwarded at 1641198764888
give action 207============================
log_to_linear:  tensor([[[0.3499]]], device='cuda:0') tensor([[[0.7839]]], device='cuda:0')
log_to_linear action at 1641198764889
bwe changes from to:  [tensor([[[0.2393]]]), tensor([[[0.1876]]])]
step into gymStat at 1641198764890
send bwe to appRecv at 1641198764890
sent bwe to appRecv at 1641198764890
wait for recv string at 1641198764890
recved string at 1641198765089
1
wait for recv [self.estimator, stat] at 1641198765089
recved [self.estimator, stat] at 1641198765089
sorted packlist at 1641198765089
packetSeq:  5289
packetSeq:  5290
packetSeq:  5291
packetSeq:  5292
packetSeq:  5293
packetSeq:  5294
packetSeq:  5295
packetSeq:  5296
packetSeq:  5297
packetSeq:  5298
processed packlist at 1641198765089
receiving_rate:  278560.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198765089
avgFrameBetween:  6
psnrStat:  [[551158, 550741, 549924, 550346, 551471, 552730]]
delayStat:  [[260, 260, 260, 258, 259, 258]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551061.6666666666] [259.1666666666667] [0]
processed state3-5 at 1641198765089
liner_to_log:  tensor([[[0.7839]]]) tensor([[[0.3499]]])
linear_to_log at 1641198765090
listState:  [0.06964, 0.12806666666666666, 0.0, 0.5510616666666667, 0.2591666666666667, 0.0, tensor([[[0.3499]]])]
state_clone_detach at 1641198765090
reward: 0.06501724591563018
state tensor([0.0696, 0.1281, 0.0000, 0.3499], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198765090
state222:  tensor([[0.0696, 0.1281, 0.0000, 0.3499]], device='cuda:0')
policy_old.forwarded at 1641198765092
give action 208============================
log_to_linear:  tensor([[[0.6088]]], device='cuda:0') tensor([[[1.1831]]], device='cuda:0')
log_to_linear action at 1641198765093
bwe changes from to:  [tensor([[[0.1876]]]), tensor([[[0.2220]]])]
step into gymStat at 1641198765093
send bwe to appRecv at 1641198765093
sent bwe to appRecv at 1641198765093
wait for recv string at 1641198765093
recved string at 1641198765292
1
wait for recv [self.estimator, stat] at 1641198765292
recved [self.estimator, stat] at 1641198765292
sorted packlist at 1641198765292
packetSeq:  5299
packetSeq:  5300
packetSeq:  5301
packetSeq:  5302
packetSeq:  5303
packetSeq:  5304
packetSeq:  5305
packetSeq:  5306
packetSeq:  5307
packetSeq:  5308
packetSeq:  5309
packetSeq:  5310
processed packlist at 1641198765292
receiving_rate:  285640.0
delay:  191.25
loss_ratio:  0.0
processed state0-2 at 1641198765292
avgFrameBetween:  6
psnrStat:  [[553140, 553928, 554972, 554348, 554245, 553700]]
delayStat:  [[258, 259, 258, 258, 258, 258]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554055.5] [258.1666666666667] [0]
processed state3-5 at 1641198765292
liner_to_log:  tensor([[[1.1831]]]) tensor([[[0.6088]]])
linear_to_log at 1641198765292
listState:  [0.07141, 0.1275, 0.0, 0.5540555, 0.2581666666666667, 0.0, tensor([[[0.6088]]])]
state_clone_detach at 1641198765293
reward: 0.07470082545907142
state tensor([0.0714, 0.1275, 0.0000, 0.6088], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198765293
state222:  tensor([[0.0714, 0.1275, 0.0000, 0.6088]], device='cuda:0')
policy_old.forwarded at 1641198765295
give action 209============================
log_to_linear:  tensor([[[0.7530]]], device='cuda:0') tensor([[[1.4568]]], device='cuda:0')
log_to_linear action at 1641198765296
bwe changes from to:  [tensor([[[0.2220]]]), tensor([[[0.3234]]])]
step into gymStat at 1641198765296
send bwe to appRecv at 1641198765296
sent bwe to appRecv at 1641198765296
wait for recv string at 1641198765296
recved string at 1641198765519
1
wait for recv [self.estimator, stat] at 1641198765519
recved [self.estimator, stat] at 1641198765519
sorted packlist at 1641198765519
packetSeq:  5311
packetSeq:  5312
packetSeq:  5313
packetSeq:  5314
packetSeq:  5315
packetSeq:  5316
packetSeq:  5317
packetSeq:  5318
packetSeq:  5319
packetSeq:  5320
processed packlist at 1641198765519
receiving_rate:  271600.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198765519
avgFrameBetween:  6
psnrStat:  [[554635, 553798, 553726, 553440, 552997, 552644, 551530]]
delayStat:  [[259, 258, 258, 259, 258, 258, 258]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553252.8571428572] [258.2857142857143] [0]
processed state3-5 at 1641198765519
liner_to_log:  tensor([[[1.4568]]]) tensor([[[0.7530]]])
linear_to_log at 1641198765520
listState:  [0.0679, 0.12814814814814815, 0.0, 0.5532528571428572, 0.2582857142857143, 0.0, tensor([[[0.7530]]])]
state_clone_detach at 1641198765520
reward: 0.05681961120567364
state tensor([0.0679, 0.1281, 0.0000, 0.7530], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198765520
state222:  tensor([[0.0679, 0.1281, 0.0000, 0.7530]], device='cuda:0')
policy_old.forwarded at 1641198765522
give action 210============================
log_to_linear:  tensor([[[0.3315]]], device='cuda:0') tensor([[[0.7607]]], device='cuda:0')
log_to_linear action at 1641198765523
bwe changes from to:  [tensor([[[0.3234]]]), tensor([[[0.2460]]])]
step into gymStat at 1641198765523
send bwe to appRecv at 1641198765523
sent bwe to appRecv at 1641198765523
wait for recv string at 1641198765523
recved string at 1641198765722
1
wait for recv [self.estimator, stat] at 1641198765722
recved [self.estimator, stat] at 1641198765722
sorted packlist at 1641198765722
packetSeq:  5321
packetSeq:  5322
packetSeq:  5323
packetSeq:  5324
packetSeq:  5325
packetSeq:  5326
packetSeq:  5327
packetSeq:  5328
packetSeq:  5329
packetSeq:  5330
packetSeq:  5331
processed packlist at 1641198765722
receiving_rate:  310520.0
delay:  192.0909090909091
loss_ratio:  0.0
processed state0-2 at 1641198765722
avgFrameBetween:  6
psnrStat:  [[551618, 551113, 549862, 550092, 549592, 550363]]
delayStat:  [[258, 259, 258, 258, 259, 258]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550440.0] [258.3333333333333] [0]
processed state3-5 at 1641198765722
liner_to_log:  tensor([[[0.7607]]]) tensor([[[0.3315]]])
linear_to_log at 1641198765723
listState:  [0.07763, 0.12806060606060607, 0.0, 0.55044, 0.2583333333333333, 0.0, tensor([[[0.3315]]])]
state_clone_detach at 1641198765723
reward: 0.10026058568204416
state tensor([0.0776, 0.1281, 0.0000, 0.3315], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198765723
state222:  tensor([[0.0776, 0.1281, 0.0000, 0.3315]], device='cuda:0')
policy_old.forwarded at 1641198765725
give action 211============================
log_to_linear:  tensor([[[0.4826]]], device='cuda:0') tensor([[[0.9726]]], device='cuda:0')
log_to_linear action at 1641198765726
bwe changes from to:  [tensor([[[0.2460]]]), tensor([[[0.2392]]])]
step into gymStat at 1641198765726
send bwe to appRecv at 1641198765726
sent bwe to appRecv at 1641198765726
wait for recv string at 1641198765726
recved string at 1641198765922
1
wait for recv [self.estimator, stat] at 1641198765922
recved [self.estimator, stat] at 1641198765922
sorted packlist at 1641198765922
packetSeq:  5332
packetSeq:  5333
packetSeq:  5334
packetSeq:  5335
packetSeq:  5336
packetSeq:  5337
packetSeq:  5338
packetSeq:  5339
packetSeq:  5340
packetSeq:  5341
processed packlist at 1641198765922
receiving_rate:  271640.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198765922
avgFrameBetween:  6
psnrStat:  [[549318, 548957, 549226, 549130, 547497, 547499]]
delayStat:  [[258, 258, 259, 258, 259, 259]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548604.5] [258.5] [0]
processed state3-5 at 1641198765922
liner_to_log:  tensor([[[0.9726]]]) tensor([[[0.4826]]])
linear_to_log at 1641198765923
listState:  [0.06791, 0.12806666666666666, 0.0, 0.5486045, 0.2585, 0.0, tensor([[[0.4826]]])]
state_clone_detach at 1641198765923
reward: 0.05711006615587272
state tensor([0.0679, 0.1281, 0.0000, 0.4826], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198765924
state222:  tensor([[0.0679, 0.1281, 0.0000, 0.4826]], device='cuda:0')
policy_old.forwarded at 1641198765925
give action 212============================
log_to_linear:  tensor([[[0.6373]]], device='cuda:0') tensor([[[1.2345]]], device='cuda:0')
log_to_linear action at 1641198765926
bwe changes from to:  [tensor([[[0.2392]]]), tensor([[[0.2954]]])]
step into gymStat at 1641198765926
send bwe to appRecv at 1641198765926
sent bwe to appRecv at 1641198765927
wait for recv string at 1641198765927
recved string at 1641198766156
1
wait for recv [self.estimator, stat] at 1641198766156
recved [self.estimator, stat] at 1641198766156
sorted packlist at 1641198766156
packetSeq:  5342
packetSeq:  5343
packetSeq:  5344
packetSeq:  5345
packetSeq:  5346
packetSeq:  5347
packetSeq:  5348
packetSeq:  5349
processed packlist at 1641198766156
receiving_rate:  250920.0
delay:  193.14285714285714
loss_ratio:  0.0
processed state0-2 at 1641198766156
avgFrameBetween:  6
psnrStat:  [[546221, 545182, 544719, 544288, 544223, 544086, 543308]]
delayStat:  [[259, 259, 258, 259, 259, 258, 258]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [544575.2857142857] [258.57142857142856] [0]
processed state3-5 at 1641198766156
liner_to_log:  tensor([[[1.2345]]]) tensor([[[0.6373]]])
linear_to_log at 1641198766156
listState:  [0.06273, 0.12876190476190477, 0.0, 0.5445752857142857, 0.25857142857142856, 0.0, tensor([[[0.6373]]])]
state_clone_detach at 1641198766156
reward: 0.030705579614436873
state tensor([0.0627, 0.1288, 0.0000, 0.6373], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198766157
state222:  tensor([[0.0627, 0.1288, 0.0000, 0.6373]], device='cuda:0')
policy_old.forwarded at 1641198766159
give action 213============================
log_to_linear:  tensor([[[0.6729]]], device='cuda:0') tensor([[[1.3006]]], device='cuda:0')
log_to_linear action at 1641198766160
bwe changes from to:  [tensor([[[0.2954]]]), tensor([[[0.3841]]])]
step into gymStat at 1641198766160
send bwe to appRecv at 1641198766160
sent bwe to appRecv at 1641198766160
wait for recv string at 1641198766160
recved string at 1641198766390
1
wait for recv [self.estimator, stat] at 1641198766390
recved [self.estimator, stat] at 1641198766390
sorted packlist at 1641198766390
packetSeq:  5350
packetSeq:  5351
packetSeq:  5352
packetSeq:  5353
packetSeq:  5354
packetSeq:  5355
packetSeq:  5356
packetSeq:  5357
packetSeq:  5358
packetSeq:  5359
packetSeq:  5360
processed packlist at 1641198766390
receiving_rate:  258440.0
delay:  191.7
loss_ratio:  0.0
processed state0-2 at 1641198766390
avgFrameBetween:  6
psnrStat:  [[541550, 541291, 542715, 543311, 545368, 545778, 547140]]
delayStat:  [[258, 258, 258, 258, 258, 258, 257]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [543879.0] [257.85714285714283] [0]
processed state3-5 at 1641198766390
liner_to_log:  tensor([[[1.3006]]]) tensor([[[0.6729]]])
linear_to_log at 1641198766390
listState:  [0.06461, 0.1278, 0.0, 0.543879, 0.25785714285714284, 0.0, tensor([[[0.6729]]])]
state_clone_detach at 1641198766391
reward: 0.04253177793853308
state tensor([0.0646, 0.1278, 0.0000, 0.6729], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198766391
state222:  tensor([[0.0646, 0.1278, 0.0000, 0.6729]], device='cuda:0')
policy_old.forwarded at 1641198766393
give action 214============================
log_to_linear:  tensor([[[0.3780]]], device='cuda:0') tensor([[[0.8210]]], device='cuda:0')
log_to_linear action at 1641198766394
bwe changes from to:  [tensor([[[0.3841]]]), tensor([[[0.3154]]])]
step into gymStat at 1641198766394
send bwe to appRecv at 1641198766394
sent bwe to appRecv at 1641198766394
wait for recv string at 1641198766394
recved string at 1641198766591
1
wait for recv [self.estimator, stat] at 1641198766591
recved [self.estimator, stat] at 1641198766591
sorted packlist at 1641198766591
packetSeq:  5361
packetSeq:  5362
packetSeq:  5363
packetSeq:  5364
packetSeq:  5365
packetSeq:  5366
packetSeq:  5367
packetSeq:  5368
packetSeq:  5369
packetSeq:  5370
processed packlist at 1641198766591
receiving_rate:  271280.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198766591
avgFrameBetween:  6
psnrStat:  [[546673, 546751, 547084, 547783, 548428, 548082]]
delayStat:  [[257, 258, 257, 257, 258, 257]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547466.8333333334] [257.3333333333333] [0]
processed state3-5 at 1641198766591
liner_to_log:  tensor([[[0.8210]]]) tensor([[[0.3780]]])
linear_to_log at 1641198766592
listState:  [0.06782, 0.12793333333333334, 0.0, 0.5474668333333333, 0.2573333333333333, 0.0, tensor([[[0.3780]]])]
state_clone_detach at 1641198766592
reward: 0.05709584453086025
state tensor([0.0678, 0.1279, 0.0000, 0.3780], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198766592
state222:  tensor([[0.0678, 0.1279, 0.0000, 0.3780]], device='cuda:0')
policy_old.forwarded at 1641198766594
give action 215============================
log_to_linear:  tensor([[[0.4216]]], device='cuda:0') tensor([[[0.8816]]], device='cuda:0')
log_to_linear action at 1641198766595
bwe changes from to:  [tensor([[[0.3154]]]), tensor([[[0.2780]]])]
step into gymStat at 1641198766595
send bwe to appRecv at 1641198766595
sent bwe to appRecv at 1641198766595
wait for recv string at 1641198766595
recved string at 1641198766818
1
wait for recv [self.estimator, stat] at 1641198766818
recved [self.estimator, stat] at 1641198766818
sorted packlist at 1641198766818
packetSeq:  5371
packetSeq:  5372
packetSeq:  5373
packetSeq:  5374
packetSeq:  5375
packetSeq:  5376
packetSeq:  5377
packetSeq:  5378
packetSeq:  5379
packetSeq:  5380
packetSeq:  5381
processed packlist at 1641198766818
receiving_rate:  259000.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198766818
avgFrameBetween:  6
psnrStat:  [[548751, 548958, 550282, 550081, 549951, 549451, 549613]]
delayStat:  [[257, 257, 257, 257, 242, 242, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549583.8571428572] [250.57142857142858] [0]
processed state3-5 at 1641198766818
liner_to_log:  tensor([[[0.8816]]]) tensor([[[0.4216]]])
linear_to_log at 1641198766819
listState:  [0.06475, 0.12793333333333334, 0.0, 0.5495838571428572, 0.25057142857142856, 0.0, tensor([[[0.4216]]])]
state_clone_detach at 1641198766819
reward: 0.042792259212965755
state tensor([0.0648, 0.1279, 0.0000, 0.4216], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198766819
state222:  tensor([[0.0648, 0.1279, 0.0000, 0.4216]], device='cuda:0')
policy_old.forwarded at 1641198766821
give action 216============================
log_to_linear:  tensor([[[0.3690]]], device='cuda:0') tensor([[[0.8089]]], device='cuda:0')
log_to_linear action at 1641198766822
bwe changes from to:  [tensor([[[0.2780]]]), tensor([[[0.2249]]])]
step into gymStat at 1641198766822
send bwe to appRecv at 1641198766822
sent bwe to appRecv at 1641198766822
wait for recv string at 1641198766822
recved string at 1641198767052
1
wait for recv [self.estimator, stat] at 1641198767052
recved [self.estimator, stat] at 1641198767052
sorted packlist at 1641198767052
packetSeq:  5382
packetSeq:  5383
packetSeq:  5384
packetSeq:  5385
packetSeq:  5386
packetSeq:  5387
packetSeq:  5388
packetSeq:  5389
packetSeq:  5390
packetSeq:  5391
packetSeq:  5392
packetSeq:  5393
processed packlist at 1641198767053
receiving_rate:  295080.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198767053
avgFrameBetween:  6
psnrStat:  [[549818, 549443, 548926, 549236, 548185, 549419, 549190]]
delayStat:  [[242, 243, 243, 242, 242, 243, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549173.8571428572] [242.42857142857142] [0]
processed state3-5 at 1641198767053
liner_to_log:  tensor([[[0.8089]]]) tensor([[[0.3690]]])
linear_to_log at 1641198767053
listState:  [0.07377, 0.12793939393939394, 0.0, 0.5491738571428572, 0.2424285714285714, 0.0, tensor([[[0.3690]]])]
state_clone_detach at 1641198767053
reward: 0.08386481119344236
state tensor([0.0738, 0.1279, 0.0000, 0.3690], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198767054
state222:  tensor([[0.0738, 0.1279, 0.0000, 0.3690]], device='cuda:0')
policy_old.forwarded at 1641198767056
give action 217============================
log_to_linear:  tensor([[[0.2660]]], device='cuda:0') tensor([[[0.6839]]], device='cuda:0')
log_to_linear action at 1641198767057
bwe changes from to:  [tensor([[[0.2249]]]), tensor([[[0.1538]]])]
step into gymStat at 1641198767057
send bwe to appRecv at 1641198767057
sent bwe to appRecv at 1641198767057
wait for recv string at 1641198767057
recved string at 1641198767255
1
wait for recv [self.estimator, stat] at 1641198767255
recved [self.estimator, stat] at 1641198767256
sorted packlist at 1641198767256
packetSeq:  5394
packetSeq:  5395
packetSeq:  5396
packetSeq:  5397
packetSeq:  5398
packetSeq:  5399
packetSeq:  5400
packetSeq:  5401
packetSeq:  5402
packetSeq:  5403
packetSeq:  5404
processed packlist at 1641198767256
receiving_rate:  281680.0
delay:  191.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198767256
avgFrameBetween:  6
psnrStat:  [[548785, 548756, 549163, 548049, 547070, 546841]]
delayStat:  [[242, 243, 242, 242, 243, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548110.6666666666] [242.33333333333334] [0]
processed state3-5 at 1641198767256
liner_to_log:  tensor([[[0.6839]]]) tensor([[[0.2660]]])
linear_to_log at 1641198767256
listState:  [0.07042, 0.1276969696969697, 0.0, 0.5481106666666666, 0.24233333333333335, 0.0, tensor([[[0.2660]]])]
state_clone_detach at 1641198767257
reward: 0.06965762295551348
state tensor([0.0704, 0.1277, 0.0000, 0.2660], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198767257
state222:  tensor([[0.0704, 0.1277, 0.0000, 0.2660]], device='cuda:0')
policy_old.forwarded at 1641198767259
give action 218============================
log_to_linear:  tensor([[[0.6480]]], device='cuda:0') tensor([[[1.2541]]], device='cuda:0')
log_to_linear action at 1641198767260
bwe changes from to:  [tensor([[[0.1538]]]), tensor([[[0.1929]]])]
step into gymStat at 1641198767260
send bwe to appRecv at 1641198767260
sent bwe to appRecv at 1641198767260
wait for recv string at 1641198767260
recved string at 1641198767457
1
wait for recv [self.estimator, stat] at 1641198767457
recved [self.estimator, stat] at 1641198767457
sorted packlist at 1641198767457
packetSeq:  5405
packetSeq:  5406
packetSeq:  5407
packetSeq:  5408
packetSeq:  5409
packetSeq:  5410
packetSeq:  5411
packetSeq:  5412
packetSeq:  5413
processed packlist at 1641198767457
receiving_rate:  258800.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198767457
avgFrameBetween:  6
psnrStat:  [[547376, 548064, 547259, 548369, 548926, 548271, 547151]]
delayStat:  [[242, 243, 242, 242, 242, 242, 243]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547916.5714285715] [242.28571428571428] [0]
processed state3-5 at 1641198767457
liner_to_log:  tensor([[[1.2541]]]) tensor([[[0.6480]]])
linear_to_log at 1641198767458
listState:  [0.0647, 0.12822222222222224, 0.0, 0.5479165714285715, 0.24228571428571427, 0.0, tensor([[[0.6480]]])]
state_clone_detach at 1641198767458
reward: 0.041689789831532764
state tensor([0.0647, 0.1282, 0.0000, 0.6480], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198767458
state222:  tensor([[0.0647, 0.1282, 0.0000, 0.6480]], device='cuda:0')
policy_old.forwarded at 1641198767460
give action 219============================
log_to_linear:  tensor([[[0.4450]]], device='cuda:0') tensor([[[0.9156]]], device='cuda:0')
log_to_linear action at 1641198767461
bwe changes from to:  [tensor([[[0.1929]]]), tensor([[[0.1766]]])]
step into gymStat at 1641198767461
send bwe to appRecv at 1641198767461
sent bwe to appRecv at 1641198767461
wait for recv string at 1641198767461
recved string at 1641198767660
1
wait for recv [self.estimator, stat] at 1641198767660
recved [self.estimator, stat] at 1641198767660
sorted packlist at 1641198767660
packetSeq:  5414
packetSeq:  5415
packetSeq:  5416
packetSeq:  5417
packetSeq:  5418
packetSeq:  5419
packetSeq:  5420
packetSeq:  5421
packetSeq:  5422
packetSeq:  5423
processed packlist at 1641198767660
receiving_rate:  293160.0
delay:  192.6
loss_ratio:  0.0
processed state0-2 at 1641198767660
avgFrameBetween:  6
psnrStat:  [[546314, 546051, 547067, 546670, 546757, 547821]]
delayStat:  [[242, 242, 242, 242, 244, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [546780.0] [242.33333333333334] [0]
processed state3-5 at 1641198767660
liner_to_log:  tensor([[[0.9156]]]) tensor([[[0.4450]]])
linear_to_log at 1641198767661
listState:  [0.07329, 0.1284, 0.0, 0.54678, 0.24233333333333335, 0.0, tensor([[[0.4450]]])]
state_clone_detach at 1641198767661
reward: 0.08036581078100852
state tensor([0.0733, 0.1284, 0.0000, 0.4450], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198767662
state222:  tensor([[0.0733, 0.1284, 0.0000, 0.4450]], device='cuda:0')
policy_old.forwarded at 1641198767663
give action 220============================
log_to_linear:  tensor([[[0.7187]]], device='cuda:0') tensor([[[1.3887]]], device='cuda:0')
log_to_linear action at 1641198767664
bwe changes from to:  [tensor([[[0.1766]]]), tensor([[[0.2453]]])]
step into gymStat at 1641198767664
send bwe to appRecv at 1641198767664
sent bwe to appRecv at 1641198767664
wait for recv string at 1641198767664
recved string at 1641198767862
1
wait for recv [self.estimator, stat] at 1641198767862
recved [self.estimator, stat] at 1641198767862
sorted packlist at 1641198767862
packetSeq:  5424
packetSeq:  5425
packetSeq:  5426
packetSeq:  5427
packetSeq:  5428
packetSeq:  5429
packetSeq:  5430
packetSeq:  5431
packetSeq:  5432
packetSeq:  5433
processed packlist at 1641198767862
receiving_rate:  276080.0
delay:  196.3
loss_ratio:  0.0
processed state0-2 at 1641198767863
avgFrameBetween:  6
psnrStat:  [[547272, 546897, 547476, 548608, 548616, 547820]]
delayStat:  [[242, 242, 242, 242, 242, 243]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547781.5] [242.16666666666666] [0]
processed state3-5 at 1641198767863
liner_to_log:  tensor([[[1.3887]]]) tensor([[[0.7187]]])
linear_to_log at 1641198767863
listState:  [0.06902, 0.1308666666666667, 0.0, 0.5477815, 0.24216666666666667, 0.0, tensor([[[0.7187]]])]
state_clone_detach at 1641198767863
reward: 0.05379542473930832
state tensor([0.0690, 0.1309, 0.0000, 0.7187], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198767864
state222:  tensor([[0.0690, 0.1309, 0.0000, 0.7187]], device='cuda:0')
policy_old.forwarded at 1641198767865
give action 221============================
log_to_linear:  tensor([[[0.5985]]], device='cuda:0') tensor([[[1.1648]]], device='cuda:0')
log_to_linear action at 1641198767866
bwe changes from to:  [tensor([[[0.2453]]]), tensor([[[0.2857]]])]
step into gymStat at 1641198767867
send bwe to appRecv at 1641198767867
sent bwe to appRecv at 1641198767867
wait for recv string at 1641198767867
recved string at 1641198768092
1
wait for recv [self.estimator, stat] at 1641198768092
recved [self.estimator, stat] at 1641198768092
sorted packlist at 1641198768092
packetSeq:  5434
packetSeq:  5435
packetSeq:  5436
packetSeq:  5437
packetSeq:  5438
packetSeq:  5439
packetSeq:  5440
packetSeq:  5441
packetSeq:  5442
packetSeq:  5443
processed packlist at 1641198768092
receiving_rate:  298280.0
delay:  197.9
loss_ratio:  0.0
processed state0-2 at 1641198768092
avgFrameBetween:  6
psnrStat:  [[547044, 546687, 545214, 544828, 544002, 544753, 544206]]
delayStat:  [[243, 243, 242, 242, 242, 242, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [545247.7142857143] [242.28571428571428] [0]
processed state3-5 at 1641198768092
liner_to_log:  tensor([[[1.1648]]]) tensor([[[0.5985]]])
linear_to_log at 1641198768093
listState:  [0.07457, 0.13193333333333335, 0.0, 0.5452477142857143, 0.24228571428571427, 0.0, tensor([[[0.5985]]])]
state_clone_detach at 1641198768093
reward: got request at 1641198764252
processed allFrame at 1641198764252
send 'asking for bwe' at 1641198764252
sent 'asking for bwe' at 1641198764252
send [estimator, stat] at 1641198764252
sent [estimator, stat] at 1641198764252
pc wait for bwe at 1641198764252
pc got bwe at 1641198764256
bandwidth:  300000
pc flushed at 1641198764257
Bwe Sent: 5 at 1641198764257
got request at 1641198764455
processed allFrame at 1641198764455
send 'asking for bwe' at 1641198764455
sent 'asking for bwe' at 1641198764455
send [estimator, stat] at 1641198764455
sent [estimator, stat] at 1641198764455
pc wait for bwe at 1641198764455
pc got bwe at 1641198764459
bandwidth:  300000
pc flushed at 1641198764459
Bwe Sent: 4 at 1641198764459
got request at 1641198764658
processed allFrame at 1641198764658
send 'asking for bwe' at 1641198764658
sent 'asking for bwe' at 1641198764658
send [estimator, stat] at 1641198764658
sent [estimator, stat] at 1641198764658
pc wait for bwe at 1641198764658
pc got bwe at 1641198764662
bandwidth:  300000
pc flushed at 1641198764662
Bwe Sent: 4 at 1641198764662
got request at 1641198764885
processed allFrame at 1641198764885
send 'asking for bwe' at 1641198764885
sent 'asking for bwe' at 1641198764885
send [estimator, stat] at 1641198764885
sent [estimator, stat] at 1641198764885
pc wait for bwe at 1641198764885
pc got bwe at 1641198764890
bandwidth:  300000
pc flushed at 1641198764890
Bwe Sent: 5 at 1641198764890
got request at 1641198765089
processed allFrame at 1641198765089
send 'asking for bwe' at 1641198765089
sent 'asking for bwe' at 1641198765089
send [estimator, stat] at 1641198765089
sent [estimator, stat] at 1641198765089
pc wait for bwe at 1641198765089
pc got bwe at 1641198765093
bandwidth:  300000
pc flushed at 1641198765093
Bwe Sent: 4 at 1641198765093
got request at 1641198765292
processed allFrame at 1641198765292
send 'asking for bwe' at 1641198765292
sent 'asking for bwe' at 1641198765292
send [estimator, stat] at 1641198765292
sent [estimator, stat] at 1641198765292
pc wait for bwe at 1641198765292
pc got bwe at 1641198765296
bandwidth:  300000
pc flushed at 1641198765296
Bwe Sent: 4 at 1641198765296
got request at 1641198765519
processed allFrame at 1641198765519
send 'asking for bwe' at 1641198765519
sent 'asking for bwe' at 1641198765519
send [estimator, stat] at 1641198765519
sent [estimator, stat] at 1641198765519
pc wait for bwe at 1641198765519
pc got bwe at 1641198765523
bandwidth:  300000
pc flushed at 1641198765523
Bwe Sent: 4 at 1641198765523
got request at 1641198765722
processed allFrame at 1641198765722
send 'asking for bwe' at 1641198765722
sent 'asking for bwe' at 1641198765722
send [estimator, stat] at 1641198765722
sent [estimator, stat] at 1641198765722
pc wait for bwe at 1641198765722
pc got bwe at 1641198765726
bandwidth:  300000
pc flushed at 1641198765726
Bwe Sent: 4 at 1641198765726
got request at 1641198765922
processed allFrame at 1641198765922
send 'asking for bwe' at 1641198765922
sent 'asking for bwe' at 1641198765922
send [estimator, stat] at 1641198765922
sent [estimator, stat] at 1641198765922
pc wait for bwe at 1641198765922
pc got bwe at 1641198765927
bandwidth:  300000
pc flushed at 1641198765927
Bwe Sent: 5 at 1641198765927
got request at 1641198766155
processed allFrame at 1641198766155
send 'asking for bwe' at 1641198766155
sent 'asking for bwe' at 1641198766156
send [estimator, stat] at 1641198766156
sent [estimator, stat] at 1641198766156
pc wait for bwe at 1641198766156
pc got bwe at 1641198766160
bandwidth:  300000
pc flushed at 1641198766160
Bwe Sent: 5 at 1641198766160
got request at 1641198766389
processed allFrame at 1641198766390
send 'asking for bwe' at 1641198766390
sent 'asking for bwe' at 1641198766390
send [estimator, stat] at 1641198766390
sent [estimator, stat] at 1641198766390
pc wait for bwe at 1641198766390
pc got bwe at 1641198766394
bandwidth:  300000
pc flushed at 1641198766394
Bwe Sent: 5 at 1641198766394
got request at 1641198766591
processed allFrame at 1641198766591
send 'asking for bwe' at 1641198766591
sent 'asking for bwe' at 1641198766591
send [estimator, stat] at 1641198766591
sent [estimator, stat] at 1641198766591
pc wait for bwe at 1641198766591
pc got bwe at 1641198766595
bandwidth:  300000
pc flushed at 1641198766595
Bwe Sent: 4 at 1641198766595
got request at 1641198766818
processed allFrame at 1641198766818
send 'asking for bwe' at 1641198766818
sent 'asking for bwe' at 1641198766818
send [estimator, stat] at 1641198766818
sent [estimator, stat] at 1641198766818
pc wait for bwe at 1641198766818
pc got bwe at 1641198766822
bandwidth:  300000
pc flushed at 1641198766822
Bwe Sent: 4 at 1641198766822
got request at 1641198767052
processed allFrame at 1641198767052
send 'asking for bwe' at 1641198767052
sent 'asking for bwe' at 1641198767052
send [estimator, stat] at 1641198767052
sent [estimator, stat] at 1641198767052
pc wait for bwe at 1641198767052
pc got bwe at 1641198767057
bandwidth:  300000
pc flushed at 1641198767057
Bwe Sent: 5 at 1641198767057
got request at 1641198767255
processed allFrame at 1641198767255
send 'asking for bwe' at 1641198767255
sent 'asking for bwe' at 1641198767255
send [estimator, stat] at 1641198767255
sent [estimator, stat] at 1641198767255
pc wait for bwe at 1641198767256
pc got bwe at 1641198767260
bandwidth:  300000
pc flushed at 1641198767260
Bwe Sent: 5 at 1641198767260
got request at 1641198767457
processed allFrame at 1641198767457
send 'asking for bwe' at 1641198767457
sent 'asking for bwe' at 1641198767457
send [estimator, stat] at 1641198767457
sent [estimator, stat] at 1641198767457
pc wait for bwe at 1641198767457
pc got bwe at 1641198767461
bandwidth:  300000
pc flushed at 1641198767461
Bwe Sent: 4 at 1641198767461
got request at 1641198767660
processed allFrame at 1641198767660
send 'asking for bwe' at 1641198767660
sent 'asking for bwe' at 1641198767660
send [estimator, stat] at 1641198767660
sent [estimator, stat] at 1641198767660
pc wait for bwe at 1641198767660
pc got bwe at 1641198767664
bandwidth:  300000
pc flushed at 1641198767664
Bwe Sent: 4 at 1641198767664
got request at 1641198767862
processed allFrame at 1641198767862
send 'asking for bwe' at 1641198767862
sent 'asking for bwe' at 1641198767862
send [estimator, stat] at 1641198767862
sent [estimator, stat] at 1641198767862
pc wait for bwe at 1641198767862
pc got bwe at 1641198767867
bandwidth:  300000
pc flushed at 1641198767867
Bwe Sent: 5 at 1641198767867
got request at 1641198768092
processed allFrame at 1641198768092
send 'asking for bwe' at 1641198768092
sent 'asking for bwe' at 1641198768092
send [estimator, stat] at 1641198768092
sent [estimator, stat] at 1641198768092
pc wait for bwe at 1641198768092
pc got bwe at 1641198768096
bandwidth:  300000
pc flushed at 1641198768096
Bwe Sent: 4 at 1641198768096
got request at 1641198768293
processed allFrame at 1641198768294
send 'asking for bwe' at 1641198768294
sent 'asking for bwe' at 1641198768294
send [estimator, stat] at 1641198768294
sent [estimator, stat] at 1641198768294
pc wait for bwe at 1641198768294
pc got bwe at 1641198768298
bandwidth:  300000
pc flushed at 1641198768298
Bwe Sent: 5 at 1641198768298
got request at 1641198768496
processed allFrame at 1641198768496
send 'asking for bwe' at 1641198768496
sent 'asking for bwe' at 1641198768496
send [estimator, stat] at 1641198768496
sent [estimator, stat] at 1641198768496
pc wait for bwe at 1641198768496
pc got bwe at 1641198768500
bandwidth:  300000
pc flushed at 1641198768500
Bwe Sent: 4 at 1641198768500
got request at 1641198768697
processed allFrame at 1641198768697
send 'asking for bwe' at 1641198768697
sent 'asking for bwe' at 1641198768697
send [estimator, stat] at 1641198768697
sent [estimator, stat] at 1641198768697
pc wait for bwe at 1641198768697
pc got bwe at 1641198768701
bandwidth:  300000
pc flushed at 1641198768701
Bwe Sent: 4 at 1641198768701
got request at 1641198768899
processed allFrame at 1641198768899
send 'asking for bwe' at 1641198768899
sent 'asking for bwe' at 1641198768899
send [estimator, stat] at 1641198768899
sent [estimator, stat] at 1641198768899
pc wait for bwe at 1641198768899
0.07539511250056036
state tensor([0.0746, 0.1319, 0.0000, 0.5985], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198768094
state222:  tensor([[0.0746, 0.1319, 0.0000, 0.5985]], device='cuda:0')
policy_old.forwarded at 1641198768095
give action 222============================
log_to_linear:  tensor([[[0.4735]]], device='cuda:0') tensor([[[0.9587]]], device='cuda:0')
log_to_linear action at 1641198768096
bwe changes from to:  [tensor([[[0.2857]]]), tensor([[[0.2739]]])]
step into gymStat at 1641198768096
send bwe to appRecv at 1641198768096
sent bwe to appRecv at 1641198768096
wait for recv string at 1641198768096
recved string at 1641198768294
1
wait for recv [self.estimator, stat] at 1641198768294
recved [self.estimator, stat] at 1641198768294
sorted packlist at 1641198768294
packetSeq:  5444
packetSeq:  5445
packetSeq:  5446
packetSeq:  5447
packetSeq:  5448
packetSeq:  5449
packetSeq:  5450
packetSeq:  5451
processed packlist at 1641198768294
receiving_rate:  230680.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198768294
avgFrameBetween:  6
psnrStat:  [[543720, 542652, 542942, 542046, 541785, 540837]]
delayStat:  [[241, 241, 241, 241, 242, 241]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [542330.3333333334] [241.16666666666666] [0]
processed state3-5 at 1641198768294
liner_to_log:  tensor([[[0.9587]]]) tensor([[[0.4735]]])
linear_to_log at 1641198768294
listState:  [0.05767, 0.132, 0.0, 0.5423303333333334, 0.24116666666666667, 0.0, tensor([[[0.4735]]])]
state_clone_detach at 1641198768294
reward: -0.003755489891488495
state tensor([0.0577, 0.1320, 0.0000, 0.4735], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198768295
state222:  tensor([[0.0577, 0.1320, 0.0000, 0.4735]], device='cuda:0')
policy_old.forwarded at 1641198768297
give action 223============================
log_to_linear:  tensor([[[0.4515]]], device='cuda:0') tensor([[[0.9253]]], device='cuda:0')
log_to_linear action at 1641198768298
bwe changes from to:  [tensor([[[0.2739]]]), tensor([[[0.2534]]])]
step into gymStat at 1641198768298
send bwe to appRecv at 1641198768298
sent bwe to appRecv at 1641198768298
wait for recv string at 1641198768298
recved string at 1641198768496
1
wait for recv [self.estimator, stat] at 1641198768496
recved [self.estimator, stat] at 1641198768496
sorted packlist at 1641198768496
packetSeq:  5452
packetSeq:  5453
packetSeq:  5454
packetSeq:  5455
packetSeq:  5456
packetSeq:  5457
packetSeq:  5458
packetSeq:  5459
packetSeq:  5460
packetSeq:  5461
processed packlist at 1641198768496
receiving_rate:  288040.0
delay:  201.0
loss_ratio:  0.0
processed state0-2 at 1641198768496
avgFrameBetween:  6
psnrStat:  [[542179, 542816, 544022, 542887, 546270, 547019]]
delayStat:  [[241, 242, 241, 242, 242, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [544198.8333333334] [241.66666666666666] [0]
processed state3-5 at 1641198768496
liner_to_log:  tensor([[[0.9253]]]) tensor([[[0.4515]]])
linear_to_log at 1641198768497
listState:  [0.07201, 0.134, 0.0, 0.5441988333333334, 0.24166666666666667, 0.0, tensor([[[0.4515]]])]
state_clone_detach at 1641198768497
reward: 0.05788319338688064
state tensor([0.0720, 0.1340, 0.0000, 0.4515], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198768497
state222:  tensor([[0.0720, 0.1340, 0.0000, 0.4515]], device='cuda:0')
policy_old.forwarded at 1641198768499
give action 224============================
log_to_linear:  tensor([[[0.2873]]], device='cuda:0') tensor([[[0.7078]]], device='cuda:0')
log_to_linear action at 1641198768500
bwe changes from to:  [tensor([[[0.2534]]]), tensor([[[0.1794]]])]
step into gymStat at 1641198768500
send bwe to appRecv at 1641198768500
sent bwe to appRecv at 1641198768500
wait for recv string at 1641198768500
recved string at 1641198768697
1
wait for recv [self.estimator, stat] at 1641198768697
recved [self.estimator, stat] at 1641198768697
sorted packlist at 1641198768697
packetSeq:  5462
packetSeq:  5463
packetSeq:  5464
packetSeq:  5465
packetSeq:  5466
packetSeq:  5467
packetSeq:  5468
packetSeq:  5469
packetSeq:  5470
packetSeq:  5471
packetSeq:  5472
processed packlist at 1641198768697
receiving_rate:  288920.0
delay:  197.27272727272728
loss_ratio:  0.0
processed state0-2 at 1641198768697
avgFrameBetween:  6
psnrStat:  [[547630, 547160, 547857, 549083, 549271, 548805]]
delayStat:  [[243, 242, 242, 242, 242, 242]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548301.0] [242.16666666666666] [0]
processed state3-5 at 1641198768697
liner_to_log:  tensor([[[0.7078]]]) tensor([[[0.2873]]])
linear_to_log at 1641198768698
listState:  [0.07223, 0.13151515151515153, 0.0, 0.548301, 0.24216666666666667, 0.0, tensor([[[0.2873]]])]
state_clone_detach at 1641198768698
reward: 0.06631827686978581
state tensor([0.0722, 0.1315, 0.0000, 0.2873], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198768698
state222:  tensor([[0.0722, 0.1315, 0.0000, 0.2873]], device='cuda:0')
policy_old.forwarded at 1641198768700
give action 225============================
log_to_linear:  tensor([[[0.2946]]], device='cuda:0') tensor([[[0.7162]]], device='cuda:0')
log_to_linear action at 1641198768701
bwe changes from to:  [tensor([[[0.1794]]]), tensor([[[0.1285]]])]
step into gymStat at 1641198768701
send bwe to appRecv at 1641198768701
sent bwe to appRecv at 1641198768701
wait for recv string at 1641198768701
recved string at 1641198768899
1
wait for recv [self.estimator, stat] at 1641198768899
recved [self.estimator, stat] at 1641198768899
sorted packlist at 1641198768899
packetSeq:  5473
packetSeq:  5474
packetSeq:  5475
packetSeq:  5476
packetSeq:  5477
packetSeq:  5478
packetSeq:  5479
packetSeq:  5480
processed packlist at 1641198768899
receiving_rate:  260480.00000000003
delay:  198.125
loss_ratio:  0.0
processed state0-2 at 1641198768899
avgFrameBetween:  6
psnrStat:  [[549563, 549714, 550095, 549617, 550518, 549797]]
delayStat:  [[219, 217, 219, 219, 218, 219]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549884.0] [218.5] [0]
processed state3-5 at 1641198768899
liner_to_log:  tensor([[[0.7162]]]) tensor([[[0.2946]]])
linear_to_log at 1641198768900
listState:  [0.06512000000000001, 0.13208333333333333, 0.0, 0.549884, 0.2185, 0.0, tensor([[[0.2946]]])]
state_clone_detach at 1641198768900
reward: 0.032084325018707005
state tensor([0.0651, 0.1321, 0.0000, 0.2946], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198768900
state222:  tensor([[0.0651, 0.1321, 0.0000, 0.2946]], device='cuda:0')
policy_old.forwarded at 1641198768902
give action 226============================
log_to_linear:  tensor([[[0.7384]]], device='cuda:0') tensor([[[1.4275]]], device='cuda:0')
log_to_linear action at 1641198768903
bwe changes from to:  [tensor([[[0.1285]]]), tensor([[[0.1834]]])]
step into gymStat at 1641198768903
send bwe to appRecv at 1641198768903
sent bwe to appRecv at 1641198768903
wait for recv string at 1641198768903
recved string at 1641198769124
1
wait for recv [self.estimator, stat] at 1641198769124
recved [self.estimator, stat] at 1641198769124
sorted packlist at 1641198769124
packetSeq:  5481
packetSeq:  5482
packetSeq:  5483
packetSeq:  5484
packetSeq:  5485
packetSeq:  5486
packetSeq:  5487
packetSeq:  5488
packetSeq:  5489
packetSeq:  5490
processed packlist at 1641198769124
receiving_rate:  289560.0
delay:  196.8
loss_ratio:  0.0
processed state0-2 at 1641198769124
avgFrameBetween:  6
psnrStat:  [[549296, 548254, 550001, 549201, 548389, 547924, 549634]]
delayStat:  [[218, 218, 218, 218, 219, 218, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548957.0] [218.14285714285714] [0]
processed state3-5 at 1641198769124
liner_to_log:  tensor([[[1.4275]]]) tensor([[[0.7384]]])
linear_to_log at 1641198769125
listState:  [0.07239, 0.1312, 0.0, 0.548957, 0.21814285714285714, 0.0, tensor([[[0.7384]]])]
state_clone_detach at 1641198769125
reward: 0.06797584484353952
state tensor([0.0724, 0.1312, 0.0000, 0.7384], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198769125
state222:  tensor([[0.0724, 0.1312, 0.0000, 0.7384]], device='cuda:0')
policy_old.forwarded at 1641198769127
give action 227============================
log_to_linear:  tensor([[[0.3641]]], device='cuda:0') tensor([[[0.8024]]], device='cuda:0')
log_to_linear action at 1641198769128
bwe changes from to:  [tensor([[[0.1834]]]), tensor([[[0.1472]]])]
step into gymStat at 1641198769128
send bwe to appRecv at 1641198769128
sent bwe to appRecv at 1641198769128
wait for recv string at 1641198769128
recved string at 1641198769330
1
wait for recv [self.estimator, stat] at 1641198769330
recved [self.estimator, stat] at 1641198769330
sorted packlist at 1641198769330
packetSeq:  5491
packetSeq:  5492
packetSeq:  5493
packetSeq:  5494
packetSeq:  5495
packetSeq:  5496
packetSeq:  5497
packetSeq:  5498
packetSeq:  5499
processed packlist at 1641198769330
receiving_rate:  278880.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198769330
avgFrameBetween:  6
psnrStat:  [[548570, 549246, 548595, 548340, 547425, 547693]]
delayStat:  [[218, 219, 218, 219, 218, 219]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548311.5] [218.5] [0]
processed state3-5 at 1641198769330
liner_to_log:  tensor([[[0.8024]]]) tensor([[[0.3641]]])
linear_to_log at 1641198769330
listState:  [0.06972, 0.132, 0.0, 0.5483115, 0.2185, 0.0, tensor([[[0.3641]]])]
state_clone_detach at 1641198769331
reward: 0.05358038709784574
state tensor([0.0697, 0.1320, 0.0000, 0.3641], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198769331
state222:  tensor([[0.0697, 0.1320, 0.0000, 0.3641]], device='cuda:0')
policy_old.forwarded at 1641198769333
give action 228============================
log_to_linear:  tensor([[[0.5600]]], device='cuda:0') tensor([[[1.0984]]], device='cuda:0')
log_to_linear action at 1641198769334
bwe changes from to:  [tensor([[[0.1472]]]), tensor([[[0.1616]]])]
step into gymStat at 1641198769334
send bwe to appRecv at 1641198769334
sent bwe to appRecv at 1641198769334
wait for recv string at 1641198769334
recved string at 1641198769555
1
wait for recv [self.estimator, stat] at 1641198769555
recved [self.estimator, stat] at 1641198769555
sorted packlist at 1641198769555
packetSeq:  5500
packetSeq:  5501
packetSeq:  5502
packetSeq:  5503
packetSeq:  5504
packetSeq:  5505
packetSeq:  5506
packetSeq:  5507
packetSeq:  5508
processed packlist at 1641198769555
receiving_rate:  292200.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198769555
avgFrameBetween:  6
psnrStat:  [[548232, 548592, 548381, 549080, 549229, 549449, 549212]]
delayStat:  [[218, 219, 218, 219, 218, 219, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548882.1428571428] [218.42857142857142] [0]
processed state3-5 at 1641198769555
liner_to_log:  tensor([[[1.0984]]]) tensor([[[0.5600]]])
linear_to_log at 1641198769555
listState:  [0.07305, 0.132, 0.0, 0.5488821428571429, 0.21842857142857142, 0.0, tensor([[[0.5600]]])]
state_clone_detach at 1641198769555
reward: 0.0685044115213912
state tensor([0.0730, 0.1320, 0.0000, 0.5600], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198769556
state222:  tensor([[0.0730, 0.1320, 0.0000, 0.5600]], device='cuda:0')
policy_old.forwarded at 1641198769558
give action 229============================
log_to_linear:  tensor([[[0.5334]]], device='cuda:0') tensor([[[1.0539]]], device='cuda:0')
log_to_linear action at 1641198769559
bwe changes from to:  [tensor([[[0.1616]]]), tensor([[[0.1703]]])]
step into gymStat at 1641198769559
send bwe to appRecv at 1641198769559
sent bwe to appRecv at 1641198769559
wait for recv string at 1641198769559
recved string at 1641198769758
1
wait for recv [self.estimator, stat] at 1641198769758
recved [self.estimator, stat] at 1641198769758
sorted packlist at 1641198769758
packetSeq:  5509
packetSeq:  5510
packetSeq:  5511
packetSeq:  5512
packetSeq:  5513
packetSeq:  5514
packetSeq:  5515
packetSeq:  5516
packetSeq:  5517
processed packlist at 1641198769758
receiving_rate:  262360.0
delay:  196.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198769758
avgFrameBetween:  6
psnrStat:  [[549229, 549451, 550361, 550162, 551355, 550003]]
delayStat:  [[219, 218, 218, 219, 218, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550093.5] [218.33333333333334] [0]
processed state3-5 at 1641198769758
liner_to_log:  tensor([[[1.0539]]]) tensor([[[0.5334]]])
linear_to_log at 1641198769759
listState:  [0.06559, 0.13125925925925927, 0.0, 0.5500935, 0.21833333333333335, 0.0, tensor([[[0.5334]]])]
state_clone_detach at 1641198769759
reward: 0.036762173282635424
state tensor([0.0656, 0.1313, 0.0000, 0.5334], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198769760
state222:  tensor([[0.0656, 0.1313, 0.0000, 0.5334]], device='cuda:0')
policy_old.forwarded at 1641198769761
give action 230============================
log_to_linear:  tensor([[[0.5571]]], device='cuda:0') tensor([[[1.0935]]], device='cuda:0')
log_to_linear action at 1641198769762
bwe changes from to:  [tensor([[[0.1703]]]), tensor([[[0.1863]]])]
step into gymStat at 1641198769762
send bwe to appRecv at 1641198769762
sent bwe to appRecv at 1641198769762
wait for recv string at 1641198769762
recved string at 1641198769994
1
wait for recv [self.estimator, stat] at 1641198769994
recved [self.estimator, stat] at 1641198769994
sorted packlist at 1641198769994
packetSeq:  5518
packetSeq:  5519
packetSeq:  5520
packetSeq:  5521
packetSeq:  5522
packetSeq:  5523
packetSeq:  5524
packetSeq:  5525
packetSeq:  5526
processed packlist at 1641198769994
receiving_rate:  246800.0
delay:  198.57142857142858
loss_ratio:  0.0
processed state0-2 at 1641198769994
avgFrameBetween:  6
psnrStat:  [[551010, 549826, 549945, 549749, 550813, 550287, 551587]]
delayStat:  [[218, 218, 218, 218, 219, 218, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550459.5714285715] [218.14285714285714] [0]
processed state3-5 at 1641198769994
liner_to_log:  tensor([[[1.0935]]]) tensor([[[0.5571]]])
linear_to_log at 1641198769995
listState:  [0.0617, 0.13238095238095238, 0.0, 0.5504595714285715, 0.21814285714285714, 0.0, tensor([[[0.5571]]])]
state_clone_detach at 1641198769995
reward: 0.014893110511356389
state tensor([0.0617, 0.1324, 0.0000, 0.5571], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198769996
state222:  tensor([[0.0617, 0.1324, 0.0000, 0.5571]], device='cuda:0')
policy_old.forwarded at 1641198769997
give action 231============================
log_to_linear:  tensor([[[0.0314]]], device='cuda:0') tensor([[[0.5062]]], device='cuda:0')
log_to_linear action at 1641198769998
bwe changes from to:  [tensor([[[0.1863]]]), tensor([[[0.0943]]])]
step into gymStat at 1641198769998
send bwe to appRecv at 1641198769998
sent bwe to appRecv at 1641198769998
wait for recv string at 1641198769998
recved string at 1641198770229
1
wait for recv [self.estimator, stat] at 1641198770229
recved [self.estimator, stat] at 1641198770229
sorted packlist at 1641198770229
packetSeq:  5527
packetSeq:  5528
packetSeq:  5529
packetSeq:  5530
packetSeq:  5531
packetSeq:  5532
packetSeq:  5533
packetSeq:  5534
packetSeq:  5535
packetSeq:  5536
processed packlist at 1641198770229
receiving_rate:  258160.00000000003
delay:  197.5
loss_ratio:  0.0
processed state0-2 at 1641198770229
avgFrameBetween:  6
psnrStat:  [[552674, 553141, 552148, 552507, 552371, 552602, 552786]]
delayStat:  [[218, 219, 218, 219, 218, 219, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552604.1428571428] [218.42857142857142] [0]
processed state3-5 at 1641198770229
liner_to_log:  tensor([[[0.5062]]]) tensor([[[0.0314]]])
linear_to_log at 1641198770229
listState:  [0.06454000000000001, 0.13166666666666665, 0.0, 0.5526041428571429, 0.21842857142857142, 0.0, tensor([[[0.0314]]])]
state_clone_detach at 1641198770229
reward: 0.03060126445388195
state tensor([0.0645, 0.1317, 0.0000, 0.0314], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198770230
state222:  tensor([[0.0645, 0.1317, 0.0000, 0.0314]], device='cuda:0')
policy_old.forwarded at 1641198770232
give action 232============================
log_to_linear:  tensor([[[0.5654]]], device='cuda:0') tensor([[[1.1076]]], device='cuda:0')
log_to_linear action at 1641198770233
bwe changes from to:  [tensor([[[0.0943]]]), tensor([[[0.1044]]])]
step into gymStat at 1641198770233
send bwe to appRecv at 1641198770233
sent bwe to appRecv at 1641198770233
wait for recv string at 1641198770233
recved string at 1641198770458
1
wait for recv [self.estimator, stat] at 1641198770458
recved [self.estimator, stat] at 1641198770458
sorted packlist at 1641198770458
packetSeq:  5537
packetSeq:  5538
packetSeq:  5539
packetSeq:  5540
packetSeq:  5541
packetSeq:  5542
packetSeq:  5543
packetSeq:  5544
packetSeq:  5545
processed packlist at 1641198770458
receiving_rate:  295840.0
delay:  199.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198770458
avgFrameBetween:  6
psnrStat:  [[551685, 551267, 552155, 552756, 552475, 553096, 552624]]
delayStat:  [[218, 218, 218, 219, 218, 218, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552294.0] [218.14285714285714] [0]
processed state3-5 at 1641198770458
liner_to_log:  tensor([[[1.1076]]]) tensor([[[0.5654]]])
linear_to_log at 1641198770459
listState:  [0.07396, 0.13281481481481483, 0.0, 0.552294, 0.21814285714285714, 0.0, tensor([[[0.5654]]])]
state_clone_detach at 1641198770459
reward: 0.0700745403425086
state tensor([0.0740, 0.1328, 0.0000, 0.5654], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198770459
state222:  tensor([[0.0740, 0.1328, 0.0000, 0.5654]], device='cuda:0')
policy_old.forwarded at 1641198770461
give action 233============================
log_to_linear:  tensor([[[0.1425]]], device='cuda:0') tensor([[[0.5684]]], device='cuda:0')
log_to_linear action at 1641198770462
bwe changes from to:  [tensor([[[0.1044]]]), tensor([[[0.0594]]])]
step into gymStat at 1641198770462
send bwe to appRecv at 1641198770462
sent bwe to appRecv at 1641198770462
wait for recv string at 1641198770462
recved string at 1641198770662
1
wait for recv [self.estimator, stat] at 1641198770662
recved [self.estimator, stat] at 1641198770662
sorted packlist at 1641198770662
packetSeq:  5546
packetSeq:  5547
packetSeq:  5548
packetSeq:  5549
packetSeq:  5550
packetSeq:  5551
packetSeq:  5552
packetSeq:  5553
packetSeq:  5554
processed packlist at 1641198770662
receiving_rate:  283400.0
delay:  197.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198770662
avgFrameBetween:  6
psnrStat:  [[552431, 552872, 553302, 552849, 553600, 554219]]
delayStat:  [[217, 218, 218, 218, 219, 218]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553212.1666666666] [218.0] [0]
processed state3-5 at 1641198770662
liner_to_log:  tensor([[[0.5684]]]) tensor([[[0.1425]]])
linear_to_log at 1641198770663
listState:  [0.07085, 0.13177777777777777, 0.0, 0.5532121666666666, 0.218, 0.0, tensor([[[0.1425]]])]
state_clone_detach at 1641198770663
reward: 0.05935308299806713
state tensor([0.0708, 0.1318, 0.0000, 0.1425], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198770663
state222:  tensor([[0.0708, 0.1318, 0.0000, 0.1425]], device='cuda:0')
policy_old.forwarded at 1641198770665
give action 234============================
log_to_linear:  tensor([[[0.5118]]], device='cuda:0') tensor([[[1.0188]]], device='cuda:0')
log_to_linear action at 1641198770666
bwe changes from to:  [tensor([[[0.0594]]]), tensor([[[0.0605]]])]
step into gymStat at 1641198770666
send bwe to appRecv at 1641198770666
sent bwe to appRecv at 1641198770666
wait for recv string at 1641198770666
recved string at 1641198770889
1
wait for recv [self.estimator, stat] at 1641198770889
recved [self.estimator, stat] at 1641198770889
sorted packlist at 1641198770889
packetSeq:  5555
packetSeq:  5556
packetSeq:  5557
packetSeq:  5558
packetSeq:  5559
packetSeq:  5560
packetSeq:  5561
packetSeq:  5562
packetSeq:  5563
packetSeq:  5564
processed packlist at 1641198770889
receiving_rate:  281480.0
delay:  197.3
loss_ratio:  0.0
processed state0-2 at 1641198770889
avgFrameBetween:  6
psnrStat:  [[554847, 555853, 556593, 556397, 557186, 556605, 556338, 556183]]
delayStat:  [[218, 218, 195, 195, 196, 196, 195, 196]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556250.25] [201.125] [0]
processed state3-5 at 1641198770889
liner_to_log:  tensor([[[1.0188]]]) tensor([[[0.5118]]])
linear_to_log at 1641198770889
listState:  [0.07037, 0.13153333333333334, 0.0, 0.55625025, 0.201125, 0.0, tensor([[[0.5118]]])]
state_clone_detach at 1641198770890
reward: 0.05792278991651639
state tensor([0.0704, 0.1315, 0.0000, 0.5118], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198770890
state222:  tensor([[0.0704, 0.1315, 0.0000, 0.5118]], device='cuda:0')
policy_old.forwarded at 1641198770892
give action 235============================
log_to_linear:  tensor([[[0.3430]]], device='cuda:0') tensor([[[0.7751]]], device='cuda:0')
log_to_linear action at 1641198770893
bwe changes from to:  [tensor([[[0.0605]]]), tensor([[[0.0469]]])]
step into gymStat at 1641198770893
send bwe to appRecv at 1641198770893
sent bwe to appRecv at 1641198770893
wait for recv string at 1641198770893
recved string at 1641198771095
1
wait for recv [self.estimator, stat] at 1641198771095
recved [self.estimator, stat] at 1641198771095
sorted packlist at 1641198771095
packetSeq:  5565
packetSeq:  5566
packetSeq:  5567
packetSeq:  5568
packetSeq:  5569
packetSeq:  5570
packetSeq:  5571
packetSeq:  5572
packetSeq:  5573
processed packlist at 1641198771095
receiving_rate:  278240.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198771095
avgFrameBetween:  6
psnrStat:  [[556866, 556834, 557312, 557617, 557168, 557168]]
delayStat:  [[195, 195, 196, 195, 195, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557160.8333333334] [195.16666666666666] [0]
processed state3-5 at 1641198771095
liner_to_log:  tensor([[[0.7751]]]) tensor([[[0.3430]]])
linear_to_log at 1641198771096
listState:  [0.06956, 0.132, 0.0, 0.5571608333333333, 0.19516666666666665, 0.0, tensor([[[0.3430]]])]
state_clone_detach at 1641198771096
reward: 0.0528538848144417
state tensor([0.0696, 0.1320, 0.0000, 0.3430], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198771096
state222:  tensor([[0.0696, 0.1320, 0.0000, 0.3430]], device='cuda:0')
policy_old.forwarded at 1641198771098
give action 236============================
log_to_linear:  tensor([[[0.3637]]], device='cuda:0') tensor([[[0.8019]]], device='cuda:0')
log_to_linear action at 1641198771099
bwe changes from to:  [tensor([[[0.0469]]]), tensor([[[0.0376]]])]
step into gymStat at 1641198771099
send bwe to appRecv at 1641198771099
sent bwe to appRecv at 1641198771099
wait for recv string at 1641198771099
recved string at 1641198771326
1
wait for recv [self.estimator, stat] at 1641198771326
recved [self.estimator, stat] at 1641198771326
sorted packlist at 1641198771326
packetSeq:  5574
packetSeq:  5575
packetSeq:  5576
packetSeq:  5577
packetSeq:  5578
packetSeq:  5579
packetSeq:  5580
packetSeq:  5581
packetSeq:  5582
packetSeq:  5583
packetSeq:  5584
processed packlist at 1641198771326
receiving_rate:  290120.0
delay:  197.7
loss_ratio:  0.0
processed state0-2 at 1641198771327
avgFrameBetween:  6
psnrStat:  [[557763, 558674, 557950, 557910, 558661, 558605, 559959]]
delayStat:  [[196, 195, 196, 196, 195, 196, 194]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558503.1428571428] [195.42857142857142] [0]
processed state3-5 at 1641198771327
liner_to_log:  tensor([[[0.8019]]]) tensor([[[0.3637]]])
linear_to_log at 1641198771327
listState:  [0.07253, 0.1318, 0.0, 0.5585031428571429, 0.19542857142857142, 0.0, tensor([[[0.3637]]])]
state_clone_detach at 1641198771327
reward: 0.06679825153835706
state tensor([0.0725, 0.1318, 0.0000, 0.3637], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198771328
state222:  tensor([[0.0725, 0.1318, 0.0000, 0.3637]], device='cuda:0')
policy_old.forwarded at 1641198771330
give action 237============================
log_to_linear:  tensor([[[0.5081]]], device='cuda:0') tensor([[[1.0129]]], device='cuda:0')
log_to_linear action at 1641198771330
bwe changes from to:  [tensor([[[0.0376]]]), tensor([[[0.0381]]])]
step into gymStat at 1641198771331
send bwe to appRecv at 1641198771331
sent bwe to appRecv at 1641198771331
wait for recv string at 1641198771331
recved string at 1641198771559
1
wait for recv [self.estimator, stat] at 1641198771559
recved [self.estimator, stat] at 1641198771559
sorted packlist at 1641198771559
packetSeq:  5585
packetSeq:  5586
packetSeq:  5587
packetSeq:  5588
packetSeq:  5589
packetSeq:  5590
packetSeq:  5591
packetSeq:  5592
packetSeq:  5593
packetSeq:  5594
packetSeq:  5595
processed packlist at 1641198771559
receiving_rate:  282720.0
delay:  197.5
loss_ratio:  0.0
processed state0-2 at 1641198771559
avgFrameBetween:  6
psnrStat:  [[559581, 559120, 559599, 558450, 557894, 558602, 557025]]
delayStat:  [[195, 196, 195, 195, 195, 195, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558610.1428571428] [195.14285714285714] [0]
processed state3-5 at 1641198771559
liner_to_log:  tensor([[[1.0129]]]) tensor([[[0.5081]]])
linear_to_log at 1641198771560
listState:  [0.07068, 0.13166666666666665, 0.0, 0.5586101428571428, 0.19514285714285715, 0.0, tensor([[[0.5081]]])]
state_clone_detach at 1641198771560
reward: 0.05892102396407234
state tensor([0.0707, 0.1317, 0.0000, 0.5081], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198771560
state222:  tensor([[0.0707, 0.1317, 0.0000, 0.5081]], device='cuda:0')
policy_old.forwarded at 1641198771562
give action 238============================
log_to_linear:  tensor([[[0.4311]]], device='cuda:0') tensor([[[0.8953]]], device='cuda:0')
log_to_linear action at 1641198771563
bwe changes from to:  [tensor([[[0.0381]]]), tensor([[[0.0341]]])]
step into gymStat at 1641198771563
send bwe to appRecv at 1641198771563
sent bwe to appRecv at 1641198771563
wait for recv string at 1641198771563
recved string at 1641198771763
1
wait for recv [self.estimator, stat] at 1641198771763
recved [self.estimator, stat] at 1641198771763
sorted packlist at 1641198771763
packetSeq:  5596
packetSeq:  5597
packetSeq:  5598
packetSeq:  5599
packetSeq:  5600
packetSeq:  5601
packetSeq:  5602
packetSeq:  5603
packetSeq:  5604
packetSeq:  5605
packetSeq:  5606
processed packlist at 1641198771763
receiving_rate:  319400.0
delay:  197.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198771763
avgFrameBetween:  6
psnrStat:  [[557275, 556941, 556686, 556339, 556198, 557129]]
delayStat:  [[195, 195, 195, 195, 195, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556761.3333333334] [195.0] [0]
processed state3-5 at 1641198771763
liner_to_log:  tensor([[[0.8953]]]) tensor([[[0.4311]]])
linear_to_log at 1641198771764
listState:  [0.07985, 0.1316969696969697, 0.0, 0.5567613333333333, 0.195, 0.0, tensor([[[0.4311]]])]
state_clone_detach at 1641198771764
reward: 0.09878244509810535
state tensor([0.0799, 0.1317, 0.0000, 0.4311], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198771764
state222:  tensor([[0.0799, 0.1317, 0.0000, 0.4311]], device='cuda:0')
policy_old.forwarded at 1641198771766
give action 239============================
log_to_linear:  tensor([[[0.4357]]], device='cuda:0') tensor([[[0.9020]]], device='cuda:0')
log_to_linear action at 1641198771767
bwe changes from to:  [tensor([[[0.0341]]]), tensor([[[0.0308]]])]
step into gymStat at 1641198771767
send bwe to appRecv at 1641198771767
sent bwe to appRecv at 1641198771767
wait for recv string at 1641198771767
recved string at 1641198771990
1
wait for recv [self.estimator, stat] at 1641198771990
recved [self.estimator, stat] at 1641198771991
sorted packlist at 1641198771991
packetSeq:  5607
packetSeq:  5608
packetSeq:  5609
packetSeq:  5610
packetSeq:  5611
packetSeq:  5612
packetSeq:  5613
packetSeq:  5614
processed packlist at 1641198771991
receiving_rate:  277960.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198771991
avgFrameBetween:  6
psnrStat:  [[555559, 556340, 555299, 554166, 554818, 554561, 553992]]
delayStat:  [[195, 195, 195, 195, 195, 196, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554962.1428571428] [195.14285714285714] [0]
processed state3-5 at 1641198771991
liner_to_log:  tensor([[[0.9020]]]) tensor([[[0.4357]]])
linear_to_log at 1641198771991
listState:  [0.06949, 0.132, 0.0, 0.5549621428571428, 0.19514285714285715, 0.0, tensor([[[0.4357]]])]
state_clone_detach at 1641198771991
reward: 0.052535763239104416
state tensor([0.0695, 0.1320, 0.0000, 0.4357], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198771992
state222:  tensor([[0.0695, 0.1320, 0.0000, 0.4357]], device='cuda:0')
policy_old.forwarded at 1641198771994
give action 240============================
log_to_linear:  tensor([[[0.4698]]], device='cuda:0') tensor([[[0.9530]]], device='cuda:0')
log_to_linear action at 1641198771994
bwe changes from to:  [tensor([[[0.0308]]]), tensor([[[0.0293]]])]
step into gymStat at 1641198771995
send bwe to appRecv at 1641198771995
sent bwe to appRecv at 1641198771995
wait for recv string at 1641198771995
recved string at 1641198772193
1
wait for recv [self.estimator, stat] at 1641198772193
recved [self.estimator, stat] at 1641198772193
sorted packlist at 1641198772193
packetSeq:  5615
packetSeq:  5616
packetSeq:  5617
packetSeq:  5618
packetSeq:  5619
packetSeq:  5620
packetSeq:  5621
packetSeq:  5622
packetSeq:  5623
processed packlist at 1641198772193
receiving_rate:  250560.0
delay:  196.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198772193
avgFrameBetween:  6
psnrStat:  [[554601, 554485, 556442, 557126, 558087, 558362]]
delayStat:  [[195, 195, 195, 196, 195, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556517.1666666666] [195.16666666666666] [0]
processed state3-5 at 1641198772193
liner_to_log:  tensor([[[0.9530]]]) tensor([[[0.4698]]])
linear_to_log at 1641198772194
listState:  [0.06264, 0.13118518518518518, 0.0, 0.5565171666666666, 0.19516666666666665, 0.0, tensor([[[0.4698]]])]
state_clone_detach at 1641198772194
reward: 0.023004376741161403
state tensor([0.0626, 0.1312, 0.0000, 0.4698], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198772195
state222:  tensor([[0.0626, 0.1312, 0.0000, 0.4698]], device='cuda:0')
policy_old.forwarded at 1641198772196
give action 241============================
log_to_linear:  tensor([[[0.5088]]], device='cuda:0') tensor([[[1.0139]]], device='cuda:0')
log_to_linear action at 1641198772197
bwe changes from to:  [tensor([[[0.0293]]]), tensor([[[0.0297]]])]
step into gymStat at 1641198772197
send bwe to appRecv at 1641198772197
sent bwe to appRecv at 1641198772197
wait for recv string at 1641198772197
recved string at 1641198772395
1
wait for recv [self.estimator, stat] at 1641198772395
recved [self.estimator, stat] at 1641198772395
sorted packlist at 1641198772395
packetSeq:  5624
packetSeq:  5625
packetSeq:  5626
packetSeq:  5627
packetSeq:  5628
packetSeq:  5629
packetSeq:  5630
packetSeq:  5631
packetSeq:  5632
packetSeq:  5633
processed packlist at 1641198772395
receiving_rate:  305680.0
delay:  197.8
loss_ratio:  0.0
processed state0-2 at 1641198772395
avgFrameBetween:  6
psnrStat:  [[557921, 558569, 558382, 558174, 557793, 557773]]
delayStat:  [[195, 195, 196, 196, 196, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558102.0] [195.5] [0]
processed state3-5 at 1641198772395
liner_to_log:  tensor([[[1.0139]]]) tensor([[[0.5088]]])
linear_to_log at 1641198772395
listState:  [0.07642, 0.1318666666666667, 0.0, 0.558102, 0.1955, 0.0, tensor([[[0.5088]]])]
state_clone_detach at 1641198772396
reward: 0.08363900268011043
state tensor([0.0764, 0.1319, 0.0000, 0.5088], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198772396
state222:  tensor([[0.0764, 0.1319, 0.0000, 0.5088]], device='cuda:0')
policy_old.forwarded at 1641198772398
give action 242============================
log_to_linear:  tensor([[[0.2112]]], device='cuda:0') tensor([[[0.6276]]], device='cuda:0')
log_to_linear action at 1641198772399
bwe changes from to:  [tensor([[[0.0297]]]), tensor([[[0.0186]]])]
step into gymStat at 1641198772399
send bwe to appRecv at 1641198772399
sent bwe to appRecv at 1641198772399
wait for recv string at 1641198772399
recved string at 1641198772602
1
wait for recv [self.estimator, stat] at 1641198772602
recved [self.estimator, stat] at 1641198772602
sorted packlist at 1641198772602
packetSeq:  5634
packetSeq:  5635
packetSeq:  5636
packetSeq:  5637
packetSeq:  5638
packetSeq:  5639
packetSeq:  5640
packetSeq:  5641
packetSeq:  5642
processed packlist at 1641198772602
receiving_rate:  250720.0
delay:  197.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198772602
avgFrameBetween:  6
psnrStat:  [[557294, 557046, 556011, 556599, 555663, 556026]]
delayStat:  [[195, 195, 195, 195, 196, 195]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556439.8333333334] [195.16666666666666] [0]
processed state3-5 at 1641198772602
liner_to_log:  tensor([[[0.6276]]]) tensor([[[0.2112]]])
linear_to_log at 1641198772603
listState:  [0.06268, 0.13140740740740742, 0.0, 0.5564398333333334, 0.19516666666666665, 0.0, tensor([[[0.2112]]])]
state_clone_detach at 1641198772603
reward: 0.022529464630297535
state tensor([0.0627, 0.1314, 0.0000, 0.2112], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198772603
state222:  tensor([[0.0627, 0.1314, 0.0000, 0.2112]], device='cuda:0')
policy_old.forwarded at 1641198772605
give action 243============================
log_to_linear:  tensor([[[0.3842]]], device='cuda:0') tensor([[[0.8293]]], device='cuda:0')
log_to_linear action at 1641198772606
bwe changes from to:  [tensor([[[0.0186]]]), tensor([[[0.0155]]])]
step into gymStat at 1641198772606
send bwe to appRecv at 1641198772606
sent bwe to appRecv at 1641198772606
wait for recv string at 1641198772606
recved string at 1641198772820
1
wait for recv [self.estimator, stat] at 1641198772820
recved [self.estimator, stat] at 1641198772820
sorted packlist at 1641198772820
packetSeq:  5643
packetSeq:  5644
packetSeq:  5645
packetSeq:  5646
packetSeq:  5647
packetSeq:  5648
packetSeq:  5649
packetSeq:  5650
packetSeq:  5651
processed packlist at 1641198772820
receiving_rate:  294400.0
delay:  195.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198772820
avgFrameBetween:  6
psnrStat:  [[556329, 557317, 557652, 558671, 558977, 559191, 560054]]
delayStat:  [[196, 195, 196, 195, 173, 173, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558313.0] [185.71428571428572] [0]
processed state3-5 at 1641198772820
liner_to_log:  tensor([[[0.8293]]]) tensor([[[0.3842]]])
linear_to_log at 1641198772821
listState:  [0.0736, 0.13037037037037036, 0.0, 0.558313, 0.18571428571428572, 0.0, tensor([[[0.3842]]])]
state_clone_detach at 1641198772821
reward: 0.07582290063792768
state tensor([0.0736, 0.1304, 0.0000, 0.3842], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198772822
state222:  tensor([[0.0736, 0.1304, 0.0000, 0.3842]], device='cuda:0')
policy_old.forwarded at 1641198772823
give action 244============================
log_to_linear:  tensor([[[0.2853]]], device='cuda:0') tensor([[[0.7055]]], device='cuda:0')
log_to_linear action at 1641198772824
bwe changes from to:  [tensor([[[0.0155]]]), tensor([[[0.0109]]])]
step into gymStat at 1641198772825
send bwe to appRecv at 1641198772825
sent bwe to appRecv at 1641198772825
wait for recv string at 1641198772825
recved string at 1641198773021
1
wait for recv [self.estimator, stat] at 1641198773021
recved [self.estimator, stat] at 1641198773021
sorted packlist at 1641198773021
packetSeq:  5652
packetSeq:  5653
packetSeq:  5654
packetSeq:  5655
packetSeq:  5656
packetSeq:  5657
packetSeq:  5658
packetSeq:  5659
packetSeq:  5660
processed packlist at 1641198773021
receiving_rate:  273320.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198773021
avgFrameBetween:  6
psnrStat:  [[561236, 561237, 561011, 560930, 560686, 560529]]
delayStat:  [[173, 175, 173, 173, 174, 173]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560938.1666666666] [173.5] [0]
processed state3-5 at 1641198773021
liner_to_log:  tensor([[[0.7055]]]) tensor([[[0.2853]]])
linear_to_log at 1641198773022
listState:  [0.06833, 0.1282962962962963, 0.0, 0.5609381666666666, 0.1735, 0.0, tensor([[[0.2853]]])]
state_clone_detach at 1641198773022
reward: 0.05835043937936846
state tensor([0.0683, 0.1283, 0.0000, 0.2853], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198773022
state222:  tensor([[0.0683, 0.1283, 0.0000, 0.2853]], device='cuda:0')
policy_old.forwarded at 1641198773024
give action 245============================
log_to_linear:  tensor([[[0.4156]]], device='cuda:0') tensor([[[0.8730]]], device='cuda:0')
log_to_linear action at 1641198773025
bwe changes from to:  [tensor([[[0.0109]]]), tensor([[[0.0095]]])]
step into gymStat at 1641198773025
send bwe to appRecv at 1641198773025
sent bwe to appRecv at 1641198773025
wait for recv string at 1641198773025
recved string at 1641198773223
1
wait for recv [self.estimator, stat] at 1641198773223
recved [self.estimator, stat] at 1641198773224
sorted packlist at 1641198773224
packetSeq:  5661
packetSeq:  5662
packetSeq:  5663
packetSeq:  5664
packetSeq:  5665
packetSeq:  5666
packetSeq:  5667
packetSeq:  5668
packetSeq:  5669
packetSeq:  5670
packetSeq:  5671
processed packlist at 1641198773224
receiving_rate:  297560.0
delay:  191.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198773224
avgFrameBetween:  6
psnrStat:  [[560214, 559905, 559170, 559847, 559018, 558768, 558480]]
delayStat:  [[173, 173, 173, 173, 174, 173, 173]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559343.1428571428] [173.14285714285714] [0]
processed state3-5 at 1641198773224
liner_to_log:  tensor([[[0.8730]]]) tensor([[[0.4156]]])
linear_to_log at 1641198773224
listState:  [0.07439, 0.12787878787878787, 0.0, 0.5593431428571428, 0.17314285714285713, 0.0, tensor([[[0.4156]]])]
state_clone_detach at 1641198773224
reward: 0.08677031248267547
state tensor([0.0744, 0.1279, 0.0000, 0.4156], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198773225
state222:  tensor([[0.0744, 0.1279, 0.0000, 0.4156]], device='cuda:0')
policy_old.forwarded at 1641198773227
give action 246============================
log_to_linear:  tensor([[[0.5848]]], device='cuda:0') tensor([[[1.1409]]], device='cuda:0')
log_to_linear action at 1641198773227
bwe changes from to:  [tensor([[[0.0095]]]), tensor([[[0.0109]]])]
step into gymStat at 1641198773228
send bwe to appRecv at 1641198773228
sent bwe to appRecv at 1641198773228
wait for recv string at 1641198773228
recved string at 1641198773427
1
wait for recv [self.estimator, stat] at 1641198773427
recved [self.estimator, stat] at 1641198773427
sorted packlist at 1641198773427
packetSeq:  5672
packetSeq:  5673
packetSeq:  5674
packetSeq:  5675
packetSeq:  5676
packetSeq:  5677
packetSeq:  5678
packetSeq:  5679
packetSeq:  5680
packetSeq:  5681
processed packlist at 1641198773427
receiving_rate:  274480.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198773427
avgFrameBetween:  6
psnrStat:  [[557783, 558016, 557373, 557473, 557574, 556782]]
delayStat:  [[174, 173, 173, 174, 173, 175]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557500.1666666666] [173.66666666666666] [0]
processed state3-5 at 1641198773427
liner_to_log:  tensor([[[1.1409]]]) pc got bwe at 1641198768903
bandwidth:  300000
pc flushed at 1641198768904
Bwe Sent: 5 at 1641198768904
got request at 1641198769124
processed allFrame at 1641198769124
send 'asking for bwe' at 1641198769124
sent 'asking for bwe' at 1641198769124
send [estimator, stat] at 1641198769124
sent [estimator, stat] at 1641198769124
pc wait for bwe at 1641198769124
pc got bwe at 1641198769128
bandwidth:  300000
pc flushed at 1641198769128
Bwe Sent: 4 at 1641198769128
got request at 1641198769330
processed allFrame at 1641198769330
send 'asking for bwe' at 1641198769330
sent 'asking for bwe' at 1641198769330
send [estimator, stat] at 1641198769330
sent [estimator, stat] at 1641198769330
pc wait for bwe at 1641198769330
pc got bwe at 1641198769334
bandwidth:  300000
pc flushed at 1641198769334
Bwe Sent: 4 at 1641198769334
got request at 1641198769554
processed allFrame at 1641198769554
send 'asking for bwe' at 1641198769554
sent 'asking for bwe' at 1641198769554
send [estimator, stat] at 1641198769554
sent [estimator, stat] at 1641198769555
pc wait for bwe at 1641198769555
pc got bwe at 1641198769559
bandwidth:  300000
pc flushed at 1641198769559
Bwe Sent: 5 at 1641198769559
got request at 1641198769758
processed allFrame at 1641198769758
send 'asking for bwe' at 1641198769758
sent 'asking for bwe' at 1641198769758
send [estimator, stat] at 1641198769758
sent [estimator, stat] at 1641198769758
pc wait for bwe at 1641198769758
pc got bwe at 1641198769763
bandwidth:  300000
pc flushed at 1641198769763
Bwe Sent: 5 at 1641198769763
got request at 1641198769994
processed allFrame at 1641198769994
send 'asking for bwe' at 1641198769994
sent 'asking for bwe' at 1641198769994
send [estimator, stat] at 1641198769994
sent [estimator, stat] at 1641198769994
pc wait for bwe at 1641198769994
pc got bwe at 1641198769998
bandwidth:  300000
pc flushed at 1641198769999
Bwe Sent: 5 at 1641198769999
got request at 1641198770228
processed allFrame at 1641198770228
send 'asking for bwe' at 1641198770228
sent 'asking for bwe' at 1641198770228
send [estimator, stat] at 1641198770228
sent [estimator, stat] at 1641198770229
pc wait for bwe at 1641198770229
pc got bwe at 1641198770233
bandwidth:  300000
pc flushed at 1641198770233
Bwe Sent: 5 at 1641198770233
got request at 1641198770458
processed allFrame at 1641198770458
send 'asking for bwe' at 1641198770458
sent 'asking for bwe' at 1641198770458
send [estimator, stat] at 1641198770458
sent [estimator, stat] at 1641198770458
pc wait for bwe at 1641198770458
pc got bwe at 1641198770462
bandwidth:  300000
pc flushed at 1641198770462
Bwe Sent: 4 at 1641198770462
got request at 1641198770662
processed allFrame at 1641198770662
send 'asking for bwe' at 1641198770662
sent 'asking for bwe' at 1641198770662
send [estimator, stat] at 1641198770662
sent [estimator, stat] at 1641198770662
pc wait for bwe at 1641198770662
pc got bwe at 1641198770666
bandwidth:  300000
pc flushed at 1641198770666
Bwe Sent: 4 at 1641198770666
got request at 1641198770889
processed allFrame at 1641198770889
send 'asking for bwe' at 1641198770889
sent 'asking for bwe' at 1641198770889
send [estimator, stat] at 1641198770889
sent [estimator, stat] at 1641198770889
pc wait for bwe at 1641198770889
pc got bwe at 1641198770893
bandwidth:  300000
pc flushed at 1641198770893
Bwe Sent: 4 at 1641198770893
got request at 1641198771095
processed allFrame at 1641198771095
send 'asking for bwe' at 1641198771095
sent 'asking for bwe' at 1641198771095
send [estimator, stat] at 1641198771095
sent [estimator, stat] at 1641198771095
pc wait for bwe at 1641198771095
pc got bwe at 1641198771099
bandwidth:  300000
pc flushed at 1641198771099
Bwe Sent: 4 at 1641198771099
got request at 1641198771326
processed allFrame at 1641198771326
send 'asking for bwe' at 1641198771326
sent 'asking for bwe' at 1641198771326
send [estimator, stat] at 1641198771326
sent [estimator, stat] at 1641198771326
pc wait for bwe at 1641198771326
pc got bwe at 1641198771331
bandwidth:  300000
pc flushed at 1641198771331
Bwe Sent: 5 at 1641198771331
got request at 1641198771559
processed allFrame at 1641198771559
send 'asking for bwe' at 1641198771559
sent 'asking for bwe' at 1641198771559
send [estimator, stat] at 1641198771559
sent [estimator, stat] at 1641198771559
pc wait for bwe at 1641198771559
pc got bwe at 1641198771563
bandwidth:  300000
pc flushed at 1641198771563
Bwe Sent: 4 at 1641198771563
got request at 1641198771763
processed allFrame at 1641198771763
send 'asking for bwe' at 1641198771763
sent 'asking for bwe' at 1641198771763
send [estimator, stat] at 1641198771763
sent [estimator, stat] at 1641198771763
pc wait for bwe at 1641198771763
pc got bwe at 1641198771767
bandwidth:  300000
pc flushed at 1641198771767
Bwe Sent: 4 at 1641198771767
got request at 1641198771990
processed allFrame at 1641198771990
send 'asking for bwe' at 1641198771990
sent 'asking for bwe' at 1641198771990
send [estimator, stat] at 1641198771990
sent [estimator, stat] at 1641198771990
pc wait for bwe at 1641198771991
pc got bwe at 1641198771995
bandwidth:  300000
pc flushed at 1641198771995
Bwe Sent: 5 at 1641198771995
got request at 1641198772193
processed allFrame at 1641198772193
send 'asking for bwe' at 1641198772193
sent 'asking for bwe' at 1641198772193
send [estimator, stat] at 1641198772193
sent [estimator, stat] at 1641198772193
pc wait for bwe at 1641198772193
pc got bwe at 1641198772197
bandwidth:  300000
pc flushed at 1641198772197
Bwe Sent: 4 at 1641198772197
got request at 1641198772395
processed allFrame at 1641198772395
send 'asking for bwe' at 1641198772395
sent 'asking for bwe' at 1641198772395
send [estimator, stat] at 1641198772395
sent [estimator, stat] at 1641198772395
pc wait for bwe at 1641198772395
pc got bwe at 1641198772399
bandwidth:  300000
pc flushed at 1641198772399
Bwe Sent: 4 at 1641198772399
got request at 1641198772602
processed allFrame at 1641198772602
send 'asking for bwe' at 1641198772602
sent 'asking for bwe' at 1641198772602
send [estimator, stat] at 1641198772602
sent [estimator, stat] at 1641198772602
pc wait for bwe at 1641198772602
pc got bwe at 1641198772606
bandwidth:  300000
pc flushed at 1641198772606
Bwe Sent: 4 at 1641198772606
got request at 1641198772820
processed allFrame at 1641198772820
send 'asking for bwe' at 1641198772820
sent 'asking for bwe' at 1641198772820
send [estimator, stat] at 1641198772820
sent [estimator, stat] at 1641198772820
pc wait for bwe at 1641198772820
pc got bwe at 1641198772825
bandwidth:  300000
pc flushed at 1641198772825
Bwe Sent: 5 at 1641198772825
got request at 1641198773021
processed allFrame at 1641198773021
send 'asking for bwe' at 1641198773021
sent 'asking for bwe' at 1641198773021
send [estimator, stat] at 1641198773021
sent [estimator, stat] at 1641198773021
pc wait for bwe at 1641198773021
pc got bwe at 1641198773025
bandwidth:  300000
pc flushed at 1641198773025
Bwe Sent: 4 at 1641198773025
got request at 1641198773223
processed allFrame at 1641198773223
send 'asking for bwe' at 1641198773223
sent 'asking for bwe' at 1641198773223
send [estimator, stat] at 1641198773223
sent [estimator, stat] at 1641198773223
pc wait for bwe at 1641198773224
pc got bwe at 1641198773228
bandwidth:  300000
pc flushed at 1641198773228
Bwe Sent: 5 at 1641198773228
got request at 1641198773427
processed allFrame at 1641198773427
send 'asking for bwe' at 1641198773427
sent 'asking for bwe' at 1641198773427
send [estimator, stat] at 1641198773427
sent [estimator, stat] at 1641198773427
pc wait for bwe at 1641198773427
pc got bwe at 1641198773431
bandwidth:  300000
pc flushed at 1641198773431
Bwe Sent: 4 at 1641198773431
got request at 1641198773655
processed allFrame at 1641198773655
send 'asking for bwe' at 1641198773655
sent 'asking for bwe' at 1641198773655
send [estimator, stat] at 1641198773655
sent [estimator, stat] at 1641198773655
pc wait for bwe at 1641198773655
pc got bwe at 1641198773659
bandwidth:  300000
pc flushed at 1641198773659
Bwe Sent: 4 at 1641198773659
got request at 1641198773892
processed allFrame at 1641198773892
send 'asking for bwe' at 1641198773892
sent 'asking for bwe' at 1641198773892
tensor([[[0.5848]]])
linear_to_log at 1641198773427
listState:  [0.06862, 0.12793333333333334, 0.0, 0.5575001666666666, 0.17366666666666666, 0.0, tensor([[[0.5848]]])]
state_clone_detach at 1641198773428
reward: 0.06076782661593111
state tensor([0.0686, 0.1279, 0.0000, 0.5848], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198773428
state222:  tensor([[0.0686, 0.1279, 0.0000, 0.5848]], device='cuda:0')
policy_old.forwarded at 1641198773430
give action 247============================
log_to_linear:  tensor([[[0.3825]]], device='cuda:0') tensor([[[0.8270]]], device='cuda:0')
log_to_linear action at 1641198773431
bwe changes from to:  [tensor([[[0.0109]]]), tensor([[[0.0090]]])]
step into gymStat at 1641198773431
send bwe to appRecv at 1641198773431
sent bwe to appRecv at 1641198773431
wait for recv string at 1641198773431
recved string at 1641198773655
1
wait for recv [self.estimator, stat] at 1641198773655
recved [self.estimator, stat] at 1641198773655
sorted packlist at 1641198773655
packetSeq:  5682
packetSeq:  5683
packetSeq:  5684
packetSeq:  5685
packetSeq:  5686
packetSeq:  5687
packetSeq:  5688
packetSeq:  5689
packetSeq:  5690
processed packlist at 1641198773655
receiving_rate:  239880.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198773655
avgFrameBetween:  6
psnrStat:  [[557034, 556584, 556511, 556026, 556390, 555894, 556633]]
delayStat:  [[173, 173, 173, 173, 173, 173, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556438.8571428572] [172.85714285714286] [0]
processed state3-5 at 1641198773655
liner_to_log:  tensor([[[0.8270]]]) tensor([[[0.3825]]])
linear_to_log at 1641198773656
listState:  [0.05997, 0.12816666666666668, 0.0, 0.5564388571428571, 0.17285714285714285, 0.0, tensor([[[0.3825]]])]
state_clone_detach at 1641198773656
reward: 0.019119727174504053
state tensor([0.0600, 0.1282, 0.0000, 0.3825], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198773656
state222:  tensor([[0.0600, 0.1282, 0.0000, 0.3825]], device='cuda:0')
policy_old.forwarded at 1641198773658
give action 248============================
log_to_linear:  tensor([[[0.1947]]], device='cuda:0') tensor([[[0.6121]]], device='cuda:0')
log_to_linear action at 1641198773659
bwe changes from to:  [tensor([[[0.0090]]]), tensor([[[0.0055]]])]
step into gymStat at 1641198773659
send bwe to appRecv at 1641198773659
sent bwe to appRecv at 1641198773659
wait for recv string at 1641198773659
recved string at 1641198773892
1
wait for recv [self.estimator, stat] at 1641198773892
recved [self.estimator, stat] at 1641198773892
sorted packlist at 1641198773892
packetSeq:  5691
packetSeq:  5692
packetSeq:  5693
packetSeq:  5694
packetSeq:  5695
packetSeq:  5696
packetSeq:  5697
packetSeq:  5698
packetSeq:  5699
packetSeq:  5700
processed packlist at 1641198773892
receiving_rate:  253640.0
delay:  192.375
loss_ratio:  0.0
processed state0-2 at 1641198773892
avgFrameBetween:  6
psnrStat:  [[556334, 557343, 556221, 557348, 556673, 556656, 557307]]
delayStat:  [[173, 172, 172, 173, 172, 173, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556840.2857142857] [172.42857142857142] [0]
processed state3-5 at 1641198773892
liner_to_log:  tensor([[[0.6121]]]) tensor([[[0.1947]]])
linear_to_log at 1641198773893
listState:  [0.06341, 0.12825, 0.0, 0.5568402857142857, 0.1724285714285714, 0.0, tensor([[[0.1947]]])]
state_clone_detach at 1641198773893
reward: 0.03549048556275203
state tensor([0.0634, 0.1283, 0.0000, 0.1947], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198773893
state222:  tensor([[0.0634, 0.1283, 0.0000, 0.1947]], device='cuda:0')
policy_old.forwarded at 1641198773895
give action 249============================
log_to_linear:  tensor([[[0.5788]]], device='cuda:0') tensor([[[1.1305]]], device='cuda:0')
log_to_linear action at 1641198773896
bwe changes from to:  [tensor([[[0.0055]]]), tensor([[[0.0062]]])]
step into gymStat at 1641198773896
send bwe to appRecv at 1641198773896
sent bwe to appRecv at 1641198773896
wait for recv string at 1641198773896
recved string at 1641198774122
1
wait for recv [self.estimator, stat] at 1641198774122
recved [self.estimator, stat] at 1641198774122
sorted packlist at 1641198774122
packetSeq:  5701
packetSeq:  5702
packetSeq:  5703
packetSeq:  5704
packetSeq:  5705
packetSeq:  5706
packetSeq:  5707
packetSeq:  5708
packetSeq:  5709
packetSeq:  5710
processed packlist at 1641198774122
receiving_rate:  270760.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198774122
avgFrameBetween:  6
psnrStat:  [[557577, 558477, 559349, 559788, 560344, 560675]]
delayStat:  [[173, 172, 173, 172, 173, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559368.3333333334] [172.5] [0]
processed state3-5 at 1641198774122
liner_to_log:  tensor([[[1.1305]]]) tensor([[[0.5788]]])
linear_to_log at 1641198774122
listState:  [0.06769, 0.128, 0.0, 0.5593683333333334, 0.1725, 0.0, tensor([[[0.5788]]])]
state_clone_detach at 1641198774123
reward: 0.05629701910016993
state tensor([0.0677, 0.1280, 0.0000, 0.5788], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198774123
state222:  tensor([[0.0677, 0.1280, 0.0000, 0.5788]], device='cuda:0')
policy_old.forwarded at 1641198774125
give action 250============================
log_to_linear:  tensor([[[0.6029]]], device='cuda:0') tensor([[[1.1726]]], device='cuda:0')
log_to_linear action at 1641198774126
bwe changes from to:  [tensor([[[0.0062]]]), tensor([[[0.0073]]])]
step into gymStat at 1641198774126
send bwe to appRecv at 1641198774126
sent bwe to appRecv at 1641198774126
wait for recv string at 1641198774126
recved string at 1641198774326
1
wait for recv [self.estimator, stat] at 1641198774326
recved [self.estimator, stat] at 1641198774326
sorted packlist at 1641198774326
packetSeq:  5711
packetSeq:  5712
packetSeq:  5713
packetSeq:  5714
packetSeq:  5715
packetSeq:  5716
packetSeq:  5717
packetSeq:  5718
packetSeq:  5719
processed packlist at 1641198774326
receiving_rate:  291200.0
delay:  192.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198774326
avgFrameBetween:  6
psnrStat:  [[560292, 560593, 561079, 562368, 561892, 562174, 561508]]
delayStat:  [[172, 172, 173, 172, 174, 172, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561415.1428571428] [172.42857142857142] [0]
processed state3-5 at 1641198774326
liner_to_log:  tensor([[[1.1726]]]) tensor([[[0.6029]]])
linear_to_log at 1641198774326
listState:  [0.0728, 0.1285185185185185, 0.0, 0.5614151428571429, 0.1724285714285714, 0.0, tensor([[[0.6029]]])]
state_clone_detach at 1641198774326
reward: 0.07784123150507638
state tensor([0.0728, 0.1285, 0.0000, 0.6029], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198774327
state222:  tensor([[0.0728, 0.1285, 0.0000, 0.6029]], device='cuda:0')
policy_old.forwarded at 1641198774329
give action 251============================
log_to_linear:  tensor([[[0.5468]]], device='cuda:0') tensor([[[1.0761]]], device='cuda:0')
log_to_linear action at 1641198774329
bwe changes from to:  [tensor([[[0.0073]]]), tensor([[[0.0078]]])]
step into gymStat at 1641198774330
send bwe to appRecv at 1641198774330
sent bwe to appRecv at 1641198774330
wait for recv string at 1641198774330
recved string at 1641198774555
1
wait for recv [self.estimator, stat] at 1641198774555
recved [self.estimator, stat] at 1641198774555
sorted packlist at 1641198774555
packetSeq:  5720
packetSeq:  5721
packetSeq:  5722
packetSeq:  5723
packetSeq:  5724
packetSeq:  5725
packetSeq:  5726
packetSeq:  5727
packetSeq:  5728
packetSeq:  5729
processed packlist at 1641198774555
receiving_rate:  306160.0
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198774555
avgFrameBetween:  6
psnrStat:  [[561611, 561740, 561121, 561959, 561505, 560461]]
delayStat:  [[172, 172, 172, 174, 172, 172]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561399.5] [172.33333333333334] [0]
processed state3-5 at 1641198774555
liner_to_log:  tensor([[[1.0761]]]) tensor([[[0.5468]]])
linear_to_log at 1641198774556
listState:  [0.07654, 0.12833333333333333, 0.0, 0.5613995, 0.17233333333333334, 0.0, tensor([[[0.5468]]])]
state_clone_detach at 1641198774556
reward: 0.0947570628648356
state tensor([0.0765, 0.1283, 0.0000, 0.5468], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198774556
state222:  tensor([[0.0765, 0.1283, 0.0000, 0.5468]], device='cuda:0')
policy_old.forwarded at 1641198774558
give action 252============================
log_to_linear:  tensor([[[0.5668]]], device='cuda:0') tensor([[[1.1100]]], device='cuda:0')
log_to_linear action at 1641198774559
bwe changes from to:  [tensor([[[0.0078]]]), tensor([[[0.0087]]])]
step into gymStat at 1641198774559
send bwe to appRecv at 1641198774559
sent bwe to appRecv at 1641198774559
wait for recv string at 1641198774559
recved string at 1641198774757
1
wait for recv [self.estimator, stat] at 1641198774757
recved [self.estimator, stat] at 1641198774757
sorted packlist at 1641198774757
packetSeq:  5730
packetSeq:  5731
packetSeq:  5732
packetSeq:  5733
packetSeq:  5734
packetSeq:  5735
packetSeq:  5736
packetSeq:  5737
packetSeq:  5738
processed packlist at 1641198774757
receiving_rate:  262360.0
delay:  191.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198774757
avgFrameBetween:  6
psnrStat:  [[560779, 560306, 559644, 559607, 559678, 559787, 560673]]
delayStat:  [[171, 172, 172, 173, 172, 147, 147]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560067.7142857143] [164.85714285714286] [0]
processed state3-5 at 1641198774757
liner_to_log:  tensor([[[1.1100]]]) tensor([[[0.5668]]])
linear_to_log at 1641198774757
listState:  [0.06559, 0.12792592592592592, 0.0, 0.5600677142857143, 0.16485714285714287, 0.0, tensor([[[0.5668]]])]
state_clone_detach at 1641198774757
reward: 0.04676217328263543
state tensor([0.0656, 0.1279, 0.0000, 0.5668], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198774758
state222:  tensor([[0.0656, 0.1279, 0.0000, 0.5668]], device='cuda:0')
policy_old.forwarded at 1641198774760
give action 253============================
log_to_linear:  tensor([[[0.7937]]], device='cuda:0') tensor([[[1.5401]]], device='cuda:0')
log_to_linear action at 1641198774761
bwe changes from to:  [tensor([[[0.0087]]]), tensor([[[0.0134]]])]
step into gymStat at 1641198774761
send bwe to appRecv at 1641198774761
sent bwe to appRecv at 1641198774761
wait for recv string at 1641198774761
recved string at 1641198774991
1
wait for recv [self.estimator, stat] at 1641198774991
recved [self.estimator, stat] at 1641198774991
sorted packlist at 1641198774991
packetSeq:  5739
packetSeq:  5740
packetSeq:  5741
packetSeq:  5742
packetSeq:  5743
packetSeq:  5744
packetSeq:  5745
packetSeq:  5746
processed packlist at 1641198774991
receiving_rate:  226040.0
delay:  193.5
loss_ratio:  0.0
processed state0-2 at 1641198774992
avgFrameBetween:  6
psnrStat:  [[560043, 560137, 560472, 561753, 563110, 562390, 561912]]
delayStat:  [[147, 147, 147, 147, 148, 147, 147]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561402.4285714285] [147.14285714285714] [0]
processed state3-5 at 1641198774992
liner_to_log:  tensor([[[1.5401]]]) tensor([[[0.7937]]])
linear_to_log at 1641198774992
listState:  [0.05651, 0.129, 0.0, 0.5614024285714285, 0.14714285714285713, 0.0, tensor([[[0.7937]]])]
state_clone_detach at 1641198774992
reward: -0.0005754201288044047
state tensor([0.0565, 0.1290, 0.0000, 0.7937], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198774993
state222:  tensor([[0.0565, 0.1290, 0.0000, 0.7937]], device='cuda:0')
policy_old.forwarded at 1641198774994
give action 254============================
log_to_linear:  tensor([[[0.1964]]], device='cuda:0') tensor([[[0.6137]]], device='cuda:0')
log_to_linear action at 1641198774995
bwe changes from to:  [tensor([[[0.0134]]]), tensor([[[0.0082]]])]
step into gymStat at 1641198774996
send bwe to appRecv at 1641198774996
sent bwe to appRecv at 1641198774996
wait for recv string at 1641198774996
recved string at 1641198775218
1
wait for recv [self.estimator, stat] at 1641198775218
recved [self.estimator, stat] at 1641198775218
sorted packlist at 1641198775218
packetSeq:  5747
packetSeq:  5748
packetSeq:  5749
packetSeq:  5750
packetSeq:  5751
packetSeq:  5752
packetSeq:  5753
packetSeq:  5754
packetSeq:  5755
packetSeq:  5756
packetSeq:  5757
packetSeq:  5758
processed packlist at 1641198775218
receiving_rate:  305400.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198775218
avgFrameBetween:  6
psnrStat:  [[561546, 561930, 561966, 561752, 562358, 561651, 561856]]
delayStat:  [[147, 147, 147, 148, 147, 147, 147]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561865.5714285715] [147.14285714285714] [0]
processed state3-5 at 1641198775218
liner_to_log:  tensor([[[0.6137]]]) tensor([[[0.1964]]])
linear_to_log at 1641198775219
listState:  [0.07635, 0.12793939393939394, 0.0, 0.5618655714285715, 0.14714285714285713, 0.0, tensor([[[0.1964]]])]
state_clone_detach at 1641198775219
reward: 0.09511841238808977
state tensor([0.0764, 0.1279, 0.0000, 0.1964], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198775219
state222:  tensor([[0.0764, 0.1279, 0.0000, 0.1964]], device='cuda:0')
policy_old.forwarded at 1641198775221
give action 255============================
log_to_linear:  tensor([[[0.8586]]], device='cuda:0') tensor([[[1.6781]]], device='cuda:0')
log_to_linear action at 1641198775222
bwe changes from to:  [tensor([[[0.0082]]]), tensor([[[0.0138]]])]
step into gymStat at 1641198775222
send bwe to appRecv at 1641198775222
sent bwe to appRecv at 1641198775222
wait for recv string at 1641198775222
recved string at 1641198775421
1
wait for recv [self.estimator, stat] at 1641198775421
recved [self.estimator, stat] at 1641198775421
sorted packlist at 1641198775421
packetSeq:  5759
packetSeq:  5760
packetSeq:  5761
packetSeq:  5762
packetSeq:  5763
packetSeq:  5764
packetSeq:  5765
packetSeq:  5766
packetSeq:  5767
packetSeq:  5768
processed packlist at 1641198775421
receiving_rate:  287280.0
delay:  191.8
loss_ratio:  0.0
processed state0-2 at 1641198775421
avgFrameBetween:  6
psnrStat:  [[561407, 561908, 561379, 561030, 560881, 560297]]
delayStat:  [[148, 148, 147, 147, 147, 148]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561150.3333333334] [147.5] [0]
processed state3-5 at 1641198775421
liner_to_log:  tensor([[[1.6781]]]) tensor([[[0.8586]]])
linear_to_log at 1641198775421
listState:  [0.07182, 0.12786666666666668, 0.0, 0.5611503333333334, 0.1475, 0.0, tensor([[[0.8586]]])]
state_clone_detach at 1641198775422
reward: 0.0754350736161577
state tensor([0.0718, 0.1279, 0.0000, 0.8586], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198775422
state222:  tensor([[0.0718, 0.1279, 0.0000, 0.8586]], device='cuda:0')
policy_old.forwarded at 1641198775424
give action 256============================
log_to_linear:  tensor([[[0.0751]]], device='cuda:0') tensor([[[0.5248]]], device='cuda:0')
log_to_linear action at 1641198775425
bwe changes from to:  [tensor([[[0.0138]]]), tensor([[[0.0073]]])]
step into gymStat at 1641198775425
send bwe to appRecv at 1641198775425
sent bwe to appRecv at 1641198775425
wait for recv string at 1641198775425
recved string at 1641198775654
1
wait for recv [self.estimator, stat] at 1641198775654
recved [self.estimator, stat] at 1641198775654
sorted packlist at 1641198775654
packetSeq:  5769
packetSeq:  5770
packetSeq:  5771
packetSeq:  5772
packetSeq:  5773
packetSeq:  5774
packetSeq:  5775
packetSeq:  5776
packetSeq:  5777
packetSeq:  5778
packetSeq:  5779
processed packlist at 1641198775654
receiving_rate:  279800.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198775654
avgFrameBetween:  6
psnrStat:  [[559606, 558910, 557746, 556577, 555777, 555933, 554358]]
delayStat:  [[147, 147, 147, 147, 147, 147, 148]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556986.7142857143] [147.14285714285714] [0]
processed state3-5 at 1641198775655
liner_to_log:  tensor([[[0.5248]]]) tensor([[[0.0751]]])
linear_to_log at 1641198775655
listState:  [0.06995, 0.1282962962962963, 0.0, 0.5569867142857143, 0.14714285714285713, 0.0, tensor([[[0.0751]]])]
state_clone_detach at 1641198775655
reward: 0.06573430649654488
state tensor([0.0699, 0.1283, 0.0000, 0.0751], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198775656
state222:  tensor([[0.0699, 0.1283, 0.0000, 0.0751]], device='cuda:0')
policy_old.forwarded at 1641198775657
give action 257============================
log_to_linear:  tensor([[[0.5309]]], device='cuda:0') tensor([[[1.0498]]], device='cuda:0')
log_to_linear action at 1641198775658
bwe changes from to:  [tensor([[[0.0073]]]), tensor([[[0.0076]]])]
step into gymStat at 1641198775659
send bwe to appRecv at 1641198775659
sent bwe to appRecv at 1641198775659
wait for recv string at 1641198775659
recved string at 1641198775890
1
wait for recv [self.estimator, stat] at 1641198775890
recved [self.estimator, stat] at 1641198775890
sorted packlist at 1641198775890
packetSeq:  5780
packetSeq:  5781
packetSeq:  5782
packetSeq:  5783
packetSeq:  5784
packetSeq:  5785
packetSeq:  5786
packetSeq:  5787
packetSeq:  5788
packetSeq:  5789
processed packlist at 1641198775890
receiving_rate:  272760.0
delay:  192.625
loss_ratio:  0.0
processed state0-2 at 1641198775890
avgFrameBetween:  6
psnrStat:  [[552967, 552503, 550547, 550612, 550771, 549137, 549349]]
delayStat:  [[147, 147, 147, 147, 147, 146, 146]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550840.8571428572] [146.71428571428572] [0]
processed state3-5 at 1641198775890
liner_to_log:  tensor([[[1.0498]]]) tensor([[[0.5309]]])
linear_to_log at 1641198775891
listState:  [0.06819, 0.12841666666666668, 0.0, 0.5508408571428571, 0.1467142857142857, 0.0, tensor([[[0.5309]]])]
state_clone_detach at 1641198775891
reward: 0.057346929919953216
state tensor([0.0682, 0.1284, 0.0000, 0.5309], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198775892
state222:  tensor([[0.0682, 0.1284, 0.0000, 0.5309]], device='cuda:0')
policy_old.forwarded at 1641198775893
give action 258============================
log_to_linear:  tensor([[[0.5600]]], device='cuda:0') tensor([[[1.0985]]], device='cuda:0')
log_to_linear action at 1641198775894
bwe changes from to:  [tensor([[[0.0076]]]), tensor([[[0.0084]]])]
step into gymStat at 1641198775894
send bwe to appRecv at 1641198775894
sent bwe to appRecv at 1641198775894
wait for recv string at 1641198775894
recved string at 1641198776123
1
wait for recv [self.estimator, stat] at 1641198776123
recved [self.estimator, stat] at 1641198776123
sorted packlist at 1641198776123
packetSeq:  5790
packetSeq:  5791
packetSeq:  5792
packetSeq:  5793
packetSeq:  5794
packetSeq:  5795
packetSeq:  5796
processed packlist at 1641198776123
receiving_rate:  262120.0
delay:  193.28571428571428
loss_ratio:  0.0
processed state0-2 at 1641198776123
avgFrameBetween:  6
psnrStat:  [[548879, 549053, 548348, 548042, 547564, 547996, 548009]]
delayStat:  [[147, 146, 146, 146, 147, 147, 146]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548270.1428571428] [146.42857142857142] [0]
processed state3-5 at 1641198776123
liner_to_log:  tensor([[[1.0985]]]) tensor([[[0.5600]]])
linear_to_log at 1641198776123
listState:  [0.06553, 0.12885714285714286, 0.0, 0.5482701428571428, 0.1464285714285714, 0.0, tensor([[[0.5600]]])]
state_clone_detach at 1641198776124
reward: 0.04368740425801054
state tensor([0.0655, 0.1289, 0.0000, 0.5600], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198776124
state222:  tensor([[0.0655, 0.1289, 0.0000, 0.5600]], device='cuda:0')
policy_old.forwarded at 1641198776126
give action 259============================
log_to_linear:  tensor([[[0.5846]]], device='cuda:0') tensor([[[1.1406]]], device='cuda:0')
log_to_linear action at 1641198776127
bwe changes from to:  [tensor([[[0.0084]]]), tensor([[[0.0095]]])]
step into gymStat at 1641198776127
send bwe to appRecv at 1641198776127
sent bwe to appRecv at 1641198776127
wait for recv string at 1641198776127
recved string at 1641198776355
1
wait for recv [self.estimator, stat] at 1641198776355
recved [self.estimator, stat] at 1641198776355
sorted packlist at 1641198776355
packetSeq:  5797
packetSeq:  5798
packetSeq:  5799
packetSeq:  5800
packetSeq:  5801
packetSeq:  5802
packetSeq:  5803
packetSeq:  5804
packetSeq:  5805
packetSeq:  5806
packetSeq:  5807
processed packlist at 1641198776355
receiving_rate:  292400.0
delay:  191.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198776356
avgFrameBetween:  6
psnrStat:  [[548588, 550266, 551598, 553460, 554039, 555158, 555402]]
delayStat:  [[146, 147, 146, 146, 146, 146, 147]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552644.4285714285] [146.28571428571428] [0]
processed state3-5 at 1641198776356
liner_to_log:  tensor([[[1.1406]]]) tensor([[[0.5846]]])
linear_to_log at 1641198776356
listState:  [0.0731, 0.12787878787878787, 0.0, 0.5526444285714285, 0.14628571428571427, 0.0, tensor([[[0.5846]]])]
state_clone_detach at 1641198776356
reward: 0.081089327622
state tensor([0.0731, 0.1279, 0.0000, 0.5846], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198776357
state222:  tensor([[0.0731, 0.1279, 0.0000, 0.5846]], device='cuda:0')
policy_old.forwarded at 1641198776358
give action 260============================
log_to_linear:  tensor([[[0.6422]]], device='cuda:0') tensor([[[1.2435]]], device='cuda:0')
log_to_linear action at 1641198776359
bwe changes from to:  [tensor([[[0.0095]]]), tensor([[[0.0119]]])]
step into gymStat at 1641198776360
send bwe to appRecv at 1641198776360
sent bwe to appRecv at 1641198776360
wait for recv string at 1641198776360
recved string at 1641198776588
1
wait for recv [self.estimator, stat] at 1641198776588
recved [self.estimator, stat] at 1641198776588
sorted packlist at 1641198776588
packetSeq:  5808
packetSeq:  5809
packetSeq:  5810
packetSeq:  5811
packetSeq:  5812
packetSeq:  5813
packetSeq:  5814
packetSeq:  5815
packetSeq:  5816
packetSeq:  5817
packetSeq:  5818
packetSeq:  5819
processed packlist at 1641198776588
receiving_rate:  278000.0
delay:  191.63636363636363
loss_ratio:  0.0
processed state0-2 at 1641198776588
avgFrameBetween:  6
psnrStat:  [[556125, 556694, 555863, 555886, 556009, 557258, 557605]]
delayStat:  [[146, 147, 146, 146, 146, 146, 146]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556491.4285714285] [146.14285714285714] [0]
processed state3-5 at 1641198776588
liner_to_log:  tensor([[[1.2435]]]) tensor([[[0.6422]]])
linear_to_log at 1641198776589
listState:  [0.0695, 0.12775757575757576, 0.0, 0.5564914285714285, 0.14614285714285713, 0.0, tensor([[[0.6422]]])]
state_clone_detach at 1641198776589
reward: 0.06530849223368179
state tensor([0.0695, 0.1278, 0.0000, 0.6422], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198776589
state222:  tensor([[0.0695, 0.1278, 0.0000, 0.6422]], device='cuda:0')
policy_old.forwarded at 1641198776591
give action 261============================
log_to_linear:  tensor([[[0.7533]]], device='cuda:0') tensor([[[1.4573]]], device='cuda:0')
log_to_linear action at 1641198776592
bwe changes from to:  [tensor([[[0.0119]]]), tensor([[[0.0173]]])]
step into gymStat at 1641198776592
send bwe to appRecv at 1641198776592
sent bwe to appRecv at 1641198776592
wait for recv string at 1641198776592
recved string at 1641198776792
1
wait for recv [self.estimator, stat] at 1641198776792
recved [self.estimator, stat] at 1641198776792
sorted packlist at 1641198776792
packetSeq:  5820
packetSeq:  5821
packetSeq:  5822
packetSeq:  5823
packetSeq:  5824
packetSeq:  5825
packetSeq:  5826
packetSeq:  5827
packetSeq:  5828
packetSeq:  5829
packetSeq:  5830
processed packlist at 1641198776792
receiving_rate:  280480.0
delay:  192.2
loss_ratio:  0.0
processed state0-2 at 1641198776792
avgFrameBetween:  6
psnrStat:  [[557741, 557350, 557589, 557233, 557459, 556752, 556854]]
delayStat:  [[147, 146, 146, 146, 122, 122, 123]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557282.5714285715] [136.0] [0]
processed state3-5 at 1641198776792
liner_to_log:  tensor([[[1.4573]]]) tensor([[[0.7533]]])
linear_to_log at 1641198776793
listState:  [0.07012, 0.12813333333333332, 0.0, 0.5572825714285715, 0.136, 0.0, tensor([[[0.7533]]])]
state_clone_detach at 1641198776793
reward: 0.06699280351901954
state tensor([0.0701, 0.1281, 0.0000, 0.7533], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198776793
state222:  tensor([[0.0701, 0.1281, 0.0000, 0.7533]], device='cuda:0')
policy_old.forwarded at 1641198776795
give action 262============================
log_to_linear:  tensor([[[0.5372]]], device='cuda:0') tensor([[[1.0603]]], device='cuda:0')
log_to_linear action at 1641198776796
bwe changes from to:  [tensor([[[0.0173]]]), tensor([[[0.0183]]])]
step into gymStat at 1641198776796
send bwe to appRecv at 1641198776796
sent bwe to appRecv at 1641198776796
wait for recv string at 1641198776796
recved string at 1641198776993
1
wait for recv [self.estimator, stat] at 1641198776993
recved [self.estimator, stat] at 1641198776993
sorted packlist at 1641198776993
packetSeq:  5831
packetSeq:  5832
packetSeq:  5833
packetSeq:  5834
packetSeq:  5835
packetSeq:  5836
packetSeq:  5837
packetSeq:  5838
packetSeq:  5839
processed packlist at 1641198776993
receiving_rate:  263120.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198776993
avgFrameBetween:  6
psnrStat:  [[556079, 555626, 556301, 556742, 556950, 557038]]
delayStat:  [[122, 122, 123, 122, 123, 122]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556456.0] [122.33333333333333] [0]
processed state3-5 at 1641198776993
liner_to_log:  tensor([[[1.0603]]]) tensor([[[0.5372]]])
linear_to_log at 1641198776993
listState:  [0.06578, 0.12814814814814815, 0.0, 0.556456, 0.12233333333333332, 0.0, tensor([[[0.5372]]])]
state_clone_detach at 1641198776994
reward: 0.04698484640511452
state tensor([0.0658, 0.1281, 0.0000, 0.5372], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198776994
state222:  tensor([[0.0658, 0.1281, 0.0000, 0.5372]], device='cuda:0')
policy_old.forwarded at 1641198776996
give action 263============================
log_to_linear:  tensor([[[0.5863]]], device='cuda:0') tensor([[[1.1436]]], device='cuda:0')
log_to_linear action at 1641198776997
bwe changes from to:  [tensor([[[0.0183]]]), tensor([[[0.0210]]])]
step into gymStat at 1641198776997
send bwe to appRecv at 1641198776997
sent bwe to appRecv at 1641198776997
wait for recv string at 1641198776997
recved string at 1641198777222
1
wait for recv [self.estimator, stat] at 1641198777222
recved [self.estimator, stat] at 1641198777222
sorted packlist at 1641198777222
packetSeq:  5840
packetSeq:  5841
packetSeq:  5842
packetSeq:  5843
packetSeq:  5844
packetSeq:  5845
packetSeq:  5846
packetSeq:  5847
packetSeq:  5848
processed packlist at 1641198777222
receiving_rate:  247680.0
delay:  192.375
loss_ratio:  0.0
processed state0-2 at 1641198777222
avgFrameBetween:  6
psnrStat:  [[556886, 556489, 555734, 555995, 555245, 555713, 554819]]
delayStat:  [[122, 122, 122, 122, 122, 122, 122]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555840.1428571428] [122.0] [0]
processed state3-5 at 1641198777223
liner_to_log:  tensor([[[1.1436]]]) tensor([[[0.5863]]])
linear_to_log at 1641198777223
listState:  [0.06192, 0.12825, 0.0, 0.5558401428571429, 0.122, 0.0, tensor([[[0.5863]]])]
state_clone_detach at 1641198777223
reward: 0.028347829406710767
state tensor([0.0619, 0.1283, 0.0000, 0.5863], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198777224
state222:  tensor([[0.0619, 0.1283, 0.0000, 0.5863]], device='cuda:0')
policy_old.forwarded at 1641198777226
give action 264============================
log_to_linear:  tensor([[[0.0557]]], device='cuda:0') tensor([[[0.5154]]], device='cuda:0')
log_to_linear action at 1641198777226
bwe changes from to:  [tensor([[[0.0210]]]), tensor([[[0.0108]]])]
step into gymStat at 1641198777227
send bwe to appRecv at 1641198777227
sent bwe to appRecv at 1641198777227
wait for recv string at 1641198777227
recved string at 1641198777454
1
wait for recv [self.estimator, stat] at 1641198777454
recved [self.estimator, stat] at 1641198777454
sorted packlist at 1641198777454
packetSeq:  5849
packetSeq:  5850
packetSeq:  5851
packetSeq:  5852
packetSeq:  5853
packetSeq:  5854
packetSeq:  5855
packetSeq:  5856
packetSeq:  5857
packetSeq:  5858
processed packlist at 1641198777454
receiving_rate:  234720.0
delay:  192.375
loss_ratio:  0.0
processed state0-2 at 1641198777454
avgFrameBetween:  6
psnrStat:  [[556211, 554887, 554979, 556261, 555439, 555572, 557013]]
delayStat:  [[122, 122, 123, 122, 122, 122, 122]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555766.0] [122.14285714285714] [0]
processed state3-5 at 1641198777454
liner_to_log:  tensor([[[0.5154]]]) tensor([[[0.0557]]])
linear_to_log at 1641198777455
listState:  [0.05868, 0.12825, 0.0, 0.555766, 0.12214285714285714, 0.0, tensor([[[0.0557]]])]
state_clone_detach at 1641198777455
reward: 0.012516292064692347
state tensor([0.0587, 0.1283, 0.0000, 0.0557], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198777455
state222:  tensor([[0.0587, 0.1283, 0.0000, 0.0557]], device='cuda:0')
policy_old.forwarded at 1641198777457
give action 265============================
log_to_linear:  tensor([[[0.6303]]], device='cuda:0') tensor([[[1.2217]]], device='cuda:0')
log_to_linear action at 1641198777458
bwe changes from to:  [tensor([[[0.0108]]]), tensor([[[0.0132]]])]
step into gymStat at 1641198777458
send bwe to appRecv at 1641198777458
sent bwe to appRecv at 1641198777458
wait for recv string at 1641198777458
recved string at 1641198777691
1
wait for recv [self.estimator, stat] at 1641198777691
recved [self.estimator, stat] at 1641198777691
sorted packlist at 1641198777691
packetSeq:  5859
packetSeq:  5860
packetSeq:  5861
packetSeq:  5862
packetSeq:  5863
packetSeq:  5864
packetSeq:  5865
packetSeq:  5866
packetSeq:  5867
packetSeq:  5868
packetSeq:  5869
processed packlist at 1641198777691
receiving_rate:  274400.0
delay:  192.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198777691
avgFrameBetween:  6
psnrStat:  [[557523, 559003, 558844, 560048, 559955, 560514, 560166]]
delayStat:  [[122, 123, 122, 123, 122, 122, 123]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559436.1428571428] [122.42857142857143] [0]
processed state3-5 at 1641198777691
liner_to_log:  tensor([[[1.2217]]]) tensor([[[0.6303]]])
linear_to_log at 1641198777691
listState:  [0.0686, 0.1285925925925926, 0.0, 0.5594361428571428, 0.12242857142857143, 0.0, tensor([[[0.6303]]])]
state_clone_detach at 1641198777691
reward: 0.05869852263860115
state tensor([0.0686, 0.1286, 0.0000, 0.6303], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198777692
state222:  tensor([[0.0686, 0.1286, 0.0000, 0.6303]], device='cuda:0')
policy_old.forwarded at 1641198777694
give action 266============================
log_to_linear:  tensor([[[0.3394]]], device='cuda:0') tensor([[[0.7706]]], device='cuda:0')
log_to_linear action at 1641198777695
bwe changes from to:  [tensor([[[0.0132]]]), tensor([[[0.0102]]])]
step into gymStat at 1641198777695
send bwe to appRecv at 1641198777695
sent bwe to appRecv at 1641198777695
wait for recv string at 1641198777695
recved string at 1641198777898
1
wait for recv [self.estimator, stat] at 1641198777898
recved [self.estimator, stat] at 1641198777898
sorted packlist at 1641198777898
packetSeq:  5870
packetSeq:  5871
packetSeq:  5872
packetSeq:  5873
packetSeq:  5874
packetSeq:  5875
packetSeq:  5876
packetSeq:  5877
send [estimator, stat] at 1641198773892
sent [estimator, stat] at 1641198773892
pc wait for bwe at 1641198773892
pc got bwe at 1641198773896
bandwidth:  300000
pc flushed at 1641198773896
Bwe Sent: 4 at 1641198773896
got request at 1641198774121
processed allFrame at 1641198774121
send 'asking for bwe' at 1641198774121
sent 'asking for bwe' at 1641198774122
send [estimator, stat] at 1641198774122
sent [estimator, stat] at 1641198774122
pc wait for bwe at 1641198774122
pc got bwe at 1641198774126
bandwidth:  300000
pc flushed at 1641198774126
Bwe Sent: 5 at 1641198774126
got request at 1641198774325
processed allFrame at 1641198774325
send 'asking for bwe' at 1641198774325
sent 'asking for bwe' at 1641198774325
send [estimator, stat] at 1641198774325
sent [estimator, stat] at 1641198774326
pc wait for bwe at 1641198774326
pc got bwe at 1641198774330
bandwidth:  300000
pc flushed at 1641198774330
Bwe Sent: 5 at 1641198774330
got request at 1641198774555
processed allFrame at 1641198774555
send 'asking for bwe' at 1641198774555
sent 'asking for bwe' at 1641198774555
send [estimator, stat] at 1641198774555
sent [estimator, stat] at 1641198774555
pc wait for bwe at 1641198774555
pc got bwe at 1641198774559
bandwidth:  300000
pc flushed at 1641198774559
Bwe Sent: 4 at 1641198774559
got request at 1641198774756
processed allFrame at 1641198774757
send 'asking for bwe' at 1641198774757
sent 'asking for bwe' at 1641198774757
send [estimator, stat] at 1641198774757
sent [estimator, stat] at 1641198774757
pc wait for bwe at 1641198774757
pc got bwe at 1641198774761
bandwidth:  300000
pc flushed at 1641198774761
Bwe Sent: 5 at 1641198774761
got request at 1641198774991
processed allFrame at 1641198774991
send 'asking for bwe' at 1641198774991
sent 'asking for bwe' at 1641198774991
send [estimator, stat] at 1641198774991
sent [estimator, stat] at 1641198774991
pc wait for bwe at 1641198774991
pc got bwe at 1641198774996
bandwidth:  300000
pc flushed at 1641198774996
Bwe Sent: 5 at 1641198774996
got request at 1641198775218
processed allFrame at 1641198775218
send 'asking for bwe' at 1641198775218
sent 'asking for bwe' at 1641198775218
send [estimator, stat] at 1641198775218
sent [estimator, stat] at 1641198775218
pc wait for bwe at 1641198775218
pc got bwe at 1641198775222
bandwidth:  300000
pc flushed at 1641198775222
Bwe Sent: 4 at 1641198775222
got request at 1641198775420
processed allFrame at 1641198775421
send 'asking for bwe' at 1641198775421
sent 'asking for bwe' at 1641198775421
send [estimator, stat] at 1641198775421
sent [estimator, stat] at 1641198775421
pc wait for bwe at 1641198775421
pc got bwe at 1641198775425
bandwidth:  300000
pc flushed at 1641198775425
Bwe Sent: 5 at 1641198775425
got request at 1641198775654
processed allFrame at 1641198775654
send 'asking for bwe' at 1641198775654
sent 'asking for bwe' at 1641198775654
send [estimator, stat] at 1641198775654
sent [estimator, stat] at 1641198775654
pc wait for bwe at 1641198775654
pc got bwe at 1641198775659
bandwidth:  300000
pc flushed at 1641198775659
Bwe Sent: 5 at 1641198775659
got request at 1641198775890
processed allFrame at 1641198775890
send 'asking for bwe' at 1641198775890
sent 'asking for bwe' at 1641198775890
send [estimator, stat] at 1641198775890
sent [estimator, stat] at 1641198775890
pc wait for bwe at 1641198775890
pc got bwe at 1641198775895
bandwidth:  300000
pc flushed at 1641198775895
Bwe Sent: 5 at 1641198775895
got request at 1641198776122
processed allFrame at 1641198776123
send 'asking for bwe' at 1641198776123
sent 'asking for bwe' at 1641198776123
send [estimator, stat] at 1641198776123
sent [estimator, stat] at 1641198776123
pc wait for bwe at 1641198776123
pc got bwe at 1641198776127
bandwidth:  300000
pc flushed at 1641198776127
Bwe Sent: 5 at 1641198776127
got request at 1641198776355
processed allFrame at 1641198776355
send 'asking for bwe' at 1641198776355
sent 'asking for bwe' at 1641198776355
send [estimator, stat] at 1641198776355
sent [estimator, stat] at 1641198776355
pc wait for bwe at 1641198776355
pc got bwe at 1641198776360
bandwidth:  300000
pc flushed at 1641198776360
Bwe Sent: 5 at 1641198776360
got request at 1641198776588
processed allFrame at 1641198776588
send 'asking for bwe' at 1641198776588
sent 'asking for bwe' at 1641198776588
send [estimator, stat] at 1641198776588
sent [estimator, stat] at 1641198776588
pc wait for bwe at 1641198776588
pc got bwe at 1641198776592
bandwidth:  300000
pc flushed at 1641198776592
Bwe Sent: 4 at 1641198776592
got request at 1641198776792
processed allFrame at 1641198776792
send 'asking for bwe' at 1641198776792
sent 'asking for bwe' at 1641198776792
send [estimator, stat] at 1641198776792
sent [estimator, stat] at 1641198776792
pc wait for bwe at 1641198776792
pc got bwe at 1641198776796
bandwidth:  300000
pc flushed at 1641198776796
Bwe Sent: 4 at 1641198776796
got request at 1641198776993
processed allFrame at 1641198776993
send 'asking for bwe' at 1641198776993
sent 'asking for bwe' at 1641198776993
send [estimator, stat] at 1641198776993
sent [estimator, stat] at 1641198776993
pc wait for bwe at 1641198776993
pc got bwe at 1641198776997
bandwidth:  300000
pc flushed at 1641198776997
Bwe Sent: 4 at 1641198776997
got request at 1641198777222
processed allFrame at 1641198777222
send 'asking for bwe' at 1641198777222
sent 'asking for bwe' at 1641198777222
send [estimator, stat] at 1641198777222
sent [estimator, stat] at 1641198777222
pc wait for bwe at 1641198777222
pc got bwe at 1641198777227
bandwidth:  300000
pc flushed at 1641198777227
Bwe Sent: 5 at 1641198777227
got request at 1641198777454
processed allFrame at 1641198777454
send 'asking for bwe' at 1641198777454
sent 'asking for bwe' at 1641198777454
send [estimator, stat] at 1641198777454
sent [estimator, stat] at 1641198777454
pc wait for bwe at 1641198777454
pc got bwe at 1641198777458
bandwidth:  300000
pc flushed at 1641198777458
Bwe Sent: 4 at 1641198777458
got request at 1641198777690
processed allFrame at 1641198777691
send 'asking for bwe' at 1641198777691
sent 'asking for bwe' at 1641198777691
send [estimator, stat] at 1641198777691
sent [estimator, stat] at 1641198777691
pc wait for bwe at 1641198777691
pc got bwe at 1641198777695
bandwidth:  300000
pc flushed at 1641198777695
Bwe Sent: 5 at 1641198777695
got request at 1641198777898
processed allFrame at 1641198777898
send 'asking for bwe' at 1641198777898
sent 'asking for bwe' at 1641198777898
send [estimator, stat] at 1641198777898
sent [estimator, stat] at 1641198777898
pc wait for bwe at 1641198777898
pc got bwe at 1641198777903
bandwidth:  300000
pc flushed at 1641198777903
Bwe Sent: 5 at 1641198777903
got request at 1641198778122
processed allFrame at 1641198778122
send 'asking for bwe' at 1641198778122
sent 'asking for bwe' at 1641198778122
send [estimator, stat] at 1641198778122
sent [estimator, stat] at 1641198778122
pc wait for bwe at 1641198778122
pc got bwe at 1641198778126
bandwidth:  300000
pc flushed at 1641198778126
Bwe Sent: 4 at 1641198778126
got request at 1641198778326
processed allFrame at 1641198778326
send 'asking for bwe' at 1641198778326
sent 'asking for bwe' at 1641198778326
send [estimator, stat] at 1641198778326
sent [estimator, stat] at 1641198778326
pc wait for bwe at 1641198778326
pc got bwe at 1641198778331
bandwidth:  300000
pc flushed at 1641198778331
Bwe Sent: 5 at 1641198778331
got request at 1641198778560
processed allFrame at 1641198778560
send 'asking for bwe' at 1641198778560
sent 'asking for bwe' at 1641198778560
send [estimator, stat] at 1641198778560
sent [estimator, stat] at 1641198778560
pc wait for bwe at 1641198778560
pc got bwe at 1641198778564
bandwidth:  300000
pc flushed at 1641198778564
Bwe Sent: 4 at 1641198778564
got request at 1641198778797
processed allFrame at 1641198778797
send 'asking for bwe' at 1641198778797
sent 'asking for bwe' at 1641198778797
send [estimator, stat] at 1641198778797
sent [estimator, stat] at 1641198778797
pc wait for bwe at 1641198778797
pc got bwe at 1641198778802
bandwidth:  300000
pc flushed at 1641198778802
Bwe Sent: 5 at 1641198778802
got request at 1641198779004
packetSeq:  5878
processed packlist at 1641198777899
receiving_rate:  280600.0
delay:  197.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198777899
avgFrameBetween:  6
psnrStat:  [[559652, 559673, 559666, 560443, 560747, 560640]]
delayStat:  [[122, 122, 123, 123, 122, 122]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560136.8333333334] [122.33333333333333] [0]
processed state3-5 at 1641198777899
liner_to_log:  tensor([[[0.7706]]]) tensor([[[0.3394]]])
linear_to_log at 1641198777899
listState:  [0.07015, 0.13155555555555556, 0.0, 0.5601368333333334, 0.12233333333333332, 0.0, tensor([[[0.3394]]])]
state_clone_detach at 1641198777899
reward: 0.05686184763816898
state tensor([0.0702, 0.1316, 0.0000, 0.3394], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198777900
state222:  tensor([[0.0702, 0.1316, 0.0000, 0.3394]], device='cuda:0')
policy_old.forwarded at 1641198777902
give action 267============================
log_to_linear:  tensor([[[0.6112]]], device='cuda:0') tensor([[[1.1874]]], device='cuda:0')
log_to_linear action at 1641198777902
bwe changes from to:  [tensor([[[0.0102]]]), tensor([[[0.0121]]])]
step into gymStat at 1641198777903
send bwe to appRecv at 1641198777903
sent bwe to appRecv at 1641198777903
wait for recv string at 1641198777903
recved string at 1641198778122
1
wait for recv [self.estimator, stat] at 1641198778122
recved [self.estimator, stat] at 1641198778122
sorted packlist at 1641198778122
packetSeq:  5879
packetSeq:  5880
packetSeq:  5881
packetSeq:  5882
packetSeq:  5883
packetSeq:  5884
packetSeq:  5885
packetSeq:  5886
packetSeq:  5887
packetSeq:  5888
packetSeq:  5889
processed packlist at 1641198778122
receiving_rate:  304520.0
delay:  197.45454545454547
loss_ratio:  0.0
processed state0-2 at 1641198778122
avgFrameBetween:  6
psnrStat:  [[561312, 560323, 560259, 560577, 559901, 559633, 558709]]
delayStat:  [[122, 122, 123, 122, 123, 122, 122]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560102.0] [122.28571428571429] [0]
processed state3-5 at 1641198778122
liner_to_log:  tensor([[[1.1874]]]) tensor([[[0.6112]]])
linear_to_log at 1641198778123
listState:  [0.07613, 0.13163636363636363, 0.0, 0.560102, 0.12228571428571429, 0.0, tensor([[[0.6112]]])]
state_clone_detach at 1641198778123
reward: 0.0830760831756664
state tensor([0.0761, 0.1316, 0.0000, 0.6112], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198778124
state222:  tensor([[0.0761, 0.1316, 0.0000, 0.6112]], device='cuda:0')
policy_old.forwarded at 1641198778125
give action 268============================
log_to_linear:  tensor([[[0.1554]]], device='cuda:0') tensor([[[0.5784]]], device='cuda:0')
log_to_linear action at 1641198778126
bwe changes from to:  [tensor([[[0.0121]]]), tensor([[[0.0070]]])]
step into gymStat at 1641198778126
send bwe to appRecv at 1641198778126
sent bwe to appRecv at 1641198778126
wait for recv string at 1641198778126
recved string at 1641198778326
1
wait for recv [self.estimator, stat] at 1641198778326
recved [self.estimator, stat] at 1641198778327
sorted packlist at 1641198778327
packetSeq:  5890
packetSeq:  5891
packetSeq:  5892
packetSeq:  5893
packetSeq:  5894
packetSeq:  5895
packetSeq:  5896
packetSeq:  5897
packetSeq:  5898
packetSeq:  5899
processed packlist at 1641198778327
receiving_rate:  286480.0
delay:  197.7
loss_ratio:  0.0
processed state0-2 at 1641198778327
avgFrameBetween:  6
psnrStat:  [[558680, 557528, 557722, 557837, 556876, 555950]]
delayStat:  [[122, 122, 122, 122, 122, 123]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557432.1666666666] [122.16666666666667] [0]
processed state3-5 at 1641198778327
liner_to_log:  tensor([[[0.5784]]]) tensor([[[0.1554]]])
linear_to_log at 1641198778327
listState:  [0.07162, 0.1318, 0.0, 0.5574321666666666, 0.12216666666666667, 0.0, tensor([[[0.1554]]])]
state_clone_detach at 1641198778327
reward: 0.06274101891340328
state tensor([0.0716, 0.1318, 0.0000, 0.1554], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198778328
state222:  tensor([[0.0716, 0.1318, 0.0000, 0.1554]], device='cuda:0')
policy_old.forwarded at 1641198778330
give action 269============================
log_to_linear:  tensor([[[0.8018]]], device='cuda:0') tensor([[[1.5570]]], device='cuda:0')
log_to_linear action at 1641198778331
bwe changes from to:  [tensor([[[0.0070]]]), tensor([[[0.0109]]])]
step into gymStat at 1641198778331
send bwe to appRecv at 1641198778331
sent bwe to appRecv at 1641198778331
wait for recv string at 1641198778331
recved string at 1641198778560
1
wait for recv [self.estimator, stat] at 1641198778560
recved [self.estimator, stat] at 1641198778560
sorted packlist at 1641198778560
packetSeq:  5900
packetSeq:  5901
packetSeq:  5902
packetSeq:  5903
packetSeq:  5904
packetSeq:  5905
packetSeq:  5906
packetSeq:  5907
packetSeq:  5908
packetSeq:  5909
processed packlist at 1641198778560
receiving_rate:  230600.0
delay:  198.5
loss_ratio:  0.0
processed state0-2 at 1641198778560
avgFrameBetween:  6
psnrStat:  [[555621, 555943, 554303, 553838, 552508, 552786, 551460]]
delayStat:  [[122, 122, 122, 122, 122, 122, 122]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553779.8571428572] [122.0] [0]
processed state3-5 at 1641198778560
liner_to_log:  tensor([[[1.5570]]]) tensor([[[0.8018]]])
linear_to_log at 1641198778561
listState:  [0.05765, 0.13233333333333333, 0.0, 0.5537798571428572, 0.122, 0.0, tensor([[[0.8018]]])]
state_clone_detach at 1641198778561
reward: -0.004855356735926741
state tensor([0.0576, 0.1323, 0.0000, 0.8018], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198778561
state222:  tensor([[0.0576, 0.1323, 0.0000, 0.8018]], device='cuda:0')
policy_old.forwarded at 1641198778563
give action 270============================
log_to_linear:  tensor([[[0.2093]]], device='cuda:0') tensor([[[0.6258]]], device='cuda:0')
log_to_linear action at 1641198778564
bwe changes from to:  [tensor([[[0.0109]]]), tensor([[[0.0068]]])]
step into gymStat at 1641198778564
send bwe to appRecv at 1641198778564
sent bwe to appRecv at 1641198778564
wait for recv string at 1641198778564
recved string at 1641198778797
1
wait for recv [self.estimator, stat] at 1641198778797
recved [self.estimator, stat] at 1641198778797
sorted packlist at 1641198778797
packetSeq:  5910
packetSeq:  5911
packetSeq:  5912
packetSeq:  5913
packetSeq:  5914
packetSeq:  5915
packetSeq:  5916
packetSeq:  5917
packetSeq:  5918
packetSeq:  5919
packetSeq:  5920
packetSeq:  5921
processed packlist at 1641198778797
receiving_rate:  345240.0
delay:  201.1
loss_ratio:  0.0
processed state0-2 at 1641198778798
avgFrameBetween:  6
psnrStat:  [[551550, 552358, 552271, 554889, 553552, 554834, 554237, 550413]]
delayStat:  [[122, 123, 123, 122, 122, 102, 102, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553013.0] [114.75] [0]
processed state3-5 at 1641198778798
liner_to_log:  tensor([[[0.6258]]]) tensor([[[0.2093]]])
linear_to_log at 1641198778798
listState:  [0.08631, 0.13406666666666667, 0.0, 0.553013, 0.11475, 0.0, tensor([[[0.2093]]])]
state_clone_detach at 1641198778798
reward: 0.11830226061976179
state tensor([0.0863, 0.1341, 0.0000, 0.2093], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198778799
state222:  tensor([[0.0863, 0.1341, 0.0000, 0.2093]], device='cuda:0')
policy_old.forwarded at 1641198778801
give action 271============================
log_to_linear:  tensor([[[0.4930]]], device='cuda:0') tensor([[[0.9889]]], device='cuda:0')
log_to_linear action at 1641198778801
bwe changes from to:  [tensor([[[0.0068]]]), tensor([[[0.0067]]])]
step into gymStat at 1641198778802
send bwe to appRecv at 1641198778802
sent bwe to appRecv at 1641198778802
wait for recv string at 1641198778802
recved string at 1641198779004
1
wait for recv [self.estimator, stat] at 1641198779004
recved [self.estimator, stat] at 1641198779004
sorted packlist at 1641198779004
packetSeq:  5922
packetSeq:  5923
packetSeq:  5924
packetSeq:  5925
packetSeq:  5926
packetSeq:  5927
packetSeq:  5928
packetSeq:  5929
packetSeq:  5930
processed packlist at 1641198779004
receiving_rate:  262520.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198779005
avgFrameBetween:  6
psnrStat:  [[550428, 548950, 549013, 549661, 549811, 551674]]
delayStat:  [[102, 103, 102, 102, 102, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549922.8333333334] [102.16666666666667] [0]
processed state3-5 at 1641198779005
liner_to_log:  tensor([[[0.9889]]]) tensor([[[0.4930]]])
linear_to_log at 1641198779005
listState:  [0.06563, 0.132, 0.0, 0.5499228333333334, 0.10216666666666667, 0.0, tensor([[[0.4930]]])]
state_clone_detach at 1641198779005
reward: 0.03472729006747732
state tensor([0.0656, 0.1320, 0.0000, 0.4930], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198779006
state222:  tensor([[0.0656, 0.1320, 0.0000, 0.4930]], device='cuda:0')
policy_old.forwarded at 1641198779008
give action 272============================
log_to_linear:  tensor([[[0.5292]]], device='cuda:0') tensor([[[1.0471]]], device='cuda:0')
log_to_linear action at 1641198779009
bwe changes from to:  [tensor([[[0.0067]]]), tensor([[[0.0070]]])]
step into gymStat at 1641198779009
send bwe to appRecv at 1641198779009
sent bwe to appRecv at 1641198779009
wait for recv string at 1641198779009
recved string at 1641198779223
1
wait for recv [self.estimator, stat] at 1641198779223
recved [self.estimator, stat] at 1641198779223
sorted packlist at 1641198779223
packetSeq:  5931
packetSeq:  5932
packetSeq:  5933
packetSeq:  5934
packetSeq:  5935
packetSeq:  5936
packetSeq:  5937
processed packlist at 1641198779223
receiving_rate:  268960.0
delay:  198.71428571428572
loss_ratio:  0.0
processed state0-2 at 1641198779223
avgFrameBetween:  6
psnrStat:  [[550812, 550407, 549604, 550513, 549218, 549486, 549272]]
delayStat:  [[102, 102, 103, 102, 102, 102, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549901.7142857143] [102.14285714285714] [0]
processed state3-5 at 1641198779223
liner_to_log:  tensor([[[1.0471]]]) tensor([[[0.5292]]])
linear_to_log at 1641198779224
listState:  [0.06724, 0.13247619047619047, 0.0, 0.5499017142857143, 0.10214285714285713, 0.0, tensor([[[0.5292]]])]
state_clone_detach at 1641198779224
reward: 0.04079096319696057
state tensor([0.0672, 0.1325, 0.0000, 0.5292], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198779224
state222:  tensor([[0.0672, 0.1325, 0.0000, 0.5292]], device='cuda:0')
policy_old.forwarded at 1641198779226
give action 273============================
log_to_linear:  tensor([[[0.8058]]], device='cuda:0') tensor([[[1.5652]]], device='cuda:0')
log_to_linear action at 1641198779227
bwe changes from to:  [tensor([[[0.0070]]]), tensor([[[0.0110]]])]
step into gymStat at 1641198779227
send bwe to appRecv at 1641198779227
sent bwe to appRecv at 1641198779227
wait for recv string at 1641198779227
recved string at 1641198779429
1
wait for recv [self.estimator, stat] at 1641198779429
recved [self.estimator, stat] at 1641198779429
sorted packlist at 1641198779429
packetSeq:  5938
packetSeq:  5939
packetSeq:  5940
packetSeq:  5941
packetSeq:  5942
packetSeq:  5943
packetSeq:  5944
packetSeq:  5945
processed packlist at 1641198779429
receiving_rate:  269720.0
delay:  198.25
loss_ratio:  0.0
processed state0-2 at 1641198779429
avgFrameBetween:  6
psnrStat:  [[549856, 548932, 551164, 549827, 549931, 549650]]
delayStat:  [[102, 103, 102, 103, 102, 103]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549893.3333333334] [102.5] [0]
processed state3-5 at 1641198779429
liner_to_log:  tensor([[[1.5652]]]) tensor([[[0.8058]]])
linear_to_log at 1641198779429
listState:  [0.06743, 0.13216666666666665, 0.0, 0.5498933333333333, 0.1025, 0.0, tensor([[[0.8058]]])]
state_clone_detach at 1641198779430
reward: 0.04259757229137345
state tensor([0.0674, 0.1322, 0.0000, 0.8058], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198779430
state222:  tensor([[0.0674, 0.1322, 0.0000, 0.8058]], device='cuda:0')
policy_old.forwarded at 1641198779432
give action 274============================
log_to_linear:  tensor([[[0.1862]]], device='cuda:0') tensor([[[0.6044]]], device='cuda:0')
log_to_linear action at 1641198779433
bwe changes from to:  [tensor([[[0.0110]]]), tensor([[[0.0067]]])]
step into gymStat at 1641198779433
send bwe to appRecv at 1641198779433
sent bwe to appRecv at 1641198779433
wait for recv string at 1641198779433
recved string at 1641198779663
1
wait for recv [self.estimator, stat] at 1641198779663
recved [self.estimator, stat] at 1641198779663
sorted packlist at 1641198779663
packetSeq:  5946
packetSeq:  5947
packetSeq:  5948
packetSeq:  5949
packetSeq:  5950
packetSeq:  5951
packetSeq:  5952
packetSeq:  5953
packetSeq:  5954
processed packlist at 1641198779663
receiving_rate:  277040.0
delay:  198.25
loss_ratio:  0.0
processed state0-2 at 1641198779663
avgFrameBetween:  6
psnrStat:  [[549282, 548788, 549747, 550346, 551960, 552500, 553522]]
delayStat:  [[103, 103, 102, 103, 102, 103, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550877.8571428572] [102.57142857142857] [0]
processed state3-5 at 1641198779664
liner_to_log:  tensor([[[0.6044]]]) tensor([[[0.1862]]])
linear_to_log at 1641198779664
listState:  [0.06926, 0.13216666666666665, 0.0, 0.5508778571428572, 0.10257142857142856, 0.0, tensor([[[0.1862]]])]
state_clone_detach at 1641198779664
reward: 0.05098931747621738
state tensor([0.0693, 0.1322, 0.0000, 0.1862], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198779665
state222:  tensor([[0.0693, 0.1322, 0.0000, 0.1862]], device='cuda:0')
policy_old.forwarded at 1641198779666
give action 275============================
log_to_linear:  tensor([[[0.3698]]], device='cuda:0') tensor([[[0.8099]]], device='cuda:0')
log_to_linear action at 1641198779667
bwe changes from to:  [tensor([[[0.0067]]]), tensor([[[0.0054]]])]
step into gymStat at 1641198779668
send bwe to appRecv at 1641198779668
sent bwe to appRecv at 1641198779668
wait for recv string at 1641198779668
recved string at 1641198779894
1
wait for recv [self.estimator, stat] at 1641198779894
recved [self.estimator, stat] at 1641198779894
sorted packlist at 1641198779894
packetSeq:  5955
packetSeq:  5956
packetSeq:  5957
packetSeq:  5958
packetSeq:  5959
packetSeq:  5960
packetSeq:  5961
processed packlist at 1641198779894
receiving_rate:  230120.0
delay:  198.83333333333334
loss_ratio:  0.0
processed state0-2 at 1641198779894
avgFrameBetween:  6
psnrStat:  [[553014, 552989, 553982, 555172, 554570, 555905, 557333]]
delayStat:  [[104, 102, 102, 102, 102, 102, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554709.2857142857] [102.28571428571429] [0]
processed state3-5 at 1641198779894
liner_to_log:  tensor([[[0.8099]]]) tensor([[[0.3698]]])
linear_to_log at 1641198779895
listState:  [0.05753, 0.13255555555555557, 0.0, 0.5547092857142857, 0.10228571428571429, 0.0, tensor([[[0.3698]]])]
state_clone_detach at 1641198779895
reward: -0.0061215737897282185
state tensor([0.0575, 0.1326, 0.0000, 0.3698], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198779895
state222:  tensor([[0.0575, 0.1326, 0.0000, 0.3698]], device='cuda:0')
policy_old.forwarded at 1641198779897
give action 276============================
log_to_linear:  tensor([[[0.5269]]], device='cuda:0') tensor([[[1.0433]]], device='cuda:0')
log_to_linear action at 1641198779898
bwe changes from to:  [tensor([[[0.0054]]]), tensor([[[0.0056]]])]
step into gymStat at 1641198779898
send bwe to appRecv at 1641198779898
sent bwe to appRecv at 1641198779898
wait for recv string at 1641198779898
recved string at 1641198780125
1
wait for recv [self.estimator, stat] at 1641198780125
recved [self.estimator, stat] at 1641198780126
sorted packlist at 1641198780126
packetSeq:  5962
packetSeq:  5963
packetSeq:  5964
packetSeq:  5965
packetSeq:  5966
packetSeq:  5967
packetSeq:  5968
packetSeq:  5969
packetSeq:  5970
packetSeq:  5971
packetSeq:  5972
processed packlist at 1641198780126
receiving_rate:  285200.0
delay:  197.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198780126
avgFrameBetween:  6
psnrStat:  [[557994, 557211, 557549, 557284, 557381, 555288, 554597]]
delayStat:  [[102, 102, 102, 102, 102, 102, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556757.7142857143] [102.0] [0]
processed state3-5 at 1641198780126
liner_to_log:  tensor([[[1.0433]]]) tensor([[[0.5269]]])
linear_to_log at 1641198780126
listState:  [0.0713, 0.13185185185185186, 0.0, 0.5567577142857143, 0.102, 0.0, tensor([[[0.5269]]])]
state_clone_detach at 1641198780126
reward: 0.06115219901550778
state tensor([0.0713, 0.1319, 0.0000, 0.5269], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198780127
state222:  tensor([[0.0713, 0.1319, 0.0000, 0.5269]], device='cuda:0')
policy_old.forwarded at 1641198780129
give action 277============================
log_to_linear:  tensor([[[0.2972]]], device='cuda:0') tensor([[[0.7193]]], device='cuda:0')
log_to_linear action at 1641198780129
bwe changes from to:  [tensor([[[0.0056]]]), tensor([[[0.0041]]])]
step into gymStat at 1641198780130
send bwe to appRecv at 1641198780130
sent bwe to appRecv at 1641198780130
wait for recv string at 1641198780130
recved string at 1641198780332
1
wait for recv [self.estimator, stat] at 1641198780332
recved [self.estimator, stat] at 1641198780332
sorted packlist at 1641198780332
packetSeq:  5973
packetSeq:  5974
packetSeq:  5975
packetSeq:  5976
packetSeq:  5977
packetSeq:  5978
packetSeq:  5979
packetSeq:  5980
packetSeq:  5981
packetSeq:  5982
packetSeq:  5983
packetSeq:  5984
processed packlist at 1641198780332
receiving_rate:  313840.0
delay:  197.25
loss_ratio:  0.0
processed state0-2 at 1641198780332
avgFrameBetween:  6
psnrStat:  [[554906, 554663, 554174, 552237, 551788, 553029]]
delayStat:  [[103, 102, 102, 102, 102, 102]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553466.1666666666] [102.16666666666667] [0]
processed state3-5 at 1641198780332
liner_to_log:  tensor([[[0.7193]]]) tensor([[[0.2972]]])
linear_to_log at 1641198780333
listState:  [0.07846, 0.1315, 0.0, 0.5534661666666666, 0.10216666666666667, 0.0, tensor([[[0.2972]]])]
state_clone_detach at 1641198780333
reward: 0.09348576326730812
state tensor([0.0785, 0.1315, 0.0000, 0.2972], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198780334
state222:  tensor([[0.0785, 0.1315, 0.0000, 0.2972]], device='cuda:0')
policy_old.forwarded at 1641198780335
give action 278============================
log_to_linear:  tensor([[[0.8679]]], device='cuda:0') tensor([[[1.6983]]], device='cuda:0')
log_to_linear action at 1641198780336
bwe changes from to:  [tensor([[[0.0041]]]), tensor([[[0.0069]]])]
step into gymStat at 1641198780336
send bwe to appRecv at 1641198780336
sent bwe to appRecv at 1641198780336
wait for recv string at 1641198780336
recved string at 1641198780561
1
wait for recv [self.estimator, stat] at 1641198780561
recved [self.estimator, stat] at 1641198780561
sorted packlist at 1641198780561
packetSeq:  5985
packetSeq:  5986
packetSeq:  5987
packetSeq:  5988
packetSeq:  5989
packetSeq:  5990
packetSeq:  5991
packetSeq:  5992
processed packlist at 1641198780561
receiving_rate:  294480.0
delay:  198.5
loss_ratio:  0.0
processed state0-2 at 1641198780561
avgFrameBetween:  6
psnrStat:  [[552530, 551395, 550279, 551550, 551021, 550473, 550423]]
delayStat:  [[102, 102, 103, 102, 102, 102, 103]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551095.8571428572] [102.28571428571429] [0]
processed state3-5 at 1641198780561
liner_to_log:  tensor([[[1.6983]]]) tensor([[[0.8679]]])
linear_to_log at 1641198780562
listState:  [0.07362, 0.13233333333333333, 0.0, 0.5510958571428571, 0.10228571428571429, 0.0, tensor([[[0.8679]]])]
state_clone_detach at 1641198780562
reward: 0.07002217573420955
state tensor([0.0736, 0.1323, 0.0000, 0.8679], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198780563
state222:  tensor([[0.0736, 0.1323, 0.0000, 0.8679]], device='cuda:0')
policy_old.forwarded at 1641198780564
give action 279============================
log_to_linear:  tensor([[[0.5969]]], device='cuda:0') tensor([[[1.1621]]], device='cuda:0')
log_to_linear action at 1641198780565
bwe changes from to:  [tensor([[[0.0069]]]), tensor([[[0.0080]]])]
step into gymStat at 1641198780565
send bwe to appRecv at 1641198780565
sent bwe to appRecv at 1641198780565
wait for recv string at 1641198780565
recved string at 1641198780779
1
wait for recv [self.estimator, stat] at 1641198780779
recved [self.estimator, stat] at 1641198780779
sorted packlist at 1641198780779
packetSeq:  5993
packetSeq:  5994
packetSeq:  5995
packetSeq:  5996
packetSeq:  5997
packetSeq:  5998
packetSeq:  5999
packetSeq:  6000
processed packlist at 1641198780779
receiving_rate:  246320.0
delay:  197.25
loss_ratio:  0.0
processed state0-2 at 1641198780780
avgFrameBetween:  6
psnrStat:  [[550214, 550540, 551056, 551350, 550621, 549944]]
delayStat:  [[102, 102, 102, 102, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550620.8333333334] [96.33333333333333] [0]
processed state3-5 at 1641198780780
liner_to_log:  tensor([[[1.1621]]]) tensor([[[0.5969]]])
linear_to_log at 1641198780780
listState:  [0.06158, 0.1315, 0.0, 0.5506208333333333, 0.09633333333333333, 0.0, tensor([[[0.5969]]])]
state_clone_detach at 1641198780780
reward: 0.016955977275556477
state tensor([0.0616, 0.1315, 0.0000, 0.5969], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198780781
state222:  tensor([[0.0616, 0.1315, 0.0000, 0.5969]], device='cuda:0')
policy_old.forwarded at 1641198780783
give action 280============================
log_to_linear:  tensor([[[0.7297]]], device='cuda:0') tensor([[[1.4104]]], device='cuda:0')
log_to_linear action at 1641198780784
bwe changes from to:  [tensor([[[0.0080]]]), tensor([[[0.0113]]])]
step into gymStat at 1641198780784
send bwe to appRecv at 1641198780784
sent bwe to appRecv at 1641198780784
wait for recv string at 1641198780784
recved string at 1641198780992
1
wait for recv [self.estimator, stat] at 1641198780992
recved [self.estimator, stat] at 1641198780993
sorted packlist at 1641198780993
packetSeq:  6001
packetSeq:  6002
packetSeq:  6003
packetSeq:  6004
packetSeq:  6005
packetSeq:  6006
packetSeq:  6007
packetSeq:  6008
packetSeq:  6009
packetSeq:  6010
processed packlist at 1641198780993
receiving_rate:  262720.0
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198780993
avgFrameBetween:  6
psnrStat:  [[549612, 549255, 549790, 549372, 550209, 548792, 550344]]
delayStat:  [[86, 85, 85, 86, 85, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549624.8571428572] [85.28571428571429] [0]
processed state3-5 at 1641198780993
liner_to_log:  tensor([[[1.4104]]]) tensor([[[0.7297]]])
linear_to_log at 1641198780993
listState:  [0.06568, 0.13133333333333333, 0.0, 0.5496248571428571, 0.0852857142857143, 0.0, tensor([[[0.7297]]])]
state_clone_detach at 1641198780993
reward: 0.036961381600235044
state tensor([0.0657, 0.1313, 0.0000, 0.7297], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198780994
state222:  tensor([[0.0657, 0.1313, 0.0000, 0.7297]], device='cuda:0')
policy_old.forwarded at 1641198780996
give action 281============================
log_to_linear:  tensor([[[0.4422]]], device='cuda:0') tensor([[[0.9115]]], device='cuda:0')
log_to_linear action at 1641198780997
bwe changes from to:  [tensor([[[0.0113]]]), tensor([[[0.0103]]])]
step into gymStat at 1641198780997
send bwe to appRecv at 1641198780997
sent bwe to appRecv at 1641198780997
wait for recv string at 1641198780997
recved string at 1641198781230
1
wait for recv [self.estimator, stat] at 1641198781230
recved [self.estimator, stat] at 1641198781230
sorted packlist at 1641198781230
packetSeq:  6011
packetSeq:  6012
packetSeq:  6013
packetSeq:  6014
packetSeq:  6015
packetSeq:  6016
packetSeq:  6017
packetSeq:  6018
packetSeq:  6019
processed packlist at 1641198781230
receiving_rate:  215680.0
delay:  196.42857142857142
loss_ratio:  0.0
processed state0-2 at 1641198781230
avgFrameBetween:  6
psnrStat:  [[551119, 551202, 551059, 551047, 552920, 553037, 554184]]
delayStat:  [[85, 85, 85, 85, 86, 86, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552081.1428571428] [85.28571428571429] [0]
processed state3-5 at 1641198781230
liner_to_log:  tensor([[[0.9115]]]) tensor([[[0.4422]]])
linear_to_log at 1641198781231
listState:  [0.05392, 0.13095238095238093, 0.0, 0.5520811428571428, 0.0852857142857143, 0.0, tensor([[[0.4422]]])]
state_clone_detach at 1641198781231
reward: -0.019634471796402653
state tensor([0.0539, 0.1310, 0.0000, 0.4422], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198781232
state222:  tensor([[0.0539, 0.1310, 0.0000, 0.4422]], device='cuda:0')
policy_old.forwarded at 1641198781233
give action 282============================
log_to_linear:  tensor([[[0.2866]]], device='cuda:0') tensor([[[0.7069]]], device='cuda:0')
log_to_linear action at 1641198781234
bwe changes from to:  [tensor([[[0.0103]]]), tensor([[[0.0073]]])]
step into gymStat at 1641198781234
send bwe to appRecv at 1641198781234
sent bwe to appRecv at 1641198781234
wait for recv string at 1641198781234
recved string at 1641198781440
1
wait for recv [self.estimator, stat] at 1641198781440
recved [self.estimator, stat] at 1641198781440
sorted packlist at 1641198781440
packetSeq:  6020
packetSeq:  6021
packetSeq:  6022
packetSeq:  6023
packetSeq:  6024
packetSeq:  6025
packetSeq:  6026
packetSeq:  6027
packetSeq:  6028
packetSeq:  6029
packetSeq:  6030
processed packlist at 1641198781440
receiving_rate:  310720.0
delay:  197.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198781440
avgFrameBetween:  6
psnrStat:  [[554024, 554545, 555356, 555893, 556222, 556250]]
delayStat:  [[85, 84, 85, 85, 85, 86]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555381.6666666666] [85.0] [0]
processed state3-5 at 1641198781440
liner_to_log:  tensor([[[0.7069]]]) tensor([[[0.2866]]])
linear_to_log at 1641198781440
listState:  [0.07768, 0.1316969696969697, 0.0, 0.5553816666666667, 0.085, 0.0, tensor([[[0.2866]]])]
state_clone_detach at 1641198781441
reward: 0.08956554350698465
state tensor([0.0777, 0.1317, 0.0000, 0.2866], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198781441
state222:  tensor([[0.0777, 0.1317, 0.0000, 0.2866]], device='cuda:0')
policy_old.forwarded at 1641198781443
give action 283============================
log_to_linear:  tensor([[[0.0583]]], device='cuda:0') tensor([[[0.5166]]], device='cuda:0')
log_to_linear action at 1641198781444
bwe changes from to:  [tensor([[[0.0073]]]), tensor([[[0.0038]]])]
step into gymStat at 1641198781444
send bwe to appRecv at 1641198781444
sent bwe to appRecv at 1641198781444
wait for recv string at 1641198781444
recved string at 1641198781643
1
wait for recv [self.estimator, stat] at 1641198781643
recved [self.estimator, stat] at 1641198781643
sorted packlist at 1641198781643
packetSeq:  6031
packetSeq:  6032
packetSeq:  6033
packetSeq:  6034
packetSeq:  6035
packetSeq:  6036
packetSeq:  6037
packetSeq:  6038
packetSeq:  6039
packetSeq:  6040
packetSeq:  6041
packetSeq:  6042
packetSeq:  6043
processed packlist at 1641198781643
receiving_rate:  315440.0
delay:  196.84615384615384
loss_ratio:  0.0
processed state0-2 at 1641198781643
avgFrameBetween:  6
psnrStat:  [[555485, 555044, 555002, 554445, 553563, 552543]]
delayStat:  [[85, 85, 85, 85, 86, 84]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554347.0] [85.0] [0]
processed state3-5 at 1641198781643
liner_to_log:  tensor([[[0.5166]]]) tensor([[[0.0583]]])
linear_to_log at 1641198781644
listState:  [0.07886, 0.13123076923076923, 0.0, 0.554347, 0.085, 0.0, tensor([[[0.0583]]])]
state_clone_detach at 1641198781644
reward: 0.09599365506914331
state tensor([0.0789, 0.1312, 0.0000, 0.0583], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198781644
state222:  tensor([[0.0789, 0.1312, 0.0000, 0.0583]], device='cuda:0')
policy_old.forwarded at 1641198781646
give action 284============================
log_to_linear:  tensor([[[0.4032]]], device='cuda:0') tensor([[[0.8555]]], device='cuda:0')
log_to_linear action at 1641198781647
bwe changes from to:  [tensor([[[0.0038]]]), tensor([[[0.0032]]])]
step into gymStat at 1641198781647
send bwe to appRecv at 1641198781647
sent bwe to appRecv at 1641198781647
wait for recv string at 1641198781647
recved string at 1641198781843
1
wait for recv [self.estimator, stat] at 1641198781843
recved [self.estimator, stat] at 1641198781844
sorted packlist at 1641198781844
packetSeq:  6044
packetSeq:  6045
packetSeq:  6046
packetSeq:  6047
packetSeq:  6048
packetSeq:  6049
packetSeq:  6050
packetSeq:  6051
packetSeq:  6052
processed packlist at 1641198781844
receiving_rate:  272280.0
delay:  197.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198781844
avgFrameBetween:  6
psnrStat:  [[552703, 551890, 551505, 551532, 550712, 548969]]
delayStat:  [[85, 85, 86, 85, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551218.5] [85.16666666666667] [0]
processed state3-5 at 1641198781844
liner_to_log:  tensor([[[0.8555]]]) tensor([[[0.4032]]])
linear_to_log at 1641198781844
listState:  [0.06807, 0.13192592592592592, 0.0, 0.5512185, 0.08516666666666667, 0.0, tensor([[[0.4032]]])]
state_clone_detach at 1641198781844
reward: 0.04626797702877711
state tensor([0.0681, 0.1319, 0.0000, 0.4032], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198781845
state222:  tensor([[0.0681, 0.1319, 0.0000, 0.4032]], device='cuda:0')
policy_old.forwarded at 1641198781847
give action 285============================
log_to_linear:  tensor([[[0.4309]]], device='cuda:0') tensor([[[0.8949]]], device='cuda:0')
log_to_linear action at 1641198781847
bwe changes from to:  [tensor([[[0.0032]]]), tensor([[[0.0029]]])]
step into gymStat at 1641198781848
send bwe to appRecv at 1641198781848
sent bwe to appRecv at 1641198781848
wait for recv string at 1641198781848
recved string at 1641198782045
1
wait for recv [self.estimator, stat] at 1641198782045
recved [self.estimator, stat] at 1641198782045
sorted packlist at 1641198782045
packetSeq:  6053
packetSeq:  6054
packetSeq:  6055
packetSeq:  6056
packetSeq:  6057
packetSeq:  6058
packetSeq:  6059
packetSeq:  6060
packetSeq:  6061
processed packlist at 1641198782045
receiving_rate:  267600.0
delay:  197.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198782045
avgFrameBetween:  6
psnrStat:  [[549457, 550021, 549039, 549727, 548652, 549181]]
delayStat:  [[85, 85, 85, 85, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549346.1666666666] [85.0] [0]
processed state3-5 at 1641198782045
liner_to_log:  tensor([[[0.8949]]]) tensor([[[0.4309]]])
linear_to_log at 1641198782045
listState:  [0.0669, 0.13155555555555556, 0.0, 0.5493461666666666, 0.085, 0.0, tensor([[[0.4309]]])]
state_clone_detach at 1641198782046
reward: 0.041978428528000045
state tensor([0.0669, 0.1316, 0.0000, 0.4309], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198782046
state222:  tensor([[0.0669, 0.1316, 0.0000, 0.4309]], device='cuda:0')
policy_old.forwarded at 1641198782048
give action 286============================
log_to_linear:  tensor([[[0.3941]]], device='cuda:0') tensor([[[0.8429]]], device='cuda:0')
log_to_linear action at 1641198782049
bwe changes from to:  [tensor([[[0.0029]]]), tensor([[[0.0024]]])]
step into gymStat at 1641198782049
send bwe to appRecv at 1641198782049
sent bwe to appRecv at 1641198782049
wait for recv string at 1641198782049
recved string at 1641198782248
1
wait for recv [self.estimator, stat] at 1641198782248
recved [self.estimator, stat] at 1641198782248
sorted packlist at 1641198782248
packetSeq:  6062
packetSeq:  6063
packetSeq:  6064
packetSeq:  6065
packetSeq:  6066
packetSeq:  6067
packetSeq:  6068
packetSeq:  6069
processed packlist at 1641198782248
receiving_rate:  245880.0
delay:  197.375
loss_ratio:  0.0
processed state0-2 at 1641198782249
avgFrameBetween:  6
psnrStat:  [[548606, 547264, 547231, 547888, 548292, 548718, 549523]]
delayStat:  [[85, 85, 85, 85, 85, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548217.4285714285] [85.0] [0]
processed state3-5 at 1641198782249
liner_to_log:  tensor([[[0.8429]]]) tensor([[[0.3941]]])
linear_to_log at 1641198782249
listState:  [0.06147, 0.13158333333333333, 0.0, 0.5482174285714285, 0.085, 0.0, tensor([[[0.3941]]])]
state_clone_detach at 1641198782249
reward: 0.01617382646461979
state tensor([0.0615, 0.1316, 0.0000, 0.3941], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198782250
state222:  tensor([[0.0615, 0.1316, 0.0000, 0.3941]], device='cuda:0')
policy_old.forwarded at 1641198782251
give action 287============================
log_to_linear:  tensor([[[0.2898]]], device='cuda:0') tensor([[[0.7107]]], device='cuda:0')
log_to_linear action at 1641198782252
bwe changes from to:  [tensor([[[0.0024]]]), tensor([[[0.0017]]])]
step into gymStat at 1641198782252
send bwe to appRecv at 1641198782252
sent bwe to appRecv at 1641198782253
wait for recv string at 1641198782253
recved string at 1641198782479
1
wait for recv [self.estimator, stat] at 1641198782479
recved [self.estimator, stat] at 1641198782479
sorted packlist at 1641198782479
packetSeq:  6070
packetSeq:  6071
packetSeq:  6072
packetSeq:  6073
packetSeq:  6074
packetSeq:  6075
packetSeq:  6076
packetSeq:  6077
packetSeq:  6078
packetSeq:  6079
packetSeq:  6080
processed packlist at 1641198782479
receiving_rate:  274160.0
delay:  197.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198782479
avgFrameBetween:  6
psnrStat:  [[550207, 551086, 551300, 551771, 552945, 552734]]
delayStat:  [[87, 85, 86, 85, 86, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551673.8333333334] [85.66666666666667] [0]
processed state3-5 at 1641198782479
liner_to_log:  tensor([[[0.7107]]]) tensor([[[0.2898]]])
linear_to_log at 1641198782480
listState:  [0.06854, 0.13185185185185186, 0.0, 0.5516738333333334, 0.08566666666666667, 0.0, tensor([[[0.2898]]])]
state_clone_detach at 1641198782480
reward: 0.04864608246188651
state tensor([0.0685, 0.1319, 0.0000, 0.2898], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198782481
state222:  tensor([[0.0685, 0.1319, 0.0000, 0.2898]], device='cuda:0')
policy_old.forwarded at 1641198782482
give action 288============================
log_to_linear:  tensor([[[0.5765]]], device='cuda:0') tensor([[[1.1266]]], device='cuda:0')
log_to_linear action at 1641198782483
bwe changes from to:  [tensor([[[0.0017]]]), tensor([[[0.0019]]])]
step into gymStat at 1641198782483
send bwe to appRecv at 1641198782483
sent bwe to appRecv at 1641198782483
wait for recv string at 1641198782483
recved string at 1641198782684
1
wait for recv [self.estimator, stat] at 1641198782684
recved [self.estimator, stat] at 1641198782684
sorted packlist at 1641198782684
packetSeq:  6081
packetSeq:  6082
packetSeq:  6083
packetSeq:  6084
packetSeq:  6085
packetSeq:  6086
packetSeq:  6087
packetSeq:  6088
packetSeq:  6089
packetSeq:  6090
processed packlist at 1641198782684
receiving_rate:  272280.0
delay:  196.4
loss_ratio:  0.0
processed state0-2 at 1641198782684
avgFrameBetween:  6
psnrStat:  [[552957, 552344, 552112, 552584, 552100, 552409]]
delayStat:  [[85, 85, 86, 85, 85, 86]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552417.6666666666] [85.33333333333333] [0]
processed state3-5 at 1641198782684
liner_to_log:  tensor([[[1.1266]]]) tensor([[[0.5765]]])
linear_to_log at 1641198782685
listState:  [0.06807, 0.13093333333333335, 0.0, 0.5524176666666666, 0.08533333333333333, 0.0, tensor([[[0.5765]]])]
state_clone_detach at 1641198782685
reward: 0.04924575480655485
state tensor([0.0681, 0.1309, 0.0000, 0.5765], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198782685
state222:  tensor([[0.0681, 0.1309, 0.0000, 0.5765]], device='cuda:0')
policy_old.forwarded at 1641198782687
give action 289============================
log_to_linear:  tensor([[[0.4087]]], device='cuda:0') tensor([[[0.8632]]], device='cuda:0')
log_to_linear action at 1641198782688
bwe changes from to:  [tensor([[[0.0019]]]), tensor([[[0.0017]]])]
step into gymStat at 1641198782689
send bwe to appRecv at 1641198782689
sent bwe to appRecv at 1641198782689
wait for recv string at 1641198782689
recved string at 1641198782902
1
wait for recv [self.estimator, stat] at 1641198782902
recved [self.estimator, stat] at 1641198782902
sorted packlist at 1641198782902
packetSeq:  6091
packetSeq:  6092
packetSeq:  6093
packetSeq:  6094
packetSeq:  6095
packetSeq:  6096
packetSeq:  6097
processed packlist at 1641198782902
receiving_rate:  255560.0
delay:  195.14285714285714
loss_ratio:  0.0
processed state0-2 at 1641198782902
avgFrameBetween:  6
psnrStat:  [[553345, 553356, 552913, 553347, 552536, 552377, 553051]]
delayStat:  [[85, 85, 85, 85, 85, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552989.2857142857] [85.0] [0]
processed state3-5 at 1641198782902
liner_to_log:  tensor([[[0.8632]]]) tensor([[[0.4087]]])
linear_to_log at 1641198782902
listState:  [0.06389, 0.1300952380952381, 0.0, 0.5529892857142856, 0.085, 0.0, tensor([[[0.4087]]])]
state_clone_detach at 1641198782903
reward: 0.03223777046429316
state tensor([0.0639, 0.1301, 0.0000, 0.4087], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198782903
state222:  tensor([[0.0639, 0.1301, 0.0000, 0.4087]], device='cuda:0')
policy_old.forwarded at 1641198782905
give action 290============================
log_to_linear:  tensor([[[0.5567]]], device='cuda:0') tensor([[[1.0929]]], device='cuda:0')
log_to_linear action at 1641198782906
bwe changes from to:  [tensor([[[0.0017]]]), tensor([[[0.0018]]])]
step into gymStat at 1641198782906
send bwe to appRecv at 1641198782906
sent bwe to appRecv at 1641198782906
wait for recv string at 1641198782906
recved string at 1641198783143
1
wait for recv [self.estimator, stat] at 1641198783143
recved [self.estimator, stat] at 1641198783143
sorted packlist at 1641198783143
packetSeq:  6098
packetSeq:  6099
packetSeq:  6100
packetSeq:  6101
packetSeq:  6102
packetSeq:  6103
packetSeq:  6104
packetSeq:  6105
packetSeq:  6106
packetSeq:  6107
processed packlist at 1641198783143
receiving_rate:  267200.0
delay:  192.875
loss_ratio:  0.0
processed state0-2 at 1641198783143
avgFrameBetween:  6
psnrStat:  [[553457, 553456, 554675, 554067, 555526, 556112, 556184]]
delayStat:  [[85, 85, 86, 85, 85, 86, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554782.4285714285] [85.28571428571429] [0]
processed state3-5 at 1641198783143
liner_to_log:  tensor([[[1.0929]]]) tensor([[[0.5567]]])
linear_to_log at 1641198783143
listState:  [0.0668, 0.12858333333333333, 0.0, 0.5547824285714286, 0.0852857142857143, 0.0, tensor([[[0.5567]]])]
state_clone_detach at 1641198783144
reward: 0.05043123742127359
state tensor([0.0668, 0.1286, 0.0000, 0.5567], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198783144
state222:  tensor([[0.0668, 0.1286, 0.0000, 0.5567]], device='cuda:0')
policy_old.forwarded at 1641198783146
give action 291============================
log_to_linear:  tensor([[[0.3375]]], device='cuda:0') tensor([[[0.7682]]], device='cuda:0')
log_to_linear action at 1641198783147
bwe changes from to:  [tensor([[[0.0018]]]), tensor([[[0.0014]]])]
step into gymStat at 1641198783147
send bwe to appRecv at 1641198783147
sent bwe to appRecv at 1641198783147
wait for recv string at 1641198783147
recved string at 1641198783371
1
wait for recv [self.estimator, stat] at 1641198783371
recved [self.estimator, stat] at 1641198783372
sorted packlist at 1641198783372
packetSeq:  6108
processed allFrame at 1641198779004
send 'asking for bwe' at 1641198779004
sent 'asking for bwe' at 1641198779004
send [estimator, stat] at 1641198779004
sent [estimator, stat] at 1641198779004
pc wait for bwe at 1641198779004
pc got bwe at 1641198779009
bandwidth:  300000
pc flushed at 1641198779009
Bwe Sent: 5 at 1641198779009
got request at 1641198779223
processed allFrame at 1641198779223
send 'asking for bwe' at 1641198779223
sent 'asking for bwe' at 1641198779223
send [estimator, stat] at 1641198779223
sent [estimator, stat] at 1641198779223
pc wait for bwe at 1641198779223
pc got bwe at 1641198779227
bandwidth:  300000
pc flushed at 1641198779227
Bwe Sent: 4 at 1641198779227
got request at 1641198779429
processed allFrame at 1641198779429
send 'asking for bwe' at 1641198779429
sent 'asking for bwe' at 1641198779429
send [estimator, stat] at 1641198779429
sent [estimator, stat] at 1641198779429
pc wait for bwe at 1641198779429
pc got bwe at 1641198779433
bandwidth:  300000
pc flushed at 1641198779433
Bwe Sent: 4 at 1641198779433
got request at 1641198779663
processed allFrame at 1641198779663
send 'asking for bwe' at 1641198779663
sent 'asking for bwe' at 1641198779663
send [estimator, stat] at 1641198779663
sent [estimator, stat] at 1641198779663
pc wait for bwe at 1641198779663
pc got bwe at 1641198779668
bandwidth:  300000
pc flushed at 1641198779668
Bwe Sent: 5 at 1641198779668
got request at 1641198779894
processed allFrame at 1641198779894
send 'asking for bwe' at 1641198779894
sent 'asking for bwe' at 1641198779894
send [estimator, stat] at 1641198779894
sent [estimator, stat] at 1641198779894
pc wait for bwe at 1641198779894
pc got bwe at 1641198779898
bandwidth:  300000
pc flushed at 1641198779898
Bwe Sent: 4 at 1641198779898
got request at 1641198780125
processed allFrame at 1641198780125
send 'asking for bwe' at 1641198780125
sent 'asking for bwe' at 1641198780125
send [estimator, stat] at 1641198780125
sent [estimator, stat] at 1641198780125
pc wait for bwe at 1641198780126
pc got bwe at 1641198780130
bandwidth:  300000
pc flushed at 1641198780130
Bwe Sent: 5 at 1641198780130
got request at 1641198780332
processed allFrame at 1641198780332
send 'asking for bwe' at 1641198780332
sent 'asking for bwe' at 1641198780332
send [estimator, stat] at 1641198780332
sent [estimator, stat] at 1641198780332
pc wait for bwe at 1641198780332
pc got bwe at 1641198780336
bandwidth:  300000
pc flushed at 1641198780337
Bwe Sent: 5 at 1641198780337
got request at 1641198780561
processed allFrame at 1641198780561
send 'asking for bwe' at 1641198780561
sent 'asking for bwe' at 1641198780561
send [estimator, stat] at 1641198780561
sent [estimator, stat] at 1641198780561
pc wait for bwe at 1641198780561
pc got bwe at 1641198780565
bandwidth:  300000
pc flushed at 1641198780565
Bwe Sent: 4 at 1641198780565
got request at 1641198780779
processed allFrame at 1641198780779
send 'asking for bwe' at 1641198780779
sent 'asking for bwe' at 1641198780779
send [estimator, stat] at 1641198780779
sent [estimator, stat] at 1641198780779
pc wait for bwe at 1641198780779
pc got bwe at 1641198780784
bandwidth:  300000
pc flushed at 1641198780784
Bwe Sent: 5 at 1641198780784
got request at 1641198780992
processed allFrame at 1641198780992
send 'asking for bwe' at 1641198780992
sent 'asking for bwe' at 1641198780992
send [estimator, stat] at 1641198780992
sent [estimator, stat] at 1641198780992
pc wait for bwe at 1641198780993
pc got bwe at 1641198780997
bandwidth:  300000
pc flushed at 1641198780997
Bwe Sent: 5 at 1641198780997
got request at 1641198781230
processed allFrame at 1641198781230
send 'asking for bwe' at 1641198781230
sent 'asking for bwe' at 1641198781230
send [estimator, stat] at 1641198781230
sent [estimator, stat] at 1641198781230
pc wait for bwe at 1641198781230
pc got bwe at 1641198781234
bandwidth:  300000
pc flushed at 1641198781234
Bwe Sent: 4 at 1641198781234
got request at 1641198781440
processed allFrame at 1641198781440
send 'asking for bwe' at 1641198781440
sent 'asking for bwe' at 1641198781440
send [estimator, stat] at 1641198781440
sent [estimator, stat] at 1641198781440
pc wait for bwe at 1641198781440
pc got bwe at 1641198781444
bandwidth:  300000
pc flushed at 1641198781444
Bwe Sent: 4 at 1641198781444
got request at 1641198781643
processed allFrame at 1641198781643
send 'asking for bwe' at 1641198781643
sent 'asking for bwe' at 1641198781643
send [estimator, stat] at 1641198781643
sent [estimator, stat] at 1641198781643
pc wait for bwe at 1641198781643
pc got bwe at 1641198781647
bandwidth:  300000
pc flushed at 1641198781647
Bwe Sent: 4 at 1641198781647
got request at 1641198781843
processed allFrame at 1641198781843
send 'asking for bwe' at 1641198781843
sent 'asking for bwe' at 1641198781843
send [estimator, stat] at 1641198781843
sent [estimator, stat] at 1641198781844
pc wait for bwe at 1641198781844
pc got bwe at 1641198781848
bandwidth:  300000
pc flushed at 1641198781848
Bwe Sent: 5 at 1641198781848
got request at 1641198782044
processed allFrame at 1641198782045
send 'asking for bwe' at 1641198782045
sent 'asking for bwe' at 1641198782045
send [estimator, stat] at 1641198782045
sent [estimator, stat] at 1641198782045
pc wait for bwe at 1641198782045
pc got bwe at 1641198782049
bandwidth:  300000
pc flushed at 1641198782049
Bwe Sent: 5 at 1641198782049
got request at 1641198782248
processed allFrame at 1641198782248
send 'asking for bwe' at 1641198782248
sent 'asking for bwe' at 1641198782248
send [estimator, stat] at 1641198782248
sent [estimator, stat] at 1641198782248
pc wait for bwe at 1641198782248
pc got bwe at 1641198782253
bandwidth:  300000
pc flushed at 1641198782253
Bwe Sent: 5 at 1641198782253
got request at 1641198782479
processed allFrame at 1641198782479
send 'asking for bwe' at 1641198782479
sent 'asking for bwe' at 1641198782479
send [estimator, stat] at 1641198782479
sent [estimator, stat] at 1641198782479
pc wait for bwe at 1641198782479
pc got bwe at 1641198782484
bandwidth:  300000
pc flushed at 1641198782484
Bwe Sent: 5 at 1641198782484
got request at 1641198782684
processed allFrame at 1641198782684
send 'asking for bwe' at 1641198782684
sent 'asking for bwe' at 1641198782684
send [estimator, stat] at 1641198782684
sent [estimator, stat] at 1641198782684
pc wait for bwe at 1641198782684
pc got bwe at 1641198782689
bandwidth:  300000
pc flushed at 1641198782689
Bwe Sent: 5 at 1641198782689
got request at 1641198782902
processed allFrame at 1641198782902
send 'asking for bwe' at 1641198782902
sent 'asking for bwe' at 1641198782902
send [estimator, stat] at 1641198782902
sent [estimator, stat] at 1641198782902
pc wait for bwe at 1641198782902
pc got bwe at 1641198782906
bandwidth:  300000
pc flushed at 1641198782906
Bwe Sent: 4 at 1641198782906
got request at 1641198783142
processed allFrame at 1641198783143
send 'asking for bwe' at 1641198783143
sent 'asking for bwe' at 1641198783143
send [estimator, stat] at 1641198783143
sent [estimator, stat] at 1641198783143
pc wait for bwe at 1641198783143
pc got bwe at 1641198783147
bandwidth:  300000
pc flushed at 1641198783147
Bwe Sent: 5 at 1641198783147
got request at 1641198783371
processed allFrame at 1641198783371
send 'asking for bwe' at 1641198783371
sent 'asking for bwe' at 1641198783371
send [estimator, stat] at 1641198783371
sent [estimator, stat] at 1641198783371
pc wait for bwe at 1641198783372
pc got bwe at 1641198783376
bandwidth:  300000
pc flushed at 1641198783376
Bwe Sent: 5 at 1641198783376
got request at 1641198783590
processed allFrame at 1641198783590
send 'asking for bwe' at 1641198783590
sent 'asking for bwe' at 1641198783590
send [estimator, stat] at 1641198783590
sent [estimator, stat] at 1641198783590
pc wait for bwe at 1641198783590
pc got bwe at 1641198783594
bandwidth:  300000
pc flushed at 1641198783594
Bwe Sent: 4 at 1641198783594
got request at 1641198783793
processed allFrame at 1641198783794
send 'asking for bwe' at 1641198783794
sent 'asking for bwe' at 1641198783794
send [estimator, stat] at 1641198783794
sent [estimator, stat] at 1641198783794
pc wait for bwe at 1641198783794
pc got bwe at 1641198783798
bandwidth:  packetSeq:  6109
packetSeq:  6110
packetSeq:  6111
packetSeq:  6112
packetSeq:  6113
packetSeq:  6114
packetSeq:  6115
packetSeq:  6116
packetSeq:  6117
packetSeq:  6118
processed packlist at 1641198783372
receiving_rate:  286960.0
delay:  192.3
loss_ratio:  0.0
processed state0-2 at 1641198783372
avgFrameBetween:  6
psnrStat:  [[556116, 557115, 556960, 557595, 557313, 557953, 557794]]
delayStat:  [[85, 86, 85, 85, 85, 85, 86]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557263.7142857143] [85.28571428571429] [0]
processed state3-5 at 1641198783372
liner_to_log:  tensor([[[0.7682]]]) tensor([[[0.3375]]])
linear_to_log at 1641198783372
listState:  [0.07174, 0.1282, 0.0, 0.5572637142857143, 0.0852857142857143, 0.0, tensor([[[0.3375]]])]
state_clone_detach at 1641198783372
reward: 0.07407761158592413
state tensor([0.0717, 0.1282, 0.0000, 0.3375], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198783373
state222:  tensor([[0.0717, 0.1282, 0.0000, 0.3375]], device='cuda:0')
policy_old.forwarded at 1641198783375
give action 292============================
log_to_linear:  tensor([[[0.6546]]], device='cuda:0') tensor([[[1.2664]]], device='cuda:0')
log_to_linear action at 1641198783375
bwe changes from to:  [tensor([[[0.0014]]]), tensor([[[0.0018]]])]
step into gymStat at 1641198783376
send bwe to appRecv at 1641198783376
sent bwe to appRecv at 1641198783376
wait for recv string at 1641198783376
recved string at 1641198783590
1
wait for recv [self.estimator, stat] at 1641198783590
recved [self.estimator, stat] at 1641198783590
sorted packlist at 1641198783590
packetSeq:  6119
packetSeq:  6120
packetSeq:  6121
packetSeq:  6122
packetSeq:  6123
packetSeq:  6124
packetSeq:  6125
packetSeq:  6126
packetSeq:  6127
packetSeq:  6128
packetSeq:  6129
processed packlist at 1641198783590
receiving_rate:  314560.0
delay:  192.1818181818182
loss_ratio:  0.0
processed state0-2 at 1641198783590
avgFrameBetween:  6
psnrStat:  [[557993, 558660, 558309, 559014, 558029, 558552, 558427]]
delayStat:  [[85, 85, 85, 85, 85, 85, 85]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558426.2857142857] [85.0] [0]
processed state3-5 at 1641198783590
liner_to_log:  tensor([[[1.2664]]]) tensor([[[0.6546]]])
linear_to_log at 1641198783591
listState:  [0.07864, 0.12812121212121214, 0.0, 0.5584262857142857, 0.085, 0.0, tensor([[[0.6546]]])]
state_clone_detach at 1641198783591
reward: 0.10438781293456062
state tensor([0.0786, 0.1281, 0.0000, 0.6546], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198783591
state222:  tensor([[0.0786, 0.1281, 0.0000, 0.6546]], device='cuda:0')
policy_old.forwarded at 1641198783593
give action 293============================
log_to_linear:  tensor([[[0.5809]]], device='cuda:0') tensor([[[1.1342]]], device='cuda:0')
log_to_linear action at 1641198783594
bwe changes from to:  [tensor([[[0.0018]]]), tensor([[[0.0020]]])]
step into gymStat at 1641198783594
send bwe to appRecv at 1641198783594
sent bwe to appRecv at 1641198783594
wait for recv string at 1641198783594
recved string at 1641198783794
1
wait for recv [self.estimator, stat] at 1641198783794
recved [self.estimator, stat] at 1641198783794
sorted packlist at 1641198783794
packetSeq:  6130
packetSeq:  6131
packetSeq:  6132
packetSeq:  6133
packetSeq:  6134
packetSeq:  6135
packetSeq:  6136
packetSeq:  6137
packetSeq:  6138
packetSeq:  6139
packetSeq:  6140
packetSeq:  6141
processed packlist at 1641198783794
receiving_rate:  289560.0
delay:  191.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198783794
avgFrameBetween:  6
psnrStat:  [[559207, 558440, 558483, 558669, 558022, 558696, 558752]]
delayStat:  [[85, 84, 85, 85, 69, 70, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558609.8571428572] [78.14285714285714] [0]
processed state3-5 at 1641198783794
liner_to_log:  tensor([[[1.1342]]]) tensor([[[0.5809]]])
linear_to_log at 1641198783794
listState:  [0.07239, 0.12777777777777777, 0.0, 0.5586098571428572, 0.07814285714285714, 0.0, tensor([[[0.5809]]])]
state_clone_detach at 1641198783795
reward: 0.07824251151020623
state tensor([0.0724, 0.1278, 0.0000, 0.5809], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198783795
state222:  tensor([[0.0724, 0.1278, 0.0000, 0.5809]], device='cuda:0')
policy_old.forwarded at 1641198783797
give action 294============================
log_to_linear:  tensor([[[0.3549]]], device='cuda:0') tensor([[[0.7905]]], device='cuda:0')
log_to_linear action at 1641198783798
bwe changes from to:  [tensor([[[0.0020]]]), tensor([[[0.0016]]])]
step into gymStat at 1641198783798
send bwe to appRecv at 1641198783798
sent bwe to appRecv at 1641198783798
wait for recv string at 1641198783798
recved string at 1641198784017
1
wait for recv [self.estimator, stat] at 1641198784017
recved [self.estimator, stat] at 1641198784017
sorted packlist at 1641198784017
packetSeq:  6142
packetSeq:  6143
packetSeq:  6144
packetSeq:  6145
packetSeq:  6146
packetSeq:  6147
packetSeq:  6148
packetSeq:  6149
packetSeq:  6150
packetSeq:  6151
packetSeq:  6152
processed packlist at 1641198784017
receiving_rate:  287960.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198784017
avgFrameBetween:  6
psnrStat:  [[558909, 558577, 558553, 557960, 558839, 558420]]
delayStat:  [[70, 69, 70, 69, 70, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558543.0] [69.5] [0]
processed state3-5 at 1641198784017
liner_to_log:  tensor([[[0.7905]]]) tensor([[[0.3549]]])
linear_to_log at 1641198784018
listState:  [0.07199, 0.12793939393939394, 0.0, 0.558543, 0.0695, 0.0, tensor([[[0.3549]]])]
state_clone_detach at 1641198784018
reward: 0.07597579223767931
state tensor([0.0720, 0.1279, 0.0000, 0.3549], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198784018
state222:  tensor([[0.0720, 0.1279, 0.0000, 0.3549]], device='cuda:0')
policy_old.forwarded at 1641198784020
give action 295============================
log_to_linear:  tensor([[[0.2888]]], device='cuda:0') tensor([[[0.7095]]], device='cuda:0')
log_to_linear action at 1641198784021
bwe changes from to:  [tensor([[[0.0016]]]), tensor([[[0.0011]]])]
step into gymStat at 1641198784021
send bwe to appRecv at 1641198784021
sent bwe to appRecv at 1641198784021
wait for recv string at 1641198784021
recved string at 1641198784220
1
wait for recv [self.estimator, stat] at 1641198784220
recved [self.estimator, stat] at 1641198784220
sorted packlist at 1641198784220
packetSeq:  6153
packetSeq:  6154
packetSeq:  6155
packetSeq:  6156
packetSeq:  6157
packetSeq:  6158
packetSeq:  6159
packetSeq:  6160
packetSeq:  6161
packetSeq:  6162
processed packlist at 1641198784220
receiving_rate:  274800.0
delay:  191.8
loss_ratio:  0.0
processed state0-2 at 1641198784220
avgFrameBetween:  6
psnrStat:  [[558381, 557447, 557269, 557178, 556241, 555642, 555321]]
delayStat:  [[70, 69, 69, 69, 70, 69, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556782.7142857143] [69.28571428571429] [0]
processed state3-5 at 1641198784220
liner_to_log:  tensor([[[0.7095]]]) tensor([[[0.2888]]])
linear_to_log at 1641198784220
listState:  [0.0687, 0.12786666666666668, 0.0, 0.5567827142857144, 0.0692857142857143, 0.0, tensor([[[0.2888]]])]
state_clone_detach at 1641198784221
reward: 0.061333791860579634
state tensor([0.0687, 0.1279, 0.0000, 0.2888], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198784221
state222:  tensor([[0.0687, 0.1279, 0.0000, 0.2888]], device='cuda:0')
policy_old.forwarded at 1641198784223
give action 296============================
log_to_linear:  tensor([[[0.2599]]], device='cuda:0') tensor([[[0.6772]]], device='cuda:0')
log_to_linear action at 1641198784224
bwe changes from to:  [tensor([[[0.0011]]]), tensor([[[0.0008]]])]
step into gymStat at 1641198784224
send bwe to appRecv at 1641198784224
sent bwe to appRecv at 1641198784224
wait for recv string at 1641198784224
recved string at 1641198784423
1
wait for recv [self.estimator, stat] at 1641198784423
recved [self.estimator, stat] at 1641198784423
sorted packlist at 1641198784423
packetSeq:  6163
packetSeq:  6164
packetSeq:  6165
packetSeq:  6166
packetSeq:  6167
packetSeq:  6168
packetSeq:  6169
packetSeq:  6170
packetSeq:  6171
packetSeq:  6172
packetSeq:  6173
processed packlist at 1641198784423
receiving_rate:  310160.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198784424
avgFrameBetween:  6
psnrStat:  [[555680, 555586, 556303, 556097, 555784, 555867]]
delayStat:  [[69, 70, 70, 69, 69, 70]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555886.1666666666] [69.5] [0]
processed state3-5 at 1641198784424
liner_to_log:  tensor([[[0.6772]]]) tensor([[[0.2599]]])
linear_to_log at 1641198784424
listState:  [0.07754, 0.12793939393939394, 0.0, 0.5558861666666667, 0.0695, 0.0, tensor([[[0.2599]]])]
state_clone_detach at 1641198784424
reward: 0.10023874178828363
state tensor([0.0775, 0.1279, 0.0000, 0.2599], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198784425
state222:  tensor([[0.0775, 0.1279, 0.0000, 0.2599]], device='cuda:0')
policy_old.forwarded at 1641198784426
give action 297============================
log_to_linear:  tensor([[[0.3949]]], device='cuda:0') tensor([[[0.8440]]], device='cuda:0')
log_to_linear action at 1641198784427
bwe changes from to:  [tensor([[[0.0008]]]), tensor([[[0.0006]]])]
step into gymStat at 1641198784428
send bwe to appRecv at 1641198784428
sent bwe to appRecv at 1641198784428
wait for recv string at 1641198784428
recved string at 1641198784652
1
wait for recv [self.estimator, stat] at 1641198784652
recved [self.estimator, stat] at 1641198784653
sorted packlist at 1641198784653
packetSeq:  6174
packetSeq:  6175
packetSeq:  6176
packetSeq:  6177
packetSeq:  6178
packetSeq:  6179
packetSeq:  6180
packetSeq:  6181
packetSeq:  6182
packetSeq:  6183
processed packlist at 1641198784653
receiving_rate:  297440.0
delay:  192.2
loss_ratio:  0.0
processed state0-2 at 1641198784653
avgFrameBetween:  6
psnrStat:  [[556106, 556269, 554742, 553734, 552641, 551968]]
delayStat:  [[70, 69, 69, 69, 69, 71]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554243.3333333334] [69.5] [0]
processed state3-5 at 1641198784653
liner_to_log:  tensor([[[0.8440]]]) tensor([[[0.3949]]])
linear_to_log at 1641198784653
listState:  [0.07436, 0.12813333333333332, 0.0, 0.5542433333333334, 0.0695, 0.0, tensor([[[0.3949]]])]
state_clone_detach at 1641198784653
reward: 0.08587516926919747
state tensor([0.0744, 0.1281, 0.0000, 0.3949], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198784654
state222:  tensor([[0.0744, 0.1281, 0.0000, 0.3949]], device='cuda:0')
policy_old.forwarded at 1641198784656
give action 298============================
log_to_linear:  tensor([[[0.4478]]], device='cuda:0') tensor([[[0.9199]]], device='cuda:0')
log_to_linear action at 1641198784656
bwe changes from to:  [tensor([[[0.0006]]]), tensor([[[0.0006]]])]
step into gymStat at 1641198784657
send bwe to appRecv at 1641198784657
sent bwe to appRecv at 1641198784657
wait for recv string at 1641198784657
recved string at 1641198784856
1
wait for recv [self.estimator, stat] at 1641198784856
recved [self.estimator, stat] at 1641198784856
sorted packlist at 1641198784856
packetSeq:  6184
packetSeq:  6185
packetSeq:  6186
packetSeq:  6187
packetSeq:  6188
packetSeq:  6189
packetSeq:  6190
processed packlist at 1641198784856
receiving_rate:  243520.0
delay:  193.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198784856
avgFrameBetween:  6
psnrStat:  [[551418, 551081, 551155, 550734, 550605, 551149, 551246]]
delayStat:  [[69, 69, 69, 69, 69, 69, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551055.4285714285] [69.0] [0]
processed state3-5 at 1641198784857
liner_to_log:  tensor([[[0.9199]]]) tensor([[[0.4478]]])
linear_to_log at 1641198784857
listState:  [0.06088, 0.1291111111111111, 0.0, 0.5510554285714285, 0.069, 0.0, tensor([[[0.4478]]])]
state_clone_detach at 1641198784857
reward: 0.02072814689014868
state tensor([0.0609, 0.1291, 0.0000, 0.4478], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198784858
state222:  tensor([[0.0609, 0.1291, 0.0000, 0.4478]], device='cuda:0')
policy_old.forwarded at 1641198784859
give action 299============================
log_to_linear:  tensor([[[0.4506]]], device='cuda:0') tensor([[[0.9241]]], device='cuda:0')
log_to_linear action at 1641198784860
bwe changes from to:  [tensor([[[0.0006]]]), tensor([[[0.0006]]])]
step into gymStat at 1641198784860
send bwe to appRecv at 1641198784860
sent bwe to appRecv at 1641198784861
wait for recv string at 1641198784861
recved string at 1641198785057
1
wait for recv [self.estimator, stat] at 1641198785057
recved [self.estimator, stat] at 1641198785058
sorted packlist at 1641198785058
packetSeq:  6191
packetSeq:  6192
packetSeq:  6193
packetSeq:  6194
packetSeq:  6195
packetSeq:  6196
packetSeq:  6197
packetSeq:  6198
processed packlist at 1641198785058
receiving_rate:  265200.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198785058
avgFrameBetween:  6
psnrStat:  [[551822, 553047, 553447, 554336, 554556, 555210]]
delayStat:  [[69, 69, 70, 69, 69, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553736.3333333334] [69.16666666666667] [0]
processed state3-5 at 1641198785058
liner_to_log:  tensor([[[0.9241]]]) tensor([[[0.4506]]])
linear_to_log at 1641198785059
listState:  [0.0663, 0.12816666666666668, 0.0, 0.5537363333333334, 0.06916666666666667, 0.0, tensor([[[0.4506]]])]
state_clone_detach at 1641198785059
reward: 0.04935655415673845
state tensor([0.0663, 0.1282, 0.0000, 0.4506], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198785060
state222:  tensor([[0.0663, 0.1282, 0.0000, 0.4506]], device='cuda:0')
policy_old.forwarded at 1641198785061
give action 300============================
log_to_linear:  tensor([[[0.3184]]], device='cuda:0') tensor([[[0.7446]]], device='cuda:0')
log_to_linear action at 1641198785062
bwe changes from to:  [tensor([[[0.0006]]]), tensor([[[0.0004]]])]
step into gymStat at 1641198785063
send bwe to appRecv at 1641198785063
sent bwe to appRecv at 1641198785063
wait for recv string at 1641198785063
recved string at 1641198785260
1
wait for recv [self.estimator, stat] at 1641198785260
recved [self.estimator, stat] at 1641198785260
sorted packlist at 1641198785260
packetSeq:  6199
packetSeq:  6200
packetSeq:  6202
packetSeq:  6203
packetSeq:  6204
packetSeq:  6205
packetSeq:  6206
packetSeq:  6207
packetSeq:  6208
packetSeq:  6209
processed packlist at 1641198785260
receiving_rate:  259880.0
delay:  191.7
loss_ratio:  0.09090909090909091
processed state0-2 at 1641198785260
avgFrameBetween:  6
psnrStat:  [[554772, 555162, 554796, 553571, 553193, 553612]]
delayStat:  [[69, 69, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554184.3333333334] [68.33333333333333] [0]
processed state3-5 at 1641198785260
liner_to_log:  tensor([[[0.7446]]]) tensor([[[0.3184]]])
linear_to_log at 1641198785261
listState:  [0.06497, 0.1278, 0.09090909090909091, 0.5541843333333334, 0.06833333333333333, 0.0, tensor([[[0.3184]]])]
state_clone_detach at 1641198785261
reward: -0.8648622174594247
state tensor([0.0650, 0.1278, 0.0909, 0.3184], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198785261
state222:  tensor([[0.0650, 0.1278, 0.0909, 0.3184]], device='cuda:0')
policy_old.forwarded at 1641198785263
give action 301============================
log_to_linear:  tensor([[[0.6498]]], device='cuda:0') tensor([[[1.2574]]], device='cuda:0')
log_to_linear action at 1641198785264
bwe changes from to:  [tensor([[[0.0004]]]), tensor([[[0.0005]]])]
step into gymStat at 1641198785264
send bwe to appRecv at 1641198785264
sent bwe to appRecv at 1641198785264
wait for recv string at 1641198785264
recved string at 1641198785489
1
wait for recv [self.estimator, stat] at 1641198785489
recved [self.estimator, stat] at 1641198785489
sorted packlist at 1641198785489
packetSeq:  6210
packetSeq:  6211
packetSeq:  6212
packetSeq:  6213
packetSeq:  6214
packetSeq:  6215
packetSeq:  6216
packetSeq:  6217
packetSeq:  6218
processed packlist at 1641198785489
receiving_rate:  258440.0
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198785489
avgFrameBetween:  6
psnrStat:  [[552796, 551920, 551790, 549991, 549767, 549320, 549657]]
delayStat:  [[68, 68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550748.7142857143] [68.0] [0]
processed state3-5 at 1641198785489
liner_to_log:  tensor([[[1.2574]]]) tensor([[[0.6498]]])
linear_to_log at 1641198785490
listState:  [0.06461, 0.1285, 0.0, 0.5507487142857144, 0.068, 0.0, tensor([[[0.6498]]])]
state_clone_detach at 1641198785490
reward: 0.04043177793853303
state tensor([0.0646, 0.1285, 0.0000, 0.6498], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198785490
state222:  tensor([[0.0646, 0.1285, 0.0000, 0.6498]], device='cuda:0')
policy_old.forwarded at 1641198785492
give action 302============================
log_to_linear:  tensor([[[0.4707]]], device='cuda:0') tensor([[[0.9543]]], device='cuda:0')
log_to_linear action at 1641198785493
bwe changes from to:  [tensor([[[0.0005]]]), tensor([[[0.0005]]])]
step into gymStat at 1641198785493
send bwe to appRecv at 1641198785493
sent bwe to appRecv at 1641198785493
wait for recv string at 1641198785493
recved string at 1641198785692
1
wait for recv [self.estimator, stat] at 1641198785692
recved [self.estimator, stat] at 1641198785692
sorted packlist at 1641198785692
packetSeq:  6219
packetSeq:  6220
packetSeq:  6221
packetSeq:  6222
packetSeq:  6223
packetSeq:  6224
packetSeq:  6225
packetSeq:  6226
packetSeq:  6227
packetSeq:  6228
packetSeq:  6229
processed packlist at 1641198785692
receiving_rate:  301400.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198785692
avgFrameBetween:  6
psnrStat:  [[549732, 549002, 549365, 548449, 547963, 547361]]
delayStat:  [[68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548645.3333333334] [68.0] [0]
processed state3-5 at 1641198785692
liner_to_log:  tensor([[[0.9543]]]) tensor([[[0.4707]]])
linear_to_log at 1641198785693
listState:  [0.07535, 0.128, 0.0, 0.5486453333333333, 0.068, 0.0, tensor([[[0.4707]]])]
state_clone_detach at 1641198785693
reward: 0.0905997484364699
state tensor([0.0754, 0.1280, 0.0000, 0.4707], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198785694
state222:  tensor([[0.0754, 0.1280, 0.0000, 0.4707]], device='cuda:0')
policy_old.forwarded at 1641198785695
give action 303============================
log_to_linear:  tensor([[[0.3272]]], device='cuda:0') tensor([[[0.7553]]], device='cuda:0')
log_to_linear action at 1641198785696
bwe changes from to:  [tensor([[[0.0005]]]), tensor([[[0.0004]]])]
step into gymStat at 1641198785696
send bwe to appRecv at 1641198785696
sent bwe to appRecv at 1641198785696
wait for recv string at 1641198785696
recved string at 1641198785920
1
wait for recv [self.estimator, stat] at 1641198785920
recved [self.estimator, stat] at 1641198785920
sorted packlist at 1641198785920
packetSeq:  6230
packetSeq:  6231
packetSeq:  6232
packetSeq:  6233
packetSeq:  6234
packetSeq:  6235
packetSeq:  6236
packetSeq:  6237
packetSeq:  6238
processed packlist at 1641198785920
receiving_rate:  280040.0
delay:  192.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198785920
avgFrameBetween:  6
psnrStat:  [[547443, 547122, 546840, 546278, 546277, 544849, 544789]]
delayStat:  [[68, 69, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [546228.2857142857] [68.14285714285714] [0]
processed state3-5 at 1641198785920
liner_to_log:  tensor([[[0.7553]]]) tensor([[[0.3272]]])
linear_to_log at 1641198785920
listState:  [0.07001, 0.12807407407407406, 0.0, 0.5462282857142857, 0.06814285714285714, 0.0, tensor([[[0.3272]]])]
state_clone_detach at 1641198785920
reward: 0.06667271229829119
state tensor([0.0700, 0.1281, 0.0000, 0.3272], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198785921
state222:  tensor([[0.0700, 0.1281, 0.0000, 0.3272]], device='cuda:0')
policy_old.forwarded at 1641198785923
give action 304============================
log_to_linear:  tensor([[[0.3734]]], device='cuda:0') tensor([[[0.8148]]], device='cuda:0')
log_to_linear action at 1641198785924
bwe changes from to:  [tensor([[[0.0004]]]), tensor([[[0.0003]]])]
step into gymStat at 1641198785924
send bwe to appRecv at 1641198785924
sent bwe to appRecv at 1641198785924
wait for recv string at 1641198785924
recved string at 1641198786122
1
wait for recv [self.estimator, stat] at 1641198786122
recved [self.estimator, stat] at 1641198786123
sorted packlist at 1641198786123
packetSeq:  6239
packetSeq:  6240
packetSeq:  6241
packetSeq:  6242
packetSeq:  6243
packetSeq:  6244
packetSeq:  6245
packetSeq:  6246
packetSeq:  6247
processed packlist at 1641198786123
receiving_rate:  277320.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198786123
avgFrameBetween:  6
psnrStat:  [[544377, 544607, 544399, 545597, 546444, 546775]]
delayStat:  [[68, 69, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [545366.5] [68.16666666666667] [0]
processed state3-5 at 1641198786123
liner_to_log:  tensor([[[0.8148]]]) tensor([[[0.3734]]])
linear_to_log at 1641198786123
listState:  [0.06933, 0.1282962962962963, 0.0, 0.5453665, 0.06816666666666667, 0.0, tensor([[[0.3734]]])]
state_clone_detach at 1641198786123
reward: 0.06291910532762479
state tensor([0.0693, 0.1283, 0.0000, 0.3734], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198786124
state222:  tensor([[0.0693, 0.1283, 0.0000, 0.3734]], device='cuda:0')
policy_old.forwarded at 1641198786126
give action 305============================
log_to_linear:  tensor([[[0.3485]]], device='cuda:0') tensor([[[0.7822]]], device='cuda:0')
log_to_linear action at 1641198786126
bwe changes from to:  [tensor([[[0.0003]]]), tensor([[[0.0002]]])]
step into gymStat at 1641198786127
send bwe to appRecv at 1641198786127
sent bwe to appRecv at 1641198786127
wait for recv string at 1641198786127
recved string at 1641198786326
1
wait for recv [self.estimator, stat] at 1641198786326
recved [self.estimator, stat] at 1641198786326
sorted packlist at 1641198786326
packetSeq:  6248
packetSeq:  6249
packetSeq:  6250
packetSeq:  6251
packetSeq:  6252
packetSeq:  6253
packetSeq:  6254
processed packlist at 1641198786326
receiving_rate:  254400.0
delay:  193.14285714285714
loss_ratio:  0.0
processed state0-2 at 1641198786326
avgFrameBetween:  6
psnrStat:  [[547599, 548237, 547908, 547804, 548633, 548406]]
delayStat:  [[68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548097.8333333334] [68.0] [0]
processed state3-5 at 1641198786326
liner_to_log:  tensor([[[0.7822]]]) tensor([[[0.3485]]])
linear_to_log at 1641198786327
listState:  [0.0636, 0.12876190476190477, 0.0, 0.5480978333333334, 0.068, 0.0, tensor([[[0.3485]]])]
state_clone_detach at 1641198786327
reward: 0.03485949745139649
state tensor([0.0636, 0.1288, 0.0000, 0.3485], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198786327
state222:  tensor([[0.0636, 0.1288, 0.0000, 0.3485]], device='cuda:0')
policy_old.forwarded at 1641198786329
give action 306============================
log_to_linear:  tensor([[[0.7091]]], device='cuda:0') tensor([[[1.3698]]], device='cuda:0')
log_to_linear action at 1641198786330
bwe changes from to:  [tensor([[[0.0002]]]), tensor([[[0.0003]]])]
step into gymStat at 1641198786330
send bwe to appRecv at 1641198786330
sent bwe to appRecv at 1641198786330
wait for recv string at 1641198786330
recved string at 1641198786557
1
wait for recv [self.estimator, stat] at 1641198786557
recved [self.estimator, stat] at 1641198786557
sorted packlist at 1641198786557
packetSeq:  6255
packetSeq:  6256
packetSeq:  6257
packetSeq:  6258
packetSeq:  6259
packetSeq:  6260
packetSeq:  6261
packetSeq:  6262
packetSeq:  6263
packetSeq:  6264
processed packlist at 1641198786557
receiving_rate:  260880.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198786557
avgFrameBetween:  6
psnrStat:  [[548798, 548975, 549755, 550451, 550098, 550334, 549691]]
delayStat:  [[68, 68, 69, 68, 68, 69, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549728.8571428572] [68.42857142857143] [0]
processed state3-5 at 1641198786557
liner_to_log:  tensor([[[1.3698]]]) tensor([[[0.7091]]])
linear_to_log at 1641198786558
listState:  [0.06522, 0.12822222222222224, 0.0, 0.5497288571428571, 0.06842857142857144, 0.0, tensor([[[0.7091]]])]
state_clone_detach at 1641198786558
reward: 0.04413761999586857
state tensor([0.0652, 0.1282, 0.0000, 0.7091], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198786558
state222:  tensor([[0.0652, 0.1282, 0.0000, 0.7091]], device='cuda:0')
policy_old.forwarded at 1641198786560
give action 307============================
log_to_linear:  tensor([[[0.5110]]], device='cuda:0') tensor([[[1.0176]]], device='cuda:0')
log_to_linear action at 1641198786561
bwe changes from to:  [tensor([[[0.0003]]]), tensor([[[0.0003]]])]
step into gymStat at 1641198786561
send bwe to appRecv at 1641198786561
sent bwe to appRecv at 1641198786561
wait for recv string at 1641198786561
recved string at 1641198786788
1
wait for recv [self.estimator, stat] at 1641198786788
recved [self.estimator, stat] at 1641198786788
sorted packlist at 1641198786788
packetSeq:  6265
packetSeq:  6266
packetSeq:  6267
packetSeq:  6268
packetSeq:  6269
packetSeq:  6270
packetSeq:  6271
packetSeq:  6272
packetSeq:  6273
packetSeq:  6274
packetSeq:  6275
processed packlist at 1641198786788
receiving_rate:  303640.0
delay:  191.72727272727272
loss_ratio:  0.0
processed state0-2 at 1641198786788
avgFrameBetween:  6
psnrStat:  [[549274, 549932, 548895, 550490, 549845, 551173, 550656]]
delayStat:  [[69, 68, 68, 69, 69, 68, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550037.8571428572] [68.57142857142857] [0]
processed state3-5 at 1641198786788
liner_to_log:  tensor([[[1.0176]]]) tensor([[[0.5110]]])
linear_to_log at 1641198786788
listState:  [0.07591, 0.1278181818181818, 0.0, 0.5500378571428571, 0.06857142857142857, 0.0, tensor([[[0.5110]]])]
state_clone_detach at 1641198786789
reward: 0.09357769744520744
state tensor([0.0759, 0.1278, 0.0000, 0.5110], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198786789
state222:  tensor([[0.0759, 0.1278, 0.0000, 0.5110]], device='cuda:0')
policy_old.forwarded at 1641198786791
give action 308============================
log_to_linear:  tensor([[[0.7408]]], device='cuda:0') tensor([[[1.4323]]], device='cuda:0')
log_to_linear action at 1641198786792
bwe changes from to:  [tensor([[[0.0003]]]), tensor([[[0.0005]]])]
step into gymStat at 1641198786792
send bwe to appRecv at 1641198786792
sent bwe to appRecv at 1641198786792
wait for recv string at 1641198786792
recved string at 1641198786991
1
wait for recv [self.estimator, stat] at 1641198786991
recved [self.estimator, stat] at 1641198786991
sorted packlist at 1641198786991
packetSeq:  6276
packetSeq:  6277
packetSeq:  6278
packetSeq:  6279
packetSeq:  6280
packetSeq:  6281
packetSeq:  6282
packetSeq:  6283
packetSeq:  6284
packetSeq:  6285
processed packlist at 1641198786991
receiving_rate:  283160.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198786991
avgFrameBetween:  6
psnrStat:  [[551914, 551136, 550740, 550503, 550303, 550051]]
delayStat:  [[68, 68, 68, 69, 68, 70]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550774.5] [68.5] [0]
processed state3-5 at 1641198786991
liner_to_log:  tensor([[[1.4323]]]) tensor([[[0.7408]]])
linear_to_log at 1641198786992
listState:  [0.07079, 0.12793333333333334, 0.0, 0.5507745, 0.0685, 0.0, tensor([[[0.7408]]])]
state_clone_detach at 1641198786992
reward: 0.0706163893241426
state tensor([0.0708, 0.1279, 0.0000, 0.7408], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198786993
state222:  tensor([[0.0708, 0.1279, 0.0000, 0.7408]], device='cuda:0')
policy_old.forwarded at 1641198786994
give action 309============================
log_to_linear:  tensor([[[0.5099]]], device='cuda:0') tensor([[[1.0157]]], device='cuda:0')
log_to_linear action at 1641198786995
bwe changes from to:  [tensor([[[0.0005]]]), tensor([[[0.0005]]])]
step into gymStat at 1641198786995
send bwe to appRecv at 1641198786995
sent bwe to appRecv at 1641198786995
wait for recv string at 1641198786995
recved string at 1641198787194
1
wait for recv [self.estimator, stat] at 1641198787194
recved [self.estimator, stat] at 1641198787194
sorted packlist at 1641198787194
packetSeq:  6286
packetSeq:  6287
packetSeq:  6288
packetSeq:  6289
packetSeq:  6290
packetSeq:  6291
packetSeq:  6292
packetSeq:  6293
packetSeq:  6294
packetSeq:  6295
processed packlist at 1641198787194
receiving_rate:  286280.0
delay:  192.3
loss_ratio:  0.0
processed state0-2 at 1641198787194
avgFrameBetween:  6
psnrStat:  [[549491, 549058, 550060, 549885, 550421, 550619]]
delayStat:  [[68, 69, 69, 68, 68, 69]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549922.3333333334] [68.5] [0]
processed state3-5 at 1641198787194
liner_to_log:  tensor([[[1.0157]]]) tensor([[[0.5099]]])
linear_to_log at 1641198787195
listState:  [0.07157, 0.1282, 0.0, 0.5499223333333334, 0.0685, 0.0, tensor([[[0.5099]]])]
state_clone_detach at 1641198787195
reward: 0.07331729692266836
state tensor([0.0716, 0.1282, 0.0000, 0.5099], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198787195
state222:  tensor([[0.0716, 0.1282, 0.0000, 0.5099]], device='cuda:0')
policy_old.forwarded at 1641198787197
give action 310============================
log_to_linear:  tensor([[[0.7645]]], device='cuda:0') tensor([[[1.4801]]], device='cuda:0')
log_to_linear action at 1641198787198
bwe changes from to:  [tensor([[[0.0005]]]), tensor([[[0.0007]]])]
step into gymStat at 1641198787198
send bwe to appRecv at 1641198787198
sent bwe to appRecv at 1641198787198
wait for recv string at 1641198787198
recved string at 1641198787423
1
wait for recv [self.estimator, stat] at 1641198787423
recved [self.estimator, stat] at 1641198787423
sorted packlist at 1641198787423
packetSeq:  6296
packetSeq:  6297
packetSeq:  6298
packetSeq:  6299
packetSeq:  6300
packetSeq:  6301
packetSeq:  6302
packetSeq:  6303
packetSeq:  6304
packetSeq:  6305
processed packlist at 1641198787423
receiving_rate:  260360.0
delay:  191.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198787423
avgFrameBetween:  6
psnrStat:  [[550475, 550647, 548941, 548684, 548547, 548184, 547440]]
delayStat:  [[68, 68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548988.2857142857] [68.0] [0]
processed state3-5 at 1641198787423
liner_to_log:  tensor([[[1.4801]]]) tensor([[[0.7645]]])
linear_to_log at 1641198787424
listState:  [0.06509, 0.12792592592592592, 0.0, 0.5489882857142857, 0.068, 0.0, tensor([[[0.7645]]])]
state_clone_detach at 1641198787424
reward: 0.04441548693699976
state tensor([0.0651, 0.1279, 0.0000, 0.7645], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198787425
state222:  tensor([[0.0651, 0.1279, 0.0000, 0.7645]], device='cuda:0')
policy_old.forwarded at 1641198787426
give action 311============================
log_to_linear:  tensor([[[0.5099]]], device='cuda:0') tensor([[[1.0157]]], device='cuda:0')
log_to_linear action at 1641198787427
bwe changes from to:  [tensor([[[0.0007]]]), tensor([[[0.0007]]])]
step into gymStat at 1641198787427
send bwe to appRecv at 1641198787427
sent bwe to appRecv at 1641198787428
wait for recv string at 1641198787428
recved string at 1641198787656
1
wait for recv [self.estimator, stat] at 1641198787656
recved [self.estimator, stat] at 1641198787656
sorted packlist at 1641198787656
packetSeq:  6306
packetSeq:  6307
packetSeq:  6308
packetSeq:  6309
packetSeq:  6310
packetSeq:  6311
packetSeq:  6312
packetSeq:  6313
packetSeq:  6314
packetSeq:  6315
processed packlist at 1641198787656
receiving_rate:  270960.0
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198787656
avgFrameBetween:  6
psnrStat:  [[547881, 549041, 548567, 549165, 548338, 549110, 548964]]
delayStat:  [[69, 68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548723.7142857143] [68.14285714285714] [0]
processed state3-5 at 1641198787656
liner_to_log:  tensor([[[1.0157]]]) tensor([[[0.5099]]])
linear_to_log at 1641198787657
listState:  [0.06774, 0.12822222222222224, 0.0, 0.5487237142857143, 0.06814285714285714, 0.0, tensor([[[0.5099]]])]
state_clone_detach at 1641198787657
reward: 0.05586074064045787
state tensor([0.0677, 0.1282, 0.0000, 0.5099], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198787657
state222:  tensor([[0.0677, 0.1282, 0.0000, 0.5099]], device='cuda:0')
policy_old.forwarded at 1641198787659
give action 312============================
log_to_linear:  tensor([[[0.4709]]], device='cuda:0') tensor([[[0.9546]]], device='cuda:0')
log_to_linear action at 1641198787660
bwe changes from to:  [tensor([[[0.0007]]]), tensor([[[0.0007]]])]
step into gymStat at 1641198787660
send bwe to appRecv at 1641198787660
sent bwe to appRecv at 1641198787660
wait for recv string at 1641198787660
recved string at 1641198787862
1
wait for recv [self.estimator, stat] at 1641198787862
recved [self.estimator, stat] at 1641198787862
sorted packlist at 1641198787862
packetSeq:  6316
packetSeq:  6317
packetSeq:  6318
packetSeq:  6319
packetSeq:  6320
packetSeq:  6321
packetSeq:  6322
packetSeq:  6323
packetSeq:  6324
packetSeq:  6325
processed packlist at 1641198787862
receiving_rate:  271360.0
delay:  193.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198787862
avgFrameBetween:  6
psnrStat:  [[547965, 547921, 547296, 546594, 546364, 546979]]
delayStat:  [[68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547186.5] [68.0] [0]
processed state3-5 at 1641198787862
liner_to_log:  tensor([[[0.9546]]]) tensor([[[0.4709]]])
linear_to_log at 1641198787862
listState:  [0.06784, 0.1291111111111111, 0.0, 0.5471865, 0.068, 0.0, tensor([[[0.4709]]])]
state_clone_detach at 1641198787862
reward: 0.05365458516379379
state tensor([0.0678, 0.1291, 0.0000, 0.4709], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198787863
state222:  tensor([[0.0678, 0.1291, 0.0000, 0.4709]], device='cuda:0')
policy_old.forwarded at 1641198787865
give action 313============================
log_to_linear:  tensor([[[0.5714]]], device='cuda:0') tensor([[[1.1178]]], device='cuda:0')
log_to_linear action at 1641198787866
bwe changes from to:  [tensor([[[0.0007]]]), tensor([[[0.0008]]])]
step into gymStat at 1641198787866
send bwe to appRecv at 1641198787866
sent bwe to appRecv at 1641198787866
wait for recv string at 1641198787866
recved string at 1641198788097
1
wait for recv [self.estimator, stat] at 1641198788097
recved [self.estimator, stat] at 1641198788097
sorted packlist at 1641198788097
packetSeq:  6326
packetSeq:  6327
packetSeq:  6328
packetSeq:  6329
packetSeq:  6330
packetSeq:  6331
packetSeq:  6332
packetSeq:  6333
packetSeq:  6334
processed packlist at 1641198788097
receiving_rate:  289160.0
delay:  199.25
loss_ratio:  0.0
processed state0-2 at 1641198788097
avgFrameBetween:  6
psnrStat:  [[546522, 546234, 545768, 544678, 545124, 544318, 543337]]
delayStat:  [[68, 68, 68, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [545140.1428571428] [68.0] [0]
processed state3-5 at 1641198788097
liner_to_log:  tensor([[[1.1178]]]) tensor([[[0.5714]]])
linear_to_log at 1641198788098
listState:  [0.07229, 0.13283333333333333, 0.0, 0.5451401428571429, 0.068, 0.0, tensor([[[0.5714]]])]
state_clone_detach at 1641198788098
reward: 0.06263087300469339
state tensor([0.0723, 0.1328, 0.0000, 0.5714], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198788099
state222:  tensor([[0.0723, 0.1328, 0.0000, 0.5714]], device='cuda:0')
policy_old.forwarded at 1641198788100
give action 314============================
log_to_linear:  tensor([[[0.8740]]], device='cuda:0') tensor([[[1.7117]]], device='cuda:0')
log_to_linear action at 1641198788101
bwe changes from to:  [tensor([[[0.0008]]]), tensor([[[0.0013]]])]
step into gymStat at 1641198788101
send bwe to appRecv at 1641198788101
sent bwe to appRecv at 1641198788101
wait for recv string at 1641198788101
recved string at 1641198788324
1
wait for recv [self.estimator, stat] at 1641198788324
recved [self.estimator, stat] at 1641198788324
sorted packlist at 1641198788324
packetSeq:  6335
packetSeq:  6336
packetSeq:  6337
packetSeq:  6338
packetSeq:  6339
packetSeq:  6340
packetSeq:  6341
packetSeq:  6342
processed packlist at 1641198788324
receiving_rate:  257600.00000000003
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198788324
avgFrameBetween:  6
psnrStat:  [[542874, 543866, 544351, 545528, 544546, 546489, 547148]]
delayStat:  [[68, 69, 69, 68, 68, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [544971.7142857143] [68.28571428571429] [0]
processed state3-5 at 1641198788324
liner_to_log:  tensor([[[1.7117]]]) tensor([[[0.8740]]])
linear_to_log at 1641198788324
listState:  [0.06440000000000001, 0.13133333333333333, 0.0, 0.5449717142857143, 0.0682857142857143, 0.0, tensor([[[0.8740]]])]
state_clone_detach at 1641198788325
reward: 0.030939690786791763
state tensor([0.0644, 0.1313, 0.0000, 0.8740], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198788325
state222:  tensor([[0.0644, 0.1313, 0.0000, 0.8740]], device='cuda:0')
policy_old.forwarded at 1641198788327
give action 315============================
log_to_linear:  tensor([[[0.2776]]], device='cuda:0') tensor([[[0.6968]]], device='cuda:0')
log_to_linear action at 1641198788328
bwe changes from to:  [tensor([[[0.0013]]]), tensor([[[0.0009]]])]
step into gymStat at 1641198788328
send bwe to appRecv at 1641198788328
sent bwe to appRecv at 1641198788328
wait for recv string at 1641198788328
recved string at 1641198788526
1
wait for recv [self.estimator, stat] at 1641198788526
recved [self.estimator, stat] at 1641198788526
sorted packlist at 1641198788526
packetSeq:  6343
packetSeq:  6344
packetSeq:  6345
packetSeq:  6346
packetSeq:  6347
packetSeq:  6348
packetSeq:  6349
packetSeq:  6350
packetSeq:  6351
packetSeq:  6352
packetSeq:  6353
processed packlist at 1641198788526
receiving_rate:  304920.0
delay:  197.0909090909091
loss_ratio:  0.0
processed state0-2 at 1641198788526
avgFrameBetween:  6
psnrStat:  [[546789, 546438, 547885, 548761, 549155, 548260]]
delayStat:  [[68, 68, 68, 69, 68, 68]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547881.3333333334] [68.16666666666667] [0]
processed state3-5 at 1641198788526
liner_to_log:  tensor([[[0.6968]]]) tensor([[[0.2776]]])
linear_to_log at 1641198788527
listState:  [0.07623, 0.1313939393939394, 0.0, 0.5478813333333333, 0.06816666666666667, 0.0, tensor([[[0.2776]]])]
state_clone_detach at 1641198788527
reward: 0.08423600662834885
state tensor([0.0762, 0.1314, 0.0000, 0.2776], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198788527
state222:  tensor([[0.0762, 0.1314, 0.0000, 0.2776]], device='cuda:0')
policy_old.forwarded at 1641198788529
give action 316============================
log_to_linear:  tensor([[[0.0430]]], device='cuda:0') tensor([[[0.5102]]], device='cuda:0')
log_to_linear action at 1641198788530
bwe changes from to:  [tensor([[[0.0009]]]), tensor([[[0.0005]]])]
step into gymStat at 1641198788530
send bwe to appRecv at 1641198788530
sent bwe to appRecv at 1641198788530
wait for recv string at 1641198788530
recved string at 1641198788733
1
300000
pc flushed at 1641198783798
Bwe Sent: 5 at 1641198783798
got request at 1641198784017
processed allFrame at 1641198784017
send 'asking for bwe' at 1641198784017
sent 'asking for bwe' at 1641198784017
send [estimator, stat] at 1641198784017
sent [estimator, stat] at 1641198784017
pc wait for bwe at 1641198784017
pc got bwe at 1641198784021
bandwidth:  300000
pc flushed at 1641198784021
Bwe Sent: 4 at 1641198784021
got request at 1641198784219
processed allFrame at 1641198784220
send 'asking for bwe' at 1641198784220
sent 'asking for bwe' at 1641198784220
send [estimator, stat] at 1641198784220
sent [estimator, stat] at 1641198784220
pc wait for bwe at 1641198784220
pc got bwe at 1641198784224
bandwidth:  300000
pc flushed at 1641198784224
Bwe Sent: 5 at 1641198784224
got request at 1641198784423
processed allFrame at 1641198784423
send 'asking for bwe' at 1641198784423
sent 'asking for bwe' at 1641198784423
send [estimator, stat] at 1641198784423
sent [estimator, stat] at 1641198784423
pc wait for bwe at 1641198784423
pc got bwe at 1641198784428
bandwidth:  300000
pc flushed at 1641198784428
Bwe Sent: 5 at 1641198784428
got request at 1641198784652
processed allFrame at 1641198784652
send 'asking for bwe' at 1641198784652
sent 'asking for bwe' at 1641198784652
send [estimator, stat] at 1641198784652
sent [estimator, stat] at 1641198784652
pc wait for bwe at 1641198784653
pc got bwe at 1641198784657
bandwidth:  300000
pc flushed at 1641198784657
Bwe Sent: 5 at 1641198784657
got request at 1641198784856
processed allFrame at 1641198784856
send 'asking for bwe' at 1641198784856
sent 'asking for bwe' at 1641198784856
send [estimator, stat] at 1641198784856
sent [estimator, stat] at 1641198784856
pc wait for bwe at 1641198784856
pc got bwe at 1641198784861
bandwidth:  300000
pc flushed at 1641198784861
Bwe Sent: 5 at 1641198784861
got request at 1641198785057
processed allFrame at 1641198785057
send 'asking for bwe' at 1641198785057
sent 'asking for bwe' at 1641198785057
send [estimator, stat] at 1641198785057
sent [estimator, stat] at 1641198785058
pc wait for bwe at 1641198785058
pc got bwe at 1641198785063
bandwidth:  300000
pc flushed at 1641198785063
Bwe Sent: 6 at 1641198785063
got request at 1641198785260
processed allFrame at 1641198785260
send 'asking for bwe' at 1641198785260
sent 'asking for bwe' at 1641198785260
send [estimator, stat] at 1641198785260
sent [estimator, stat] at 1641198785260
pc wait for bwe at 1641198785260
pc got bwe at 1641198785264
bandwidth:  300000
pc flushed at 1641198785264
Bwe Sent: 4 at 1641198785264
got request at 1641198785489
processed allFrame at 1641198785489
send 'asking for bwe' at 1641198785489
sent 'asking for bwe' at 1641198785489
send [estimator, stat] at 1641198785489
sent [estimator, stat] at 1641198785489
pc wait for bwe at 1641198785489
pc got bwe at 1641198785493
bandwidth:  300000
pc flushed at 1641198785493
Bwe Sent: 4 at 1641198785493
got request at 1641198785692
processed allFrame at 1641198785692
send 'asking for bwe' at 1641198785692
sent 'asking for bwe' at 1641198785692
send [estimator, stat] at 1641198785692
sent [estimator, stat] at 1641198785692
pc wait for bwe at 1641198785692
pc got bwe at 1641198785696
bandwidth:  300000
pc flushed at 1641198785696
Bwe Sent: 4 at 1641198785696
got request at 1641198785919
processed allFrame at 1641198785919
send 'asking for bwe' at 1641198785919
sent 'asking for bwe' at 1641198785920
send [estimator, stat] at 1641198785920
sent [estimator, stat] at 1641198785920
pc wait for bwe at 1641198785920
pc got bwe at 1641198785924
bandwidth:  300000
pc flushed at 1641198785924
Bwe Sent: 5 at 1641198785924
got request at 1641198786122
processed allFrame at 1641198786122
send 'asking for bwe' at 1641198786122
sent 'asking for bwe' at 1641198786122
send [estimator, stat] at 1641198786122
sent [estimator, stat] at 1641198786122
pc wait for bwe at 1641198786123
pc got bwe at 1641198786127
bandwidth:  300000
pc flushed at 1641198786127
Bwe Sent: 5 at 1641198786127
got request at 1641198786326
processed allFrame at 1641198786326
send 'asking for bwe' at 1641198786326
sent 'asking for bwe' at 1641198786326
send [estimator, stat] at 1641198786326
sent [estimator, stat] at 1641198786326
pc wait for bwe at 1641198786326
pc got bwe at 1641198786330
bandwidth:  300000
pc flushed at 1641198786330
Bwe Sent: 4 at 1641198786330
got request at 1641198786557
processed allFrame at 1641198786557
send 'asking for bwe' at 1641198786557
sent 'asking for bwe' at 1641198786557
send [estimator, stat] at 1641198786557
sent [estimator, stat] at 1641198786557
pc wait for bwe at 1641198786557
pc got bwe at 1641198786561
bandwidth:  300000
pc flushed at 1641198786561
Bwe Sent: 4 at 1641198786561
got request at 1641198786787
processed allFrame at 1641198786788
send 'asking for bwe' at 1641198786788
sent 'asking for bwe' at 1641198786788
send [estimator, stat] at 1641198786788
sent [estimator, stat] at 1641198786788
pc wait for bwe at 1641198786788
pc got bwe at 1641198786792
bandwidth:  300000
pc flushed at 1641198786792
Bwe Sent: 5 at 1641198786792
got request at 1641198786991
processed allFrame at 1641198786991
send 'asking for bwe' at 1641198786991
sent 'asking for bwe' at 1641198786991
send [estimator, stat] at 1641198786991
sent [estimator, stat] at 1641198786991
pc wait for bwe at 1641198786991
pc got bwe at 1641198786995
bandwidth:  300000
pc flushed at 1641198786995
Bwe Sent: 4 at 1641198786995
got request at 1641198787194
processed allFrame at 1641198787194
send 'asking for bwe' at 1641198787194
sent 'asking for bwe' at 1641198787194
send [estimator, stat] at 1641198787194
sent [estimator, stat] at 1641198787194
pc wait for bwe at 1641198787194
pc got bwe at 1641198787198
bandwidth:  300000
pc flushed at 1641198787198
Bwe Sent: 4 at 1641198787198
got request at 1641198787423
processed allFrame at 1641198787423
send 'asking for bwe' at 1641198787423
sent 'asking for bwe' at 1641198787423
send [estimator, stat] at 1641198787423
sent [estimator, stat] at 1641198787423
pc wait for bwe at 1641198787423
pc got bwe at 1641198787428
bandwidth:  300000
pc flushed at 1641198787428
Bwe Sent: 5 at 1641198787428
got request at 1641198787656
processed allFrame at 1641198787656
send 'asking for bwe' at 1641198787656
sent 'asking for bwe' at 1641198787656
send [estimator, stat] at 1641198787656
sent [estimator, stat] at 1641198787656
pc wait for bwe at 1641198787656
pc got bwe at 1641198787660
bandwidth:  300000
pc flushed at 1641198787660
Bwe Sent: 4 at 1641198787660
got request at 1641198787861
processed allFrame at 1641198787861
send 'asking for bwe' at 1641198787861
sent 'asking for bwe' at 1641198787861
send [estimator, stat] at 1641198787861
sent [estimator, stat] at 1641198787862
pc wait for bwe at 1641198787862
pc got bwe at 1641198787866
bandwidth:  300000
pc flushed at 1641198787866
Bwe Sent: 5 at 1641198787866
got request at 1641198788097
processed allFrame at 1641198788097
send 'asking for bwe' at 1641198788097
sent 'asking for bwe' at 1641198788097
send [estimator, stat] at 1641198788097
sent [estimator, stat] at 1641198788097
pc wait for bwe at 1641198788097
pc got bwe at 1641198788101
bandwidth:  300000
pc flushed at 1641198788101
Bwe Sent: 4 at 1641198788101
got request at 1641198788323
processed allFrame at 1641198788324
send 'asking for bwe' at 1641198788324
sent 'asking for bwe' at 1641198788324
send [estimator, stat] at 1641198788324
sent [estimator, stat] at 1641198788324
pc wait for bwe at 1641198788324
pc got bwe at 1641198788328
bandwidth:  300000
pc flushed at 1641198788328
Bwe Sent: 5 at 1641198788328
got request at 1641198788526
processed allFrame at 1641198788526
send 'asking for bwe' at 1641198788526
sent 'asking for bwe' at 1641198788526
send [estimator, stat] at 1641198788526
sent [estimator, stat] at 1641198788526
pc wait for bwe at 1641198788526
pc got bwe at 1641198788530
bandwidth:  300000
pc flushed at 1641198788530
Bwe Sent: 4 at 1641198788530
got request at 1641198788733
processed allFrame at 1641198788733
send 'asking for bwe' at 1641198788733
sent 'asking for bwe' at 1641198788733
send [estimator, stat] at 1641198788733
wait for recv [self.estimator, stat] at 1641198788733
recved [self.estimator, stat] at 1641198788734
sorted packlist at 1641198788734
packetSeq:  6354
packetSeq:  6355
packetSeq:  6356
packetSeq:  6357
packetSeq:  6358
packetSeq:  6359
packetSeq:  6360
packetSeq:  6361
packetSeq:  6362
packetSeq:  6363
processed packlist at 1641198788734
receiving_rate:  276400.0
delay:  200.6
loss_ratio:  0.0
processed state0-2 at 1641198788734
avgFrameBetween:  6
psnrStat:  [[550138, 549266, 549949, 549153, 548870, 549176]]
delayStat:  [[68, 69, 68, 68, 68, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549425.3333333334] [65.83333333333333] [0]
processed state3-5 at 1641198788734
liner_to_log:  tensor([[[0.5102]]]) tensor([[[0.0430]]])
linear_to_log at 1641198788734
listState:  [0.0691, 0.13373333333333334, 0.0, 0.5494253333333333, 0.06583333333333333, 0.0, tensor([[[0.0430]]])]
state_clone_detach at 1641198788734
reward: 0.04556027728931661
state tensor([0.0691, 0.1337, 0.0000, 0.0430], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198788735
state222:  tensor([[0.0691, 0.1337, 0.0000, 0.0430]], device='cuda:0')
policy_old.forwarded at 1641198788737
give action 317============================
log_to_linear:  tensor([[[-0.0977]]], device='cuda:0') tensor([[[nan]]], device='cuda:0')
log_to_linear action at 1641198788738
bwe changes from to:  [tensor([[[0.0005]]]), tensor([[[0.0002]]])]
step into gymStat at 1641198788738
send bwe to appRecv at 1641198788738
sent bwe to appRecv at 1641198788738
wait for recv string at 1641198788738
recved string at 1641198788961
1
wait for recv [self.estimator, stat] at 1641198788961
recved [self.estimator, stat] at 1641198788961
sorted packlist at 1641198788961
packetSeq:  6364
packetSeq:  6365
packetSeq:  6366
packetSeq:  6367
packetSeq:  6368
packetSeq:  6369
packetSeq:  6370
packetSeq:  6371
packetSeq:  6372
packetSeq:  6373
processed packlist at 1641198788961
receiving_rate:  263680.0
delay:  197.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198788961
avgFrameBetween:  6
psnrStat:  [[547905, 548673, 547963, 548609, 547534, 547393, 548933]]
delayStat:  [[53, 54, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548144.2857142857] [53.142857142857146] [0]
processed state3-5 at 1641198788962
liner_to_log:  tensor([[[0.5000]]]) tensor([[[0.]]])
linear_to_log at 1641198788962
listState:  [0.06592, 0.13155555555555556, 0.0, 0.5481442857142856, 0.053142857142857144, 0.0, tensor([[[0.]]])]
state_clone_detach at 1641198788962
reward: 0.037417084707461246
state tensor([0.0659, 0.1316, 0.0000, 0.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198788963
state222:  tensor([[0.0659, 0.1316, 0.0000, 0.0000]], device='cuda:0')
policy_old.forwarded at 1641198788964
give action 318============================
log_to_linear:  tensor([[[0.6253]]], device='cuda:0') tensor([[[1.2126]]], device='cuda:0')
log_to_linear action at 1641198788965
bwe changes from to:  [tensor([[[0.0002]]]), tensor([[[0.0003]]])]
step into gymStat at 1641198788965
send bwe to appRecv at 1641198788965
sent bwe to appRecv at 1641198788965
wait for recv string at 1641198788965
recved string at 1641198789197
1
wait for recv [self.estimator, stat] at 1641198789197
recved [self.estimator, stat] at 1641198789197
sorted packlist at 1641198789197
packetSeq:  6374
packetSeq:  6375
packetSeq:  6376
packetSeq:  6377
packetSeq:  6378
packetSeq:  6379
packetSeq:  6380
packetSeq:  6381
packetSeq:  6382
packetSeq:  6383
packetSeq:  6384
processed packlist at 1641198789197
receiving_rate:  287280.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198789197
avgFrameBetween:  6
psnrStat:  [[547513, 547727, 547926, 547849, 547628, 548757, 549168]]
delayStat:  [[53, 53, 53, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548081.1428571428] [53.142857142857146] [0]
processed state3-5 at 1641198789197
liner_to_log:  tensor([[[1.2126]]]) tensor([[[0.6253]]])
linear_to_log at 1641198789197
listState:  [0.07182, 0.132, 0.0, 0.5480811428571428, 0.053142857142857144, 0.0, tensor([[[0.6253]]])]
state_clone_detach at 1641198789198
reward: 0.06303507361615768
state tensor([0.0718, 0.1320, 0.0000, 0.6253], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198789198
state222:  tensor([[0.0718, 0.1320, 0.0000, 0.6253]], device='cuda:0')
policy_old.forwarded at 1641198789200
give action 319============================
log_to_linear:  tensor([[[-0.1003]]], device='cuda:0') tensor([[[nan]]], device='cuda:0')
log_to_linear action at 1641198789200
bwe changes from to:  [tensor([[[0.0003]]]), tensor([[[0.0001]]])]
step into gymStat at 1641198789201
send bwe to appRecv at 1641198789201
sent bwe to appRecv at 1641198789201
wait for recv string at 1641198789201
recved string at 1641198789424
1
wait for recv [self.estimator, stat] at 1641198789424
recved [self.estimator, stat] at 1641198789424
sorted packlist at 1641198789424
packetSeq:  6385
packetSeq:  6386
packetSeq:  6387
packetSeq:  6388
packetSeq:  6389
packetSeq:  6390
packetSeq:  6391
packetSeq:  6392
packetSeq:  6393
packetSeq:  6394
processed packlist at 1641198789424
receiving_rate:  242200.0
delay:  196.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198789424
avgFrameBetween:  6
psnrStat:  [[548600, 549441, 549312, 549407, 550134, 549979, 550428]]
delayStat:  [[53, 53, 53, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549614.4285714285] [53.142857142857146] [0]
processed state3-5 at 1641198789424
liner_to_log:  tensor([[[0.5000]]]) tensor([[[0.]]])
linear_to_log at 1641198789425
listState:  [0.06055, 0.1308888888888889, 0.0, 0.5496144285714285, 0.053142857142857144, 0.0, tensor([[[0.]]])]
state_clone_detach at 1641198789425
reward: 0.013787865390158749
state tensor([0.0606, 0.1309, 0.0000, 0.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198789425
state222:  tensor([[0.0606, 0.1309, 0.0000, 0.0000]], device='cuda:0')
policy_old.forwarded at 1641198789427
give action 320============================
log_to_linear:  tensor([[[0.6262]]], device='cuda:0') tensor([[[1.2143]]], device='cuda:0')
log_to_linear action at 1641198789428
bwe changes from to:  [tensor([[[0.0001]]]), tensor([[[0.0002]]])]
step into gymStat at 1641198789428
send bwe to appRecv at 1641198789428
sent bwe to appRecv at 1641198789428
wait for recv string at 1641198789428
recved string at 1641198789627
1
wait for recv [self.estimator, stat] at 1641198789627
recved [self.estimator, stat] at 1641198789627
sorted packlist at 1641198789627
packetSeq:  6395
packetSeq:  6396
packetSeq:  6397
packetSeq:  6398
packetSeq:  6399
packetSeq:  6400
packetSeq:  6401
packetSeq:  6402
packetSeq:  6403
packetSeq:  6404
packetSeq:  6405
packetSeq:  6406
processed packlist at 1641198789627
receiving_rate:  303600.0
delay:  197.25
loss_ratio:  0.0
processed state0-2 at 1641198789627
avgFrameBetween:  6
psnrStat:  [[550476, 551963, 551051, 550838, 550832, 551068]]
delayStat:  [[53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551038.0] [53.0] [0]
processed state3-5 at 1641198789627
liner_to_log:  tensor([[[1.2143]]]) tensor([[[0.6262]]])
linear_to_log at 1641198789628
listState:  [0.0759, 0.1315, 0.0, 0.551038, 0.053, 0.0, tensor([[[0.6262]]])]
state_clone_detach at 1641198789628
reward: 0.08248889186472796
state tensor([0.0759, 0.1315, 0.0000, 0.6262], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198789628
state222:  tensor([[0.0759, 0.1315, 0.0000, 0.6262]], device='cuda:0')
policy_old.forwarded at 1641198789630
give action 321============================
log_to_linear:  tensor([[[0.0545]]], device='cuda:0') tensor([[[0.5149]]], device='cuda:0')
log_to_linear action at 1641198789631
bwe changes from to:  [tensor([[[0.0002]]]), tensor([[[8.8801e-05]]])]
step into gymStat at 1641198789631
send bwe to appRecv at 1641198789631
sent bwe to appRecv at 1641198789631
wait for recv string at 1641198789631
recved string at 1641198789833
1
wait for recv [self.estimator, stat] at 1641198789833
recved [self.estimator, stat] at 1641198789833
sorted packlist at 1641198789833
packetSeq:  6407
packetSeq:  6408
packetSeq:  6409
packetSeq:  6410
packetSeq:  6411
packetSeq:  6412
packetSeq:  6413
packetSeq:  6414
processed packlist at 1641198789833
receiving_rate:  259560.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198789833
avgFrameBetween:  6
psnrStat:  [[550784, 549681, 549451, 550546, 550204, 550908]]
delayStat:  [[53, 53, 54, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550262.3333333334] [53.333333333333336] [0]
processed state3-5 at 1641198789833
liner_to_log:  tensor([[[0.5149]]]) tensor([[[0.0545]]])
linear_to_log at 1641198789833
listState:  [0.06489, 0.132, 0.0, 0.5502623333333334, 0.05333333333333334, 0.0, tensor([[[0.0545]]])]
state_clone_detach at 1641198789834
reward: 0.031252014227603764
state tensor([0.0649, 0.1320, 0.0000, 0.0545], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198789834
state222:  tensor([[0.0649, 0.1320, 0.0000, 0.0545]], device='cuda:0')
policy_old.forwarded at 1641198789836
give action 322============================
log_to_linear:  tensor([[[0.4214]]], device='cuda:0') tensor([[[0.8812]]], device='cuda:0')
log_to_linear action at 1641198789837
bwe changes from to:  [tensor([[[8.8801e-05]]]), tensor([[[7.8255e-05]]])]
step into gymStat at 1641198789837
send bwe to appRecv at 1641198789837
sent bwe to appRecv at 1641198789837
wait for recv string at 1641198789837
recved string at 1641198790058
1
wait for recv [self.estimator, stat] at 1641198790058
recved [self.estimator, stat] at 1641198790058
sorted packlist at 1641198790058
packetSeq:  6415
packetSeq:  6416
packetSeq:  6417
packetSeq:  6418
packetSeq:  6419
packetSeq:  6420
packetSeq:  6421
packetSeq:  6422
packetSeq:  6423
packetSeq:  6424
processed packlist at 1641198790058
receiving_rate:  247960.0
delay:  196.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198790058
avgFrameBetween:  6
psnrStat:  [[552807, 553063, 553893, 554058, 554510, 554244, 553583]]
delayStat:  [[53, 53, 53, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553736.8571428572] [53.142857142857146] [0]
processed state3-5 at 1641198790058
liner_to_log:  tensor([[[0.8812]]]) tensor([[[0.4214]]])
linear_to_log at 1641198790058
listState:  [0.06199, 0.13118518518518518, 0.0, 0.5537368571428571, 0.053142857142857144, 0.0, tensor([[[0.4214]]])]
state_clone_detach at 1641198790059
reward: 0.019879745235247714
state tensor([0.0620, 0.1312, 0.0000, 0.4214], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198790059
state222:  tensor([[0.0620, 0.1312, 0.0000, 0.4214]], device='cuda:0')
policy_old.forwarded at 1641198790061
give action 323============================
log_to_linear:  tensor([[[0.3598]]], device='cuda:0') tensor([[[0.7967]]], device='cuda:0')
log_to_linear action at 1641198790062
bwe changes from to:  [tensor([[[7.8255e-05]]]), tensor([[[6.2349e-05]]])]
step into gymStat at 1641198790062
send bwe to appRecv at 1641198790062
sent bwe to appRecv at 1641198790062
wait for recv string at 1641198790062
recved string at 1641198790260
1
wait for recv [self.estimator, stat] at 1641198790260
recved [self.estimator, stat] at 1641198790260
sorted packlist at 1641198790260
packetSeq:  6425
packetSeq:  6426
packetSeq:  6427
packetSeq:  6428
packetSeq:  6429
packetSeq:  6430
packetSeq:  6431
packetSeq:  6432
packetSeq:  6433
processed packlist at 1641198790260
receiving_rate:  277480.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198790260
avgFrameBetween:  6
psnrStat:  [[553497, 553916, 555069, 555376, 555640, 556284]]
delayStat:  [[53, 54, 53, 54, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554963.6666666666] [53.5] [0]
processed state3-5 at 1641198790260
liner_to_log:  tensor([[[0.7967]]]) tensor([[[0.3598]]])
linear_to_log at 1641198790261
listState:  [0.06937, 0.132, 0.0, 0.5549636666666666, 0.0535, 0.0, tensor([[[0.3598]]])]
state_clone_detach at 1641198790261
reward: 0.05199001924635105
state tensor([0.0694, 0.1320, 0.0000, 0.3598], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198790262
state222:  tensor([[0.0694, 0.1320, 0.0000, 0.3598]], device='cuda:0')
policy_old.forwarded at 1641198790263
give action 324============================
log_to_linear:  tensor([[[0.3562]]], device='cuda:0') tensor([[[0.7922]]], device='cuda:0')
log_to_linear action at 1641198790264
bwe changes from to:  [tensor([[[6.2349e-05]]]), tensor([[[4.9390e-05]]])]
step into gymStat at 1641198790264
send bwe to appRecv at 1641198790264
sent bwe to appRecv at 1641198790264
wait for recv string at 1641198790264
recved string at 1641198790467
1
wait for recv [self.estimator, stat] at 1641198790467
recved [self.estimator, stat] at 1641198790467
sorted packlist at 1641198790467
packetSeq:  6434
packetSeq:  6435
packetSeq:  6436
packetSeq:  6437
packetSeq:  6438
packetSeq:  6439
packetSeq:  6440
packetSeq:  6441
packetSeq:  6442
processed packlist at 1641198790467
receiving_rate:  263360.0
delay:  197.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198790467
avgFrameBetween:  6
psnrStat:  [[556744, 556866, 558092, 557580, 557139, 557429]]
delayStat:  [[53, 53, 54, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557308.3333333334] [53.333333333333336] [0]
processed state3-5 at 1641198790467
liner_to_log:  tensor([[[0.7922]]]) tensor([[[0.3562]]])
linear_to_log at 1641198790467
listState:  [0.06584, 0.13155555555555556, 0.0, 0.5573083333333334, 0.05333333333333334, 0.0, tensor([[[0.3562]]])]
state_clone_detach at 1641198790468
reward: 0.03704319468134143
state tensor([0.0658, 0.1316, 0.0000, 0.3562], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198790468
state222:  tensor([[0.0658, 0.1316, 0.0000, 0.3562]], device='cuda:0')
policy_old.forwarded at 1641198790470
give action 325============================
log_to_linear:  tensor([[[0.3222]]], device='cuda:0') tensor([[[0.7492]]], device='cuda:0')
log_to_linear action at 1641198790471
bwe changes from to:  [tensor([[[4.9390e-05]]]), tensor([[[3.7004e-05]]])]
step into gymStat at 1641198790471
send bwe to appRecv at 1641198790471
sent bwe to appRecv at 1641198790471
wait for recv string at 1641198790471
recved string at 1641198790668
1
wait for recv [self.estimator, stat] at 1641198790668
recved [self.estimator, stat] at 1641198790668
sorted packlist at 1641198790668
packetSeq:  6443
packetSeq:  6444
packetSeq:  6445
packetSeq:  6446
packetSeq:  6447
packetSeq:  6448
packetSeq:  6449
packetSeq:  6450
packetSeq:  6451
packetSeq:  6452
packetSeq:  6453
processed packlist at 1641198790668
receiving_rate:  297320.0
delay:  197.63636363636363
loss_ratio:  0.0
processed state0-2 at 1641198790668
avgFrameBetween:  6
psnrStat:  [[557782, 558407, 558217, 559476, 559163, 559586, 559455]]
delayStat:  [[54, 53, 53, 53, 53, 53, 52]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558869.4285714285] [53.0] [0]
processed state3-5 at 1641198790668
liner_to_log:  tensor([[[0.7492]]]) tensor([[[0.3222]]])
linear_to_log at 1641198790669
listState:  [0.07433, 0.13175757575757574, 0.0, 0.5588694285714285, 0.053, 0.0, tensor([[[0.3222]]])]
state_clone_detach at 1641198790669
reward: 0.07487090631689047
state tensor([0.0743, 0.1318, 0.0000, 0.3222], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198790669
state222:  tensor([[0.0743, 0.1318, 0.0000, 0.3222]], device='cuda:0')
policy_old.forwarded at 1641198790671
give action 326============================
log_to_linear:  tensor([[[0.7627]]], device='cuda:0') tensor([[[1.4763]]], device='cuda:0')
log_to_linear action at 1641198790672
bwe changes from to:  [tensor([[[3.7004e-05]]]), tensor([[[5.4631e-05]]])]
step into gymStat at 1641198790672
send bwe to appRecv at 1641198790672
sent bwe to appRecv at 1641198790672
wait for recv string at 1641198790672
recved string at 1641198790893
1
wait for recv [self.estimator, stat] at 1641198790893
recved [self.estimator, stat] at 1641198790893
sorted packlist at 1641198790893
packetSeq:  6454
packetSeq:  6455
packetSeq:  6456
packetSeq:  6457
packetSeq:  6458
packetSeq:  6459
packetSeq:  6460
packetSeq:  6461
packetSeq:  6462
packetSeq:  6463
processed packlist at 1641198790893
receiving_rate:  242840.0
delay:  196.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198790893
avgFrameBetween:  6
psnrStat:  [[559017, 558258, 559176, 558706, 558560, 559013]]
delayStat:  [[53, 52, 53, 54, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558788.3333333334] [53.166666666666664] [0]
processed state3-5 at 1641198790893
liner_to_log:  tensor([[[1.4763]]]) tensor([[[0.7627]]])
linear_to_log at 1641198790894
listState:  [0.06071, 0.13125925925925927, 0.0, 0.5587883333333333, 0.05316666666666667, 0.0, tensor([[[0.7627]]])]
state_clone_detach at 1641198790894
reward: 0.013456418158754868
state tensor([0.0607, 0.1313, 0.0000, 0.7627], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198790895
state222:  tensor([[0.0607, 0.1313, 0.0000, 0.7627]], device='cuda:0')
policy_old.forwarded at 1641198790896
give action 327============================
log_to_linear:  tensor([[[0.3402]]], device='cuda:0') tensor([[[0.7716]]], device='cuda:0')
log_to_linear action at 1641198790897
bwe changes from to:  [tensor([[[5.4631e-05]]]), tensor([[[4.2154e-05]]])]
step into gymStat at 1641198790898
send bwe to appRecv at 1641198790898
sent bwe to appRecv at 1641198790898
wait for recv string at 1641198790898
recved string at 1641198791125
1
wait for recv [self.estimator, stat] at 1641198791125
recved [self.estimator, stat] at 1641198791125
sorted packlist at 1641198791125
packetSeq:  6464
packetSeq:  6465
packetSeq:  6466
packetSeq:  6467
packetSeq:  6468
packetSeq:  6469
packetSeq:  6470
packetSeq:  6471
packetSeq:  6472
packetSeq:  6473
packetSeq:  6474
processed packlist at 1641198791125
receiving_rate:  246960.0
delay:  196.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198791125
avgFrameBetween:  6
psnrStat:  [[558358, 559034, 559897, 559878, 561163, 561111, 562151]]
delayStat:  [[52, 53, 53, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560227.4285714285] [53.0] [0]
processed state3-5 at 1641198791125
liner_to_log:  tensor([[[0.7716]]]) tensor([[[0.3402]]])
linear_to_log at 1641198791125
listState:  [0.06174, 0.1308888888888889, 0.0, 0.5602274285714285, 0.053, 0.0, tensor([[[0.3402]]])]
state_clone_detach at 1641198791126
reward: 0.01956250659217884
state tensor([0.0617, 0.1309, 0.0000, 0.3402], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198791126
state222:  tensor([[0.0617, 0.1309, 0.0000, 0.3402]], device='cuda:0')
policy_old.forwarded at 1641198791128
give action 328============================
log_to_linear:  tensor([[[0.7652]]], device='cuda:0') tensor([[[1.4815]]], device='cuda:0')
log_to_linear action at 1641198791129
bwe changes from to:  [tensor([[[4.2154e-05]]]), tensor([[[6.2449e-05]]])]
step into gymStat at 1641198791129
send bwe to appRecv at 1641198791129
sent bwe to appRecv at 1641198791129
wait for recv string at 1641198791129
recved string at 1641198791332
1
wait for recv [self.estimator, stat] at 1641198791332
recved [self.estimator, stat] at 1641198791332
sorted packlist at 1641198791332
packetSeq:  6475
packetSeq:  6476
packetSeq:  6477
packetSeq:  6478
packetSeq:  6479
packetSeq:  6480
packetSeq:  6481
packetSeq:  6482
packetSeq:  6483
packetSeq:  6484
packetSeq:  6485
packetSeq:  6486
packetSeq:  6487
processed packlist at 1641198791332
receiving_rate:  326160.0
delay:  197.0
loss_ratio:  0.0
processed state0-2 at 1641198791332
avgFrameBetween:  6
psnrStat:  [[561923, 563196, 562337, 562011, 561899, 561983]]
delayStat:  [[53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [562224.8333333334] [53.0] [0]
processed state3-5 at 1641198791332
liner_to_log:  tensor([[[1.4815]]]) tensor([[[0.7652]]])
linear_to_log at 1641198791333
listState:  [0.08154, 0.13133333333333333, 0.0, 0.5622248333333334, 0.053, 0.0, tensor([[[0.7652]]])]
state_clone_detach at 1641198791333
reward: 0.10695468922299123
state tensor([0.0815, 0.1313, 0.0000, 0.7652], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198791333
state222:  tensor([[0.0815, 0.1313, 0.0000, 0.7652]], device='cuda:0')
policy_old.forwarded at 1641198791335
give action 329============================
log_to_linear:  tensor([[[0.3731]]], device='cuda:0') tensor([[[0.8144]]], device='cuda:0')
log_to_linear action at 1641198791336
bwe changes from to:  [tensor([[[6.2449e-05]]]), tensor([[[5.0859e-05]]])]
step into gymStat at 1641198791337
send bwe to appRecv at 1641198791337
sent bwe to appRecv at 1641198791337
wait for recv string at 1641198791337
recved string at 1641198791537
1
wait for recv [self.estimator, stat] at 1641198791537
recved [self.estimator, stat] at 1641198791537
sorted packlist at 1641198791537
packetSeq:  6488
packetSeq:  6489
packetSeq:  6490
packetSeq:  6491
packetSeq:  6492
packetSeq:  6493
packetSeq:  6494
packetSeq:  6495
packetSeq:  6496
processed packlist at 1641198791537
receiving_rate:  266760.0
delay:  197.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198791537
avgFrameBetween:  6
psnrStat:  [[560875, 560671, 559616, 559996, 558888, 558223, 557373]]
delayStat:  [[53, 53, 53, 54, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559377.4285714285] [53.57142857142857] [0]
processed state3-5 at 1641198791537
liner_to_log:  tensor([[[0.8144]]]) tensor([[[0.3731]]])
linear_to_log at 1641198791537
listState:  [0.06669, 0.13185185185185186, 0.0, 0.5593774285714285, 0.05357142857142857, 0.0, tensor([[[0.3731]]])]
state_clone_detach at 1641198791538
reward: 0.04011502378235687
state tensor([0.0667, 0.1319, 0.0000, 0.3731], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198791538
state222:  tensor([[0.0667, 0.1319, 0.0000, 0.3731]], device='cuda:0')
policy_old.forwarded at 1641198791540
give action 330============================
log_to_linear:  tensor([[[0.3763]]], device='cuda:0') tensor([[[0.8187]]], device='cuda:0')
log_to_linear action at 1641198791541
bwe changes from to:  [tensor([[[5.0859e-05]]]), tensor([[[4.1636e-05]]])]
step into gymStat at 1641198791541
send bwe to appRecv at 1641198791541
sent bwe to appRecv at 1641198791541
wait for recv string at 1641198791541
recved string at 1641198791763
1
wait for recv [self.estimator, stat] at 1641198791763
recved [self.estimator, stat] at 1641198791764
sorted packlist at 1641198791764
packetSeq:  6497
packetSeq:  6498
packetSeq:  6499
packetSeq:  6500
packetSeq:  6501
packetSeq:  6502
packetSeq:  6503
packetSeq:  6504
packetSeq:  6505
packetSeq:  6506
packetSeq:  6507
processed packlist at 1641198791764
receiving_rate:  315200.0
delay:  198.6
loss_ratio:  0.0
processed state0-2 at 1641198791764
avgFrameBetween:  6
psnrStat:  [[557775, 557731, 556879, 555588, 556346, 554268]]
delayStat:  [[54, 54, 54, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556431.1666666666] [54.0] [0]
processed state3-5 at 1641198791764
liner_to_log:  tensor([[[0.8187]]]) tensor([[[0.3763]]])
linear_to_log at 1641198791764
listState:  [0.0788, 0.1324, 0.0, 0.5564311666666666, 0.054, 0.0, tensor([[[0.3763]]])]
state_clone_detach at 1641198791764
reward: 0.09223123976887548
state tensor([0.0788, 0.1324, 0.0000, 0.3763], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198791765
state222:  tensor([[0.0788, 0.1324, 0.0000, 0.3763]], device='cuda:0')
policy_old.forwarded at 1641198791767
give action 331============================
log_to_linear:  tensor([[[0.2392]]], device='cuda:0') tensor([[[0.6554]]], device='cuda:0')
log_to_linear action at 1641198791768
bwe changes from to:  [tensor([[[4.1636e-05]]]), tensor([[[2.7288e-05]]])]
step into gymStat at 1641198791768
send bwe to appRecv at 1641198791768
sent bwe to appRecv at 1641198791768
wait for recv string at 1641198791768
recved string at 1641198791995
1
wait for recv [self.estimator, stat] at 1641198791995
recved [self.estimator, stat] at 1641198791995
sorted packlist at 1641198791995
packetSeq:  6508
packetSeq:  6509
packetSeq:  6510
packetSeq:  6511
packetSeq:  6512
packetSeq:  6513
packetSeq:  6514
packetSeq:  6515
processed packlist at 1641198791995
receiving_rate:  230240.0
delay:  197.71428571428572
loss_ratio:  0.0
processed state0-2 at 1641198791995
avgFrameBetween:  6
psnrStat:  [[555137, 554432, 554617, 554397, 554606, 554836, 556040]]
delayStat:  [[54, 54, 55, 54, 55, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554866.4285714285] [54.42857142857143] [0]
processed state3-5 at 1641198791995
liner_to_log:  tensor([[[0.6554]]]) tensor([[[0.2392]]])
linear_to_log at 1641198791995
listState:  [0.05756, 0.13180952380952382, 0.0, 0.5548664285714285, 0.05442857142857143, 0.0, tensor([[[0.2392]]])]
state_clone_detach at 1641198791996
reward: -0.003733534779766734
state tensor([0.0576, 0.1318, 0.0000, 0.2392], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198791996
state222:  tensor([[0.0576, 0.1318, 0.0000, 0.2392]], device='cuda:0')
policy_old.forwarded at 1641198791998
give action 332============================
log_to_linear:  tensor([[[0.2966]]], device='cuda:0') tensor([[[0.7185]]], device='cuda:0')
log_to_linear action at 1641198791999
bwe changes from to:  [tensor([[[2.7288e-05]]]), tensor([[[1.9607e-05]]])]
step into gymStat at 1641198791999
send bwe to appRecv at 1641198791999
sent bwe to appRecv at 1641198791999
wait for recv string at 1641198791999
recved string at 1641198792225
1
wait for recv [self.estimator, stat] at 1641198792225
recved [self.estimator, stat] at 1641198792226
sorted packlist at 1641198792226
packetSeq:  6516
packetSeq:  6517
packetSeq:  6518
packetSeq:  6519
packetSeq:  6520
packetSeq:  6521
packetSeq:  6522
packetSeq:  6523
packetSeq:  6524
processed packlist at 1641198792226
receiving_rate:  273640.0
delay:  197.625
loss_ratio:  0.0
processed state0-2 at 1641198792226
avgFrameBetween:  6
psnrStat:  [[556771, 556951, 557790, 557306, 558402, 558495, 558393]]
delayStat:  [[54, 54, 54, 55, 54, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557729.7142857143] [54.285714285714285] [0]
processed state3-5 at 1641198792226
liner_to_log:  tensor([[[0.7185]]]) tensor([[[0.2966]]])
linear_to_log at 1641198792226
listState:  [0.06841, 0.13175, 0.0, 0.5577297142857143, 0.054285714285714284, 0.0, tensor([[[0.2966]]])]
state_clone_detach at 1641198792226
reward: 0.048356104469791106
state tensor([0.0684, 0.1318, 0.0000, 0.2966], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198792227
state222:  tensor([[0.0684, 0.1318, 0.0000, 0.2966]], device='cuda:0')
policy_old.forwarded at 1641198792229
give action 333============================
log_to_linear:  tensor([[[0.6359]]], device='cuda:0') tensor([[[1.2320]]], device='cuda:0')
log_to_linear action at 1641198792229
bwe changes from to:  [tensor([[[1.9607e-05]]]), tensor([[[2.4155e-05]]])]
step into gymStat at 1641198792230
send bwe to appRecv at 1641198792230
sent bwe to appRecv at 1641198792230
wait for recv string at 1641198792230
recved string at 1641198792458
1
wait for recv [self.estimator, stat] at 1641198792458
recved [self.estimator, stat] at 1641198792458
sorted packlist at 1641198792458
packetSeq:  6525
packetSeq:  6526
packetSeq:  6527
packetSeq:  6528
packetSeq:  6529
packetSeq:  6530
packetSeq:  6531
packetSeq:  6532
packetSeq:  6533
packetSeq:  6534
packetSeq:  6535
processed packlist at 1641198792458
receiving_rate:  263680.0
delay:  197.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198792458
avgFrameBetween:  6
psnrStat:  [[557595, 557900, 557659, 557167, 557700, 557058, 557656]]
delayStat:  [[54, 54, 54, 54, 54, 54, 56]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557533.5714285715] [54.285714285714285] [0]
processed state3-5 at 1641198792458
liner_to_log:  tensor([[[1.2320]]]) tensor([[[0.6359]]])
linear_to_log at 1641198792459
listState:  [0.06592, 0.13162962962962962, 0.0, 0.5575335714285715, 0.054285714285714284, 0.0, tensor([[[0.6359]]])]
state_clone_detach at 1641198792459
reward: 0.03719486248523901
state tensor([0.0659, 0.1316, 0.0000, 0.6359], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198792460
state222:  tensor([[0.0659, 0.1316, 0.0000, 0.6359]], device='cuda:0')
policy_old.forwarded at 1641198792461
give action 334============================
log_to_linear:  tensor([[[0.3422]]], device='cuda:0') tensor([[[0.7741]]], device='cuda:0')
log_to_linear action at 1641198792462
bwe changes from to:  [tensor([[[2.4155e-05]]]), tensor([[[1.8699e-05]]])]
step into gymStat at 1641198792462
send bwe to appRecv at 1641198792462
sent bwe to appRecv at 1641198792462
wait for recv string at 1641198792462
recved string at 1641198792664
1
wait for recv [self.estimator, stat] at 1641198792664
recved [self.estimator, stat] at 1641198792664
sorted packlist at 1641198792664
packetSeq:  6536
packetSeq:  6537
packetSeq:  6538
packetSeq:  6539
packetSeq:  6540
packetSeq:  6541
packetSeq:  6542
packetSeq:  6543
processed packlist at 1641198792664
receiving_rate:  271720.0
delay:  197.875
loss_ratio:  0.0
processed state0-2 at 1641198792664
avgFrameBetween:  6
psnrStat:  [[557898, 558837, 558705, 559228, 559770, 559637]]
delayStat:  [[54, 54, 55, 55, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559012.5] [54.5] [0]
processed state3-5 at 1641198792664
liner_to_log:  tensor([[[0.7741]]]) tensor([[[0.3422]]])
linear_to_log at 1641198792665
listState:  [0.06793, 0.13191666666666665, 0.0, 0.5590125, 0.0545, 0.0, tensor([[[0.3422]]])]
state_clone_detach at 1641198792665
reward: 0.045652076583901524
state tensor([0.0679, 0.1319, 0.0000, 0.3422], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198792666
state222:  tensor([[0.0679, 0.1319, 0.0000, 0.3422]], device='cuda:0')
policy_old.forwarded at 1641198792667
give action 335============================
log_to_linear:  tensor([[[0.1707]]], device='cuda:0') tensor([[[0.5910]]], device='cuda:0')
log_to_linear action at 1641198792668
bwe changes from to:  [tensor([[[1.8699e-05]]]), tensor([[[1.1051e-05]]])]
step into gymStat at 1641198792669
send bwe to appRecv at 1641198792669
sent bwe to appRecv at 1641198792669
wait for recv string at 1641198792669
recved string at 1641198792886
1
wait for recv [self.estimator, stat] at 1641198792886
recved [self.estimator, stat] at 1641198792886
sorted packlist at 1641198792886
packetSeq:  6544
packetSeq:  6545
packetSeq:  6546
packetSeq:  6547
packetSeq:  6548
packetSeq:  6549
packetSeq:  6550
packetSeq:  6551
packetSeq:  6552
packetSeq:  6553
processed packlist at 1641198792886
receiving_rate:  295840.0
delay:  195.7
loss_ratio:  0.0
processed state0-2 at 1641198792886
avgFrameBetween:  6
psnrStat:  [[559984, 560028, 560583, 560488, 560283, 559862, 559990]]
delayStat:  [[54, 55, 54, 54, 54, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560174.0] [54.285714285714285] [0]
processed state3-5 at 1641198792886
liner_to_log:  tensor([[[0.5910]]]) tensor([[[0.1707]]])
linear_to_log at 1641198792887
listState:  [0.07396, 0.13046666666666665, 0.0, 0.560174, 0.054285714285714284, 0.0, tensor([[[0.1707]]])]
state_clone_detach at 1641198792887
reward: 0.07711898478695306
state tensor([0.0740, 0.1305, 0.0000, 0.1707], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198792888
state222:  tensor([[0.0740, 0.1305, 0.0000, 0.1707]], device='cuda:0')
policy_old.forwarded at 1641198792889
give action 336============================
log_to_linear:  tensor([[[0.4116]]], device='cuda:0') sent [estimator, stat] at 1641198788733
pc wait for bwe at 1641198788734
pc got bwe at 1641198788738
bandwidth:  300000
pc flushed at 1641198788738
Bwe Sent: 5 at 1641198788738
got request at 1641198788961
processed allFrame at 1641198788961
send 'asking for bwe' at 1641198788961
sent 'asking for bwe' at 1641198788961
send [estimator, stat] at 1641198788961
sent [estimator, stat] at 1641198788961
pc wait for bwe at 1641198788961
pc got bwe at 1641198788966
bandwidth:  300000
pc flushed at 1641198788966
Bwe Sent: 5 at 1641198788966
got request at 1641198789196
processed allFrame at 1641198789196
send 'asking for bwe' at 1641198789196
sent 'asking for bwe' at 1641198789197
send [estimator, stat] at 1641198789197
sent [estimator, stat] at 1641198789197
pc wait for bwe at 1641198789197
pc got bwe at 1641198789201
bandwidth:  300000
pc flushed at 1641198789201
Bwe Sent: 5 at 1641198789201
got request at 1641198789424
processed allFrame at 1641198789424
send 'asking for bwe' at 1641198789424
sent 'asking for bwe' at 1641198789424
send [estimator, stat] at 1641198789424
sent [estimator, stat] at 1641198789424
pc wait for bwe at 1641198789424
pc got bwe at 1641198789428
bandwidth:  300000
pc flushed at 1641198789428
Bwe Sent: 4 at 1641198789428
got request at 1641198789627
processed allFrame at 1641198789627
send 'asking for bwe' at 1641198789627
sent 'asking for bwe' at 1641198789627
send [estimator, stat] at 1641198789627
sent [estimator, stat] at 1641198789627
pc wait for bwe at 1641198789627
pc got bwe at 1641198789631
bandwidth:  300000
pc flushed at 1641198789631
Bwe Sent: 4 at 1641198789631
got request at 1641198789833
processed allFrame at 1641198789833
send 'asking for bwe' at 1641198789833
sent 'asking for bwe' at 1641198789833
send [estimator, stat] at 1641198789833
sent [estimator, stat] at 1641198789833
pc wait for bwe at 1641198789833
pc got bwe at 1641198789837
bandwidth:  300000
pc flushed at 1641198789837
Bwe Sent: 4 at 1641198789837
got request at 1641198790058
processed allFrame at 1641198790058
send 'asking for bwe' at 1641198790058
sent 'asking for bwe' at 1641198790058
send [estimator, stat] at 1641198790058
sent [estimator, stat] at 1641198790058
pc wait for bwe at 1641198790058
pc got bwe at 1641198790062
bandwidth:  300000
pc flushed at 1641198790062
Bwe Sent: 4 at 1641198790062
got request at 1641198790260
processed allFrame at 1641198790260
send 'asking for bwe' at 1641198790260
sent 'asking for bwe' at 1641198790260
send [estimator, stat] at 1641198790260
sent [estimator, stat] at 1641198790260
pc wait for bwe at 1641198790260
pc got bwe at 1641198790264
bandwidth:  300000
pc flushed at 1641198790265
Bwe Sent: 5 at 1641198790265
got request at 1641198790466
processed allFrame at 1641198790466
send 'asking for bwe' at 1641198790466
sent 'asking for bwe' at 1641198790467
send [estimator, stat] at 1641198790467
sent [estimator, stat] at 1641198790467
pc wait for bwe at 1641198790467
pc got bwe at 1641198790471
bandwidth:  300000
pc flushed at 1641198790471
Bwe Sent: 5 at 1641198790471
got request at 1641198790668
processed allFrame at 1641198790668
send 'asking for bwe' at 1641198790668
sent 'asking for bwe' at 1641198790668
send [estimator, stat] at 1641198790668
sent [estimator, stat] at 1641198790668
pc wait for bwe at 1641198790668
pc got bwe at 1641198790672
bandwidth:  300000
pc flushed at 1641198790672
Bwe Sent: 4 at 1641198790672
got request at 1641198790893
processed allFrame at 1641198790893
send 'asking for bwe' at 1641198790893
sent 'asking for bwe' at 1641198790893
send [estimator, stat] at 1641198790893
sent [estimator, stat] at 1641198790893
pc wait for bwe at 1641198790893
pc got bwe at 1641198790898
bandwidth:  300000
pc flushed at 1641198790898
Bwe Sent: 5 at 1641198790898
got request at 1641198791125
processed allFrame at 1641198791125
send 'asking for bwe' at 1641198791125
sent 'asking for bwe' at 1641198791125
send [estimator, stat] at 1641198791125
sent [estimator, stat] at 1641198791125
pc wait for bwe at 1641198791125
pc got bwe at 1641198791129
bandwidth:  300000
pc flushed at 1641198791129
Bwe Sent: 4 at 1641198791129
got request at 1641198791332
processed allFrame at 1641198791332
send 'asking for bwe' at 1641198791332
sent 'asking for bwe' at 1641198791332
send [estimator, stat] at 1641198791332
sent [estimator, stat] at 1641198791332
pc wait for bwe at 1641198791332
pc got bwe at 1641198791337
bandwidth:  300000
pc flushed at 1641198791337
Bwe Sent: 5 at 1641198791337
got request at 1641198791537
processed allFrame at 1641198791537
send 'asking for bwe' at 1641198791537
sent 'asking for bwe' at 1641198791537
send [estimator, stat] at 1641198791537
sent [estimator, stat] at 1641198791537
pc wait for bwe at 1641198791537
pc got bwe at 1641198791541
bandwidth:  300000
pc flushed at 1641198791541
Bwe Sent: 4 at 1641198791541
got request at 1641198791763
processed allFrame at 1641198791763
send 'asking for bwe' at 1641198791763
sent 'asking for bwe' at 1641198791763
send [estimator, stat] at 1641198791763
sent [estimator, stat] at 1641198791763
pc wait for bwe at 1641198791764
pc got bwe at 1641198791768
bandwidth:  300000
pc flushed at 1641198791768
Bwe Sent: 5 at 1641198791768
got request at 1641198791995
processed allFrame at 1641198791995
send 'asking for bwe' at 1641198791995
sent 'asking for bwe' at 1641198791995
send [estimator, stat] at 1641198791995
sent [estimator, stat] at 1641198791995
pc wait for bwe at 1641198791995
pc got bwe at 1641198791999
bandwidth:  300000
pc flushed at 1641198791999
Bwe Sent: 4 at 1641198791999
got request at 1641198792225
processed allFrame at 1641198792225
send 'asking for bwe' at 1641198792225
sent 'asking for bwe' at 1641198792225
send [estimator, stat] at 1641198792225
sent [estimator, stat] at 1641198792225
pc wait for bwe at 1641198792226
pc got bwe at 1641198792230
bandwidth:  300000
pc flushed at 1641198792230
Bwe Sent: 5 at 1641198792230
got request at 1641198792458
processed allFrame at 1641198792458
send 'asking for bwe' at 1641198792458
sent 'asking for bwe' at 1641198792458
send [estimator, stat] at 1641198792458
sent [estimator, stat] at 1641198792458
pc wait for bwe at 1641198792458
pc got bwe at 1641198792462
bandwidth:  300000
pc flushed at 1641198792463
Bwe Sent: 5 at 1641198792463
got request at 1641198792664
processed allFrame at 1641198792664
send 'asking for bwe' at 1641198792664
sent 'asking for bwe' at 1641198792664
send [estimator, stat] at 1641198792664
sent [estimator, stat] at 1641198792664
pc wait for bwe at 1641198792664
pc got bwe at 1641198792669
bandwidth:  300000
pc flushed at 1641198792669
Bwe Sent: 5 at 1641198792669
got request at 1641198792886
processed allFrame at 1641198792886
send 'asking for bwe' at 1641198792886
sent 'asking for bwe' at 1641198792886
send [estimator, stat] at 1641198792886
sent [estimator, stat] at 1641198792886
pc wait for bwe at 1641198792886
pc got bwe at 1641198792891
bandwidth:  300000
pc flushed at 1641198792891
Bwe Sent: 5 at 1641198792891
got request at 1641198793087
processed allFrame at 1641198793087
send 'asking for bwe' at 1641198793087
sent 'asking for bwe' at 1641198793087
send [estimator, stat] at 1641198793087
sent [estimator, stat] at 1641198793087
pc wait for bwe at 1641198793087
pc got bwe at 1641198793091
bandwidth:  300000
pc flushed at 1641198793091
Bwe Sent: 4 at 1641198793091
got request at 1641198793290
processed allFrame at 1641198793290
send 'asking for bwe' at 1641198793290
sent 'asking for bwe' at 1641198793290
send [estimator, stat] at 1641198793290
sent [estimator, stat] at 1641198793290
pc wait for bwe at 1641198793290
pc got bwe at 1641198793294
bandwidth:  300000
pc flushed at 1641198793294
Bwe Sent: 4 at 1641198793294
got request at 1641198793522
processed allFrame at 1641198793522
send 'asking for bwe' at 1641198793522
sent 'asking for bwe' at 1641198793522
send [estimator, stat] at 1641198793522
sent [estimator, stat] at 1641198793522
pc wait for bwe at 1641198793522
pc got bwe at 1641198793526
bandwidth:  300000
pc flushed at 1641198793527
Bwe Sent: 5 at 1641198793527
got request at 1641198793725
processed allFrame at 1641198793725
tensor([[[0.8674]]], device='cuda:0')
log_to_linear action at 1641198792890
bwe changes from to:  [tensor([[[1.1051e-05]]]), tensor([[[9.5851e-06]]])]
step into gymStat at 1641198792890
send bwe to appRecv at 1641198792890
sent bwe to appRecv at 1641198792891
wait for recv string at 1641198792891
recved string at 1641198793087
1
wait for recv [self.estimator, stat] at 1641198793087
recved [self.estimator, stat] at 1641198793087
sorted packlist at 1641198793087
packetSeq:  6554
packetSeq:  6555
packetSeq:  6556
packetSeq:  6557
packetSeq:  6558
packetSeq:  6559
packetSeq:  6560
packetSeq:  6561
packetSeq:  6562
packetSeq:  6563
packetSeq:  6564
processed packlist at 1641198793087
receiving_rate:  307080.0
delay:  191.72727272727272
loss_ratio:  0.0
processed state0-2 at 1641198793087
avgFrameBetween:  6
psnrStat:  [[559170, 559583, 558631, 559032, 558607, 558600]]
delayStat:  [[55, 54, 54, 55, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558937.1666666666] [54.5] [0]
processed state3-5 at 1641198793087
liner_to_log:  tensor([[[0.8674]]]) tensor([[[0.4116]]])
linear_to_log at 1641198793087
listState:  [0.07677, 0.1278181818181818, 0.0, 0.5589371666666666, 0.0545, 0.0, tensor([[[0.4116]]])]
state_clone_detach at 1641198793088
reward: 0.09729421753904144
state tensor([0.0768, 0.1278, 0.0000, 0.4116], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198793088
state222:  tensor([[0.0768, 0.1278, 0.0000, 0.4116]], device='cuda:0')
policy_old.forwarded at 1641198793090
give action 337============================
log_to_linear:  tensor([[[0.4695]]], device='cuda:0') tensor([[[0.9526]]], device='cuda:0')
log_to_linear action at 1641198793091
bwe changes from to:  [tensor([[[9.5851e-06]]]), tensor([[[9.1305e-06]]])]
step into gymStat at 1641198793091
send bwe to appRecv at 1641198793091
sent bwe to appRecv at 1641198793091
wait for recv string at 1641198793091
recved string at 1641198793290
1
wait for recv [self.estimator, stat] at 1641198793290
recved [self.estimator, stat] at 1641198793290
sorted packlist at 1641198793290
packetSeq:  6565
packetSeq:  6566
packetSeq:  6567
packetSeq:  6568
packetSeq:  6569
packetSeq:  6570
packetSeq:  6571
packetSeq:  6572
processed packlist at 1641198793290
receiving_rate:  258399.99999999997
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198793290
avgFrameBetween:  6
psnrStat:  [[558059, 557482, 557308, 556647, 556661, 556981]]
delayStat:  [[54, 55, 54, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [557189.6666666666] [54.333333333333336] [0]
processed state3-5 at 1641198793290
liner_to_log:  tensor([[[0.9526]]]) tensor([[[0.4695]]])
linear_to_log at 1641198793291
listState:  [0.06459999999999999, 0.12833333333333333, 0.0, 0.5571896666666666, 0.05433333333333334, 0.0, tensor([[[0.4695]]])]
state_clone_detach at 1641198793291
reward: 0.04088457287183145
state tensor([0.0646, 0.1283, 0.0000, 0.4695], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198793291
state222:  tensor([[0.0646, 0.1283, 0.0000, 0.4695]], device='cuda:0')
policy_old.forwarded at 1641198793293
give action 338============================
log_to_linear:  tensor([[[0.4992]]], device='cuda:0') tensor([[[0.9988]]], device='cuda:0')
log_to_linear action at 1641198793294
bwe changes from to:  [tensor([[[9.1305e-06]]]), tensor([[[9.1191e-06]]])]
step into gymStat at 1641198793294
send bwe to appRecv at 1641198793294
sent bwe to appRecv at 1641198793294
wait for recv string at 1641198793294
recved string at 1641198793522
1
wait for recv [self.estimator, stat] at 1641198793522
recved [self.estimator, stat] at 1641198793522
sorted packlist at 1641198793522
packetSeq:  6573
packetSeq:  6574
packetSeq:  6575
packetSeq:  6576
packetSeq:  6577
packetSeq:  6578
packetSeq:  6579
packetSeq:  6580
packetSeq:  6581
packetSeq:  6582
processed packlist at 1641198793522
receiving_rate:  314600.0
delay:  192.6
loss_ratio:  0.0
processed state0-2 at 1641198793522
avgFrameBetween:  6
psnrStat:  [[555916, 555734, 555955, 555721, 556547, 556560, 557402]]
delayStat:  [[54, 56, 54, 54, 54, 55, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556262.1428571428] [54.57142857142857] [0]
processed state3-5 at 1641198793522
liner_to_log:  tensor([[[0.9988]]]) tensor([[[0.4992]]])
linear_to_log at 1641198793523
listState:  [0.07865, 0.1284, 0.0, 0.5562621428571428, 0.05457142857142857, 0.0, tensor([[[0.4992]]])]
state_clone_detach at 1641198793523
reward: 0.10359395877428551
state tensor([0.0786, 0.1284, 0.0000, 0.4992], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198793524
state222:  tensor([[0.0786, 0.1284, 0.0000, 0.4992]], device='cuda:0')
policy_old.forwarded at 1641198793525
give action 339============================
log_to_linear:  tensor([[[0.6970]]], device='cuda:0') tensor([[[1.3465]]], device='cuda:0')
log_to_linear action at 1641198793526
bwe changes from to:  [tensor([[[9.1191e-06]]]), tensor([[[1.2279e-05]]])]
step into gymStat at 1641198793526
send bwe to appRecv at 1641198793526
sent bwe to appRecv at 1641198793526
wait for recv string at 1641198793526
recved string at 1641198793725
1
wait for recv [self.estimator, stat] at 1641198793725
recved [self.estimator, stat] at 1641198793725
sorted packlist at 1641198793725
packetSeq:  6583
packetSeq:  6584
packetSeq:  6585
packetSeq:  6586
packetSeq:  6587
packetSeq:  6588
packetSeq:  6589
packetSeq:  6590
packetSeq:  6591
processed packlist at 1641198793725
receiving_rate:  269000.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198793726
avgFrameBetween:  6
psnrStat:  [[557053, 557027, 557488, 557015, 556549, 556812]]
delayStat:  [[54, 54, 54, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556990.6666666666] [54.166666666666664] [0]
processed state3-5 at 1641198793726
liner_to_log:  tensor([[[1.3465]]]) tensor([[[0.6970]]])
linear_to_log at 1641198793726
listState:  [0.06725, 0.128, 0.0, 0.5569906666666666, 0.05416666666666666, 0.0, tensor([[[0.6970]]])]
state_clone_detach at 1641198793726
reward: 0.054265779182185336
state tensor([0.0672, 0.1280, 0.0000, 0.6970], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198793727
state222:  tensor([[0.0672, 0.1280, 0.0000, 0.6970]], device='cuda:0')
policy_old.forwarded at 1641198793729
give action 340============================
log_to_linear:  tensor([[[0.6561]]], device='cuda:0') tensor([[[1.2692]]], device='cuda:0')
log_to_linear action at 1641198793729
bwe changes from to:  [tensor([[[1.2279e-05]]]), tensor([[[1.5584e-05]]])]
step into gymStat at 1641198793730
send bwe to appRecv at 1641198793730
sent bwe to appRecv at 1641198793730
wait for recv string at 1641198793730
recved string at 1641198793954
1
wait for recv [self.estimator, stat] at 1641198793954
recved [self.estimator, stat] at 1641198793954
sorted packlist at 1641198793954
packetSeq:  6592
packetSeq:  6593
packetSeq:  6594
packetSeq:  6595
packetSeq:  6596
packetSeq:  6597
packetSeq:  6598
packetSeq:  6599
processed packlist at 1641198793954
receiving_rate:  283680.0
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198793954
avgFrameBetween:  6
psnrStat:  [[556626, 557579, 558495, 559315, 560357, 560655, 561598]]
delayStat:  [[54, 55, 54, 54, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [559232.1428571428] [53.857142857142854] [0]
processed state3-5 at 1641198793955
liner_to_log:  tensor([[[1.2692]]]) tensor([[[0.6561]]])
linear_to_log at 1641198793955
listState:  [0.07092, 0.1285, 0.0, 0.5592321428571428, 0.05385714285714285, 0.0, tensor([[[0.6561]]])]
state_clone_detach at 1641198793955
reward: 0.06950129432340801
state tensor([0.0709, 0.1285, 0.0000, 0.6561], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198793956
state222:  tensor([[0.0709, 0.1285, 0.0000, 0.6561]], device='cuda:0')
policy_old.forwarded at 1641198793958
give action 341============================
log_to_linear:  tensor([[[0.4883]]], device='cuda:0') tensor([[[0.9816]]], device='cuda:0')
log_to_linear action at 1641198793958
bwe changes from to:  [tensor([[[1.5584e-05]]]), tensor([[[1.5298e-05]]])]
step into gymStat at 1641198793959
send bwe to appRecv at 1641198793959
sent bwe to appRecv at 1641198793959
wait for recv string at 1641198793959
recved string at 1641198794160
1
wait for recv [self.estimator, stat] at 1641198794160
recved [self.estimator, stat] at 1641198794160
sorted packlist at 1641198794160
packetSeq:  6600
packetSeq:  6601
packetSeq:  6602
packetSeq:  6603
packetSeq:  6604
packetSeq:  6605
packetSeq:  6606
packetSeq:  6607
packetSeq:  6608
packetSeq:  6609
packetSeq:  6610
processed packlist at 1641198794160
receiving_rate:  291720.0
delay:  191.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198794160
avgFrameBetween:  6
psnrStat:  [[561352, 561241, 561815, 561582, 562774, 562348]]
delayStat:  [[55, 53, 54, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561852.0] [53.5] [0]
processed state3-5 at 1641198794160
liner_to_log:  tensor([[[0.9816]]]) tensor([[[0.4883]]])
linear_to_log at 1641198794160
listState:  [0.07293, 0.12787878787878787, 0.0, 0.561852, 0.0535, 0.0, tensor([[[0.4883]]])]
state_clone_detach at 1641198794160
reward: 0.08033664331859086
state tensor([0.0729, 0.1279, 0.0000, 0.4883], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198794161
state222:  tensor([[0.0729, 0.1279, 0.0000, 0.4883]], device='cuda:0')
policy_old.forwarded at 1641198794163
give action 342============================
log_to_linear:  tensor([[[0.4980]]], device='cuda:0') tensor([[[0.9968]]], device='cuda:0')
log_to_linear action at 1641198794164
bwe changes from to:  [tensor([[[1.5298e-05]]]), tensor([[[1.5249e-05]]])]
step into gymStat at 1641198794164
send bwe to appRecv at 1641198794164
sent bwe to appRecv at 1641198794164
wait for recv string at 1641198794164
recved string at 1641198794389
1
wait for recv [self.estimator, stat] at 1641198794389
recved [self.estimator, stat] at 1641198794389
sorted packlist at 1641198794389
packetSeq:  6611
packetSeq:  6612
packetSeq:  6613
packetSeq:  6614
packetSeq:  6615
packetSeq:  6616
packetSeq:  6617
packetSeq:  6618
packetSeq:  6619
packetSeq:  6620
processed packlist at 1641198794389
receiving_rate:  252320.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198794389
avgFrameBetween:  6
psnrStat:  [[562685, 561942, 561554, 561522, 561180, 561876, 561241]]
delayStat:  [[53, 53, 54, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561714.2857142857] [53.142857142857146] [0]
processed state3-5 at 1641198794389
liner_to_log:  tensor([[[0.9968]]]) tensor([[[0.4980]]])
linear_to_log at 1641198794389
listState:  [0.06308, 0.12814814814814815, 0.0, 0.5617142857142857, 0.053142857142857144, 0.0, tensor([[[0.4980]]])]
state_clone_detach at 1641198794390
reward: 0.034221424921720034
state tensor([0.0631, 0.1281, 0.0000, 0.4980], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198794390
state222:  tensor([[0.0631, 0.1281, 0.0000, 0.4980]], device='cuda:0')
policy_old.forwarded at 1641198794392
give action 343============================
log_to_linear:  tensor([[[-0.0356]]], device='cuda:0') tensor([[[nan]]], device='cuda:0')
log_to_linear action at 1641198794393
bwe changes from to:  [tensor([[[1.5249e-05]]]), tensor([[[7.6247e-06]]])]
step into gymStat at 1641198794393
send bwe to appRecv at 1641198794393
sent bwe to appRecv at 1641198794393
wait for recv string at 1641198794393
recved string at 1641198794622
1
wait for recv [self.estimator, stat] at 1641198794622
recved [self.estimator, stat] at 1641198794623
sorted packlist at 1641198794623
packetSeq:  6621
packetSeq:  6622
packetSeq:  6623
packetSeq:  6624
packetSeq:  6625
packetSeq:  6626
packetSeq:  6627
packetSeq:  6628
packetSeq:  6629
packetSeq:  6630
processed packlist at 1641198794623
receiving_rate:  257519.99999999997
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198794623
avgFrameBetween:  6
psnrStat:  [[561568, 560868, 560488, 560498, 560609, 560486, 562337]]
delayStat:  [[53, 53, 53, 53, 54, 55, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560979.1428571428] [53.42857142857143] [0]
processed state3-5 at 1641198794623
liner_to_log:  tensor([[[0.5000]]]) tensor([[[0.]]])
linear_to_log at 1641198794623
listState:  [0.06437999999999999, 0.12833333333333333, 0.0, 0.5609791428571428, 0.05342857142857143, 0.0, tensor([[[0.]]])]
state_clone_detach at 1641198794623
reward: 0.039845120686509194
state tensor([0.0644, 0.1283, 0.0000, 0.0000], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198794624
state222:  tensor([[0.0644, 0.1283, 0.0000, 0.0000]], device='cuda:0')
policy_old.forwarded at 1641198794625
give action 344============================
log_to_linear:  tensor([[[0.3758]]], device='cuda:0') tensor([[[0.8180]]], device='cuda:0')
log_to_linear action at 1641198794626
bwe changes from to:  [tensor([[[7.6247e-06]]]), tensor([[[6.2373e-06]]])]
step into gymStat at 1641198794627
send bwe to appRecv at 1641198794627
sent bwe to appRecv at 1641198794627
wait for recv string at 1641198794627
recved string at 1641198794825
1
wait for recv [self.estimator, stat] at 1641198794825
recved [self.estimator, stat] at 1641198794825
sorted packlist at 1641198794825
packetSeq:  6631
packetSeq:  6632
packetSeq:  6633
packetSeq:  6634
packetSeq:  6635
packetSeq:  6636
packetSeq:  6637
packetSeq:  6638
packetSeq:  6639
packetSeq:  6640
processed packlist at 1641198794825
receiving_rate:  289720.0
delay:  192.1
loss_ratio:  0.0
processed state0-2 at 1641198794825
avgFrameBetween:  6
psnrStat:  [[562538, 562439, 562320, 562267, 562453, 562081]]
delayStat:  [[55, 53, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [562349.6666666666] [53.5] [0]
processed state3-5 at 1641198794825
liner_to_log:  tensor([[[0.8180]]]) tensor([[[0.3758]]])
linear_to_log at 1641198794826
listState:  [0.07243, 0.12806666666666666, 0.0, 0.5623496666666666, 0.0535, 0.0, tensor([[[0.3758]]])]
state_clone_detach at 1641198794826
reward: 0.07755374122020409
state tensor([0.0724, 0.1281, 0.0000, 0.3758], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198794826
state222:  tensor([[0.0724, 0.1281, 0.0000, 0.3758]], device='cuda:0')
policy_old.forwarded at 1641198794828
give action 345============================
log_to_linear:  tensor([[[0.7612]]], device='cuda:0') tensor([[[1.4733]]], device='cuda:0')
log_to_linear action at 1641198794829
bwe changes from to:  [tensor([[[6.2373e-06]]]), tensor([[[9.1894e-06]]])]
step into gymStat at 1641198794829
send bwe to appRecv at 1641198794829
sent bwe to appRecv at 1641198794829
wait for recv string at 1641198794829
recved string at 1641198795055
1
wait for recv [self.estimator, stat] at 1641198795055
recved [self.estimator, stat] at 1641198795055
sorted packlist at 1641198795055
packetSeq:  6641
packetSeq:  6642
packetSeq:  6643
packetSeq:  6644
packetSeq:  6645
packetSeq:  6646
packetSeq:  6647
packetSeq:  6648
packetSeq:  6649
packetSeq:  6650
processed packlist at 1641198795055
receiving_rate:  282720.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198795055
avgFrameBetween:  6
psnrStat:  [[562674, 562242, 562008, 563088, 562544, 562788, 561982]]
delayStat:  [[53, 53, 53, 53, 54, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [562475.1428571428] [53.285714285714285] [0]
processed state3-5 at 1641198795055
liner_to_log:  tensor([[[1.4733]]]) tensor([[[0.7612]]])
linear_to_log at 1641198795055
listState:  [0.07068, 0.1282962962962963, 0.0, 0.5624751428571428, 0.053285714285714283, 0.0, tensor([[[0.7612]]])]
state_clone_detach at 1641198795056
reward: 0.06903213507518347
state tensor([0.0707, 0.1283, 0.0000, 0.7612], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198795056
state222:  tensor([[0.0707, 0.1283, 0.0000, 0.7612]], device='cuda:0')
policy_old.forwarded at 1641198795058
give action 346============================
log_to_linear:  tensor([[[0.4555]]], device='cuda:0') tensor([[[0.9313]]], device='cuda:0')
log_to_linear action at 1641198795059
bwe changes from to:  [tensor([[[9.1894e-06]]]), tensor([[[8.5583e-06]]])]
step into gymStat at 1641198795059
send bwe to appRecv at 1641198795059
sent bwe to appRecv at 1641198795059
wait for recv string at 1641198795059
recved string at 1641198795257
1
wait for recv [self.estimator, stat] at 1641198795257
recved [self.estimator, stat] at 1641198795257
sorted packlist at 1641198795257
packetSeq:  6651
packetSeq:  6652
packetSeq:  6653
packetSeq:  6654
packetSeq:  6655
packetSeq:  6656
packetSeq:  6657
packetSeq:  6658
packetSeq:  6659
packetSeq:  6660
packetSeq:  6661
processed packlist at 1641198795257
receiving_rate:  288440.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198795257
avgFrameBetween:  6
psnrStat:  [[561580, 561600, 561540, 561362, 561414, 561235]]
delayStat:  [[53, 53, 53, 54, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561455.1666666666] [53.333333333333336] [0]
processed state3-5 at 1641198795257
liner_to_log:  tensor([[[0.9313]]]) tensor([[[0.4555]]])
linear_to_log at 1641198795258
listState:  [0.07211, 0.12793939393939394, 0.0, 0.5614551666666666, 0.05333333333333334, 0.0, tensor([[[0.4555]]])]
state_clone_detach at 1641198795258
reward: 0.07651090933000565
state tensor([0.0721, 0.1279, 0.0000, 0.4555], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198795258
state222:  tensor([[0.0721, 0.1279, 0.0000, 0.4555]], device='cuda:0')
policy_old.forwarded at 1641198795260
give action 347============================
log_to_linear:  tensor([[[0.3310]]], device='cuda:0') tensor([[[0.7600]]], device='cuda:0')
log_to_linear action at 1641198795261
bwe changes from to:  [tensor([[[8.5583e-06]]]), tensor([[[6.5046e-06]]])]
step into gymStat at 1641198795261
send bwe to appRecv at 1641198795261
sent bwe to appRecv at 1641198795261
wait for recv string at 1641198795261
recved string at 1641198795457
1
wait for recv [self.estimator, stat] at 1641198795457
recved [self.estimator, stat] at 1641198795457
sorted packlist at 1641198795457
packetSeq:  6662
packetSeq:  6663
packetSeq:  6664
packetSeq:  6665
packetSeq:  6666
packetSeq:  6667
packetSeq:  6668
packetSeq:  6669
packetSeq:  6670
packetSeq:  6671
processed packlist at 1641198795458
receiving_rate:  286160.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198795458
avgFrameBetween:  6
psnrStat:  [[561419, 560751, 560810, 560607, 559028, 557691]]
delayStat:  [[53, 53, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560051.0] [53.166666666666664] [0]
processed state3-5 at 1641198795458
liner_to_log:  tensor([[[0.7600]]]) tensor([[[0.3310]]])
linear_to_log at 1641198795458
listState:  [0.07154, 0.128, 0.0, 0.560051, 0.05316666666666667, 0.0, tensor([[[0.3310]]])]
state_clone_detach at 1641198795458
reward: 0.07378302367605227
state tensor([0.0715, 0.1280, 0.0000, 0.3310], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198795459
state222:  tensor([[0.0715, 0.1280, 0.0000, 0.3310]], device='cuda:0')
policy_old.forwarded at 1641198795461
give action 348============================
log_to_linear:  tensor([[[0.5079]]], device='cuda:0') tensor([[[1.0126]]], device='cuda:0')
log_to_linear action at 1641198795461
bwe changes from to:  [tensor([[[6.5046e-06]]]), tensor([[[6.5869e-06]]])]
step into gymStat at 1641198795462
send bwe to appRecv at 1641198795462
sent bwe to appRecv at 1641198795462
wait for recv string at 1641198795462
recved string at 1641198795660
1
wait for recv [self.estimator, stat] at 1641198795660
recved [self.estimator, stat] at 1641198795660
sorted packlist at 1641198795660
packetSeq:  6672
packetSeq:  6673
packetSeq:  6674
packetSeq:  6675
packetSeq:  6676
packetSeq:  6677
packetSeq:  6678
packetSeq:  6679
packetSeq:  6680
packetSeq:  6681
processed packlist at 1641198795660
receiving_rate:  293280.0
delay:  192.3
loss_ratio:  0.0
processed state0-2 at 1641198795660
avgFrameBetween:  6
psnrStat:  [[556672, 556373, 555500, 553692, 553374, 552388]]
delayStat:  [[53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554666.5] [53.0] [0]
processed state3-5 at 1641198795660
liner_to_log:  tensor([[[1.0126]]]) tensor([[[0.5079]]])
linear_to_log at 1641198795661
listState:  [0.07332, 0.1282, 0.0, 0.5546665, 0.053, 0.0, tensor([[[0.5079]]])]
state_clone_detach at 1641198795661
reward: 0.08109835376597369
state tensor([0.0733, 0.1282, 0.0000, 0.5079], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198795661
state222:  tensor([[0.0733, 0.1282, 0.0000, 0.5079]], device='cuda:0')
policy_old.forwarded at 1641198795663
give action 349============================
log_to_linear:  tensor([[[0.4610]]], device='cuda:0') tensor([[[0.9396]]], device='cuda:0')
log_to_linear action at 1641198795664
bwe changes from to:  [tensor([[[6.5869e-06]]]), tensor([[[6.1891e-06]]])]
step into gymStat at 1641198795664
send bwe to appRecv at 1641198795664
sent bwe to appRecv at 1641198795664
wait for recv string at 1641198795664
recved string at 1641198795892
1
wait for recv [self.estimator, stat] at 1641198795892
recved [self.estimator, stat] at 1641198795892
sorted packlist at 1641198795892
packetSeq:  6682
packetSeq:  6683
packetSeq:  6684
packetSeq:  6685
packetSeq:  6686
packetSeq:  6687
packetSeq:  6688
packetSeq:  6689
packetSeq:  6690
processed packlist at 1641198795892
receiving_rate:  257920.00000000003
delay:  193.0
loss_ratio:  0.0
processed state0-2 at 1641198795892
avgFrameBetween:  6
psnrStat:  [[551530, 550394, 549981, 549045, 549362, 549927, 549483]]
delayStat:  [[53, 53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549960.2857142857] [53.0] [0]
processed state3-5 at 1641198795892
liner_to_log:  tensor([[[0.9396]]]) tensor([[[0.4610]]])
linear_to_log at 1641198795893
listState:  [0.06448000000000001, 0.12866666666666668, 0.0, 0.5499602857142857, 0.053, 0.0, tensor([[[0.4610]]])]
state_clone_detach at 1641198795893
reward: 0.03931782219996888
state tensor([0.0645, 0.1287, 0.0000, 0.4610], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198795894
state222:  tensor([[0.0645, 0.1287, 0.0000, 0.4610]], device='cuda:0')
policy_old.forwarded at 1641198795895
give action 350============================
log_to_linear:  tensor([[[0.7434]]], device='cuda:0') tensor([[[1.4375]]], device='cuda:0')
log_to_linear action at 1641198795896
bwe changes from to:  [tensor([[[6.1891e-06]]]), tensor([[[8.8966e-06]]])]
step into gymStat at 1641198795896
send bwe to appRecv at 1641198795896
sent bwe to appRecv at 1641198795896
wait for recv string at 1641198795896
recved string at 1641198796124
1
wait for recv [self.estimator, stat] at 1641198796124
recved [self.estimator, stat] at 1641198796124
sorted packlist at 1641198796124
packetSeq:  6691
packetSeq:  6692
packetSeq:  6693
packetSeq:  6694
packetSeq:  6695
packetSeq:  6696
packetSeq:  6697
processed packlist at 1641198796124
receiving_rate:  215480.0
delay:  192.83333333333334
loss_ratio:  0.0
processed state0-2 at 1641198796124
avgFrameBetween:  6
psnrStat:  [[549949, 550091, 550301, 550607, 551712, 552973, 552947]]
delayStat:  [[53, 54, 53, 54, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551225.7142857143] [53.42857142857143] [0]
processed state3-5 at 1641198796124
liner_to_log:  tensor([[[1.4375]]]) tensor([[[0.7434]]])
linear_to_log at 1641198796124
listState:  [0.05387, 0.12855555555555556, 0.0, 0.5512257142857143, 0.05342857142857143, 0.0, tensor([[[0.7434]]])]
state_clone_detach at 1641198796124
reward: -0.012701738640088889
state tensor([0.0539, 0.1286, 0.0000, 0.7434], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198796125
state222:  tensor([[0.0539, 0.1286, 0.0000, 0.7434]], device='cuda:0')
policy_old.forwarded at 1641198796127
give action 351============================
log_to_linear:  tensor([[[0.0516]]], device='cuda:0') tensor([[[0.5137]]], device='cuda:0')
log_to_linear action at 1641198796128
bwe changes from to:  [tensor([[[8.8966e-06]]]), tensor([[[4.5699e-06]]])]
step into gymStat at 1641198796128
send bwe to appRecv at 1641198796128
sent bwe to appRecv at 1641198796128
wait for recv string at 1641198796128
recved string at 1641198796325
1
wait for recv [self.estimator, stat] at 1641198796325
recved [self.estimator, stat] at 1641198796325
sorted packlist at 1641198796325
packetSeq:  6698
packetSeq:  6699
packetSeq:  6700
packetSeq:  6701
packetSeq:  6702
packetSeq:  6703
packetSeq:  6704
packetSeq:  6705
packetSeq:  6706
packetSeq:  6707
processed packlist at 1641198796325
receiving_rate:  276680.0
delay:  191.7
loss_ratio:  0.0
processed state0-2 at 1641198796325
avgFrameBetween:  6
psnrStat:  [[554624, 555393, 555998, 557245, 557283, 558066]]
delayStat:  [[53, 53, 54, 53, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556434.8333333334] [53.5] [0]
processed state3-5 at 1641198796325
liner_to_log:  tensor([[[0.5137]]]) tensor([[[0.0516]]])
linear_to_log at 1641198796325
listState:  [0.06917, 0.1278, 0.0, 0.5564348333333333, 0.0535, 0.0, tensor([[[0.0516]]])]
state_clone_detach at 1641198796326
reward: 0.06367934137553799
state tensor([0.0692, 0.1278, 0.0000, 0.0516], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198796326
state222:  tensor([[0.0692, 0.1278, 0.0000, 0.0516]], device='cuda:0')
policy_old.forwarded at 1641198796328
give action 352============================
log_to_linear:  tensor([[[0.7554]]], device='cuda:0') tensor([[[1.4616]]], device='cuda:0')
log_to_linear action at 1641198796329
bwe changes from to:  [tensor([[[4.5699e-06]]]), tensor([[[6.6795e-06]]])]
step into gymStat at 1641198796329
send bwe to appRecv at 1641198796329
sent bwe to appRecv at 1641198796329
wait for recv string at 1641198796329
recved string at 1641198796553
1
wait for recv [self.estimator, stat] at 1641198796553
recved [self.estimator, stat] at 1641198796553
sorted packlist at 1641198796553
packetSeq:  6708
packetSeq:  6709
packetSeq:  6710
packetSeq:  6711
packetSeq:  6712
packetSeq:  6713
packetSeq:  6714
packetSeq:  6715
packetSeq:  6716
processed packlist at 1641198796553
receiving_rate:  286120.0
delay:  192.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198796553
avgFrameBetween:  6
psnrStat:  [[558222, 558010, 557720, 559163, 559454, 559547, 559734]]
delayStat:  [[54, 53, 53, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558835.7142857143] [53.285714285714285] [0]
processed state3-5 at 1641198796553
liner_to_log:  tensor([[[1.4616]]]) tensor([[[0.7554]]])
linear_to_log at 1641198796554
listState:  [0.07153, 0.12844444444444444, 0.0, 0.5588357142857143, 0.053285714285714283, 0.0, tensor([[[0.7554]]])]
state_clone_detach at 1641198796554
reward: 0.0724049259154963
state tensor([0.0715, 0.1284, 0.0000, 0.7554], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198796554
state222:  tensor([[0.0715, 0.1284, 0.0000, 0.7554]], device='cuda:0')
policy_old.forwarded at 1641198796556
give action 353============================
log_to_linear:  tensor([[[0.9056]]], device='cuda:0') tensor([[[1.7819]]], device='cuda:0')
log_to_linear action at 1641198796557
bwe changes from to:  [tensor([[[6.6795e-06]]]), tensor([[[1.1902e-05]]])]
step into gymStat at 1641198796557
send bwe to appRecv at 1641198796557
sent bwe to appRecv at 1641198796557
wait for recv string at 1641198796557
recved string at 1641198796754
1
wait for recv [self.estimator, stat] at 1641198796754
recved [self.estimator, stat] at 1641198796754
sorted packlist at 1641198796754
packetSeq:  6717
packetSeq:  6718
packetSeq:  6719
packetSeq:  6720
packetSeq:  6721
packetSeq:  6722
packetSeq:  6723
packetSeq:  6724
packetSeq:  6725
processed packlist at 1641198796754
receiving_rate:  279200.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198796754
avgFrameBetween:  6
psnrStat:  [[558975, 559113, 559211, 558872, 558687, 558260]]
delayStat:  [[54, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558853.0] [53.166666666666664] [0]
processed state3-5 at 1641198796754
liner_to_log:  tensor([[[1.7819]]]) tensor([[[0.9056]]])
linear_to_log at 1641198796755
listState:  [0.0698, 0.1282962962962963, 0.0, 0.558853, 0.05316666666666667, 0.0, tensor([[[0.9056]]])]
state_clone_detach at 1641198796755
reward: 0.06505441973824366
state tensor([0.0698, 0.1283, 0.0000, 0.9056], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198796756
state222:  tensor([[0.0698, 0.1283, 0.0000, 0.9056]], device='cuda:0')
policy_old.forwarded at 1641198796757
give action 354============================
log_to_linear:  tensor([[[0.3518]]], device='cuda:0') tensor([[[0.7864]]], device='cuda:0')
log_to_linear action at 1641198796758
bwe changes from to:  [tensor([[[1.1902e-05]]]), tensor([[[9.3595e-06]]])]
step into gymStat at 1641198796758
send bwe to appRecv at 1641198796758
sent bwe to appRecv at 1641198796758
wait for recv string at 1641198796759
recved string at 1641198796956
1
wait for recv [self.estimator, stat] at 1641198796956
recved [self.estimator, stat] at 1641198796956
sorted packlist at 1641198796956
packetSeq:  6726
packetSeq:  6727
packetSeq:  6728
packetSeq:  6729
packetSeq:  6730
packetSeq:  6731
packetSeq:  6732
packetSeq:  6733
packetSeq:  6734
packetSeq:  6735
processed packlist at 1641198796956
receiving_rate:  280720.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198796956
avgFrameBetween:  6
psnrStat:  [[557923, 558110, 558773, 558576, 559503, 559989]]
delayStat:  [[54, 54, 53, 55, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558812.3333333334] [53.666666666666664] [0]
processed state3-5 at 1641198796956
liner_to_log:  tensor([[[0.7864]]]) tensor([[[0.3518]]])
linear_to_log at 1641198796956
listState:  [0.07018, 0.12793333333333334, 0.0, 0.5588123333333334, 0.05366666666666666, 0.0, tensor([[[0.3518]]])]
state_clone_detach at 1641198796957
reward: 0.06786419440189756
state tensor([0.0702, 0.1279, 0.0000, 0.3518], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198796957
state222:  tensor([[0.0702, 0.1279, 0.0000, 0.3518]], device='cuda:0')
policy_old.forwarded at 1641198796959
give action 355============================
log_to_linear:  tensor([[[0.2844]]], device='cuda:0') tensor([[[0.7044]]], device='cuda:0')
log_to_linear action at 1641198796960
bwe changes from to:  [tensor([[[9.3595e-06]]]), tensor([[[6.5930e-06]]])]
step into gymStat at 1641198796960
send bwe to appRecv at 1641198796960
sent bwe to appRecv at 1641198796960
wait for recv string at 1641198796960
recved string at 1641198797187
1
wait for recv [self.estimator, stat] at 1641198797187
recved [self.estimator, stat] at 1641198797187
sorted packlist at 1641198797187
packetSeq:  6736
packetSeq:  6737
packetSeq:  6738
packetSeq:  6739
packetSeq:  6740
packetSeq:  6741
packetSeq:  6742
packetSeq:  6743
packetSeq:  6744
processed packlist at 1641198797187
receiving_rate:  256480.00000000003
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198797187
avgFrameBetween:  6
psnrStat:  [[559648, 558567, 558576, 558255, 558181, 557485, 557860]]
delayStat:  [[53, 54, 53, 54, 53, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558367.4285714285] [53.57142857142857] [0]
processed state3-5 at 1641198797187
liner_to_log:  tensor([[[0.7044]]]) tensor([[[0.2844]]])
linear_to_log at 1641198797187
listState:  [0.06412000000000001, 0.1285, 0.0, 0.5583674285714285, 0.05357142857142857, 0.0, tensor([[[0.2844]]])]
state_clone_detach at 1641198797188
reward: 0.03811435102417482
state tensor([0.0641, 0.1285, 0.0000, 0.2844], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198797188
state222:  tensor([[0.0641, 0.1285, 0.0000, 0.2844]], device='cuda:0')
policy_old.forwarded at 1641198797190
give action 356============================
log_to_linear:  tensor([[[0.3863]]], device='cuda:0') tensor([[[0.8321]]], device='cuda:0')
log_to_linear action at 1641198797191
bwe changes from to:  [tensor([[[6.5930e-06]]]), tensor([[[5.4863e-06]]])]
step into gymStat at 1641198797191
send bwe to appRecv at 1641198797191
sent bwe to appRecv at 1641198797191
wait for recv string at 1641198797191
recved string at 1641198797423
1
wait for recv [self.estimator, stat] at 1641198797423
recved [self.estimator, stat] at 1641198797424
sorted packlist at 1641198797424
packetSeq:  6745
packetSeq:  6746
packetSeq:  6747
packetSeq:  6748
packetSeq:  6749
packetSeq:  6750
packetSeq:  6751
packetSeq:  6752
packetSeq:  6753
processed packlist at 1641198797424
receiving_rate:  241320.0
delay:  192.71428571428572
loss_ratio:  0.0
processed state0-2 at 1641198797424
avgFrameBetween:  6
psnrStat:  [[557263, 557697, 557515, 558051, 558321, 559094, 559563]]
delayStat:  [[53, 53, 54, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558214.8571428572] [53.285714285714285] [0]
processed state3-5 at 1641198797424
liner_to_log:  tensor([[[0.8321]]]) tensor([[[0.3863]]])
linear_to_log at 1641198797424
listState:  [0.06033, 0.1284761904761905, 0.0, 0.5582148571428571, 0.053285714285714283, 0.0, tensor([[[0.3863]]])]
state_clone_detach at 1641198797424
reward: 0.019952265856616858
state tensor([0.0603, 0.1285, 0.0000, 0.3863], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198797425
state222:  tensor([[0.0603, 0.1285, 0.0000, 0.3863]], device='cuda:0')
policy_old.forwarded at 1641198797427
give action 357============================
log_to_linear:  tensor([[[0.4616]]], device='cuda:0') tensor([[[0.9405]]], device='cuda:0')
log_to_linear action at 1641198797427
bwe changes from to:  [tensor([[[5.4863e-06]]]), tensor([[[5.1599e-06]]])]
step into gymStat at 1641198797428
send bwe to appRecv at 1641198797428
sent bwe to appRecv at 1641198797428
wait for recv string at 1641198797428
recved string at 1641198797653
1
wait for recv [self.estimator, stat] at 1641198797653
recved [self.estimator, stat] at 1641198797654
sorted packlist at 1641198797654
packetSeq:  6754
packetSeq:  6755
packetSeq:  6756
packetSeq:  6757
packetSeq:  6758
packetSeq:  6759
packetSeq:  6760
packetSeq:  6761
packetSeq:  6762
packetSeq:  6763
packetSeq:  6764
processed packlist at 1641198797654
receiving_rate:  318600.0
delay:  192.36363636363637
loss_ratio:  0.0
processed state0-2 at 1641198797654
avgFrameBetween:  6
psnrStat:  [[560448, 560526, 561061, 560568, 560028, 560652, 559927]]
delayStat:  [[53, 54, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [560458.5714285715] [53.142857142857146] [0]
processed state3-5 at 1641198797654
liner_to_log:  tensor([[[0.9405]]]) tensor([[[0.4616]]])
linear_to_log at 1641198797654
listState:  [0.07965, 0.12824242424242424, 0.0, 0.5604585714285715, 0.053142857142857144, 0.0, tensor([[[0.4616]]])]
state_clone_detach at 1641198797654
reward: 0.10830249608551895
state tensor([0.0796, 0.1282, 0.0000, 0.4616], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198797655
state222:  tensor([[0.0796, 0.1282, 0.0000, 0.4616]], device='cuda:0')
policy_old.forwarded at 1641198797657
give action 358============================
log_to_linear:  tensor([[[0.4611]]], device='cuda:0') tensor([[[0.9398]]], device='cuda:0')
log_to_linear action at 1641198797657
bwe changes from to:  [tensor([[[5.1599e-06]]]), tensor([[[4.8494e-06]]])]
step into gymStat at 1641198797658
send bwe to appRecv at 1641198797658
sent bwe to appRecv at 1641198797658
wait for recv string at 1641198797658
recved string at 1641198797864
1
wait for recv [self.estimator, stat] at 1641198797864
recved [self.estimator, stat] at 1641198797864
sorted packlist at 1641198797864
packetSeq:  6765
packetSeq:  6766
packetSeq:  6767
packetSeq:  6768
packetSeq:  6769
packetSeq:  6770
packetSeq:  6771
packetSeq:  6772
packetSeq:  6773
processed packlist at 1641198797864
receiving_rate:  241920.0
delay:  194.75
loss_ratio:  0.0
processed state0-2 at 1641198797864
avgFrameBetween:  6
psnrStat:  [[560383, 560828, 561597, 562981, 562671, 563176]]
delayStat:  [[53, 54, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [561939.3333333334] [53.333333333333336] [0]
processed state3-5 at 1641198797864
liner_to_log:  tensor([[[0.9398]]]) tensor([[[0.4611]]])
linear_to_log at 1641198797865
listState:  [0.06048, 0.12983333333333333, 0.0, 0.5619393333333333, 0.05333333333333334, 0.0, tensor([[[0.4611]]])]
state_clone_detach at 1641198797865
reward: 0.016613110269780684
state tensor([0.0605, 0.1298, 0.0000, 0.4611], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198797865
state222:  tensor([[0.0605, 0.1298, 0.0000, 0.4611]], device='cuda:0')
policy_old.forwarded at 1641198797867
give action 359============================
log_to_linear:  tensor([[[0.4322]]], device='cuda:0') tensor([[[0.8968]]], device='cuda:0')
log_to_linear action at 1641198797868
bwe changes from to:  [tensor([[[4.8494e-06]]]), tensor([[[4.3492e-06]]])]
step into gymStat at 1641198797868
send bwe to appRecv at 1641198797868
sent bwe to appRecv at 1641198797868
wait for recv string at 1641198797868
recved string at 1641198798065
1
wait for recv [self.estimator, stat] at 1641198798065
recved [self.estimator, stat] at 1641198798065
sorted packlist at 1641198798065
packetSeq:  6774
packetSeq:  6775
packetSeq:  6776
packetSeq:  6777
packetSeq:  6778
packetSeq:  6779
packetSeq:  6780
packetSeq:  6781
packetSeq:  6782
packetSeq:  6783
processed packlist at 1641198798065
receiving_rate:  301480.0
delay:  198.4
loss_ratio:  0.0
processed state0-2 at 1641198798065
avgFrameBetween:  6
psnrStat:  [[563041, 562551, 563172, 562426, 561447, 560740]]
delayStat:  [[53, 53, 53, 53, 52, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [562229.5] [52.833333333333336] [0]
processed state3-5 at 1641198798065
liner_to_log:  tensor([[[0.8968]]]) tensor([[[0.4322]]])
linear_to_log at 1641198798066
listState:  [0.07537, 0.13226666666666667, 0.0, 0.5622295, 0.052833333333333336, 0.0, tensor([[[0.4322]]])]
state_clone_detach at 1641198798066
reward: 0.07788679326470904
state tensor([0.0754, 0.1323, 0.0000, 0.4322], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198798066
state222:  tensor([[0.0754, 0.1323, 0.0000, 0.4322]], device='cuda:0')
policy_old.forwarded at 1641198798068
give action 360============================
log_to_linear:  tensor([[[0.8485]]], device='cuda:0') tensor([[[1.6561]]], device='cuda:0')
log_to_linear action at 1641198798069
bwe changes from to:  [tensor([[[4.3492e-06]]]), tensor([[[7.2026e-06]]])]
step into gymStat at 1641198798069
send bwe to appRecv at 1641198798069
sent bwe to appRecv at 1641198798069
wait for recv string at 1641198798069
recved string at 1641198798292
1
wait for recv [self.estimator, stat] at 1641198798292
recved [self.estimator, stat] at 1641198798292
sorted packlist at 1641198798292
packetSeq:  6784
packetSeq:  6785
packetSeq:  6786
packetSeq:  6787
packetSeq:  6788
packetSeq:  6789
packetSeq:  6790
packetSeq:  6791
packetSeq:  6792
packetSeq:  6793
packetSeq:  6794
processed packlist at 1641198798292
receiving_rate:  270520.0
delay:  196.8
loss_ratio:  0.0
processed state0-2 at 1641198798292
avgFrameBetween:  6
psnrStat:  [[559686, 559413, 558885, 558559, 557370, 556122, 556504]]
delayStat:  [[54, 53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [558077.0] [53.142857142857146] [0]
processed state3-5 at 1641198798292
liner_to_log:  tensor([[[1.6561]]]) tensor([[[0.8485]]])
linear_to_log at 1641198798293
listState:  [0.06763, 0.1312, 0.0, 0.558077, 0.053142857142857144, 0.0, tensor([[[0.8485]]])]
state_clone_detach at 1641198798293
reward: 0.04642043642778604
state send 'asking for bwe' at 1641198793725
sent 'asking for bwe' at 1641198793725
send [estimator, stat] at 1641198793725
sent [estimator, stat] at 1641198793725
pc wait for bwe at 1641198793725
pc got bwe at 1641198793730
bandwidth:  300000
pc flushed at 1641198793730
Bwe Sent: 5 at 1641198793730
got request at 1641198793954
processed allFrame at 1641198793954
send 'asking for bwe' at 1641198793954
sent 'asking for bwe' at 1641198793954
send [estimator, stat] at 1641198793954
sent [estimator, stat] at 1641198793954
pc wait for bwe at 1641198793954
pc got bwe at 1641198793959
bandwidth:  300000
pc flushed at 1641198793959
Bwe Sent: 5 at 1641198793959
got request at 1641198794159
processed allFrame at 1641198794160
send 'asking for bwe' at 1641198794160
sent 'asking for bwe' at 1641198794160
send [estimator, stat] at 1641198794160
sent [estimator, stat] at 1641198794160
pc wait for bwe at 1641198794160
pc got bwe at 1641198794164
bandwidth:  300000
pc flushed at 1641198794164
Bwe Sent: 5 at 1641198794164
got request at 1641198794388
processed allFrame at 1641198794389
send 'asking for bwe' at 1641198794389
sent 'asking for bwe' at 1641198794389
send [estimator, stat] at 1641198794389
sent [estimator, stat] at 1641198794389
pc wait for bwe at 1641198794389
pc got bwe at 1641198794393
bandwidth:  300000
pc flushed at 1641198794393
Bwe Sent: 5 at 1641198794393
got request at 1641198794622
processed allFrame at 1641198794622
send 'asking for bwe' at 1641198794622
sent 'asking for bwe' at 1641198794622
send [estimator, stat] at 1641198794622
sent [estimator, stat] at 1641198794622
pc wait for bwe at 1641198794622
pc got bwe at 1641198794627
bandwidth:  300000
pc flushed at 1641198794627
Bwe Sent: 5 at 1641198794627
got request at 1641198794825
processed allFrame at 1641198794825
send 'asking for bwe' at 1641198794825
sent 'asking for bwe' at 1641198794825
send [estimator, stat] at 1641198794825
sent [estimator, stat] at 1641198794825
pc wait for bwe at 1641198794825
pc got bwe at 1641198794829
bandwidth:  300000
pc flushed at 1641198794829
Bwe Sent: 4 at 1641198794829
got request at 1641198795055
processed allFrame at 1641198795055
send 'asking for bwe' at 1641198795055
sent 'asking for bwe' at 1641198795055
send [estimator, stat] at 1641198795055
sent [estimator, stat] at 1641198795055
pc wait for bwe at 1641198795055
pc got bwe at 1641198795059
bandwidth:  300000
pc flushed at 1641198795059
Bwe Sent: 4 at 1641198795059
got request at 1641198795257
processed allFrame at 1641198795257
send 'asking for bwe' at 1641198795257
sent 'asking for bwe' at 1641198795257
send [estimator, stat] at 1641198795257
sent [estimator, stat] at 1641198795257
pc wait for bwe at 1641198795257
pc got bwe at 1641198795261
bandwidth:  300000
pc flushed at 1641198795261
Bwe Sent: 4 at 1641198795261
got request at 1641198795457
processed allFrame at 1641198795457
send 'asking for bwe' at 1641198795457
sent 'asking for bwe' at 1641198795457
send [estimator, stat] at 1641198795457
sent [estimator, stat] at 1641198795457
pc wait for bwe at 1641198795457
pc got bwe at 1641198795462
bandwidth:  300000
pc flushed at 1641198795462
Bwe Sent: 5 at 1641198795462
got request at 1641198795660
processed allFrame at 1641198795660
send 'asking for bwe' at 1641198795660
sent 'asking for bwe' at 1641198795660
send [estimator, stat] at 1641198795660
sent [estimator, stat] at 1641198795660
pc wait for bwe at 1641198795660
pc got bwe at 1641198795664
bandwidth:  300000
pc flushed at 1641198795664
Bwe Sent: 4 at 1641198795664
got request at 1641198795892
processed allFrame at 1641198795892
send 'asking for bwe' at 1641198795892
sent 'asking for bwe' at 1641198795892
send [estimator, stat] at 1641198795892
sent [estimator, stat] at 1641198795892
pc wait for bwe at 1641198795892
pc got bwe at 1641198795896
bandwidth:  300000
pc flushed at 1641198795897
Bwe Sent: 5 at 1641198795897
got request at 1641198796123
processed allFrame at 1641198796123
send 'asking for bwe' at 1641198796123
sent 'asking for bwe' at 1641198796123
send [estimator, stat] at 1641198796123
sent [estimator, stat] at 1641198796124
pc wait for bwe at 1641198796124
pc got bwe at 1641198796128
bandwidth:  300000
pc flushed at 1641198796128
Bwe Sent: 5 at 1641198796128
got request at 1641198796325
processed allFrame at 1641198796325
send 'asking for bwe' at 1641198796325
sent 'asking for bwe' at 1641198796325
send [estimator, stat] at 1641198796325
sent [estimator, stat] at 1641198796325
pc wait for bwe at 1641198796325
pc got bwe at 1641198796329
bandwidth:  300000
pc flushed at 1641198796329
Bwe Sent: 4 at 1641198796329
got request at 1641198796553
processed allFrame at 1641198796553
send 'asking for bwe' at 1641198796553
sent 'asking for bwe' at 1641198796553
send [estimator, stat] at 1641198796553
sent [estimator, stat] at 1641198796553
pc wait for bwe at 1641198796553
pc got bwe at 1641198796557
bandwidth:  300000
pc flushed at 1641198796557
Bwe Sent: 4 at 1641198796557
got request at 1641198796754
processed allFrame at 1641198796754
send 'asking for bwe' at 1641198796754
sent 'asking for bwe' at 1641198796754
send [estimator, stat] at 1641198796754
sent [estimator, stat] at 1641198796754
pc wait for bwe at 1641198796754
pc got bwe at 1641198796759
bandwidth:  300000
pc flushed at 1641198796759
Bwe Sent: 5 at 1641198796759
got request at 1641198796956
processed allFrame at 1641198796956
send 'asking for bwe' at 1641198796956
sent 'asking for bwe' at 1641198796956
send [estimator, stat] at 1641198796956
sent [estimator, stat] at 1641198796956
pc wait for bwe at 1641198796956
pc got bwe at 1641198796960
bandwidth:  300000
pc flushed at 1641198796960
Bwe Sent: 4 at 1641198796960
got request at 1641198797187
processed allFrame at 1641198797187
send 'asking for bwe' at 1641198797187
sent 'asking for bwe' at 1641198797187
send [estimator, stat] at 1641198797187
sent [estimator, stat] at 1641198797187
pc wait for bwe at 1641198797187
pc got bwe at 1641198797191
bandwidth:  300000
pc flushed at 1641198797191
Bwe Sent: 4 at 1641198797191
got request at 1641198797423
processed allFrame at 1641198797423
send 'asking for bwe' at 1641198797423
sent 'asking for bwe' at 1641198797423
send [estimator, stat] at 1641198797423
sent [estimator, stat] at 1641198797423
pc wait for bwe at 1641198797423
pc got bwe at 1641198797428
bandwidth:  300000
pc flushed at 1641198797428
Bwe Sent: 5 at 1641198797428
got request at 1641198797653
processed allFrame at 1641198797653
send 'asking for bwe' at 1641198797653
sent 'asking for bwe' at 1641198797653
send [estimator, stat] at 1641198797653
sent [estimator, stat] at 1641198797653
pc wait for bwe at 1641198797653
pc got bwe at 1641198797658
bandwidth:  300000
pc flushed at 1641198797658
Bwe Sent: 5 at 1641198797658
got request at 1641198797864
processed allFrame at 1641198797864
send 'asking for bwe' at 1641198797864
sent 'asking for bwe' at 1641198797864
send [estimator, stat] at 1641198797864
sent [estimator, stat] at 1641198797864
pc wait for bwe at 1641198797864
pc got bwe at 1641198797868
bandwidth:  300000
pc flushed at 1641198797868
Bwe Sent: 4 at 1641198797868
got request at 1641198798065
processed allFrame at 1641198798065
send 'asking for bwe' at 1641198798065
sent 'asking for bwe' at 1641198798065
send [estimator, stat] at 1641198798065
sent [estimator, stat] at 1641198798065
pc wait for bwe at 1641198798065
pc got bwe at 1641198798069
bandwidth:  300000
pc flushed at 1641198798070
Bwe Sent: 5 at 1641198798070
got request at 1641198798292
processed allFrame at 1641198798292
send 'asking for bwe' at 1641198798292
sent 'asking for bwe' at 1641198798292
send [estimator, stat] at 1641198798292
sent [estimator, stat] at 1641198798292
pc wait for bwe at 1641198798292
pc got bwe at 1641198798296
bandwidth:  300000
pc flushed at 1641198798296
Bwe Sent: 4 at 1641198798296
got request at 1641198798493
processed allFrame at 1641198798493
send 'asking for bwe' at 1641198798493
sent 'asking for bwe' at 1641198798493
send [estimator, stat] at 1641198798493
sent [estimator, stat] at 1641198798493
pc wait for bwe at 1641198798493
pc got bwe at 1641198798497
bandwidth:  300000
pc flushed at 1641198798497
tensor([0.0676, 0.1312, 0.0000, 0.8485], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198798293
state222:  tensor([[0.0676, 0.1312, 0.0000, 0.8485]], device='cuda:0')
policy_old.forwarded at 1641198798295
give action 361============================
log_to_linear:  tensor([[[0.1986]]], device='cuda:0') tensor([[[0.6157]]], device='cuda:0')
log_to_linear action at 1641198798296
bwe changes from to:  [tensor([[[7.2026e-06]]]), tensor([[[4.4347e-06]]])]
step into gymStat at 1641198798296
send bwe to appRecv at 1641198798296
sent bwe to appRecv at 1641198798296
wait for recv string at 1641198798296
recved string at 1641198798493
1
wait for recv [self.estimator, stat] at 1641198798493
recved [self.estimator, stat] at 1641198798493
sorted packlist at 1641198798493
packetSeq:  6795
packetSeq:  6796
packetSeq:  6797
packetSeq:  6798
packetSeq:  6799
packetSeq:  6800
packetSeq:  6801
packetSeq:  6802
processed packlist at 1641198798493
receiving_rate:  274960.0
delay:  198.75
loss_ratio:  0.0
processed state0-2 at 1641198798493
avgFrameBetween:  6
psnrStat:  [[555196, 555185, 554079, 552976, 553097, 552572]]
delayStat:  [[53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553850.8333333334] [53.0] [0]
processed state3-5 at 1641198798493
liner_to_log:  tensor([[[0.6157]]]) tensor([[[0.1986]]])
linear_to_log at 1641198798494
listState:  [0.06874, 0.1325, 0.0, 0.5538508333333334, 0.053, 0.0, tensor([[[0.1986]]])]
state_clone_detach at 1641198798494
reward: 0.047616690810327045
state tensor([0.0687, 0.1325, 0.0000, 0.1986], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198798494
state222:  tensor([[0.0687, 0.1325, 0.0000, 0.1986]], device='cuda:0')
policy_old.forwarded at 1641198798496
give action 362============================
log_to_linear:  tensor([[[0.3379]]], device='cuda:0') tensor([[[0.7686]]], device='cuda:0')
log_to_linear action at 1641198798497
bwe changes from to:  [tensor([[[4.4347e-06]]]), tensor([[[3.4087e-06]]])]
step into gymStat at 1641198798497
send bwe to appRecv at 1641198798497
sent bwe to appRecv at 1641198798497
wait for recv string at 1641198798497
recved string at 1641198798702
1
wait for recv [self.estimator, stat] at 1641198798702
recved [self.estimator, stat] at 1641198798702
sorted packlist at 1641198798702
packetSeq:  6803
packetSeq:  6804
packetSeq:  6805
packetSeq:  6806
packetSeq:  6807
packetSeq:  6808
packetSeq:  6809
packetSeq:  6810
processed packlist at 1641198798702
receiving_rate:  278640.0
delay:  205.75
loss_ratio:  0.0
processed state0-2 at 1641198798703
avgFrameBetween:  6
psnrStat:  [[552434, 553719, 553374, 555831, 554995, 554982]]
delayStat:  [[54, 54, 54, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554222.5] [53.5] [0]
processed state3-5 at 1641198798703
liner_to_log:  tensor([[[0.7686]]]) tensor([[[0.3379]]])
linear_to_log at 1641198798703
listState:  [0.06966, 0.13716666666666666, 0.0, 0.5542225, 0.0535, 0.0, tensor([[[0.3379]]])]
state_clone_detach at 1641198798703
reward: 0.037808051818187305
state tensor([0.0697, 0.1372, 0.0000, 0.3379], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198798704
state222:  tensor([[0.0697, 0.1372, 0.0000, 0.3379]], device='cuda:0')
policy_old.forwarded at 1641198798705
give action 363============================
log_to_linear:  tensor([[[0.4303]]], device='cuda:0') tensor([[[0.8942]]], device='cuda:0')
log_to_linear action at 1641198798706
bwe changes from to:  [tensor([[[3.4087e-06]]]), tensor([[[3.0479e-06]]])]
step into gymStat at 1641198798706
send bwe to appRecv at 1641198798706
sent bwe to appRecv at 1641198798707
wait for recv string at 1641198798707
recved string at 1641198798923
1
wait for recv [self.estimator, stat] at 1641198798923
recved [self.estimator, stat] at 1641198798923
sorted packlist at 1641198798923
packetSeq:  6811
packetSeq:  6812
packetSeq:  6813
packetSeq:  6814
packetSeq:  6815
packetSeq:  6816
packetSeq:  6817
packetSeq:  6818
packetSeq:  6819
packetSeq:  6820
packetSeq:  6821
packetSeq:  6822
processed packlist at 1641198798923
receiving_rate:  326680.0
delay:  204.8181818181818
loss_ratio:  0.0
processed state0-2 at 1641198798923
avgFrameBetween:  6
psnrStat:  [[558064, 553312, 552279, 551991, 551593, 552202, 552476]]
delayStat:  [[81, 56, 53, 53, 53, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553131.0] [57.57142857142857] [0]
processed state3-5 at 1641198798923
liner_to_log:  tensor([[[0.8942]]]) tensor([[[0.4303]]])
linear_to_log at 1641198798923
listState:  [0.08167, 0.13654545454545455, 0.0, 0.553131, 0.05757142857142857, 0.0, tensor([[[0.4303]]])]
state_clone_detach at 1641198798924
reward: 0.09185959761251045
state tensor([0.0817, 0.1365, 0.0000, 0.4303], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198798924
state222:  tensor([[0.0817, 0.1365, 0.0000, 0.4303]], device='cuda:0')
policy_old.forwarded at 1641198798926
give action 364============================
log_to_linear:  tensor([[[0.2599]]], device='cuda:0') tensor([[[0.6773]]], device='cuda:0')
log_to_linear action at 1641198798927
bwe changes from to:  [tensor([[[3.0479e-06]]]), tensor([[[2.0643e-06]]])]
step into gymStat at 1641198798927
send bwe to appRecv at 1641198798927
sent bwe to appRecv at 1641198798927
wait for recv string at 1641198798927
recved string at 1641198799127
1
wait for recv [self.estimator, stat] at 1641198799127
recved [self.estimator, stat] at 1641198799127
sorted packlist at 1641198799127
packetSeq:  6823
packetSeq:  6824
packetSeq:  6825
packetSeq:  6826
packetSeq:  6827
packetSeq:  6828
packetSeq:  6829
packetSeq:  6830
processed packlist at 1641198799127
receiving_rate:  267080.0
delay:  198.25
loss_ratio:  0.0
processed state0-2 at 1641198799127
avgFrameBetween:  6
psnrStat:  [[553830, 551531, 551979, 551383, 551166, 549757]]
delayStat:  [[53, 53, 54, 54, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551607.6666666666] [53.5] [0]
processed state3-5 at 1641198799127
liner_to_log:  tensor([[[0.6773]]]) tensor([[[0.2599]]])
linear_to_log at 1641198799127
listState:  [0.06677, 0.13216666666666665, 0.0, 0.5516076666666666, 0.0535, 0.0, tensor([[[0.2599]]])]
state_clone_detach at 1641198799128
reward: 0.03954201013210029
state tensor([0.0668, 0.1322, 0.0000, 0.2599], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198799128
state222:  tensor([[0.0668, 0.1322, 0.0000, 0.2599]], device='cuda:0')
policy_old.forwarded at 1641198799130
give action 365============================
log_to_linear:  tensor([[[0.8606]]], device='cuda:0') tensor([[[1.6823]]], device='cuda:0')
log_to_linear action at 1641198799131
bwe changes from to:  [tensor([[[2.0643e-06]]]), tensor([[[3.4729e-06]]])]
step into gymStat at 1641198799131
send bwe to appRecv at 1641198799131
sent bwe to appRecv at 1641198799131
wait for recv string at 1641198799131
recved string at 1641198799359
1
wait for recv [self.estimator, stat] at 1641198799359
recved [self.estimator, stat] at 1641198799359
sorted packlist at 1641198799359
packetSeq:  6831
packetSeq:  6832
packetSeq:  6833
packetSeq:  6834
packetSeq:  6835
packetSeq:  6836
packetSeq:  6837
packetSeq:  6838
processed packlist at 1641198799359
receiving_rate:  258320.0
delay:  199.71428571428572
loss_ratio:  0.0
processed state0-2 at 1641198799359
avgFrameBetween:  6
psnrStat:  [[550051, 549597, 549894, 549279, 551134, 549296, 549233]]
delayStat:  [[53, 54, 54, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549783.4285714285] [53.42857142857143] [0]
processed state3-5 at 1641198799359
liner_to_log:  tensor([[[1.6823]]]) tensor([[[0.8606]]])
linear_to_log at 1641198799360
listState:  [0.06458, 0.13314285714285715, 0.0, 0.5497834285714285, 0.05342857142857143, 0.0, tensor([[[0.8606]]])]
state_clone_detach at 1641198799360
reward: 0.026361580166804233
state tensor([0.0646, 0.1331, 0.0000, 0.8606], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198799361
state222:  tensor([[0.0646, 0.1331, 0.0000, 0.8606]], device='cuda:0')
policy_old.forwarded at 1641198799362
give action 366============================
log_to_linear:  tensor([[[0.7601]]], device='cuda:0') tensor([[[1.4710]]], device='cuda:0')
log_to_linear action at 1641198799363
bwe changes from to:  [tensor([[[3.4729e-06]]]), tensor([[[5.1087e-06]]])]
step into gymStat at 1641198799363
send bwe to appRecv at 1641198799363
sent bwe to appRecv at 1641198799363
wait for recv string at 1641198799363
recved string at 1641198799594
1
wait for recv [self.estimator, stat] at 1641198799594
recved [self.estimator, stat] at 1641198799594
sorted packlist at 1641198799594
packetSeq:  6839
packetSeq:  6840
packetSeq:  6841
packetSeq:  6842
packetSeq:  6843
packetSeq:  6844
packetSeq:  6845
packetSeq:  6846
packetSeq:  6847
processed packlist at 1641198799594
receiving_rate:  252760.0
delay:  197.375
loss_ratio:  0.0
processed state0-2 at 1641198799594
avgFrameBetween:  6
psnrStat:  [[548481, 548526, 548180, 549093, 549617, 549888, 552017]]
delayStat:  [[53, 54, 53, 53, 54, 52, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549400.2857142857] [53.142857142857146] [0]
processed state3-5 at 1641198799594
liner_to_log:  tensor([[[1.4710]]]) tensor([[[0.7601]]])
linear_to_log at 1641198799594
listState:  [0.06319, 0.13158333333333333, 0.0, 0.5494002857142857, 0.053142857142857144, 0.0, tensor([[[0.7601]]])]
state_clone_detach at 1641198799595
reward: 0.024441200716634193
state tensor([0.0632, 0.1316, 0.0000, 0.7601], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198799595
state222:  tensor([[0.0632, 0.1316, 0.0000, 0.7601]], device='cuda:0')
policy_old.forwarded at 1641198799597
give action 367============================
log_to_linear:  tensor([[[0.5005]]], device='cuda:0') tensor([[[1.0008]]], device='cuda:0')
log_to_linear action at 1641198799598
bwe changes from to:  [tensor([[[5.1087e-06]]]), tensor([[[5.1126e-06]]])]
step into gymStat at 1641198799598
send bwe to appRecv at 1641198799598
sent bwe to appRecv at 1641198799598
wait for recv string at 1641198799598
recved string at 1641198799827
1
wait for recv [self.estimator, stat] at 1641198799827
recved [self.estimator, stat] at 1641198799827
sorted packlist at 1641198799827
packetSeq:  6848
packetSeq:  6849
packetSeq:  6850
packetSeq:  6851
packetSeq:  6852
packetSeq:  6853
packetSeq:  6854
packetSeq:  6855
processed packlist at 1641198799827
receiving_rate:  243560.0
delay:  198.0
loss_ratio:  0.0
processed state0-2 at 1641198799827
avgFrameBetween:  6
psnrStat:  [[552812, 551731, 551494, 552355, 553399, 553781, 555468]]
delayStat:  [[54, 53, 53, 54, 55, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553005.7142857143] [53.714285714285715] [0]
processed state3-5 at 1641198799827
liner_to_log:  tensor([[[1.0008]]]) tensor([[[0.5005]]])
linear_to_log at 1641198799828
listState:  [0.06089, 0.132, 0.0, 0.5530057142857143, 0.053714285714285714, 0.0, tensor([[[0.5005]]])]
state_clone_detach at 1641198799828
reward: 0.012110108500851868
state tensor([0.0609, 0.1320, 0.0000, 0.5005], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198799828
state222:  tensor([[0.0609, 0.1320, 0.0000, 0.5005]], device='cuda:0')
policy_old.forwarded at 1641198799830
give action 368============================
log_to_linear:  tensor([[[0.6484]]], device='cuda:0') tensor([[[1.2548]]], device='cuda:0')
log_to_linear action at 1641198799831
bwe changes from to:  [tensor([[[5.1126e-06]]]), tensor([[[6.4153e-06]]])]
step into gymStat at 1641198799831
send bwe to appRecv at 1641198799831
sent bwe to appRecv at 1641198799831
wait for recv string at 1641198799831
recved string at 1641198800055
1
wait for recv [self.estimator, stat] at 1641198800055
recved [self.estimator, stat] at 1641198800055
sorted packlist at 1641198800055
packetSeq:  6856
packetSeq:  6857
packetSeq:  6858
packetSeq:  6859
packetSeq:  6860
packetSeq:  6861
packetSeq:  6862
packetSeq:  6863
packetSeq:  6864
packetSeq:  6865
packetSeq:  6866
processed packlist at 1641198800055
receiving_rate:  303760.0
delay:  197.1818181818182
loss_ratio:  0.0
processed state0-2 at 1641198800055
avgFrameBetween:  6
psnrStat:  [[556208, 557296, 556310, 557387, 556494, 557393, 554943]]
delayStat:  [[53, 53, 54, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [556575.8571428572] [53.285714285714285] [0]
processed state3-5 at 1641198800055
liner_to_log:  tensor([[[1.2548]]]) tensor([[[0.6484]]])
linear_to_log at 1641198800056
listState:  [0.07594, 0.13145454545454546, 0.0, 0.5565758571428572, 0.053285714285714283, 0.0, tensor([[[0.6484]]])]
state_clone_detach at 1641198800056
reward: 0.08279864085123628
state tensor([0.0759, 0.1315, 0.0000, 0.6484], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198800057
state222:  tensor([[0.0759, 0.1315, 0.0000, 0.6484]], device='cuda:0')
policy_old.forwarded at 1641198800058
give action 369============================
log_to_linear:  tensor([[[0.4851]]], device='cuda:0') tensor([[[0.9766]]], device='cuda:0')
log_to_linear action at 1641198800059
bwe changes from to:  [tensor([[[6.4153e-06]]]), tensor([[[6.2653e-06]]])]
step into gymStat at 1641198800059
send bwe to appRecv at 1641198800059
sent bwe to appRecv at 1641198800060
wait for recv string at 1641198800060
recved string at 1641198800262
1
wait for recv [self.estimator, stat] at 1641198800262
recved [self.estimator, stat] at 1641198800263
sorted packlist at 1641198800263
packetSeq:  6867
packetSeq:  6868
packetSeq:  6869
packetSeq:  6870
packetSeq:  6871
packetSeq:  6872
packetSeq:  6873
packetSeq:  6874
packetSeq:  6875
packetSeq:  6876
packetSeq:  6877
packetSeq:  6878
processed packlist at 1641198800263
receiving_rate:  288960.0
delay:  196.54545454545453
loss_ratio:  0.0
processed state0-2 at 1641198800263
avgFrameBetween:  6
psnrStat:  [[553682, 553638, 554034, 553291, 551747, 551228]]
delayStat:  [[53, 54, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552936.6666666666] [53.333333333333336] [0]
processed state3-5 at 1641198800263
liner_to_log:  tensor([[[0.9766]]]) tensor([[[0.4851]]])
linear_to_log at 1641198800263
listState:  [0.07224, 0.13103030303030302, 0.0, 0.5529366666666666, 0.05333333333333334, 0.0, tensor([[[0.4851]]])]
state_clone_detach at 1641198800263
reward: 0.06781735418318569
state tensor([0.0722, 0.1310, 0.0000, 0.4851], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198800264
state222:  tensor([[0.0722, 0.1310, 0.0000, 0.4851]], device='cuda:0')
policy_old.forwarded at 1641198800266
give action 370============================
log_to_linear:  tensor([[[0.2971]]], device='cuda:0') tensor([[[0.7191]]], device='cuda:0')
log_to_linear action at 1641198800267
bwe changes from to:  [tensor([[[6.2653e-06]]]), tensor([[[4.5051e-06]]])]
step into gymStat at 1641198800267
send bwe to appRecv at 1641198800267
sent bwe to appRecv at 1641198800267
wait for recv string at 1641198800267
recved string at 1641198800491
1
wait for recv [self.estimator, stat] at 1641198800491
recved [self.estimator, stat] at 1641198800491
sorted packlist at 1641198800491
packetSeq:  6879
packetSeq:  6880
packetSeq:  6881
packetSeq:  6882
packetSeq:  6883
packetSeq:  6884
packetSeq:  6885
packetSeq:  6886
packetSeq:  6887
processed packlist at 1641198800491
receiving_rate:  297400.0
delay:  197.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198800491
avgFrameBetween:  6
psnrStat:  [[552667, 551077, 549841, 549947, 550682, 549770, 549346]]
delayStat:  [[53, 53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550475.7142857143] [53.0] [0]
processed state3-5 at 1641198800491
liner_to_log:  tensor([[[0.7191]]]) tensor([[[0.2971]]])
linear_to_log at 1641198800491
listState:  [0.07435, 0.13140740740740742, 0.0, 0.5504757142857143, 0.053, 0.0, tensor([[[0.2971]]])]
state_clone_detach at 1641198800492
reward: 0.07600910502437708
state tensor([0.0743, 0.1314, 0.0000, 0.2971], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198800492
state222:  tensor([[0.0743, 0.1314, 0.0000, 0.2971]], device='cuda:0')
policy_old.forwarded at 1641198800494
give action 371============================
log_to_linear:  tensor([[[0.0271]]], device='cuda:0') tensor([[[0.5049]]], device='cuda:0')
log_to_linear action at 1641198800495
bwe changes from to:  [tensor([[[4.5051e-06]]]), tensor([[[2.2747e-06]]])]
step into gymStat at 1641198800495
send bwe to appRecv at 1641198800495
sent bwe to appRecv at 1641198800495
wait for recv string at 1641198800495
recved string at 1641198800692
1
wait for recv [self.estimator, stat] at 1641198800692
recved [self.estimator, stat] at 1641198800692
sorted packlist at 1641198800692
packetSeq:  6888
packetSeq:  6889
packetSeq:  6890
packetSeq:  6891
packetSeq:  6892
packetSeq:  6893
packetSeq:  6894
packetSeq:  6895
packetSeq:  6896
packetSeq:  6897
packetSeq:  6898
processed packlist at 1641198800693
receiving_rate:  268680.0
delay:  196.36363636363637
loss_ratio:  0.0
processed state0-2 at 1641198800693
avgFrameBetween:  6
psnrStat:  [[550083, 549278, 549989, 550210, 550017, 550104]]
delayStat:  [[53, 54, 54, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549946.8333333334] [53.833333333333336] [0]
processed state3-5 at 1641198800693
liner_to_log:  tensor([[[0.5049]]]) tensor([[[0.0271]]])
linear_to_log at 1641198800693
listState:  [0.06717, 0.13090909090909092, 0.0, 0.5499468333333334, 0.05383333333333334, 0.0, tensor([[[0.0271]]])]
state_clone_detach at 1641198800693
reward: 0.045168450163789564
state tensor([0.0672, 0.1309, 0.0000, 0.0271], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198800694
state222:  tensor([[0.0672, 0.1309, 0.0000, 0.0271]], device='cuda:0')
policy_old.forwarded at 1641198800696
give action 372============================
log_to_linear:  tensor([[[0.4560]]], device='cuda:0') tensor([[[0.9321]]], device='cuda:0')
log_to_linear action at 1641198800697
bwe changes from to:  [tensor([[[2.2747e-06]]]), tensor([[[2.1203e-06]]])]
step into gymStat at 1641198800697
send bwe to appRecv at 1641198800697
sent bwe to appRecv at 1641198800697
wait for recv string at 1641198800697
recved string at 1641198800923
1
wait for recv [self.estimator, stat] at 1641198800923
recved [self.estimator, stat] at 1641198800923
sorted packlist at 1641198800923
packetSeq:  6899
packetSeq:  6900
packetSeq:  6901
packetSeq:  6902
packetSeq:  6903
packetSeq:  6904
packetSeq:  6905
processed packlist at 1641198800923
receiving_rate:  268120.0
delay:  198.71428571428572
loss_ratio:  0.0
processed state0-2 at 1641198800923
avgFrameBetween:  6
psnrStat:  [[550039, 548923, 549863, 549962, 549854, 549306, 549010]]
delayStat:  [[54, 54, 55, 54, 54, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549565.2857142857] [54.285714285714285] [0]
processed state3-5 at 1641198800923
liner_to_log:  tensor([[[0.9321]]]) tensor([[[0.4560]]])
linear_to_log at 1641198800924
listState:  [0.06703, 0.13247619047619047, 0.0, 0.5495652857142856, 0.054285714285714284, 0.0, tensor([[[0.4560]]])]
state_clone_detach at 1641198800924
reward: 0.03981900325131388
state tensor([0.0670, 0.1325, 0.0000, 0.4560], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198800924
state222:  tensor([[0.0670, 0.1325, 0.0000, 0.4560]], device='cuda:0')
policy_old.forwarded at 1641198800926
give action 373============================
log_to_linear:  tensor([[[0.3802]]], device='cuda:0') tensor([[[0.8239]]], device='cuda:0')
log_to_linear action at 1641198800927
bwe changes from to:  [tensor([[[2.1203e-06]]]), tensor([[[1.7470e-06]]])]
step into gymStat at 1641198800927
send bwe to appRecv at 1641198800927
sent bwe to appRecv at 1641198800927
wait for recv string at 1641198800927
recved string at 1641198801124
1
wait for recv [self.estimator, stat] at 1641198801124
recved [self.estimator, stat] at 1641198801124
sorted packlist at 1641198801124
packetSeq:  6906
packetSeq:  6907
packetSeq:  6908
packetSeq:  6909
packetSeq:  6910
packetSeq:  6911
packetSeq:  6912
packetSeq:  6913
packetSeq:  6914
processed packlist at 1641198801124
receiving_rate:  274520.0
delay:  197.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198801124
avgFrameBetween:  6
psnrStat:  [[550064, 549999, 550568, 550276, 550730, 552245]]
delayStat:  [[54, 55, 54, 54, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550647.0] [54.333333333333336] [0]
processed state3-5 at 1641198801124
liner_to_log:  tensor([[[0.8239]]]) tensor([[[0.3802]]])
linear_to_log at 1641198801125
listState:  [0.06863, 0.13170370370370368, 0.0, 0.550647, 0.05433333333333334, 0.0, tensor([[[0.3802]]])]
state_clone_detach at 1641198801125
reward: 0.04950247336947511
state tensor([0.0686, 0.1317, 0.0000, 0.3802], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198801126
state222:  tensor([[0.0686, 0.1317, 0.0000, 0.3802]], device='cuda:0')
policy_old.forwarded at 1641198801127
give action 374============================
log_to_linear:  tensor([[[0.3586]]], device='cuda:0') tensor([[[0.7953]]], device='cuda:0')
log_to_linear action at 1641198801128
bwe changes from to:  [tensor([[[1.7470e-06]]]), tensor([[[1.3893e-06]]])]
step into gymStat at 1641198801128
send bwe to appRecv at 1641198801128
sent bwe to appRecv at 1641198801128
wait for recv string at 1641198801128
recved string at 1641198801329
1
wait for recv [self.estimator, stat] at 1641198801329
recved [self.estimator, stat] at 1641198801329
sorted packlist at 1641198801329
packetSeq:  6915
packetSeq:  6916
packetSeq:  6917
packetSeq:  6918
packetSeq:  6919
packetSeq:  6920
packetSeq:  6921
packetSeq:  6922
processed packlist at 1641198801329
receiving_rate:  270480.0
delay:  198.125
loss_ratio:  0.0
processed state0-2 at 1641198801329
avgFrameBetween:  6
psnrStat:  [[552678, 552826, 553281, 554380, 554448, 555162]]
delayStat:  [[54, 54, 54, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553795.8333333334] [54.0] [0]
processed state3-5 at 1641198801329
liner_to_log:  tensor([[[0.7953]]]) tensor([[[0.3586]]])
linear_to_log at 1641198801330
listState:  [0.06762, 0.13208333333333333, 0.0, 0.5537958333333334, 0.054, 0.0, tensor([[[0.3586]]])]
state_clone_detach at 1641198801330
reward: 0.04372432691767697
state tensor([0.0676, 0.1321, 0.0000, 0.3586], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198801330
state222:  tensor([[0.0676, 0.1321, 0.0000, 0.3586]], device='cuda:0')
policy_old.forwarded at 1641198801332
give action 375============================
log_to_linear:  tensor([[[0.6204]]], device='cuda:0') tensor([[[1.2038]]], device='cuda:0')
log_to_linear action at 1641198801333
bwe changes from to:  [tensor([[[1.3893e-06]]]), tensor([[[1.6725e-06]]])]
step into gymStat at 1641198801333
send bwe to appRecv at 1641198801333
sent bwe to appRecv at 1641198801333
wait for recv string at 1641198801333
recved string at 1641198801553
1
wait for recv [self.estimator, stat] at 1641198801553
recved [self.estimator, stat] at 1641198801553
sorted packlist at 1641198801553
packetSeq:  6923
packetSeq:  6924
packetSeq:  6925
packetSeq:  6926
packetSeq:  6927
packetSeq:  6928
packetSeq:  6929
packetSeq:  6930
packetSeq:  6931
packetSeq:  6932
packetSeq:  6933
processed packlist at 1641198801553
receiving_rate:  311360.0
delay:  197.45454545454547
loss_ratio:  0.0
processed state0-2 at 1641198801553
avgFrameBetween:  6
psnrStat:  [[555202, 555366, 554731, 554236, 554314, 554176, 553706]]
delayStat:  [[54, 55, 53, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554533.0] [54.142857142857146] [0]
processed state3-5 at 1641198801553
liner_to_log:  tensor([[[1.2038]]]) tensor([[[0.6204]]])
linear_to_log at 1641198801554
listState:  [0.07784, 0.13163636363636363, 0.0, 0.554533, 0.054142857142857145, 0.0, tensor([[[0.6204]]])]
state_clone_detach at 1641198801554
reward: 0.09043180493998121
state tensor([0.0778, 0.1316, 0.0000, 0.6204], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198801555
state222:  tensor([[0.0778, 0.1316, 0.0000, 0.6204]], device='cuda:0')
policy_old.forwarded at 1641198801556
give action 376============================
log_to_linear:  tensor([[[0.5183]]], device='cuda:0') tensor([[[1.0293]]], device='cuda:0')
log_to_linear action at 1641198801557
bwe changes from to:  [tensor([[[1.6725e-06]]]), tensor([[[1.7215e-06]]])]
step into gymStat at 1641198801557
send bwe to appRecv at 1641198801557
sent bwe to appRecv at 1641198801557
wait for recv string at 1641198801557
recved string at 1641198801757
1
wait for recv [self.estimator, stat] at 1641198801757
recved [self.estimator, stat] at 1641198801757
sorted packlist at 1641198801757
packetSeq:  6934
packetSeq:  6935
packetSeq:  6936
packetSeq:  6937
packetSeq:  6938
packetSeq:  6939
packetSeq:  6940
packetSeq:  6941
packetSeq:  6942
packetSeq:  6943
processed packlist at 1641198801757
receiving_rate:  284040.0
delay:  197.2
loss_ratio:  0.0
processed state0-2 at 1641198801757
avgFrameBetween:  6
psnrStat:  [[552060, 552162, 551924, 551015, 550937, 549331]]
delayStat:  [[55, 54, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551238.1666666666] [54.333333333333336] [0]
processed state3-5 at 1641198801757
liner_to_log:  tensor([[[1.0293]]]) tensor([[[0.5183]]])
linear_to_log at 1641198801757
listState:  [0.07101, 0.13146666666666665, 0.0, 0.5512381666666666, 0.05433333333333334, 0.0, tensor([[[0.5183]]])]
state_clone_detach at 1641198801757
reward: 0.06100589479578877
state tensor([0.0710, 0.1315, 0.0000, 0.5183], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198801758
state222:  tensor([[0.0710, 0.1315, 0.0000, 0.5183]], device='cuda:0')
policy_old.forwarded at 1641198801760
give action 377============================
log_to_linear:  tensor([[[0.5249]]], device='cuda:0') tensor([[[1.0401]]], device='cuda:0')
log_to_linear action at 1641198801761
bwe changes from to:  [tensor([[[1.7215e-06]]]), tensor([[[1.7906e-06]]])]
step into gymStat at 1641198801761
send bwe to appRecv at 1641198801761
sent bwe to appRecv at 1641198801761
wait for recv string at 1641198801761
recved string at 1641198801958
1
wait for recv [self.estimator, stat] at 1641198801958
recved [self.estimator, stat] at 1641198801958
sorted packlist at 1641198801958
packetSeq:  6944
packetSeq:  6945
packetSeq:  6946
packetSeq:  6947
packetSeq:  6948
packetSeq:  6949
packetSeq:  6950
packetSeq:  6951
packetSeq:  6952
processed packlist at 1641198801958
receiving_rate:  283840.0
delay:  198.11111111111111
loss_ratio:  0.0
processed state0-2 at 1641198801958
avgFrameBetween:  6
psnrStat:  [[548370, 549944, 549592, 549392, 549002, 549061]]
delayStat:  [[55, 54, 54, 56, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549226.8333333334] [54.5] [0]
processed state3-5 at 1641198801959
liner_to_log:  tensor([[[1.0401]]]) tensor([[[0.5249]]])
linear_to_log at 1641198801959
listState:  [0.07096, 0.13207407407407407, 0.0, 0.5492268333333333, 0.0545, 0.0, tensor([[[0.5249]]])]
state_clone_detach at 1641198801959
reward: 0.05895892821764054
state tensor([0.0710, 0.1321, 0.0000, 0.5249], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198801960
state222:  tensor([[0.0710, 0.1321, 0.0000, 0.5249]], device='cuda:0')
policy_old.forwarded at 1641198801961
give action 378============================
log_to_linear:  tensor([[[0.5995]]], device='cuda:0') tensor([[[1.1666]]], device='cuda:0')
log_to_linear action at 1641198801962
bwe changes from to:  [tensor([[[1.7906e-06]]]), tensor([[[2.0889e-06]]])]
step into gymStat at 1641198801963
send bwe to appRecv at 1641198801963
sent bwe to appRecv at 1641198801963
wait for recv string at 1641198801963
recved string at 1641198802160
1
wait for recv [self.estimator, stat] at 1641198802160
recved [self.estimator, stat] at 1641198802160
sorted packlist at 1641198802160
packetSeq:  6953
packetSeq:  6954
packetSeq:  6955
packetSeq:  6956
packetSeq:  6957
packetSeq:  6958
packetSeq:  6959
packetSeq:  6960
processed packlist at 1641198802160
receiving_rate:  259200.0
delay:  197.75
loss_ratio:  0.0
processed state0-2 at 1641198802160
avgFrameBetween:  6
psnrStat:  [[548912, 548606, 548481, 547024, 548398, 547901]]
delayStat:  [[55, 54, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548220.3333333334] [54.333333333333336] [0]
processed state3-5 at 1641198802160
liner_to_log:  tensor([[[1.1666]]]) tensor([[[0.5995]]])
linear_to_log at 1641198802161
listState:  [0.0648, 0.13183333333333333, 0.0, 0.5482203333333334, 0.05433333333333334, 0.0, tensor([[[0.5995]]])]
state_clone_detach at 1641198802161
reward: 0.03132796929260295
state tensor([0.0648, 0.1318, 0.0000, 0.5995], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198802161
state222:  tensor([[0.0648, 0.1318, 0.0000, 0.5995]], device='cuda:0')
policy_old.forwarded at 1641198802163
give action 379============================
log_to_linear:  tensor([[[0.4362]]], device='cuda:0') tensor([[[0.9027]]], device='cuda:0')
log_to_linear action at 1641198802164
bwe changes from to:  [tensor([[[2.0889e-06]]]), tensor([[[1.8857e-06]]])]
step into gymStat at 1641198802164
send bwe to appRecv at 1641198802164
sent bwe to appRecv at 1641198802164
wait for recv string at 1641198802164
recved string at 1641198802361
1
wait for recv [self.estimator, stat] at 1641198802361
recved [self.estimator, stat] at 1641198802361
sorted packlist at 1641198802361
packetSeq:  6961
packetSeq:  6962
packetSeq:  6963
packetSeq:  6964
packetSeq:  6965
packetSeq:  6966
packetSeq:  6967
packetSeq:  6968
processed packlist at 1641198802361
receiving_rate:  248760.0
delay:  197.625
loss_ratio:  0.0
processed state0-2 at 1641198802362
avgFrameBetween:  6
psnrStat:  [[549177, 549549, 549899, 550971, 550831, 550815]]
delayStat:  [[55, 55, 55, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550207.0] [54.666666666666664] [0]
processed state3-5 at 1641198802362
liner_to_log:  tensor([[[0.9027]]]) tensor([[[0.4362]]])
linear_to_log at 1641198802362
listState:  [0.06219, 0.13175, 0.0, 0.550207, 0.05466666666666666, 0.0, tensor([[[0.4362]]])]
state_clone_detach at 1641198802362
reward: 0.01914846015968341
state tensor([0.0622, 0.1318, 0.0000, 0.4362], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198802363
state222:  tensor([[0.0622, 0.1318, 0.0000, 0.4362]], device='cuda:0')
policy_old.forwarded at 1641198802364
give action 380============================
log_to_linear:  tensor([[[0.8603]]], device='cuda:0') tensor([[[1.6816]]], device='cuda:0')
log_to_linear action at 1641198802365
bwe changes from to:  [tensor([[[1.8857e-06]]]), tensor([[[3.1711e-06]]])]
step into gymStat at 1641198802366
send bwe to appRecv at 1641198802366
sent bwe to appRecv at 1641198802366
wait for recv string at 1641198802366
recved string at 1641198802586
1
wait for recv [self.estimator, stat] at 1641198802586
recved [self.estimator, stat] at 1641198802586
sorted packlist at 1641198802586
packetSeq:  6969
packetSeq:  6970
packetSeq:  6971
packetSeq:  6972
packetSeq:  6973
packetSeq:  6974
packetSeq:  6975
packetSeq:  6976
packetSeq:  6977
packetSeq:  6978
processed packlist at 1641198802586
receiving_rate:  297680.0
delay:  197.3
loss_ratio:  0.0
processed state0-2 at 1641198802587
avgFrameBetween:  6
psnrStat:  [[551449, 551662, 552273, 552627, 551383, 550244, 549963]]
delayStat:  [[55, 54, 54, 54, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551371.5714285715] [54.285714285714285] [0]
processed state3-5 at 1641198802587
liner_to_log:  tensor([[[1.6816]]]) tensor([[[0.8603]]])
linear_to_log at 1641198802587
listState:  Bwe Sent: 4 at 1641198798497
got request at 1641198798702
processed allFrame at 1641198798702
send 'asking for bwe' at 1641198798702
sent 'asking for bwe' at 1641198798702
send [estimator, stat] at 1641198798702
sent [estimator, stat] at 1641198798702
pc wait for bwe at 1641198798702
pc got bwe at 1641198798707
bandwidth:  300000
pc flushed at 1641198798707
Bwe Sent: 5 at 1641198798707
got request at 1641198798923
processed allFrame at 1641198798923
send 'asking for bwe' at 1641198798923
sent 'asking for bwe' at 1641198798923
send [estimator, stat] at 1641198798923
sent [estimator, stat] at 1641198798923
pc wait for bwe at 1641198798923
pc got bwe at 1641198798927
bandwidth:  300000
pc flushed at 1641198798927
Bwe Sent: 4 at 1641198798927
got request at 1641198799127
processed allFrame at 1641198799127
send 'asking for bwe' at 1641198799127
sent 'asking for bwe' at 1641198799127
send [estimator, stat] at 1641198799127
sent [estimator, stat] at 1641198799127
pc wait for bwe at 1641198799127
pc got bwe at 1641198799131
bandwidth:  300000
pc flushed at 1641198799131
Bwe Sent: 4 at 1641198799131
got request at 1641198799359
processed allFrame at 1641198799359
send 'asking for bwe' at 1641198799359
sent 'asking for bwe' at 1641198799359
send [estimator, stat] at 1641198799359
sent [estimator, stat] at 1641198799359
pc wait for bwe at 1641198799359
pc got bwe at 1641198799363
bandwidth:  300000
pc flushed at 1641198799363
Bwe Sent: 4 at 1641198799363
got request at 1641198799594
processed allFrame at 1641198799594
send 'asking for bwe' at 1641198799594
sent 'asking for bwe' at 1641198799594
send [estimator, stat] at 1641198799594
sent [estimator, stat] at 1641198799594
pc wait for bwe at 1641198799594
pc got bwe at 1641198799598
bandwidth:  300000
pc flushed at 1641198799598
Bwe Sent: 4 at 1641198799598
got request at 1641198799827
processed allFrame at 1641198799827
send 'asking for bwe' at 1641198799827
sent 'asking for bwe' at 1641198799827
send [estimator, stat] at 1641198799827
sent [estimator, stat] at 1641198799827
pc wait for bwe at 1641198799827
pc got bwe at 1641198799831
bandwidth:  300000
pc flushed at 1641198799831
Bwe Sent: 4 at 1641198799831
got request at 1641198800055
processed allFrame at 1641198800055
send 'asking for bwe' at 1641198800055
sent 'asking for bwe' at 1641198800055
send [estimator, stat] at 1641198800055
sent [estimator, stat] at 1641198800055
pc wait for bwe at 1641198800055
pc got bwe at 1641198800060
bandwidth:  300000
pc flushed at 1641198800060
Bwe Sent: 5 at 1641198800060
got request at 1641198800262
processed allFrame at 1641198800262
send 'asking for bwe' at 1641198800262
sent 'asking for bwe' at 1641198800262
send [estimator, stat] at 1641198800262
sent [estimator, stat] at 1641198800263
pc wait for bwe at 1641198800263
pc got bwe at 1641198800267
bandwidth:  300000
pc flushed at 1641198800267
Bwe Sent: 5 at 1641198800267
got request at 1641198800490
processed allFrame at 1641198800490
send 'asking for bwe' at 1641198800490
sent 'asking for bwe' at 1641198800491
send [estimator, stat] at 1641198800491
sent [estimator, stat] at 1641198800491
pc wait for bwe at 1641198800491
pc got bwe at 1641198800495
bandwidth:  300000
pc flushed at 1641198800495
Bwe Sent: 5 at 1641198800495
got request at 1641198800692
processed allFrame at 1641198800692
send 'asking for bwe' at 1641198800692
sent 'asking for bwe' at 1641198800692
send [estimator, stat] at 1641198800692
sent [estimator, stat] at 1641198800692
pc wait for bwe at 1641198800692
pc got bwe at 1641198800697
bandwidth:  300000
pc flushed at 1641198800697
Bwe Sent: 5 at 1641198800697
got request at 1641198800923
processed allFrame at 1641198800923
send 'asking for bwe' at 1641198800923
sent 'asking for bwe' at 1641198800923
send [estimator, stat] at 1641198800923
sent [estimator, stat] at 1641198800923
pc wait for bwe at 1641198800923
pc got bwe at 1641198800927
bandwidth:  300000
pc flushed at 1641198800927
Bwe Sent: 4 at 1641198800927
got request at 1641198801124
processed allFrame at 1641198801124
send 'asking for bwe' at 1641198801124
sent 'asking for bwe' at 1641198801124
send [estimator, stat] at 1641198801124
sent [estimator, stat] at 1641198801124
pc wait for bwe at 1641198801124
pc got bwe at 1641198801128
bandwidth:  300000
pc flushed at 1641198801128
Bwe Sent: 4 at 1641198801128
got request at 1641198801329
processed allFrame at 1641198801329
send 'asking for bwe' at 1641198801329
sent 'asking for bwe' at 1641198801329
send [estimator, stat] at 1641198801329
sent [estimator, stat] at 1641198801329
pc wait for bwe at 1641198801329
pc got bwe at 1641198801333
bandwidth:  300000
pc flushed at 1641198801333
Bwe Sent: 4 at 1641198801333
got request at 1641198801553
processed allFrame at 1641198801553
send 'asking for bwe' at 1641198801553
sent 'asking for bwe' at 1641198801553
send [estimator, stat] at 1641198801553
sent [estimator, stat] at 1641198801553
pc wait for bwe at 1641198801553
pc got bwe at 1641198801558
bandwidth:  300000
pc flushed at 1641198801558
Bwe Sent: 5 at 1641198801558
got request at 1641198801756
processed allFrame at 1641198801756
send 'asking for bwe' at 1641198801756
sent 'asking for bwe' at 1641198801756
send [estimator, stat] at 1641198801757
sent [estimator, stat] at 1641198801757
pc wait for bwe at 1641198801757
pc got bwe at 1641198801761
bandwidth:  300000
pc flushed at 1641198801761
Bwe Sent: 5 at 1641198801761
got request at 1641198801958
processed allFrame at 1641198801958
send 'asking for bwe' at 1641198801958
sent 'asking for bwe' at 1641198801958
send [estimator, stat] at 1641198801958
sent [estimator, stat] at 1641198801958
pc wait for bwe at 1641198801958
pc got bwe at 1641198801963
bandwidth:  300000
pc flushed at 1641198801963
Bwe Sent: 5 at 1641198801963
got request at 1641198802160
processed allFrame at 1641198802160
send 'asking for bwe' at 1641198802160
sent 'asking for bwe' at 1641198802160
send [estimator, stat] at 1641198802160
sent [estimator, stat] at 1641198802160
pc wait for bwe at 1641198802160
pc got bwe at 1641198802164
bandwidth:  300000
pc flushed at 1641198802164
Bwe Sent: 4 at 1641198802164
got request at 1641198802361
processed allFrame at 1641198802361
send 'asking for bwe' at 1641198802361
sent 'asking for bwe' at 1641198802361
send [estimator, stat] at 1641198802361
sent [estimator, stat] at 1641198802361
pc wait for bwe at 1641198802361
pc got bwe at 1641198802366
bandwidth:  300000
pc flushed at 1641198802366
Bwe Sent: 5 at 1641198802366
got request at 1641198802586
processed allFrame at 1641198802586
send 'asking for bwe' at 1641198802586
sent 'asking for bwe' at 1641198802586
send [estimator, stat] at 1641198802586
sent [estimator, stat] at 1641198802586
pc wait for bwe at 1641198802586
pc got bwe at 1641198802591
bandwidth:  300000
pc flushed at 1641198802591
Bwe Sent: 5 at 1641198802591
got request at 1641198802789
processed allFrame at 1641198802789
send 'asking for bwe' at 1641198802789
sent 'asking for bwe' at 1641198802789
send [estimator, stat] at 1641198802789
sent [estimator, stat] at 1641198802789
pc wait for bwe at 1641198802789
pc got bwe at 1641198802794
bandwidth:  300000
pc flushed at 1641198802794
Bwe Sent: 5 at 1641198802794
got request at 1641198802990
processed allFrame at 1641198802990
send 'asking for bwe' at 1641198802990
sent 'asking for bwe' at 1641198802990
send [estimator, stat] at 1641198802990
sent [estimator, stat] at 1641198802990
pc wait for bwe at 1641198802990
pc got bwe at 1641198802994
bandwidth:  300000
pc flushed at 1641198802994
Bwe Sent: 4 at 1641198802994
got request at 1641198803220
processed allFrame at 1641198803220
send 'asking for bwe' at 1641198803220
sent 'asking for bwe' at 1641198803220
send [estimator, stat] at 1641198803220
sent [estimator, stat] at 1641198803220
pc wait for bwe at 1641198803220
pc got bwe at 1641198803224
bandwidth:  300000
pc flushed at 1641198803224
Bwe Sent: 4 at 1641198803224
got request at 1641198803422
processed allFrame at 1641198803422
send 'asking for bwe' at 1641198803422
sent 'asking for bwe' at 1641198803422
send [estimator, stat] at 1641198803422
sent [estimator, stat] at 1641198803422
[0.07442, 0.13153333333333334, 0.0, 0.5513715714285715, 0.054285714285714284, 0.0, tensor([[[0.8603]]])]
state_clone_detach at 1641198802587
reward: 0.07593815415177718
state tensor([0.0744, 0.1315, 0.0000, 0.8603], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198802588
state222:  tensor([[0.0744, 0.1315, 0.0000, 0.8603]], device='cuda:0')
policy_old.forwarded at 1641198802590
give action 381============================
log_to_linear:  tensor([[[0.0311]]], device='cuda:0') tensor([[[0.5061]]], device='cuda:0')
log_to_linear action at 1641198802590
bwe changes from to:  [tensor([[[3.1711e-06]]]), tensor([[[1.6050e-06]]])]
step into gymStat at 1641198802591
send bwe to appRecv at 1641198802591
sent bwe to appRecv at 1641198802591
wait for recv string at 1641198802591
recved string at 1641198802789
1
wait for recv [self.estimator, stat] at 1641198802789
recved [self.estimator, stat] at 1641198802789
sorted packlist at 1641198802789
packetSeq:  6979
packetSeq:  6980
packetSeq:  6981
packetSeq:  6982
packetSeq:  6983
packetSeq:  6984
packetSeq:  6985
packetSeq:  6986
packetSeq:  6987
packetSeq:  6988
processed packlist at 1641198802789
receiving_rate:  262840.0
delay:  196.5
loss_ratio:  0.0
processed state0-2 at 1641198802789
avgFrameBetween:  6
psnrStat:  [[549802, 550458, 550106, 550420, 551426, 551218]]
delayStat:  [[54, 54, 55, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [550571.6666666666] [54.333333333333336] [0]
processed state3-5 at 1641198802789
liner_to_log:  tensor([[[0.5061]]]) tensor([[[0.0311]]])
linear_to_log at 1641198802790
listState:  [0.06571, 0.131, 0.0, 0.5505716666666667, 0.05433333333333334, 0.0, tensor([[[0.0311]]])]
state_clone_detach at 1641198802790
reward: 0.03810179269335201
state tensor([0.0657, 0.1310, 0.0000, 0.0311], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198802791
state222:  tensor([[0.0657, 0.1310, 0.0000, 0.0311]], device='cuda:0')
policy_old.forwarded at 1641198802792
give action 382============================
log_to_linear:  tensor([[[0.3254]]], device='cuda:0') tensor([[[0.7531]]], device='cuda:0')
log_to_linear action at 1641198802793
bwe changes from to:  [tensor([[[1.6050e-06]]]), tensor([[[1.2087e-06]]])]
step into gymStat at 1641198802793
send bwe to appRecv at 1641198802793
sent bwe to appRecv at 1641198802793
wait for recv string at 1641198802794
recved string at 1641198802990
1
wait for recv [self.estimator, stat] at 1641198802990
recved [self.estimator, stat] at 1641198802990
sorted packlist at 1641198802990
packetSeq:  6989
packetSeq:  6990
packetSeq:  6991
packetSeq:  6992
packetSeq:  6993
packetSeq:  6994
packetSeq:  6995
packetSeq:  6996
packetSeq:  6997
processed packlist at 1641198802990
receiving_rate:  277440.0
delay:  193.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198802990
avgFrameBetween:  6
psnrStat:  [[550540, 551599, 551364, 552483, 552803, 553342]]
delayStat:  [[54, 54, 56, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [552021.8333333334] [54.5] [0]
processed state3-5 at 1641198802990
liner_to_log:  tensor([[[0.7531]]]) tensor([[[0.3254]]])
linear_to_log at 1641198802990
listState:  [0.06936, 0.12925925925925927, 0.0, 0.5520218333333333, 0.0545, 0.0, tensor([[[0.3254]]])]
state_clone_detach at 1641198802991
reward: 0.060166740387900874
state tensor([0.0694, 0.1293, 0.0000, 0.3254], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198802991
state222:  tensor([[0.0694, 0.1293, 0.0000, 0.3254]], device='cuda:0')
policy_old.forwarded at 1641198802993
give action 383============================
log_to_linear:  tensor([[[0.2220]]], device='cuda:0') tensor([[[0.6380]]], device='cuda:0')
log_to_linear action at 1641198802994
bwe changes from to:  [tensor([[[1.2087e-06]]]), tensor([[[7.7118e-07]]])]
step into gymStat at 1641198802994
send bwe to appRecv at 1641198802994
sent bwe to appRecv at 1641198802994
wait for recv string at 1641198802994
recved string at 1641198803220
1
wait for recv [self.estimator, stat] at 1641198803220
recved [self.estimator, stat] at 1641198803220
sorted packlist at 1641198803220
packetSeq:  6998
packetSeq:  6999
packetSeq:  7000
packetSeq:  7001
packetSeq:  7002
packetSeq:  7003
packetSeq:  7004
packetSeq:  7005
packetSeq:  7006
processed packlist at 1641198803220
receiving_rate:  302040.0
delay:  192.55555555555554
loss_ratio:  0.0
processed state0-2 at 1641198803220
avgFrameBetween:  6
psnrStat:  [[552997, 553348, 553595, 554792, 555199, 555901, 555682]]
delayStat:  [[54, 54, 55, 54, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554502.0] [54.142857142857146] [0]
processed state3-5 at 1641198803220
liner_to_log:  tensor([[[0.6380]]]) tensor([[[0.2220]]])
linear_to_log at 1641198803221
listState:  [0.07551, 0.12837037037037036, 0.0, 0.554502, 0.054142857142857145, 0.0, tensor([[[0.2220]]])]
state_clone_detach at 1641198803221
reward: 0.09018464262344461
state tensor([0.0755, 0.1284, 0.0000, 0.2220], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198803221
state222:  tensor([[0.0755, 0.1284, 0.0000, 0.2220]], device='cuda:0')
policy_old.forwarded at 1641198803223
give action 384============================
log_to_linear:  tensor([[[0.3476]]], device='cuda:0') tensor([[[0.7810]]], device='cuda:0')
log_to_linear action at 1641198803224
bwe changes from to:  [tensor([[[7.7118e-07]]]), tensor([[[6.0231e-07]]])]
step into gymStat at 1641198803224
send bwe to appRecv at 1641198803224
sent bwe to appRecv at 1641198803224
wait for recv string at 1641198803224
recved string at 1641198803422
1
wait for recv [self.estimator, stat] at 1641198803422
recved [self.estimator, stat] at 1641198803423
sorted packlist at 1641198803423
packetSeq:  7007
packetSeq:  7008
packetSeq:  7009
packetSeq:  7010
packetSeq:  7011
packetSeq:  7012
packetSeq:  7013
packetSeq:  7014
packetSeq:  7015
packetSeq:  7016
processed packlist at 1641198803423
receiving_rate:  270760.0
delay:  191.9
loss_ratio:  0.0
processed state0-2 at 1641198803423
avgFrameBetween:  6
psnrStat:  [[555599, 555744, 556247, 555550, 556002, 556116]]
delayStat:  [[54, 55, 54, 54, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555876.3333333334] [54.333333333333336] [0]
processed state3-5 at 1641198803423
liner_to_log:  tensor([[[0.7810]]]) tensor([[[0.3476]]])
linear_to_log at 1641198803423
listState:  [0.06769, 0.12793333333333334, 0.0, 0.5558763333333334, 0.05433333333333334, 0.0, tensor([[[0.3476]]])]
state_clone_detach at 1641198803423
reward: 0.05649701910016991
state tensor([0.0677, 0.1279, 0.0000, 0.3476], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198803424
state222:  tensor([[0.0677, 0.1279, 0.0000, 0.3476]], device='cuda:0')
policy_old.forwarded at 1641198803426
give action 385============================
log_to_linear:  tensor([[[0.5256]]], device='cuda:0') tensor([[[1.0412]]], device='cuda:0')
log_to_linear action at 1641198803427
bwe changes from to:  [tensor([[[6.0231e-07]]]), tensor([[[6.2713e-07]]])]
step into gymStat at 1641198803427
send bwe to appRecv at 1641198803427
sent bwe to appRecv at 1641198803427
wait for recv string at 1641198803427
recved string at 1641198803653
1
wait for recv [self.estimator, stat] at 1641198803653
recved [self.estimator, stat] at 1641198803653
sorted packlist at 1641198803653
packetSeq:  7017
packetSeq:  7018
packetSeq:  7019
packetSeq:  7020
packetSeq:  7021
packetSeq:  7022
packetSeq:  7023
packetSeq:  7024
packetSeq:  7025
packetSeq:  7026
processed packlist at 1641198803653
receiving_rate:  272160.0
delay:  192.22222222222223
loss_ratio:  0.0
processed state0-2 at 1641198803654
avgFrameBetween:  6
psnrStat:  [[555852, 556249, 555777, 556289, 555430, 556670, 555468]]
delayStat:  [[54, 55, 54, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555962.1428571428] [54.285714285714285] [0]
processed state3-5 at 1641198803654
liner_to_log:  tensor([[[1.0412]]]) tensor([[[0.5256]]])
linear_to_log at 1641198803654
listState:  [0.06804, 0.12814814814814815, 0.0, 0.5559621428571428, 0.054285714285714284, 0.0, tensor([[[0.5256]]])]
state_clone_detach at 1641198803654
reward: 0.05746343743438481
state tensor([0.0680, 0.1281, 0.0000, 0.5256], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198803655
state222:  tensor([[0.0680, 0.1281, 0.0000, 0.5256]], device='cuda:0')
policy_old.forwarded at 1641198803657
give action 386============================
log_to_linear:  tensor([[[0.5697]]], device='cuda:0') tensor([[[1.1149]]], device='cuda:0')
log_to_linear action at 1641198803657
bwe changes from to:  [tensor([[[6.2713e-07]]]), tensor([[[6.9916e-07]]])]
step into gymStat at 1641198803658
send bwe to appRecv at 1641198803658
sent bwe to appRecv at 1641198803658
wait for recv string at 1641198803658
recved string at 1641198803855
1
wait for recv [self.estimator, stat] at 1641198803855
recved [self.estimator, stat] at 1641198803855
sorted packlist at 1641198803855
packetSeq:  7027
packetSeq:  7028
packetSeq:  7029
packetSeq:  7030
packetSeq:  7031
packetSeq:  7032
packetSeq:  7033
packetSeq:  7034
packetSeq:  7035
packetSeq:  7036
packetSeq:  7037
processed packlist at 1641198803855
receiving_rate:  308320.0
delay:  194.27272727272728
loss_ratio:  0.0
processed state0-2 at 1641198803855
avgFrameBetween:  6
psnrStat:  [[556093, 555742, 555879, 555672, 556044, 555537]]
delayStat:  [[54, 54, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555827.8333333334] [54.166666666666664] [0]
processed state3-5 at 1641198803855
liner_to_log:  tensor([[[1.1149]]]) tensor([[[0.5697]]])
linear_to_log at 1641198803856
listState:  [0.07708, 0.12951515151515153, 0.0, 0.5558278333333334, 0.05416666666666666, 0.0, tensor([[[0.5697]]])]
state_clone_detach at 1641198803856
reward: 0.09353735956253517
state tensor([0.0771, 0.1295, 0.0000, 0.5697], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198803856
state222:  tensor([[0.0771, 0.1295, 0.0000, 0.5697]], device='cuda:0')
policy_old.forwarded at 1641198803858
give action 387============================
log_to_linear:  tensor([[[0.9604]]], device='cuda:0') tensor([[[1.9068]]], device='cuda:0')
log_to_linear action at 1641198803859
bwe changes from to:  [tensor([[[6.9916e-07]]]), tensor([[[1.3332e-06]]])]
step into gymStat at 1641198803859
send bwe to appRecv at 1641198803859
sent bwe to appRecv at 1641198803859
wait for recv string at 1641198803859
recved string at 1641198804085
1
wait for recv [self.estimator, stat] at 1641198804085
recved [self.estimator, stat] at 1641198804085
sorted packlist at 1641198804085
packetSeq:  7038
packetSeq:  7039
packetSeq:  7040
packetSeq:  7041
packetSeq:  7042
packetSeq:  7043
packetSeq:  7044
packetSeq:  7045
processed packlist at 1641198804085
receiving_rate:  280600.0
delay:  193.25
loss_ratio:  0.0
processed state0-2 at 1641198804085
avgFrameBetween:  6
psnrStat:  [[555797, 555244, 555471, 554922, 555319, 554247, 554565]]
delayStat:  [[55, 54, 54, 55, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [555080.7142857143] [54.285714285714285] [0]
processed state3-5 at 1641198804085
liner_to_log:  tensor([[[1.9068]]]) tensor([[[0.9604]]])
linear_to_log at 1641198804086
listState:  [0.07015, 0.12883333333333333, 0.0, 0.5550807142857144, 0.054285714285714284, 0.0, tensor([[[0.9604]]])]
state_clone_detach at 1641198804086
reward: 0.06502851430483564
state tensor([0.0702, 0.1288, 0.0000, 0.9604], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198804086
state222:  tensor([[0.0702, 0.1288, 0.0000, 0.9604]], device='cuda:0')
policy_old.forwarded at 1641198804088
give action 388============================
log_to_linear:  tensor([[[0.2334]]], device='cuda:0') tensor([[[0.6495]]], device='cuda:0')
log_to_linear action at 1641198804089
bwe changes from to:  [tensor([[[1.3332e-06]]]), tensor([[[8.6593e-07]]])]
step into gymStat at 1641198804089
send bwe to appRecv at 1641198804089
sent bwe to appRecv at 1641198804089
wait for recv string at 1641198804089
recved string at 1641198804318
1
wait for recv [self.estimator, stat] at 1641198804318
recved [self.estimator, stat] at 1641198804318
sorted packlist at 1641198804318
packetSeq:  7046
packetSeq:  7047
packetSeq:  7048
packetSeq:  7049
packetSeq:  7050
packetSeq:  7051
packetSeq:  7052
packetSeq:  7053
packetSeq:  7054
packetSeq:  7055
processed packlist at 1641198804318
receiving_rate:  299720.0
delay:  192.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198804318
avgFrameBetween:  6
psnrStat:  [[554644, 554560, 553818, 554571, 553747, 554400, 554335]]
delayStat:  [[54, 55, 55, 54, 54, 55, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [554296.4285714285] [54.42857142857143] [0]
processed state3-5 at 1641198804318
liner_to_log:  tensor([[[0.6495]]]) tensor([[[0.2334]]])
linear_to_log at 1641198804318
listState:  [0.07493, 0.1285925925925926, 0.0, 0.5542964285714285, 0.05442857142857143, 0.0, tensor([[[0.2334]]])]
state_clone_detach at 1641198804319
reward: 0.08699110611686495
state tensor([0.0749, 0.1286, 0.0000, 0.2334], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198804319
state222:  tensor([[0.0749, 0.1286, 0.0000, 0.2334]], device='cuda:0')
policy_old.forwarded at 1641198804321
give action 389============================
log_to_linear:  tensor([[[0.5861]]], device='cuda:0') tensor([[[1.1431]]], device='cuda:0')
log_to_linear action at 1641198804322
bwe changes from to:  [tensor([[[8.6593e-07]]]), tensor([[[9.8986e-07]]])]
step into gymStat at 1641198804322
send bwe to appRecv at 1641198804322
sent bwe to appRecv at 1641198804322
wait for recv string at 1641198804322
recved string at 1641198804551
1
wait for recv [self.estimator, stat] at 1641198804551
recved [self.estimator, stat] at 1641198804551
sorted packlist at 1641198804551
packetSeq:  7056
packetSeq:  7057
packetSeq:  7058
packetSeq:  7059
packetSeq:  7060
packetSeq:  7061
packetSeq:  7062
packetSeq:  7063
packetSeq:  7064
packetSeq:  7065
packetSeq:  7066
packetSeq:  7067
processed packlist at 1641198804551
receiving_rate:  311120.0
delay:  192.36363636363637
loss_ratio:  0.0
processed state0-2 at 1641198804552
avgFrameBetween:  6
psnrStat:  [[554376, 554678, 554012, 554492, 554067, 552827, 552084]]
delayStat:  [[54, 55, 54, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553790.8571428572] [54.285714285714285] [0]
processed state3-5 at 1641198804552
liner_to_log:  tensor([[[1.1431]]]) tensor([[[0.5861]]])
linear_to_log at 1641198804552
listState:  [0.07778, 0.12824242424242424, 0.0, 0.5537908571428571, 0.054285714285714284, 0.0, tensor([[[0.5861]]])]
state_clone_detach at 1641198804552
reward: 0.10035704839050025
state tensor([0.0778, 0.1282, 0.0000, 0.5861], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198804553
state222:  tensor([[0.0778, 0.1282, 0.0000, 0.5861]], device='cuda:0')
policy_old.forwarded at 1641198804554
give action 390============================
log_to_linear:  tensor([[[0.5801]]], device='cuda:0') tensor([[[1.1328]]], device='cuda:0')
log_to_linear action at 1641198804555
bwe changes from to:  [tensor([[[9.8986e-07]]]), tensor([[[1.1213e-06]]])]
step into gymStat at 1641198804556
send bwe to appRecv at 1641198804556
sent bwe to appRecv at 1641198804556
wait for recv string at 1641198804556
recved string at 1641198804784
1
wait for recv [self.estimator, stat] at 1641198804784
recved [self.estimator, stat] at 1641198804784
sorted packlist at 1641198804784
packetSeq:  7068
packetSeq:  7069
packetSeq:  7070
packetSeq:  7071
packetSeq:  7072
packetSeq:  7073
packetSeq:  7074
packetSeq:  7075
packetSeq:  7076
packetSeq:  7077
processed packlist at 1641198804784
receiving_rate:  271960.0
delay:  192.44444444444446
loss_ratio:  0.0
processed state0-2 at 1641198804784
avgFrameBetween:  6
psnrStat:  [[550243, 549748, 549228, 548797, 549474, 548052, 549749]]
delayStat:  [[55, 54, 54, 55, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549327.2857142857] [54.42857142857143] [0]
processed state3-5 at 1641198804785
liner_to_log:  tensor([[[1.1328]]]) tensor([[[0.5801]]])
linear_to_log at 1641198804785
listState:  [0.06799, 0.1282962962962963, 0.0, 0.5493272857142857, 0.05442857142857143, 0.0, tensor([[[0.5801]]])]
state_clone_detach at 1641198804785
reward: 0.05678913435019084
state tensor([0.0680, 0.1283, 0.0000, 0.5801], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198804786
state222:  tensor([[0.0680, 0.1283, 0.0000, 0.5801]], device='cuda:0')
policy_old.forwarded at 1641198804787
give action 391============================
log_to_linear:  tensor([[[0.0778]]], device='cuda:0') tensor([[[0.5262]]], device='cuda:0')
log_to_linear action at 1641198804788
bwe changes from to:  [tensor([[[1.1213e-06]]]), tensor([[[5.9003e-07]]])]
step into gymStat at 1641198804789
send bwe to appRecv at 1641198804789
sent bwe to appRecv at 1641198804789
wait for recv string at 1641198804789
recved string at 1641198804987
1
wait for recv [self.estimator, stat] at 1641198804987
recved [self.estimator, stat] at 1641198804987
sorted packlist at 1641198804987
packetSeq:  7078
packetSeq:  7079
packetSeq:  7080
packetSeq:  7081
packetSeq:  7082
packetSeq:  7083
packetSeq:  7084
packetSeq:  7085
packetSeq:  7086
processed packlist at 1641198804987
receiving_rate:  262040.00000000003
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198804987
avgFrameBetween:  6
psnrStat:  [[549108, 549819, 550738, 551188, 552211, 552992]]
delayStat:  [[55, 54, 55, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551009.3333333334] [54.5] [0]
processed state3-5 at 1641198804987
liner_to_log:  tensor([[[0.5262]]]) tensor([[[0.0778]]])
linear_to_log at 1641198804988
listState:  [0.06551000000000001, 0.12822222222222224, 0.0, 0.5510093333333334, 0.0545, 0.0, tensor([[[0.0778]]])]
state_clone_detach at 1641198804988
reward: 0.04549843080873084
state tensor([0.0655, 0.1282, 0.0000, 0.0778], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198804988
state222:  tensor([[0.0655, 0.1282, 0.0000, 0.0778]], device='cuda:0')
policy_old.forwarded at 1641198804990
give action 392============================
log_to_linear:  tensor([[[0.7569]]], device='cuda:0') tensor([[[1.4648]]], device='cuda:0')
log_to_linear action at 1641198804991
bwe changes from to:  [tensor([[[5.9003e-07]]]), tensor([[[8.6425e-07]]])]
step into gymStat at 1641198804991
send bwe to appRecv at 1641198804991
sent bwe to appRecv at 1641198804991
wait for recv string at 1641198804991
recved string at 1641198805190
1
wait for recv [self.estimator, stat] at 1641198805190
recved [self.estimator, stat] at 1641198805190
sorted packlist at 1641198805191
packetSeq:  7087
packetSeq:  7088
packetSeq:  7089
packetSeq:  7090
packetSeq:  7091
packetSeq:  7092
packetSeq:  7093
packetSeq:  7094
packetSeq:  7095
packetSeq:  7096
packetSeq:  7097
processed packlist at 1641198805191
receiving_rate:  304840.0
delay:  192.1818181818182
loss_ratio:  0.0
processed state0-2 at 1641198805191
avgFrameBetween:  6
psnrStat:  [[552964, 553475, 553095, 552986, 552621, 553470]]
delayStat:  [[54, 54, 55, 54, 54, 55]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [553101.8333333334] [54.333333333333336] [0]
processed state3-5 at 1641198805191
liner_to_log:  tensor([[[1.4648]]]) tensor([[[0.7569]]])
linear_to_log at 1641198805191
listState:  [0.07621, 0.12812121212121214, 0.0, 0.5531018333333334, 0.05433333333333334, 0.0, tensor([[[0.7569]]])]
state_clone_detach at 1641198805191
reward: 0.09396768325606936
state tensor([0.0762, 0.1281, 0.0000, 0.7569], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198805192
state222:  tensor([[0.0762, 0.1281, 0.0000, 0.7569]], device='cuda:0')
policy_old.forwarded at 1641198805195
give action 393============================
log_to_linear:  tensor([[[0.7739]]], device='cuda:0') tensor([[[1.4992]]], device='cuda:0')
log_to_linear action at 1641198805196
bwe changes from to:  [tensor([[[8.6425e-07]]]), tensor([[[1.2957e-06]]])]
step into gymStat at 1641198805196
send bwe to appRecv at 1641198805196
sent bwe to appRecv at 1641198805196
wait for recv string at 1641198805196
recved string at 1641198805419
1
wait for recv [self.estimator, stat] at 1641198805419
recved [self.estimator, stat] at 1641198805419
sorted packlist at 1641198805419
packetSeq:  7098
packetSeq:  7099
packetSeq:  7100
packetSeq:  7101
packetSeq:  7102
packetSeq:  7103
packetSeq:  7104
packetSeq:  7105
packetSeq:  7106
packetSeq:  7107
processed packlist at 1641198805419
receiving_rate:  292920.0
delay:  192.3
loss_ratio:  0.0
processed state0-2 at 1641198805419
avgFrameBetween:  6
psnrStat:  [[553623, 553016, 553055, 550736, 551384, 549909, 549245]]
delayStat:  [[54, 54, 55, 54, 54, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [551566.8571428572] [54.142857142857146] [0]
processed state3-5 at 1641198805419
liner_to_log:  tensor([[[1.4992]]]) tensor([[[0.7739]]])
linear_to_log at 1641198805420
listState:  [0.07323, 0.1282, 0.0, 0.5515668571428571, 0.054142857142857145, 0.0, tensor([[[0.7739]]])]
state_clone_detach at 1641198805420
reward: 0.08070063694051083
state tensor([0.0732, 0.1282, 0.0000, 0.7739], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198805421
state222:  tensor([[0.0732, 0.1282, 0.0000, 0.7739]], device='cuda:0')
policy_old.forwarded at 1641198805422
give action 394============================
log_to_linear:  tensor([[[0.5872]]], device='cuda:0') tensor([[[1.1451]]], device='cuda:0')
log_to_linear action at 1641198805423
bwe changes from to:  [tensor([[[1.2957e-06]]]), tensor([[[1.4837e-06]]])]
step into gymStat at 1641198805423
send bwe to appRecv at 1641198805423
sent bwe to appRecv at 1641198805423
wait for recv string at 1641198805423
recved string at 1641198805651
1
wait for recv [self.estimator, stat] at 1641198805651
recved [self.estimator, stat] at 1641198805651
sorted packlist at 1641198805651
packetSeq:  7108
packetSeq:  7109
packetSeq:  7110
packetSeq:  7111
packetSeq:  7112
packetSeq:  7113
packetSeq:  7114
packetSeq:  7115
packetSeq:  7116
packetSeq:  7117
packetSeq:  7118
packetSeq:  7119
processed packlist at 1641198805651
receiving_rate:  303480.0
delay:  191.9090909090909
loss_ratio:  0.0
processed state0-2 at 1641198805651
avgFrameBetween:  6
psnrStat:  [[549261, 549556, 548854, 548517, 548363, 547683, 547774]]
delayStat:  [[54, 55, 54, 54, 55, 54, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548572.5714285715] [54.285714285714285] [0]
processed state3-5 at 1641198805651
liner_to_log:  tensor([[[1.1451]]]) tensor([[[0.5872]]])
linear_to_log at 1641198805652
listState:  [0.07587, 0.12793939393939394, 0.0, 0.5485725714285715, 0.054285714285714284, 0.0, tensor([[[0.5872]]])]
state_clone_detach at 1641198805652
reward: 0.09304063814518043
state tensor([0.0759, 0.1279, 0.0000, 0.5872], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198805652
state222:  tensor([[0.0759, 0.1279, 0.0000, 0.5872]], device='cuda:0')
policy_old.forwarded at 1641198805654
give action 395============================
log_to_linear:  tensor([[[0.4834]]], device='cuda:0') tensor([[[0.9740]]], device='cuda:0')
log_to_linear action at 1641198805655
bwe changes from to:  [tensor([[[1.4837e-06]]]), tensor([[[1.4452e-06]]])]
step into gymStat at 1641198805655
send bwe to appRecv at 1641198805655
sent bwe to appRecv at 1641198805655
wait for recv string at 1641198805655
recved string at 1641198805852
1
wait for recv [self.estimator, stat] at 1641198805852
recved [self.estimator, stat] at 1641198805852
sorted packlist at 1641198805852
packetSeq:  7120
packetSeq:  7121
packetSeq:  7122
packetSeq:  7123
packetSeq:  7124
packetSeq:  7125
packetSeq:  7126
packetSeq:  7127
packetSeq:  7128
processed packlist at 1641198805852
receiving_rate:  287400.0
delay:  192.77777777777777
loss_ratio:  0.0
processed state0-2 at 1641198805852
avgFrameBetween:  6
psnrStat:  [[546472, 546298, 546202, 545565, 545756, 545106]]
delayStat:  [[55, 54, 54, 54, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [545899.8333333334] [54.0] [0]
processed state3-5 at 1641198805852
liner_to_log:  tensor([[[0.9740]]]) tensor([[[0.4834]]])
linear_to_log at 1641198805852
listState:  [0.07185, 0.1285185185185185, 0.0, 0.5458998333333334, 0.054, 0.0, tensor([[[0.4834]]])]
state_clone_detach at 1641198805853
reward: 0.07361351143593103
state tensor([0.0719, 0.1285, 0.0000, 0.4834], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198805853
state222:  tensor([[0.0719, 0.1285, 0.0000, 0.4834]], device='cuda:0')
policy_old.forwarded at 1641198805855
give action 396============================
log_to_linear:  tensor([[[0.2313]]], device='cuda:0') tensor([[[0.6473]]], device='cuda:0')
log_to_linear action at 1641198805856
bwe changes from to:  [tensor([[[1.4452e-06]]]), tensor([[[9.3553e-07]]])]
step into gymStat at 1641198805856
send bwe to appRecv at 1641198805856
sent bwe to appRecv at 1641198805856
wait for recv string at 1641198805856
recved string at 1641198806057
1
wait for recv [self.estimator, stat] at 1641198806057
recved [self.estimator, stat] at 1641198806057
sorted packlist at 1641198806057
packetSeq:  7129
packetSeq:  7130
packetSeq:  7131
packetSeq:  7132
packetSeq:  7133
packetSeq:  7134
packetSeq:  7135
packetSeq:  7136
processed packlist at 1641198806057
receiving_rate:  248960.0
delay:  192.75
loss_ratio:  0.0
processed state0-2 at 1641198806057
avgFrameBetween:  6
psnrStat:  [[544505, 544326, 543721, 543982, 542960, 544778]]
delayStat:  [[54, 53, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [544045.3333333334] [53.333333333333336] [0]
processed state3-5 at 1641198806057
liner_to_log:  tensor([[[0.6473]]]) tensor([[[0.2313]]])
linear_to_log at 1641198806058
listState:  [0.06224, 0.1285, 0.0, 0.5440453333333334, 0.05333333333333334, 0.0, tensor([[[0.2313]]])]
state_clone_detach at 1641198806058
reward: 0.029139008613372375
state tensor([0.0622, 0.1285, 0.0000, 0.2313], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198806058
state222:  tensor([[0.0622, 0.1285, 0.0000, 0.2313]], device='cuda:0')
policy_old.forwarded at 1641198806060
give action 397============================
log_to_linear:  tensor([[[0.7735]]], device='cuda:0') tensor([[[1.4984]]], device='cuda:0')
log_to_linear action at 1641198806061
bwe changes from to:  [tensor([[[9.3553e-07]]]), tensor([[[1.4018e-06]]])]
step into gymStat at 1641198806061
send bwe to appRecv at 1641198806061
sent bwe to appRecv at 1641198806061
wait for recv string at 1641198806061
recved string at 1641198806287
1
wait for recv [self.estimator, stat] at 1641198806287
recved [self.estimator, stat] at 1641198806287
sorted packlist at 1641198806287
packetSeq:  7137
packetSeq:  7138
packetSeq:  7139
packetSeq:  7140
packetSeq:  7141
packetSeq:  7142
packetSeq:  7143
packetSeq:  7144
packetSeq:  7145
processed packlist at 1641198806287
receiving_rate:  259200.0
delay:  192.25
loss_ratio:  0.0
processed state0-2 at 1641198806287
avgFrameBetween:  6
psnrStat:  [[544966, 547099, 546843, 547759, 547469, 547500, 547653]]
delayStat:  [[54, 54, 53, 54, 53, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547041.2857142857] [53.57142857142857] [0]
processed state3-5 at 1641198806287
liner_to_log:  tensor([[[1.4984]]]) tensor([[[0.7735]]])
linear_to_log at 1641198806288
listState:  [0.0648, 0.12816666666666668, 0.0, 0.5470412857142857, 0.05357142857142857, 0.0, tensor([[[0.7735]]])]
state_clone_detach at 1641198806288
reward: 0.04232796929260296
state tensor([0.0648, 0.1282, 0.0000, 0.7735], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198806288
state222:  tensor([[0.0648, 0.1282, 0.0000, 0.7735]], device='cuda:0')
policy_old.forwarded at 1641198806290
give action 398============================
log_to_linear:  tensor([[[0.6911]]], device='cuda:0') tensor([[[1.3351]]], device='cuda:0')
log_to_linear action at 1641198806291
bwe changes from to:  [tensor([[[1.4018e-06]]]), tensor([[[1.8716e-06]]])]
step into gymStat at 1641198806291
send bwe to appRecv at 1641198806291
sent bwe to appRecv at 1641198806291
wait for recv string at 1641198806291
recved string at 1641198806521
1
wait for recv [self.estimator, stat] at 1641198806521
recved [self.estimator, stat] at 1641198806521
sorted packlist at 1641198806521
packetSeq:  7146
packetSeq:  7147
packetSeq:  7148
packetSeq:  7149
packetSeq:  7150
packetSeq:  7151
packetSeq:  7152
packetSeq:  7153
packetSeq:  7154
processed packlist at 1641198806521
receiving_rate:  261880.0
delay:  193.0
loss_ratio:  0.0
processed state0-2 at 1641198806521
avgFrameBetween:  6
psnrStat:  [[548066, 547895, 548229, 549059, 549822, 550440, 549351]]
delayStat:  [[53, 53, 54, 53, 53, 54, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548980.2857142857] [53.285714285714285] [0]
processed state3-5 at 1641198806521
liner_to_log:  tensor([[[1.3351]]]) tensor([[[0.6911]]])
linear_to_log at 1641198806521
listState:  [0.06547, 0.12866666666666668, 0.0, 0.5489802857142857, 0.053285714285714283, 0.0, tensor([[[0.6911]]])]
state_clone_detach at 1641198806521
reward: 0.04397758282428926
state tensor([0.0655, 0.1287, 0.0000, 0.6911], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198806522
state222:  tensor([[0.0655, 0.1287, 0.0000, 0.6911]], device='cuda:0')
policy_old.forwarded at 1641198806524
give action 399============================
log_to_linear:  tensor([[[0.6153]]], device='cuda:0') tensor([[[1.1947]]], device='cuda:0')
log_to_linear action at 1641198806525
bwe changes from to:  [tensor([[[1.8716e-06]]]), tensor([[[2.2359e-06]]])]
step into gymStat at 1641198806525
send bwe to appRecv at 1641198806525
sent bwe to appRecv at 1641198806525
wait for recv string at 1641198806525
recved string at 1641198806752
1
wait for recv [self.estimator, stat] at 1641198806752
recved [self.estimator, stat] at 1641198806752
sorted packlist at 1641198806752
packetSeq:  7155
packetSeq:  7156
packetSeq:  7157
packetSeq:  7158
packetSeq:  7159
packetSeq:  7160
packetSeq:  7161
packetSeq:  7162
packetSeq:  7163
packetSeq:  7164
processed packlist at 1641198806752
receiving_rate:  260399.99999999997
delay:  192.33333333333334
loss_ratio:  0.0
processed state0-2 at 1641198806752
avgFrameBetween:  6
psnrStat:  [[548908, 548396, 549516, 548499, 549101, 548370, 549852]]
delayStat:  [[53, 54, 54, 53, 54, 53, 54]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548948.8571428572] [53.57142857142857] [0]
processed state3-5 at 1641198806752
liner_to_log:  tensor([[[1.1947]]]) tensor([[[0.6153]]])
linear_to_log at 1641198806753
listState:  [0.06509999999999999, 0.12822222222222224, 0.0, 0.5489488571428571, 0.05357142857142857, 0.0, tensor([[[0.6153]]])]
state_clone_detach at 1641198806753
reward: 0.04357362183433344
state tensor([0.0651, 0.1282, 0.0000, 0.6153], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198806754
state222:  tensor([[0.0651, 0.1282, 0.0000, 0.6153]], device='cuda:0')
policy_old.forwarded at 1641198806755
give action 400============================
log_to_linear:  tensor([[[0.4173]]], device='cuda:0') tensor([[[0.8755]]], device='cuda:0')
log_to_linear action at 1641198806756
bwe changes from to:  [tensor([[[2.2359e-06]]]), tensor([[[1.9575e-06]]])]
step into gymStat at 1641198806756
send bwe to appRecv at 1641198806756
sent bwe to appRecv at 1641198806756
wait for recv string at 1641198806756
recved string at 1641198806953
1
wait for recv [self.estimator, stat] at 1641198806953
recved [self.estimator, stat] at 1641198806953
sorted packlist at 1641198806953
packetSeq:  7165
packetSeq:  7166
packetSeq:  7167
packetSeq:  7168
packetSeq:  7169
packetSeq:  7170
packetSeq:  7171
packetSeq:  7172
packetSeq:  7173
pc wait for bwe at 1641198803423
pc got bwe at 1641198803427
bandwidth:  300000
pc flushed at 1641198803427
Bwe Sent: 5 at 1641198803427
got request at 1641198803653
processed allFrame at 1641198803653
send 'asking for bwe' at 1641198803653
sent 'asking for bwe' at 1641198803653
send [estimator, stat] at 1641198803653
sent [estimator, stat] at 1641198803653
pc wait for bwe at 1641198803653
pc got bwe at 1641198803658
bandwidth:  300000
pc flushed at 1641198803658
Bwe Sent: 5 at 1641198803658
got request at 1641198803855
processed allFrame at 1641198803855
send 'asking for bwe' at 1641198803855
sent 'asking for bwe' at 1641198803855
send [estimator, stat] at 1641198803855
sent [estimator, stat] at 1641198803855
pc wait for bwe at 1641198803855
pc got bwe at 1641198803859
bandwidth:  300000
pc flushed at 1641198803859
Bwe Sent: 4 at 1641198803859
got request at 1641198804085
processed allFrame at 1641198804085
send 'asking for bwe' at 1641198804085
sent 'asking for bwe' at 1641198804085
send [estimator, stat] at 1641198804085
sent [estimator, stat] at 1641198804085
pc wait for bwe at 1641198804085
pc got bwe at 1641198804089
bandwidth:  300000
pc flushed at 1641198804089
Bwe Sent: 4 at 1641198804089
got request at 1641198804318
processed allFrame at 1641198804318
send 'asking for bwe' at 1641198804318
sent 'asking for bwe' at 1641198804318
send [estimator, stat] at 1641198804318
sent [estimator, stat] at 1641198804318
pc wait for bwe at 1641198804318
pc got bwe at 1641198804322
bandwidth:  300000
pc flushed at 1641198804322
Bwe Sent: 4 at 1641198804322
got request at 1641198804551
processed allFrame at 1641198804551
send 'asking for bwe' at 1641198804551
sent 'asking for bwe' at 1641198804551
send [estimator, stat] at 1641198804551
sent [estimator, stat] at 1641198804551
pc wait for bwe at 1641198804551
pc got bwe at 1641198804556
bandwidth:  300000
pc flushed at 1641198804556
Bwe Sent: 5 at 1641198804556
got request at 1641198804784
processed allFrame at 1641198804784
send 'asking for bwe' at 1641198804784
sent 'asking for bwe' at 1641198804784
send [estimator, stat] at 1641198804784
sent [estimator, stat] at 1641198804784
pc wait for bwe at 1641198804784
pc got bwe at 1641198804789
bandwidth:  300000
pc flushed at 1641198804789
Bwe Sent: 5 at 1641198804789
got request at 1641198804987
processed allFrame at 1641198804987
send 'asking for bwe' at 1641198804987
sent 'asking for bwe' at 1641198804987
send [estimator, stat] at 1641198804987
sent [estimator, stat] at 1641198804987
pc wait for bwe at 1641198804987
pc got bwe at 1641198804991
bandwidth:  300000
pc flushed at 1641198804991
Bwe Sent: 4 at 1641198804991
got request at 1641198805190
processed allFrame at 1641198805190
send 'asking for bwe' at 1641198805190
sent 'asking for bwe' at 1641198805190
send [estimator, stat] at 1641198805190
sent [estimator, stat] at 1641198805190
pc wait for bwe at 1641198805190
pc got bwe at 1641198805196
bandwidth:  300000
pc flushed at 1641198805196
Bwe Sent: 6 at 1641198805196
got request at 1641198805419
processed allFrame at 1641198805419
send 'asking for bwe' at 1641198805419
sent 'asking for bwe' at 1641198805419
send [estimator, stat] at 1641198805419
sent [estimator, stat] at 1641198805419
pc wait for bwe at 1641198805419
pc got bwe at 1641198805423
bandwidth:  300000
pc flushed at 1641198805423
Bwe Sent: 4 at 1641198805423
got request at 1641198805651
processed allFrame at 1641198805651
send 'asking for bwe' at 1641198805651
sent 'asking for bwe' at 1641198805651
send [estimator, stat] at 1641198805651
sent [estimator, stat] at 1641198805651
pc wait for bwe at 1641198805651
pc got bwe at 1641198805655
bandwidth:  300000
pc flushed at 1641198805655
Bwe Sent: 4 at 1641198805655
got request at 1641198805852
processed allFrame at 1641198805852
send 'asking for bwe' at 1641198805852
sent 'asking for bwe' at 1641198805852
send [estimator, stat] at 1641198805852
sent [estimator, stat] at 1641198805852
pc wait for bwe at 1641198805852
pc got bwe at 1641198805856
bandwidth:  300000
pc flushed at 1641198805856
Bwe Sent: 4 at 1641198805856
got request at 1641198806057
processed allFrame at 1641198806057
send 'asking for bwe' at 1641198806057
sent 'asking for bwe' at 1641198806057
send [estimator, stat] at 1641198806057
sent [estimator, stat] at 1641198806057
pc wait for bwe at 1641198806057
pc got bwe at 1641198806061
bandwidth:  300000
pc flushed at 1641198806061
Bwe Sent: 4 at 1641198806061
got request at 1641198806287
processed allFrame at 1641198806287
send 'asking for bwe' at 1641198806287
sent 'asking for bwe' at 1641198806287
send [estimator, stat] at 1641198806287
sent [estimator, stat] at 1641198806287
pc wait for bwe at 1641198806287
pc got bwe at 1641198806291
bandwidth:  300000
pc flushed at 1641198806291
Bwe Sent: 4 at 1641198806291
got request at 1641198806520
processed allFrame at 1641198806520
send 'asking for bwe' at 1641198806520
sent 'asking for bwe' at 1641198806521
send [estimator, stat] at 1641198806521
sent [estimator, stat] at 1641198806521
pc wait for bwe at 1641198806521
pc got bwe at 1641198806525
bandwidth:  300000
pc flushed at 1641198806525
Bwe Sent: 5 at 1641198806525
got request at 1641198806752
processed allFrame at 1641198806752
send 'asking for bwe' at 1641198806752
sent 'asking for bwe' at 1641198806752
send [estimator, stat] at 1641198806752
sent [estimator, stat] at 1641198806752
pc wait for bwe at 1641198806752
pc got bwe at 1641198806757
bandwidth:  300000
pc flushed at 1641198806757
Bwe Sent: 5 at 1641198806757
got request at 1641198806953
processed allFrame at 1641198806953
send 'asking for bwe' at 1641198806953
sent 'asking for bwe' at 1641198806953
send [estimator, stat] at 1641198806953
sent [estimator, stat] at 1641198806953
pc wait for bwe at 1641198806953
pc got bwe at 1641198806957
bandwidth:  300000
pc flushed at 1641198806957
Bwe Sent: 4 at 1641198806957
got request at 1641198807158
processed allFrame at 1641198807158
send 'asking for bwe' at 1641198807158
sent 'asking for bwe' at 1641198807158
send [estimator, stat] at 1641198807158
sent [estimator, stat] at 1641198807158
pc wait for bwe at 1641198807158
pc got bwe at 1641198807162
bandwidth:  300000
pc flushed at 1641198807162
Bwe Sent: 4 at 1641198807162
got request at 1641198807388
processed allFrame at 1641198807389
send 'asking for bwe' at 1641198807389
sent 'asking for bwe' at 1641198807389
send [estimator, stat] at 1641198807389
sent [estimator, stat] at 1641198807389
pc wait for bwe at 1641198807389
pc got bwe at 1641198807393
bandwidth:  300000
pc flushed at 1641198807393
Bwe Sent: 5 at 1641198807393
got request at 1641198807590
processed allFrame at 1641198807590
send 'asking for bwe' at 1641198807590
sent 'asking for bwe' at 1641198807590
send [estimator, stat] at 1641198807590
sent [estimator, stat] at 1641198807590
pc wait for bwe at 1641198807590
pc got bwe at 1641198807594
bandwidth:  300000
pc flushed at 1641198807595
Bwe Sent: 5 at 1641198807595
packetSeq:  7174
packetSeq:  7175
packetSeq:  7176
processed packlist at 1641198806953
receiving_rate:  307640.0
delay:  191.66666666666666
loss_ratio:  0.0
processed state0-2 at 1641198806953
avgFrameBetween:  6
psnrStat:  [[548338, 550120, 549458, 549089, 548974, 550343]]
delayStat:  [[55, 53, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [549387.0] [53.5] [0]
processed state3-5 at 1641198806953
liner_to_log:  tensor([[[0.8755]]]) tensor([[[0.4173]]])
linear_to_log at 1641198806954
listState:  [0.07691, 0.12777777777777777, 0.0, 0.549387, 0.0535, 0.0, tensor([[[0.4173]]])]
state_clone_detach at 1641198806954
reward: 0.09801827168025118
state tensor([0.0769, 0.1278, 0.0000, 0.4173], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198806954
state222:  tensor([[0.0769, 0.1278, 0.0000, 0.4173]], device='cuda:0')
policy_old.forwarded at 1641198806956
give action 401============================
log_to_linear:  tensor([[[0.5958]]], device='cuda:0') tensor([[[1.1602]]], device='cuda:0')
log_to_linear action at 1641198806957
bwe changes from to:  [tensor([[[1.9575e-06]]]), tensor([[[2.2710e-06]]])]
step into gymStat at 1641198806957
send bwe to appRecv at 1641198806957
sent bwe to appRecv at 1641198806957
wait for recv string at 1641198806957
recved string at 1641198807158
1
wait for recv [self.estimator, stat] at 1641198807158
recved [self.estimator, stat] at 1641198807158
sorted packlist at 1641198807158
packetSeq:  7177
packetSeq:  7178
packetSeq:  7179
packetSeq:  7180
packetSeq:  7181
packetSeq:  7182
packetSeq:  7183
packetSeq:  7184
packetSeq:  7185
packetSeq:  7186
processed packlist at 1641198807158
receiving_rate:  278240.0
delay:  192.0
loss_ratio:  0.0
processed state0-2 at 1641198807158
avgFrameBetween:  6
psnrStat:  [[549149, 548356, 547483, 548228, 548272, 549837]]
delayStat:  [[54, 53, 53, 54, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548554.1666666666] [53.333333333333336] [0]
processed state3-5 at 1641198807158
liner_to_log:  tensor([[[1.1602]]]) tensor([[[0.5958]]])
linear_to_log at 1641198807158
listState:  [0.06956, 0.128, 0.0, 0.5485541666666667, 0.05333333333333334, 0.0, tensor([[[0.5958]]])]
state_clone_detach at 1641198807159
reward: 0.06485388481444171
state tensor([0.0696, 0.1280, 0.0000, 0.5958], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198807159
state222:  tensor([[0.0696, 0.1280, 0.0000, 0.5958]], device='cuda:0')
policy_old.forwarded at 1641198807161
give action 402============================
log_to_linear:  tensor([[[0.5123]]], device='cuda:0') tensor([[[1.0196]]], device='cuda:0')
log_to_linear action at 1641198807162
bwe changes from to:  [tensor([[[2.2710e-06]]]), tensor([[[2.3156e-06]]])]
step into gymStat at 1641198807162
send bwe to appRecv at 1641198807162
sent bwe to appRecv at 1641198807162
wait for recv string at 1641198807162
recved string at 1641198807389
1
wait for recv [self.estimator, stat] at 1641198807389
recved [self.estimator, stat] at 1641198807389
sorted packlist at 1641198807389
packetSeq:  7187
packetSeq:  7188
packetSeq:  7189
packetSeq:  7190
packetSeq:  7191
packetSeq:  7192
packetSeq:  7193
packetSeq:  7194
packetSeq:  7195
processed packlist at 1641198807389
receiving_rate:  268120.0
delay:  192.5
loss_ratio:  0.0
processed state0-2 at 1641198807389
avgFrameBetween:  6
psnrStat:  [[550195, 550238, 548820, 548755, 548045, 547738, 547633]]
delayStat:  [[53, 53, 53, 53, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [548774.8571428572] [53.0] [0]
processed state3-5 at 1641198807389
liner_to_log:  tensor([[[1.0196]]]) tensor([[[0.5123]]])
linear_to_log at 1641198807389
listState:  [0.06703, 0.12833333333333333, 0.0, 0.5487748571428571, 0.053, 0.0, tensor([[[0.5123]]])]
state_clone_detach at 1641198807390
reward: 0.052247574679885334
state tensor([0.0670, 0.1283, 0.0000, 0.5123], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198807390
state222:  tensor([[0.0670, 0.1283, 0.0000, 0.5123]], device='cuda:0')
policy_old.forwarded at 1641198807392
give action 403============================
log_to_linear:  tensor([[[0.9116]]], device='cuda:0') tensor([[[1.7953]]], device='cuda:0')
log_to_linear action at 1641198807393
bwe changes from to:  [tensor([[[2.3156e-06]]]), tensor([[[4.1571e-06]]])]
step into gymStat at 1641198807393
send bwe to appRecv at 1641198807393
sent bwe to appRecv at 1641198807393
wait for recv string at 1641198807393
recved string at 1641198807590
1
wait for recv [self.estimator, stat] at 1641198807590
recved [self.estimator, stat] at 1641198807590
sorted packlist at 1641198807590
packetSeq:  7196
packetSeq:  7197
packetSeq:  7198
packetSeq:  7199
packetSeq:  7200
packetSeq:  7201
packetSeq:  7202
packetSeq:  7203
packetSeq:  7204
processed packlist at 1641198807590
receiving_rate:  265880.0
delay:  191.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198807590
avgFrameBetween:  6
psnrStat:  [[547055, 546944, 548010, 547607, 546941, 546832]]
delayStat:  [[53, 53, 54, 53, 53, 53]]
skipStat:  [[1, 1, 1, 1, 1, 1]]
skipCount:  0
state:  [547231.5] [53.166666666666664] [0]
processed state3-5 at 1641198807590
liner_to_log:  tensor([[[1.7953]]]) tensor([[[0.9116]]])
linear_to_log at 1641198807591
listState:  [0.06647, 0.12792592592592592, 0.0, 0.5472315, 0.05316666666666667, 0.0, tensor([[[0.9116]]])]
state_clone_detach at 1641198807591
reward: 0.05087017968258867
state tensor([0.0665, 0.1279, 0.0000, 0.9116], device='cuda:0')
one action ended============================

selectttttt
FloatTensor_reshaped at 1641198807592
state222:  tensor([[0.0665, 0.1279, 0.0000, 0.9116]], device='cuda:0')
policy_old.forwarded at 1641198807593
give action 404============================
log_to_linear:  tensor([[[0.7885]]], device='cuda:0') tensor([[[1.5293]]], device='cuda:0')
log_to_linear action at 1641198807594
bwe changes from to:  [tensor([[[4.1571e-06]]]), tensor([[[6.3575e-06]]])]
step into gymStat at 1641198807594
send bwe to appRecv at 1641198807594
sent bwe to appRecv at 1641198807594
wait for recv string at 1641198807594
recved string at 1641198807685
0
sorted packlist at 1641198807716
processed packlist at 1641198807716
receiving_rate:  265880.0
delay:  191.88888888888889
loss_ratio:  0.0
processed state0-2 at 1641198807716
avgFrameBetween:  6
stat is empty!!!
psnrStat:  [[547055, 546944, 548010, 547607, 546941, 546832]]
delayStat:  [[253, 253, 254, 253, 253, 253]]
skipStat:  [[-1, -1, -1, -1, -1, -1]]
skipCount:  0
state:  [547231.5] [253.16666666666666] [32]
processed state3-5 at 1641198807716
liner_to_log:  tensor([[[1.5293]]]) tensor([[[0.7885]]])
linear_to_log at 1641198807716
listState:  [0.06647, 0.12792592592592592, 0.0, 0.5472315, 0.25316666666666665, 1.0, tensor([[[0.7885]]])]
state_clone_detach at 1641198807716
one rtc ended. 405 actions in this RTC. ============================

reward: 0.05087017968258867
state tensor([0.0665, 0.1279, 0.0000, 0.7885], device='cuda:0')
one action ended============================

storage: 
get_value
returns:  [-1.10173501e+00  3.70711830e-01  9.85185217e-01  1.12731738e+00
  1.02541233e+00  9.76676783e-01  9.48286781e-01  9.21074212e-01
  8.66211070e-01  8.62091323e-01  8.14964778e-01  7.34813229e-01
  6.81716244e-01  6.32303820e-01  6.20550797e-01  5.41746066e-01
  4.48789202e-01  4.75551370e-01  5.30835152e-01  5.21455711e-01
  5.15008285e-01  4.66506526e-01  1.34575493e+00  1.16699519e+00
  1.10700995e+00  1.07767810e+00  1.02910422e+00  9.53466895e-01
  8.68836766e-01  7.66238614e-01  7.18448869e-01  6.67376034e-01
  6.13452241e-01  5.47451277e-01  4.99281260e-01  4.39697011e-01
  3.67125615e-01  2.96493113e-01  2.20051715e-01  1.70702888e-01
  1.32809410e-01  7.72432569e-02  5.44828424e-02 -4.12755516e-03
 -8.58940649e-02 -1.48078944e-01 -1.73780394e-01 -2.17422787e-01
 -3.02598304e-01 -3.78857004e-01 -4.35596598e-01 -4.75424326e-01
 -5.70307603e-01  3.95918843e-01  3.47341355e-01  3.19965401e-01
  2.46258044e-01  1.53545450e-01  8.31041229e-02  5.60371193e-02
  7.94246748e-03 -2.85897191e-02 -1.20329387e-01 -1.62146568e-01
 -2.30584339e-01 -3.10898076e-01  7.93327991e-01  1.76040211e+00
  2.92174050e+00  3.91469409e+00  4.03093595e+00  4.08677937e+00
  4.07713009e+00  4.07750148e+00  4.08093221e+00  4.07685329e+00
  4.08299201e+00  4.06219458e+00  4.03906831e+00  4.01950957e+00
  3.99344652e+00  3.94570172e+00  3.93203792e+00  3.92185122e+00
  3.85041460e+00  3.83131133e+00  3.77979936e+00  3.77758467e+00
  3.67438959e+00  3.63904290e+00  3.61406314e+00  3.63134490e+00
  3.64308455e+00  3.65305686e+00  3.60963135e+00  3.57446368e+00
  3.55826253e+00  3.54145956e+00  3.54186100e+00  3.55114606e+00
  3.53918988e+00  3.48478528e+00  3.45491687e+00  3.43574302e+00
  3.41922054e+00  3.38855845e+00  3.37095202e+00  3.36849173e+00
  3.34871308e+00  3.33135810e+00  3.39056921e+00  3.30684917e+00
  3.28470631e+00  4.21418020e+00  5.30517271e+00  5.23049248e+00
  5.21360926e+00  5.19007846e+00  5.16735226e+00  5.13414275e+00
  5.11723418e+00  5.10070125e+00  5.12294834e+00  5.11078914e+00
  5.09685862e+00  5.10398305e+00  5.10880016e+00  5.12601332e+00
  5.09672917e+00  5.09557447e+00  5.12759300e+00  5.14053130e+00
  5.10924911e+00  5.13127227e+00  5.14587821e+00  5.14533113e+00
  5.16283667e+00  5.16722171e+00  5.16040047e+00  5.18245859e+00
  5.16825401e+00  5.17044843e+00  5.15299221e+00  5.18150829e+00
  5.16598531e+00  5.15795054e+00  5.14516713e+00  5.15755727e+00
  5.15610928e+00  5.12431341e+00  5.16163049e+00  5.12074947e+00
  5.09510318e+00  5.04789537e+00  5.02870793e+00  5.02968586e+00
  5.01013100e+00  4.95810383e+00  4.91371507e+00  4.90071405e+00
  4.87590662e+00  4.88884053e+00  4.85393296e+00  4.86352757e+00
  4.80330096e+00  4.78453681e+00  4.81838358e+00  4.87749556e+00
  4.82975876e+00  4.79197990e+00  4.77096044e+00  4.76594586e+00
  4.78925842e+00  4.76777189e+00  4.77591921e+00  4.73241884e+00
  4.72932311e+00  4.70215190e+00  4.71622726e+00  4.65319697e+00
  4.62287438e+00  4.63071088e+00  4.64501843e+00  4.67431351e+00
  4.66846590e+00  4.65277560e+00  4.60724230e+00  4.60055299e+00
  4.57657182e+00  4.60609627e+00  4.63681505e+00  4.64193114e+00
  4.60571247e+00  4.58956246e+00  4.58947254e+00  4.59035574e+00
  4.55412318e+00  4.57115167e+00  4.55847450e+00  4.56678667e+00
  4.59640162e+00  4.57306126e+00  4.58653111e+00  4.52897424e+00
  4.50726777e+00  4.45793124e+00  4.43415186e+00  4.46544616e+00
  4.44487769e+00  4.41432007e+00  4.40151561e+00  4.34470205e+00
  4.33090099e+00  4.34363173e+00  4.34454541e+00  4.33075713e+00
  4.33127765e+00  4.29031600e+00  4.26329129e+00  4.26424394e+00
  4.22613952e+00  4.21448899e+00  4.18090290e+00  4.22692767e+00
  4.21115604e+00  4.18670481e+00  4.19658635e+00  4.17031364e+00
  4.15831642e+00  4.13112324e+00  4.13571825e+00  4.16244963e+00
  4.17358421e+00  4.14495926e+00  4.12687493e+00  4.11005266e+00
  4.09818059e+00  4.07210337e+00  4.05371954e+00  3.99488595e+00
  3.98217191e+00  3.99915912e+00  3.95507083e+00  3.97226401e+00
  3.93579910e+00  3.91661480e+00  3.86852979e+00  3.84622420e+00
  3.86576210e+00  3.86896123e+00  3.85117597e+00  3.81144923e+00
  3.75423451e+00  3.74492155e+00  3.78333027e+00  3.72546653e+00
  3.68690046e+00  3.65774359e+00  3.63676430e+00  3.62937060e+00
  3.58412250e+00  3.55435758e+00  3.52259069e+00  3.51071297e+00
  3.51754055e+00  3.54042854e+00  3.51689901e+00  3.49498703e+00
  3.44637469e+00  3.41781179e+00  3.45723954e+00  3.37266392e+00
  3.37165316e+00  3.36450727e+00  3.35546435e+00  3.33785356e+00
  3.37775266e+00  3.35010148e+00  3.28951082e+00  3.25200874e+00
  3.26773006e+00  3.26340270e+00  3.31619917e+00  3.25922588e+00
  3.19518407e+00  3.18072333e+00  3.17044939e+00  3.18613693e+00
  3.16918268e+00  3.15145144e+00  3.15072088e+00  3.13160570e+00
  3.08841221e+00  3.01416605e+00  2.96557934e+00  2.91879146e+00
  2.88632087e+00  2.81422438e+00  2.75590829e+00  2.76280823e+00
  2.74086027e+00  3.64214393e+00  3.63809308e+00  3.58332660e+00
  3.55217565e+00  3.52450156e+00  3.52489097e+00  3.51591247e+00
  3.45690381e+00  3.42049235e+00  3.38098490e+00  3.37027213e+00
  3.34789030e+00  3.32751082e+00  3.29785853e+00  3.29991802e+00
  3.24816365e+00  3.23495290e+00  3.22983416e+00  3.19878696e+00
  3.21717080e+00  3.16634536e+00  3.16676096e+00  3.17866789e+00
  3.15826047e+00  3.15274473e+00  3.10896346e+00  3.12677478e+00
  3.13859826e+00  3.06226623e+00  3.05267799e+00  2.99035025e+00
  3.02432706e+00  3.00603127e+00  2.99882465e+00  2.98300260e+00
  2.93523597e+00  2.86660784e+00  2.85426592e+00  2.77845653e+00
  2.75170783e+00  2.70929953e+00  2.65551807e+00  2.64777438e+00
  2.63427198e+00  2.58254368e+00  2.53890055e+00  2.48726226e+00
  2.43785782e+00  2.38056512e+00  2.36489626e+00  2.40161414e+00
  2.36155030e+00  2.31226805e+00  2.26991276e+00  2.22429148e+00
  2.20825973e+00  2.21041158e+00  2.12334250e+00  2.12800949e+00
  2.07083101e+00  2.04485916e+00  2.01741664e+00  1.99960463e+00
  1.92701519e+00  1.90653856e+00  1.89916867e+00  1.89366411e+00
  1.90055960e+00  1.83612218e+00  1.78616649e+00  1.72743170e+00
  1.69925581e+00  1.67619879e+00  1.64312759e+00  1.61555886e+00
  1.54053237e+00  1.49447119e+00  1.45001239e+00  1.43301456e+00
  1.42814758e+00  1.36586811e+00  1.34117809e+00  1.29395086e+00
  1.21592548e+00  1.17113985e+00  1.12492567e+00  1.04180638e+00
  9.86644307e-01  9.08740607e-01  8.16549049e-01  7.67434257e-01
  7.29228107e-01  6.41677196e-01  5.66642989e-01  4.78386213e-01
  4.08861315e-01  3.83557885e-01  3.44676683e-01  3.03736464e-01
  2.62790750e-01  1.66436847e-01  1.02609053e-01  5.08701797e-02
 -7.58337259e+00]
rewards:  [tensor([-1.1017], dtype=torch.float64), tensor([0.3707], dtype=torch.float64), tensor([0.9852], dtype=torch.float64), tensor([1.1273], dtype=torch.float64), tensor([1.0254], dtype=torch.float64), tensor([0.9767], dtype=torch.float64), tensor([0.9483], dtype=torch.float64), tensor([0.9211], dtype=torch.float64), tensor([0.8662], dtype=torch.float64), tensor([0.8621], dtype=torch.float64), tensor([0.8150], dtype=torch.float64), tensor([0.7348], dtype=torch.float64), tensor([0.6817], dtype=torch.float64), tensor([0.6323], dtype=torch.float64), tensor([0.6206], dtype=torch.float64), tensor([0.5417], dtype=torch.float64), tensor([0.4488], dtype=torch.float64), tensor([0.4756], dtype=torch.float64), tensor([0.5308], dtype=torch.float64), tensor([0.5215], dtype=torch.float64), tensor([0.5150], dtype=torch.float64), tensor([0.4665], dtype=torch.float64), tensor([1.3458], dtype=torch.float64), tensor([1.1670], dtype=torch.float64), tensor([1.1070], dtype=torch.float64), tensor([1.0777], dtype=torch.float64), tensor([1.0291], dtype=torch.float64), tensor([0.9535], dtype=torch.float64), tensor([0.8688], dtype=torch.float64), tensor([0.7662], dtype=torch.float64), tensor([0.7184], dtype=torch.float64), tensor([0.6674], dtype=torch.float64), tensor([0.6135], dtype=torch.float64), tensor([0.5475], dtype=torch.float64), tensor([0.4993], dtype=torch.float64), tensor([0.4397], dtype=torch.float64), tensor([0.3671], dtype=torch.float64), tensor([0.2965], dtype=torch.float64), tensor([0.2201], dtype=torch.float64), tensor([0.1707], dtype=torch.float64), tensor([0.1328], dtype=torch.float64), tensor([0.0772], dtype=torch.float64), tensor([0.0545], dtype=torch.float64), tensor([-0.0041], dtype=torch.float64), tensor([-0.0859], dtype=torch.float64), tensor([-0.1481], dtype=torch.float64), tensor([-0.1738], dtype=torch.float64), tensor([-0.2174], dtype=torch.float64), tensor([-0.3026], dtype=torch.float64), tensor([-0.3789], dtype=torch.float64), tensor([-0.4356], dtype=torch.float64), tensor([-0.4754], dtype=torch.float64), tensor([-0.5703], dtype=torch.float64), tensor([0.3959], dtype=torch.float64), tensor([0.3473], dtype=torch.float64), tensor([0.3200], dtype=torch.float64), tensor([0.2463], dtype=torch.float64), tensor([0.1535], dtype=torch.float64), tensor([0.0831], dtype=torch.float64), tensor([0.0560], dtype=torch.float64), tensor([0.0079], dtype=torch.float64), tensor([-0.0286], dtype=torch.float64), tensor([-0.1203], dtype=torch.float64), tensor([-0.1621], dtype=torch.float64), tensor([-0.2306], dtype=torch.float64), tensor([-0.3109], dtype=torch.float64), tensor([0.7933], dtype=torch.float64), tensor([1.7604], dtype=torch.float64), tensor([2.9217], dtype=torch.float64), tensor([3.9147], dtype=torch.float64), tensor([4.0309], dtype=torch.float64), tensor([4.0868], dtype=torch.float64), tensor([4.0771], dtype=torch.float64), tensor([4.0775], dtype=torch.float64), tensor([4.0809], dtype=torch.float64), tensor([4.0769], dtype=torch.float64), tensor([4.0830], dtype=torch.float64), tensor([4.0622], dtype=torch.float64), tensor([4.0391], dtype=torch.float64), tensor([4.0195], dtype=torch.float64), tensor([3.9934], dtype=torch.float64), tensor([3.9457], dtype=torch.float64), tensor([3.9320], dtype=torch.float64), tensor([3.9219], dtype=torch.float64), tensor([3.8504], dtype=torch.float64), tensor([3.8313], dtype=torch.float64), tensor([3.7798], dtype=torch.float64), tensor([3.7776], dtype=torch.float64), tensor([3.6744], dtype=torch.float64), tensor([3.6390], dtype=torch.float64), tensor([3.6141], dtype=torch.float64), tensor([3.6313], dtype=torch.float64), tensor([3.6431], dtype=torch.float64), tensor([3.6531], dtype=torch.float64), tensor([3.6096], dtype=torch.float64), tensor([3.5745], dtype=torch.float64), tensor([3.5583], dtype=torch.float64), tensor([3.5415], dtype=torch.float64), tensor([3.5419], dtype=torch.float64), tensor([3.5511], dtype=torch.float64), tensor([3.5392], dtype=torch.float64), tensor([3.4848], dtype=torch.float64), tensor([3.4549], dtype=torch.float64), tensor([3.4357], dtype=torch.float64), tensor([3.4192], dtype=torch.float64), tensor([3.3886], dtype=torch.float64), tensor([3.3710], dtype=torch.float64), tensor([3.3685], dtype=torch.float64), tensor([3.3487], dtype=torch.float64), tensor([3.3314], dtype=torch.float64), tensor([3.3906], dtype=torch.float64), tensor([3.3068], dtype=torch.float64), tensor([3.2847], dtype=torch.float64), tensor([4.2142], dtype=torch.float64), tensor([5.3052], dtype=torch.float64), tensor([5.2305], dtype=torch.float64), tensor([5.2136], dtype=torch.float64), tensor([5.1901], dtype=torch.float64), tensor([5.1674], dtype=torch.float64), tensor([5.1341], dtype=torch.float64), tensor([5.1172], dtype=torch.float64), tensor([5.1007], dtype=torch.float64), tensor([5.1229], dtype=torch.float64), tensor([5.1108], dtype=torch.float64), tensor([5.0969], dtype=torch.float64), tensor([5.1040], dtype=torch.float64), tensor([5.1088], dtype=torch.float64), tensor([5.1260], dtype=torch.float64), tensor([5.0967], dtype=torch.float64), tensor([5.0956], dtype=torch.float64), tensor([5.1276], dtype=torch.float64), tensor([5.1405], dtype=torch.float64), tensor([5.1092], dtype=torch.float64), tensor([5.1313], dtype=torch.float64), tensor([5.1459], dtype=torch.float64), tensor([5.1453], dtype=torch.float64), tensor([5.1628], dtype=torch.float64), tensor([5.1672], dtype=torch.float64), tensor([5.1604], dtype=torch.float64), tensor([5.1825], dtype=torch.float64), tensor([5.1683], dtype=torch.float64), tensor([5.1704], dtype=torch.float64), tensor([5.1530], dtype=torch.float64), tensor([5.1815], dtype=torch.float64), tensor([5.1660], dtype=torch.float64), tensor([5.1580], dtype=torch.float64), tensor([5.1452], dtype=torch.float64), tensor([5.1576], dtype=torch.float64), tensor([5.1561], dtype=torch.float64), tensor([5.1243], dtype=torch.float64), tensor([5.1616], dtype=torch.float64), tensor([5.1207], dtype=torch.float64), tensor([5.0951], dtype=torch.float64), tensor([5.0479], dtype=torch.float64), tensor([5.0287], dtype=torch.float64), tensor([5.0297], dtype=torch.float64), tensor([5.0101], dtype=torch.float64), tensor([4.9581], dtype=torch.float64), tensor([4.9137], dtype=torch.float64), tensor([4.9007], dtype=torch.float64), tensor([4.8759], dtype=torch.float64), tensor([4.8888], dtype=torch.float64), tensor([4.8539], dtype=torch.float64), tensor([4.8635], dtype=torch.float64), tensor([4.8033], dtype=torch.float64), tensor([4.7845], dtype=torch.float64), tensor([4.8184], dtype=torch.float64), tensor([4.8775], dtype=torch.float64), tensor([4.8298], dtype=torch.float64), tensor([4.7920], dtype=torch.float64), tensor([4.7710], dtype=torch.float64), tensor([4.7659], dtype=torch.float64), tensor([4.7893], dtype=torch.float64), tensor([4.7678], dtype=torch.float64), tensor([4.7759], dtype=torch.float64), tensor([4.7324], dtype=torch.float64), tensor([4.7293], dtype=torch.float64), tensor([4.7022], dtype=torch.float64), tensor([4.7162], dtype=torch.float64), tensor([4.6532], dtype=torch.float64), tensor([4.6229], dtype=torch.float64), tensor([4.6307], dtype=torch.float64), tensor([4.6450], dtype=torch.float64), tensor([4.6743], dtype=torch.float64), tensor([4.6685], dtype=torch.float64), tensor([4.6528], dtype=torch.float64), tensor([4.6072], dtype=torch.float64), tensor([4.6006], dtype=torch.float64), tensor([4.5766], dtype=torch.float64), tensor([4.6061], dtype=torch.float64), tensor([4.6368], dtype=torch.float64), tensor([4.6419], dtype=torch.float64), tensor([4.6057], dtype=torch.float64), tensor([4.5896], dtype=torch.float64), tensor([4.5895], dtype=torch.float64), tensor([4.5904], dtype=torch.float64), tensor([4.5541], dtype=torch.float64), tensor([4.5712], dtype=torch.float64), tensor([4.5585], dtype=torch.float64), tensor([4.5668], dtype=torch.float64), tensor([4.5964], dtype=torch.float64), tensor([4.5731], dtype=torch.float64), tensor([4.5865], dtype=torch.float64), tensor([4.5290], dtype=torch.float64), tensor([4.5073], dtype=torch.float64), tensor([4.4579], dtype=torch.float64), tensor([4.4342], dtype=torch.float64), tensor([4.4654], dtype=torch.float64), tensor([4.4449], dtype=torch.float64), tensor([4.4143], dtype=torch.float64), tensor([4.4015], dtype=torch.float64), tensor([4.3447], dtype=torch.float64), tensor([4.3309], dtype=torch.float64), tensor([4.3436], dtype=torch.float64), tensor([4.3445], dtype=torch.float64), tensor([4.3308], dtype=torch.float64), tensor([4.3313], dtype=torch.float64), tensor([4.2903], dtype=torch.float64), tensor([4.2633], dtype=torch.float64), tensor([4.2642], dtype=torch.float64), tensor([4.2261], dtype=torch.float64), tensor([4.2145], dtype=torch.float64), tensor([4.1809], dtype=torch.float64), tensor([4.2269], dtype=torch.float64), tensor([4.2112], dtype=torch.float64), tensor([4.1867], dtype=torch.float64), tensor([4.1966], dtype=torch.float64), tensor([4.1703], dtype=torch.float64), tensor([4.1583], dtype=torch.float64), tensor([4.1311], dtype=torch.float64), tensor([4.1357], dtype=torch.float64), tensor([4.1624], dtype=torch.float64), tensor([4.1736], dtype=torch.float64), tensor([4.1450], dtype=torch.float64), tensor([4.1269], dtype=torch.float64), tensor([4.1101], dtype=torch.float64), tensor([4.0982], dtype=torch.float64), tensor([4.0721], dtype=torch.float64), tensor([4.0537], dtype=torch.float64), tensor([3.9949], dtype=torch.float64), tensor([3.9822], dtype=torch.float64), tensor([3.9992], dtype=torch.float64), tensor([3.9551], dtype=torch.float64), tensor([3.9723], dtype=torch.float64), tensor([3.9358], dtype=torch.float64), tensor([3.9166], dtype=torch.float64), tensor([3.8685], dtype=torch.float64), tensor([3.8462], dtype=torch.float64), tensor([3.8658], dtype=torch.float64), tensor([3.8690], dtype=torch.float64), tensor([3.8512], dtype=torch.float64), tensor([3.8114], dtype=torch.float64), tensor([3.7542], dtype=torch.float64), tensor([3.7449], dtype=torch.float64), tensor([3.7833], dtype=torch.float64), tensor([3.7255], dtype=torch.float64), tensor([3.6869], dtype=torch.float64), tensor([3.6577], dtype=torch.float64), tensor([3.6368], dtype=torch.float64), tensor([3.6294], dtype=torch.float64), tensor([3.5841], dtype=torch.float64), tensor([3.5544], dtype=torch.float64), tensor([3.5226], dtype=torch.float64), tensor([3.5107], dtype=torch.float64), tensor([3.5175], dtype=torch.float64), tensor([3.5404], dtype=torch.float64), tensor([3.5169], dtype=torch.float64), tensor([3.4950], dtype=torch.float64), tensor([3.4464], dtype=torch.float64), tensor([3.4178], dtype=torch.float64), tensor([3.4572], dtype=torch.float64), tensor([3.3727], dtype=torch.float64), tensor([3.3717], dtype=torch.float64), tensor([3.3645], dtype=torch.float64), tensor([3.3555], dtype=torch.float64), tensor([3.3379], dtype=torch.float64), tensor([3.3778], dtype=torch.float64), tensor([3.3501], dtype=torch.float64), tensor([3.2895], dtype=torch.float64), tensor([3.2520], dtype=torch.float64), tensor([3.2677], dtype=torch.float64), tensor([3.2634], dtype=torch.float64), tensor([3.3162], dtype=torch.float64), tensor([3.2592], dtype=torch.float64), tensor([3.1952], dtype=torch.float64), tensor([3.1807], dtype=torch.float64), tensor([3.1704], dtype=torch.float64), tensor([3.1861], dtype=torch.float64), tensor([3.1692], dtype=torch.float64), tensor([3.1515], dtype=torch.float64), tensor([3.1507], dtype=torch.float64), tensor([3.1316], dtype=torch.float64), tensor([3.0884], dtype=torch.float64), tensor([3.0142], dtype=torch.float64), tensor([2.9656], dtype=torch.float64), tensor([2.9188], dtype=torch.float64), tensor([2.8863], dtype=torch.float64), tensor([2.8142], dtype=torch.float64), tensor([2.7559], dtype=torch.float64), tensor([2.7628], dtype=torch.float64), tensor([2.7409], dtype=torch.float64), tensor([3.6421], dtype=torch.float64), tensor([3.6381], dtype=torch.float64), tensor([3.5833], dtype=torch.float64), tensor([3.5522], dtype=torch.float64), tensor([3.5245], dtype=torch.float64), tensor([3.5249], dtype=torch.float64), tensor([3.5159], dtype=torch.float64), tensor([3.4569], dtype=torch.float64), tensor([3.4205], dtype=torch.float64), tensor([3.3810], dtype=torch.float64), tensor([3.3703], dtype=torch.float64), tensor([3.3479], dtype=torch.float64), tensor([3.3275], dtype=torch.float64), tensor([3.2979], dtype=torch.float64), tensor([3.2999], dtype=torch.float64), tensor([3.2482], dtype=torch.float64), tensor([3.2350], dtype=torch.float64), tensor([3.2298], dtype=torch.float64), tensor([3.1988], dtype=torch.float64), tensor([3.2172], dtype=torch.float64), tensor([3.1663], dtype=torch.float64), tensor([3.1668], dtype=torch.float64), tensor([3.1787], dtype=torch.float64), tensor([3.1583], dtype=torch.float64), tensor([3.1527], dtype=torch.float64), tensor([3.1090], dtype=torch.float64), tensor([3.1268], dtype=torch.float64), tensor([3.1386], dtype=torch.float64), tensor([3.0623], dtype=torch.float64), tensor([3.0527], dtype=torch.float64), tensor([2.9904], dtype=torch.float64), tensor([3.0243], dtype=torch.float64), tensor([3.0060], dtype=torch.float64), tensor([2.9988], dtype=torch.float64), tensor([2.9830], dtype=torch.float64), tensor([2.9352], dtype=torch.float64), tensor([2.8666], dtype=torch.float64), tensor([2.8543], dtype=torch.float64), tensor([2.7785], dtype=torch.float64), tensor([2.7517], dtype=torch.float64), tensor([2.7093], dtype=torch.float64), tensor([2.6555], dtype=torch.float64), tensor([2.6478], dtype=torch.float64), tensor([2.6343], dtype=torch.float64), tensor([2.5825], dtype=torch.float64), tensor([2.5389], dtype=torch.float64), tensor([2.4873], dtype=torch.float64), tensor([2.4379], dtype=torch.float64), tensor([2.3806], dtype=torch.float64), tensor([2.3649], dtype=torch.float64), tensor([2.4016], dtype=torch.float64), tensor([2.3616], dtype=torch.float64), tensor([2.3123], dtype=torch.float64), tensor([2.2699], dtype=torch.float64), tensor([2.2243], dtype=torch.float64), tensor([2.2083], dtype=torch.float64), tensor([2.2104], dtype=torch.float64), tensor([2.1233], dtype=torch.float64), tensor([2.1280], dtype=torch.float64), tensor([2.0708], dtype=torch.float64), tensor([2.0449], dtype=torch.float64), tensor([2.0174], dtype=torch.float64), tensor([1.9996], dtype=torch.float64), tensor([1.9270], dtype=torch.float64), tensor([1.9065], dtype=torch.float64), tensor([1.8992], dtype=torch.float64), tensor([1.8937], dtype=torch.float64), tensor([1.9006], dtype=torch.float64), tensor([1.8361], dtype=torch.float64), tensor([1.7862], dtype=torch.float64), tensor([1.7274], dtype=torch.float64), tensor([1.6993], dtype=torch.float64), tensor([1.6762], dtype=torch.float64), tensor([1.6431], dtype=torch.float64), tensor([1.6156], dtype=torch.float64), tensor([1.5405], dtype=torch.float64), tensor([1.4945], dtype=torch.float64), tensor([1.4500], dtype=torch.float64), tensor([1.4330], dtype=torch.float64), tensor([1.4281], dtype=torch.float64), tensor([1.3659], dtype=torch.float64), tensor([1.3412], dtype=torch.float64), tensor([1.2940], dtype=torch.float64), tensor([1.2159], dtype=torch.float64), tensor([1.1711], dtype=torch.float64), tensor([1.1249], dtype=torch.float64), tensor([1.0418], dtype=torch.float64), tensor([0.9866], dtype=torch.float64), tensor([0.9087], dtype=torch.float64), tensor([0.8165], dtype=torch.float64), tensor([0.7674], dtype=torch.float64), tensor([0.7292], dtype=torch.float64), tensor([0.6417], dtype=torch.float64), tensor([0.5666], dtype=torch.float64), tensor([0.4784], dtype=torch.float64), tensor([0.4089], dtype=torch.float64), tensor([0.3836], dtype=torch.float64), tensor([0.3447], dtype=torch.float64), tensor([0.3037], dtype=torch.float64), tensor([0.2628], dtype=torch.float64), tensor([0.1664], dtype=torch.float64), tensor([0.1026], dtype=torch.float64), tensor([0.0509], dtype=torch.float64)]
values:  [tensor([[[-2.7617]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7867]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.6916]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.5776]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.8719]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.3302]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.8395]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.9167]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.7975]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.2815]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.4606]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.4284]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.9816]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.7439]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.1526]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.7770]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.4920]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-2.8906]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.3643]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.6653]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.9582]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-4.1025]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2181]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.3870]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.1009]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-4.6264]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.1001]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.0914]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.4441]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.6996]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-4.2312]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.1603]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.2949]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-4.8633]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.7533]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.8884]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.2791]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.3920]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.2850]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.1797]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.4149]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.7093]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.5449]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.2619]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.9673]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.9260]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.6807]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.6661]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.8074]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.2043]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2395]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.9568]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.0612]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-5.0728]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.0201]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.8047]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.3907]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.4800]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.3231]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.8484]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.3641]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.5101]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8904]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5001]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.6511]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8784]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-13.6993]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-13.1266]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-14.5462]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-14.4491]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-20.9114]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.0246]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8604]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5202]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.8090]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5134]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.6571]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8520]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0059]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9815]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9041]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3850]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1688]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0681]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6275]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.6375]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2081]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9417]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9141]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2905]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9866]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8535]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1482]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2394]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5390]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8092]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1852]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4277]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4131]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1625]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3756]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7436]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3123]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6894]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.3290]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9319]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.3879]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8381]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1187]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3126]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4441]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7110]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3595]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-12.0277]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-13.7040]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-16.7017]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0362]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8043]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6475]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8818]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8793]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7846]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5775]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7736]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4410]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5328]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1899]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2406]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4371]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8586]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7656]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4815]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5244]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5872]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.8286]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3697]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0040]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5598]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6744]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4650]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9150]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7205]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7492]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7562]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9681]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0002]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.6043]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4118]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7152]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2042]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5371]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8388]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0144]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7426]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9632]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5176]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4196]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6623]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6332]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8383]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3594]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7839]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0845]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8495]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8005]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0058]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3343]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1792]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6361]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0796]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1009]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2525]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4049]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8965]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8555]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.4121]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2228]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1932]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0021]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.5861]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1231]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8520]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5495]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9841]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8752]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2562]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.4577]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9451]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5393]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2403]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5477]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7293]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.6870]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8948]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0572]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4678]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8395]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4420]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7641]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5101]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.4547]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9280]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5281]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2226]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8584]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8059]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7711]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5756]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5489]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8360]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3916]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5793]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9368]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5283]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3744]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5631]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7687]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8853]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2382]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5959]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0654]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6770]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9532]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6708]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2007]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4884]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3765]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7638]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7940]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1165]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8873]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7131]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5240]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1194]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8767]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1037]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8060]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0297]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2174]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.5080]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.2589]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0100]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.3649]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9335]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0516]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3657]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1979]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9277]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5712]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7177]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7570]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8873]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0934]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7206]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8581]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7106]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9835]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.4259]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7571]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6942]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8376]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6175]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3574]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5426]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4083]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2721]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5758]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5956]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0534]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8368]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9675]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2072]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9213]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9327]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2524]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.6623]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3981]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0054]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6473]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2376]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3886]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2984]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3134]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3989]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8476]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0042]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0456]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7792]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3734]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8569]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7703]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7584]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5606]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9624]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8294]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6551]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2285]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2830]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0476]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8396]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9142]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-6.4828]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2154]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9779]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4516]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7258]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4826]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4161]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9503]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4734]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7813]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1332]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5766]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7940]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9353]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.7668]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0818]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5729]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.4882]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8632]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.3977]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9282]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5333]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6946]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7532]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6519]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6798]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2152]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1818]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7305]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5696]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0518]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8272]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2512]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5992]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4902]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8010]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0454]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8436]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1963]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4748]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4885]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8167]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5845]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.3441]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7030]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4068]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8182]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4233]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9484]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7661]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8926]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2844]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2799]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.4395]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1135]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8364]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3061]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.1263]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6710]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.0963]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9796]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5728]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5829]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-9.4359]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1245]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8576]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9163]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.2939]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6163]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8188]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3113]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.4554]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8653]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.7455]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6347]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.9604]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8642]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8601]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5082]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.5490]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9987]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2141]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.1785]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8635]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3714]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6816]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.8917]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.1421]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5397]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6772]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4529]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.2886]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.3353]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0930]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4994]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6704]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.5644]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.8807]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-7.9935]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.0811]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.6674]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4599]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-8.4735]]], device='cuda:0', grad_fn=<AddBackward0>)]
actions:  [tensor([[[0.5067]]], device='cuda:0'), tensor([[[0.5770]]], device='cuda:0'), tensor([[[0.3262]]], device='cuda:0'), tensor([[[0.3593]]], device='cuda:0'), tensor([[[0.1002]]], device='cuda:0'), tensor([[[0.4624]]], device='cuda:0'), tensor([[[0.3697]]], device='cuda:0'), tensor([[[0.1311]]], device='cuda:0'), tensor([[[0.5043]]], device='cuda:0'), tensor([[[0.7176]]], device='cuda:0'), tensor([[[0.2519]]], device='cuda:0'), tensor([[[0.4879]]], device='cuda:0'), tensor([[[0.2463]]], device='cuda:0'), tensor([[[0.5759]]], device='cuda:0'), tensor([[[0.9700]]], device='cuda:0'), tensor([[[0.6739]]], device='cuda:0'), tensor([[[0.1536]]], device='cuda:0'), tensor([[[0.6280]]], device='cuda:0'), tensor([[[0.6404]]], device='cuda:0'), tensor([[[0.7263]]], device='cuda:0'), tensor([[[0.6351]]], device='cuda:0'), tensor([[[0.3844]]], device='cuda:0'), tensor([[[0.4349]]], device='cuda:0'), tensor([[[0.5820]]], device='cuda:0'), tensor([[[0.3312]]], device='cuda:0'), tensor([[[0.4572]]], device='cuda:0'), tensor([[[0.7257]]], device='cuda:0'), tensor([[[0.4563]]], device='cuda:0'), tensor([[[0.4922]]], device='cuda:0'), tensor([[[1.0820]]], device='cuda:0'), tensor([[[0.6723]]], device='cuda:0'), tensor([[[0.7181]]], device='cuda:0'), tensor([[[0.1626]]], device='cuda:0'), tensor([[[0.3958]]], device='cuda:0'), tensor([[[0.3726]]], device='cuda:0'), tensor([[[0.4704]]], device='cuda:0'), tensor([[[0.1525]]], device='cuda:0'), tensor([[[0.5828]]], device='cuda:0'), tensor([[[0.3508]]], device='cuda:0'), tensor([[[0.5499]]], device='cuda:0'), tensor([[[0.5099]]], device='cuda:0'), tensor([[[0.4723]]], device='cuda:0'), tensor([[[0.7926]]], device='cuda:0'), tensor([[[0.2150]]], device='cuda:0'), tensor([[[0.4959]]], device='cuda:0'), tensor([[[0.3862]]], device='cuda:0'), tensor([[[0.6804]]], device='cuda:0'), tensor([[[0.3400]]], device='cuda:0'), tensor([[[0.1508]]], device='cuda:0'), tensor([[[0.4804]]], device='cuda:0'), tensor([[[0.6206]]], device='cuda:0'), tensor([[[0.3326]]], device='cuda:0'), tensor([[[0.5839]]], device='cuda:0'), tensor([[[0.4928]]], device='cuda:0'), tensor([[[0.3314]]], device='cuda:0'), tensor([[[0.6069]]], device='cuda:0'), tensor([[[0.9298]]], device='cuda:0'), tensor([[[0.1745]]], device='cuda:0'), tensor([[[0.7630]]], device='cuda:0'), tensor([[[0.4204]]], device='cuda:0'), tensor([[[0.1834]]], device='cuda:0'), tensor([[[0.4517]]], device='cuda:0'), tensor([[[0.5339]]], device='cuda:0'), tensor([[[0.1724]]], device='cuda:0'), tensor([[[0.5067]]], device='cuda:0'), tensor([[[0.3231]]], device='cuda:0'), tensor([[[0.2117]]], device='cuda:0'), tensor([[[0.3207]]], device='cuda:0'), tensor([[[0.5421]]], device='cuda:0'), tensor([[[0.7296]]], device='cuda:0'), tensor([[[0.0714]]], device='cuda:0'), tensor([[[0.5427]]], device='cuda:0'), tensor([[[0.3482]]], device='cuda:0'), tensor([[[0.1872]]], device='cuda:0'), tensor([[[0.7259]]], device='cuda:0'), tensor([[[0.5236]]], device='cuda:0'), tensor([[[0.5523]]], device='cuda:0'), tensor([[[0.5002]]], device='cuda:0'), tensor([[[0.5451]]], device='cuda:0'), tensor([[[0.3745]]], device='cuda:0'), tensor([[[0.4587]]], device='cuda:0'), tensor([[[0.4904]]], device='cuda:0'), tensor([[[0.3844]]], device='cuda:0'), tensor([[[0.6012]]], device='cuda:0'), tensor([[[0.8250]]], device='cuda:0'), tensor([[[0.1865]]], device='cuda:0'), tensor([[[0.3587]]], device='cuda:0'), tensor([[[0.6691]]], device='cuda:0'), tensor([[[0.6568]]], device='cuda:0'), tensor([[[0.3339]]], device='cuda:0'), tensor([[[0.6868]]], device='cuda:0'), tensor([[[0.4679]]], device='cuda:0'), tensor([[[0.4214]]], device='cuda:0'), tensor([[[0.3844]]], device='cuda:0'), tensor([[[0.2629]]], device='cuda:0'), tensor([[[0.7094]]], device='cuda:0'), tensor([[[0.5189]]], device='cuda:0'), tensor([[[0.4757]]], device='cuda:0'), tensor([[[0.6352]]], device='cuda:0'), tensor([[[0.5216]]], device='cuda:0'), tensor([[[0.4212]]], device='cuda:0'), tensor([[[0.3302]]], device='cuda:0'), tensor([[[0.4506]]], device='cuda:0'), tensor([[[0.0663]]], device='cuda:0'), tensor([[[0.2699]]], device='cuda:0'), tensor([[[0.1519]]], device='cuda:0'), tensor([[[0.3036]]], device='cuda:0'), tensor([[[0.3328]]], device='cuda:0'), tensor([[[0.3633]]], device='cuda:0'), tensor([[[0.5771]]], device='cuda:0'), tensor([[[0.6692]]], device='cuda:0'), tensor([[[0.5708]]], device='cuda:0'), tensor([[[0.3394]]], device='cuda:0'), tensor([[[0.2648]]], device='cuda:0'), tensor([[[0.2898]]], device='cuda:0'), tensor([[[-0.0286]]], device='cuda:0'), tensor([[[0.6333]]], device='cuda:0'), tensor([[[0.3840]]], device='cuda:0'), tensor([[[0.5357]]], device='cuda:0'), tensor([[[0.4694]]], device='cuda:0'), tensor([[[0.4088]]], device='cuda:0'), tensor([[[0.4358]]], device='cuda:0'), tensor([[[0.4307]]], device='cuda:0'), tensor([[[0.6895]]], device='cuda:0'), tensor([[[0.4277]]], device='cuda:0'), tensor([[[0.7286]]], device='cuda:0'), tensor([[[0.3825]]], device='cuda:0'), tensor([[[0.3176]]], device='cuda:0'), tensor([[[0.2233]]], device='cuda:0'), tensor([[[0.8588]]], device='cuda:0'), tensor([[[0.5396]]], device='cuda:0'), tensor([[[0.0662]]], device='cuda:0'), tensor([[[0.5572]]], device='cuda:0'), tensor([[[1.0053]]], device='cuda:0'), tensor([[[0.5329]]], device='cuda:0'), tensor([[[0.7633]]], device='cuda:0'), tensor([[[0.4947]]], device='cuda:0'), tensor([[[0.5668]]], device='cuda:0'), tensor([[[0.6170]]], device='cuda:0'), tensor([[[0.4601]]], device='cuda:0'), tensor([[[0.3832]]], device='cuda:0'), tensor([[[0.1347]]], device='cuda:0'), tensor([[[0.4808]]], device='cuda:0'), tensor([[[0.5327]]], device='cuda:0'), tensor([[[0.5147]]], device='cuda:0'), tensor([[[0.0373]]], device='cuda:0'), tensor([[[0.3219]]], device='cuda:0'), tensor([[[0.6842]]], device='cuda:0'), tensor([[[0.4688]]], device='cuda:0'), tensor([[[0.9013]]], device='cuda:0'), tensor([[[0.4941]]], device='cuda:0'), tensor([[[0.2622]]], device='cuda:0'), tensor([[[0.3475]]], device='cuda:0'), tensor([[[0.4179]]], device='cuda:0'), tensor([[[0.9137]]], device='cuda:0'), tensor([[[0.6066]]], device='cuda:0'), tensor([[[0.6707]]], device='cuda:0'), tensor([[[0.3549]]], device='cuda:0'), tensor([[[0.4066]]], device='cuda:0'), tensor([[[0.3004]]], device='cuda:0'), tensor([[[0.2282]]], device='cuda:0'), tensor([[[0.4397]]], device='cuda:0'), tensor([[[0.4885]]], device='cuda:0'), tensor([[[0.0743]]], device='cuda:0'), tensor([[[0.5072]]], device='cuda:0'), tensor([[[0.3435]]], device='cuda:0'), tensor([[[0.6355]]], device='cuda:0'), tensor([[[0.3481]]], device='cuda:0'), tensor([[[0.4126]]], device='cuda:0'), tensor([[[0.4702]]], device='cuda:0'), tensor([[[0.2921]]], device='cuda:0'), tensor([[[0.3468]]], device='cuda:0'), tensor([[[0.5592]]], device='cuda:0'), tensor([[[0.4209]]], device='cuda:0'), tensor([[[0.4607]]], device='cuda:0'), tensor([[[0.4530]]], device='cuda:0'), tensor([[[0.5580]]], device='cuda:0'), tensor([[[0.4851]]], device='cuda:0'), tensor([[[0.5177]]], device='cuda:0'), tensor([[[0.3711]]], device='cuda:0'), tensor([[[0.6056]]], device='cuda:0'), tensor([[[0.0249]]], device='cuda:0'), tensor([[[0.2445]]], device='cuda:0'), tensor([[[0.3723]]], device='cuda:0'), tensor([[[0.4800]]], device='cuda:0'), tensor([[[0.4373]]], device='cuda:0'), tensor([[[0.6454]]], device='cuda:0'), tensor([[[0.7865]]], device='cuda:0'), tensor([[[0.7174]]], device='cuda:0'), tensor([[[0.4005]]], device='cuda:0'), tensor([[[0.1807]]], device='cuda:0'), tensor([[[-0.0410]]], device='cuda:0'), tensor([[[0.6637]]], device='cuda:0'), tensor([[[0.2341]]], device='cuda:0'), tensor([[[0.7758]]], device='cuda:0'), tensor([[[0.6410]]], device='cuda:0'), tensor([[[0.6729]]], device='cuda:0'), tensor([[[0.2345]]], device='cuda:0'), tensor([[[0.5977]]], device='cuda:0'), tensor([[[0.1697]]], device='cuda:0'), tensor([[[0.4483]]], device='cuda:0'), tensor([[[0.3590]]], device='cuda:0'), tensor([[[0.5238]]], device='cuda:0'), tensor([[[0.6049]]], device='cuda:0'), tensor([[[0.6782]]], device='cuda:0'), tensor([[[0.5594]]], device='cuda:0'), tensor([[[0.2082]]], device='cuda:0'), tensor([[[0.3499]]], device='cuda:0'), tensor([[[0.6088]]], device='cuda:0'), tensor([[[0.7530]]], device='cuda:0'), tensor([[[0.3315]]], device='cuda:0'), tensor([[[0.4826]]], device='cuda:0'), tensor([[[0.6373]]], device='cuda:0'), tensor([[[0.6729]]], device='cuda:0'), tensor([[[0.3780]]], device='cuda:0'), tensor([[[0.4216]]], device='cuda:0'), tensor([[[0.3690]]], device='cuda:0'), tensor([[[0.2660]]], device='cuda:0'), tensor([[[0.6480]]], device='cuda:0'), tensor([[[0.4450]]], device='cuda:0'), tensor([[[0.7187]]], device='cuda:0'), tensor([[[0.5985]]], device='cuda:0'), tensor([[[0.4735]]], device='cuda:0'), tensor([[[0.4515]]], device='cuda:0'), tensor([[[0.2873]]], device='cuda:0'), tensor([[[0.2946]]], device='cuda:0'), tensor([[[0.7384]]], device='cuda:0'), tensor([[[0.3641]]], device='cuda:0'), tensor([[[0.5600]]], device='cuda:0'), tensor([[[0.5334]]], device='cuda:0'), tensor([[[0.5571]]], device='cuda:0'), tensor([[[0.0314]]], device='cuda:0'), tensor([[[0.5654]]], device='cuda:0'), tensor([[[0.1425]]], device='cuda:0'), tensor([[[0.5118]]], device='cuda:0'), tensor([[[0.3430]]], device='cuda:0'), tensor([[[0.3637]]], device='cuda:0'), tensor([[[0.5081]]], device='cuda:0'), tensor([[[0.4311]]], device='cuda:0'), tensor([[[0.4357]]], device='cuda:0'), tensor([[[0.4698]]], device='cuda:0'), tensor([[[0.5088]]], device='cuda:0'), tensor([[[0.2112]]], device='cuda:0'), tensor([[[0.3842]]], device='cuda:0'), tensor([[[0.2853]]], device='cuda:0'), tensor([[[0.4156]]], device='cuda:0'), tensor([[[0.5848]]], device='cuda:0'), tensor([[[0.3825]]], device='cuda:0'), tensor([[[0.1947]]], device='cuda:0'), tensor([[[0.5788]]], device='cuda:0'), tensor([[[0.6029]]], device='cuda:0'), tensor([[[0.5468]]], device='cuda:0'), tensor([[[0.5668]]], device='cuda:0'), tensor([[[0.7937]]], device='cuda:0'), tensor([[[0.1964]]], device='cuda:0'), tensor([[[0.8586]]], device='cuda:0'), tensor([[[0.0751]]], device='cuda:0'), tensor([[[0.5309]]], device='cuda:0'), tensor([[[0.5600]]], device='cuda:0'), tensor([[[0.5846]]], device='cuda:0'), tensor([[[0.6422]]], device='cuda:0'), tensor([[[0.7533]]], device='cuda:0'), tensor([[[0.5372]]], device='cuda:0'), tensor([[[0.5863]]], device='cuda:0'), tensor([[[0.0557]]], device='cuda:0'), tensor([[[0.6303]]], device='cuda:0'), tensor([[[0.3394]]], device='cuda:0'), tensor([[[0.6112]]], device='cuda:0'), tensor([[[0.1554]]], device='cuda:0'), tensor([[[0.8018]]], device='cuda:0'), tensor([[[0.2093]]], device='cuda:0'), tensor([[[0.4930]]], device='cuda:0'), tensor([[[0.5292]]], device='cuda:0'), tensor([[[0.8058]]], device='cuda:0'), tensor([[[0.1862]]], device='cuda:0'), tensor([[[0.3698]]], device='cuda:0'), tensor([[[0.5269]]], device='cuda:0'), tensor([[[0.2972]]], device='cuda:0'), tensor([[[0.8679]]], device='cuda:0'), tensor([[[0.5969]]], device='cuda:0'), tensor([[[0.7297]]], device='cuda:0'), tensor([[[0.4422]]], device='cuda:0'), tensor([[[0.2866]]], device='cuda:0'), tensor([[[0.0583]]], device='cuda:0'), tensor([[[0.4032]]], device='cuda:0'), tensor([[[0.4309]]], device='cuda:0'), tensor([[[0.3941]]], device='cuda:0'), tensor([[[0.2898]]], device='cuda:0'), tensor([[[0.5765]]], device='cuda:0'), tensor([[[0.4087]]], device='cuda:0'), tensor([[[0.5567]]], device='cuda:0'), tensor([[[0.3375]]], device='cuda:0'), tensor([[[0.6546]]], device='cuda:0'), tensor([[[0.5809]]], device='cuda:0'), tensor([[[0.3549]]], device='cuda:0'), tensor([[[0.2888]]], device='cuda:0'), tensor([[[0.2599]]], device='cuda:0'), tensor([[[0.3949]]], device='cuda:0'), tensor([[[0.4478]]], device='cuda:0'), tensor([[[0.4506]]], device='cuda:0'), tensor([[[0.3184]]], device='cuda:0'), tensor([[[0.6498]]], device='cuda:0'), tensor([[[0.4707]]], device='cuda:0'), tensor([[[0.3272]]], device='cuda:0'), tensor([[[0.3734]]], device='cuda:0'), tensor([[[0.3485]]], device='cuda:0'), tensor([[[0.7091]]], device='cuda:0'), tensor([[[0.5110]]], device='cuda:0'), tensor([[[0.7408]]], device='cuda:0'), tensor([[[0.5099]]], device='cuda:0'), tensor([[[0.7645]]], device='cuda:0'), tensor([[[0.5099]]], device='cuda:0'), tensor([[[0.4709]]], device='cuda:0'), tensor([[[0.5714]]], device='cuda:0'), tensor([[[0.8740]]], device='cuda:0'), tensor([[[0.2776]]], device='cuda:0'), tensor([[[0.0430]]], device='cuda:0'), tensor([[[-0.0977]]], device='cuda:0'), tensor([[[0.6253]]], device='cuda:0'), tensor([[[-0.1003]]], device='cuda:0'), tensor([[[0.6262]]], device='cuda:0'), tensor([[[0.0545]]], device='cuda:0'), tensor([[[0.4214]]], device='cuda:0'), tensor([[[0.3598]]], device='cuda:0'), tensor([[[0.3562]]], device='cuda:0'), tensor([[[0.3222]]], device='cuda:0'), tensor([[[0.7627]]], device='cuda:0'), tensor([[[0.3402]]], device='cuda:0'), tensor([[[0.7652]]], device='cuda:0'), tensor([[[0.3731]]], device='cuda:0'), tensor([[[0.3763]]], device='cuda:0'), tensor([[[0.2392]]], device='cuda:0'), tensor([[[0.2966]]], device='cuda:0'), tensor([[[0.6359]]], device='cuda:0'), tensor([[[0.3422]]], device='cuda:0'), tensor([[[0.1707]]], device='cuda:0'), tensor([[[0.4116]]], device='cuda:0'), tensor([[[0.4695]]], device='cuda:0'), tensor([[[0.4992]]], device='cuda:0'), tensor([[[0.6970]]], device='cuda:0'), tensor([[[0.6561]]], device='cuda:0'), tensor([[[0.4883]]], device='cuda:0'), tensor([[[0.4980]]], device='cuda:0'), tensor([[[-0.0356]]], device='cuda:0'), tensor([[[0.3758]]], device='cuda:0'), tensor([[[0.7612]]], device='cuda:0'), tensor([[[0.4555]]], device='cuda:0'), tensor([[[0.3310]]], device='cuda:0'), tensor([[[0.5079]]], device='cuda:0'), tensor([[[0.4610]]], device='cuda:0'), tensor([[[0.7434]]], device='cuda:0'), tensor([[[0.0516]]], device='cuda:0'), tensor([[[0.7554]]], device='cuda:0'), tensor([[[0.9056]]], device='cuda:0'), tensor([[[0.3518]]], device='cuda:0'), tensor([[[0.2844]]], device='cuda:0'), tensor([[[0.3863]]], device='cuda:0'), tensor([[[0.4616]]], device='cuda:0'), tensor([[[0.4611]]], device='cuda:0'), tensor([[[0.4322]]], device='cuda:0'), tensor([[[0.8485]]], device='cuda:0'), tensor([[[0.1986]]], device='cuda:0'), tensor([[[0.3379]]], device='cuda:0'), tensor([[[0.4303]]], device='cuda:0'), tensor([[[0.2599]]], device='cuda:0'), tensor([[[0.8606]]], device='cuda:0'), tensor([[[0.7601]]], device='cuda:0'), tensor([[[0.5005]]], device='cuda:0'), tensor([[[0.6484]]], device='cuda:0'), tensor([[[0.4851]]], device='cuda:0'), tensor([[[0.2971]]], device='cuda:0'), tensor([[[0.0271]]], device='cuda:0'), tensor([[[0.4560]]], device='cuda:0'), tensor([[[0.3802]]], device='cuda:0'), tensor([[[0.3586]]], device='cuda:0'), tensor([[[0.6204]]], device='cuda:0'), tensor([[[0.5183]]], device='cuda:0'), tensor([[[0.5249]]], device='cuda:0'), tensor([[[0.5995]]], device='cuda:0'), tensor([[[0.4362]]], device='cuda:0'), tensor([[[0.8603]]], device='cuda:0'), tensor([[[0.0311]]], device='cuda:0'), tensor([[[0.3254]]], device='cuda:0'), tensor([[[0.2220]]], device='cuda:0'), tensor([[[0.3476]]], device='cuda:0'), tensor([[[0.5256]]], device='cuda:0'), tensor([[[0.5697]]], device='cuda:0'), tensor([[[0.9604]]], device='cuda:0'), tensor([[[0.2334]]], device='cuda:0'), tensor([[[0.5861]]], device='cuda:0'), tensor([[[0.5801]]], device='cuda:0'), tensor([[[0.0778]]], device='cuda:0'), tensor([[[0.7569]]], device='cuda:0'), tensor([[[0.7739]]], device='cuda:0'), tensor([[[0.5872]]], device='cuda:0'), tensor([[[0.4834]]], device='cuda:0'), tensor([[[0.2313]]], device='cuda:0'), tensor([[[0.7735]]], device='cuda:0'), tensor([[[0.6911]]], device='cuda:0'), tensor([[[0.6153]]], device='cuda:0'), tensor([[[0.4173]]], device='cuda:0'), tensor([[[0.5958]]], device='cuda:0'), tensor([[[0.5123]]], device='cuda:0'), tensor([[[0.9116]]], device='cuda:0')]
advantages:  tensor([-3.1615e+00, -9.8092e-01, -1.4778e+00, -2.4752e+00, -2.7462e+00,
        -2.9444e+00, -2.7830e+00, -2.7662e+00, -2.8246e+00, -2.6636e+00,
        -2.6193e+00, -2.6570e+00, -2.4892e+00, -2.5855e+00, -2.4523e+00,
        -2.6048e+00, -2.3961e+00, -2.5889e+00, -2.4114e+00, -2.3135e+00,
        -2.2174e+00, -2.1852e+00, -8.4450e-01, -1.7660e-01, -1.6352e+00,
        -1.8043e+00, -1.6616e+00, -1.6899e+00, -1.5999e+00, -1.5486e+00,
        -2.0575e+00, -1.7628e+00, -1.7357e+00, -1.9027e+00, -1.6202e+00,
        -1.5949e+00, -1.4881e+00, -1.8095e+00, -1.5354e+00, -1.5874e+00,
        -1.5212e+00, -1.4410e+00, -1.5038e+00, -1.6185e+00, -1.7448e+00,
        -1.4439e+00, -1.5348e+00, -1.5544e+00, -1.5355e+00, -1.7635e+00,
        -1.4352e+00, -1.5434e+00, -1.5402e+00, -1.8833e+00, -1.2460e+00,
        -1.3275e+00, -1.1556e+00, -1.4924e+00, -1.5686e+00, -1.4014e+00,
        -1.2445e+00, -1.5434e+00, -1.1109e+00, -1.2559e+00, -1.5638e+00,
        -1.1789e+00,  1.1452e+00,  1.2776e+00,  2.1438e+00,  2.4444e+00,
         4.6523e+00,  1.0474e-02,  2.8771e-01,  1.7367e-01, -6.3848e-02,
         1.7117e-01,  2.2146e-01,  2.7990e-01,  3.2377e-01,  3.0904e-01,
         2.7430e-01,  4.1969e-01,  3.4254e-01,  3.0532e-01,  4.6909e-01,
         1.3042e-01, -3.0982e-02,  2.1448e-01,  5.0619e-01,  2.8504e-01,
         1.7467e-01,  1.3580e-01,  2.3864e-01,  2.7261e-01,  3.5858e-01,
         1.0184e-01,  2.2260e-01,  2.9832e-01,  2.9356e-01,  2.1259e-01,
         2.8010e-01,  3.8535e-01,  2.3055e-01,  3.5069e-01, -1.1141e-01,
         8.0614e-02, -1.0784e-01,  4.2402e-02,  1.2995e-01,  1.8920e-01,
         2.5319e-01,  3.1468e-01,  1.8927e-01,  1.7323e+00,  2.6610e+00,
         3.6420e+00,  7.2813e-01,  9.7801e-01,  9.1777e-01,  9.8525e-01,
         9.7873e-01,  9.4142e-01,  8.7936e-01,  9.4109e-01,  8.2481e-01,
         8.5800e-01,  7.4453e-01,  7.6734e-01,  8.2345e-01,  6.2891e-01,
         6.0847e-01,  8.5305e-01,  5.2134e-01,  8.8542e-01,  3.0012e-01,
         8.1713e-01,  7.0029e-01,  8.8827e-01,  9.2447e-01,  8.6158e-01,
         1.0079e+00,  9.4329e-01,  6.1147e-01,  9.5901e-01,  1.0249e+00,
         1.0330e+00,  5.6020e-01,  8.3538e-01,  9.3672e-01,  1.0902e+00,
         5.4320e-01,  9.6635e-01,  6.8106e-01,  9.0960e-01,  9.7718e-01,
         4.9237e-01,  7.8851e-01,  8.5250e-01,  8.2784e-01,  8.9231e-01,
         7.2327e-01,  5.3448e-01,  9.5926e-01,  8.8361e-01,  5.1131e-01,
         9.0954e-01,  6.9554e-01,  6.6331e-01,  8.0064e-01,  9.3681e-01,
         9.3690e-01,  6.5049e-01,  7.0947e-01,  8.6724e-01,  8.5621e-01,
         1.0284e+00,  9.6384e-01,  9.4478e-01,  8.8538e-01,  1.0602e+00,
         8.9464e-01,  8.0631e-01,  3.7396e-01,  5.2964e-01,  8.2675e-01,
         9.4936e-01,  1.0017e+00,  8.2743e-01,  6.8320e-01,  5.9275e-01,
         7.0623e-01,  4.3329e-01,  4.0692e-01,  8.0686e-01,  5.2570e-01,
         6.6381e-01,  7.7640e-01,  6.4870e-01,  4.1695e-01,  6.7009e-01,
         3.2584e-01,  8.1245e-01,  6.8276e-01,  8.9653e-01,  7.6701e-01,
         7.3285e-01,  7.1318e-01,  3.2248e-01,  6.4222e-01,  7.2830e-01,
         5.7487e-01,  6.1880e-01,  7.3415e-01,  6.0131e-01,  5.4997e-01,
         6.0867e-01,  6.7783e-01,  7.0324e-01,  4.7699e-01,  5.9737e-01,
         7.4213e-01,  6.0788e-01,  6.8932e-01,  6.0997e-01,  7.8254e-01,
         5.3526e-01,  5.0102e-01,  6.2218e-01,  6.2831e-01,  7.2740e-01,
         6.5201e-01,  6.0252e-01,  2.0721e-01,  7.3303e-01,  3.0988e-01,
         7.1602e-01,  6.1213e-01,  6.7845e-01,  7.3528e-01,  8.1307e-01,
         7.2520e-01,  6.4737e-01,  7.5167e-01,  2.7705e-01,  6.4006e-01,
         4.0343e-01,  6.6658e-01,  5.6844e-01,  4.5535e-01,  1.6996e-01,
         5.1281e-01,  5.4318e-01,  5.9317e-01,  4.6494e-01,  1.8835e-01,
         1.1941e-01,  1.9809e-01,  1.1421e-03,  4.4087e-01,  4.1727e-01,
         4.5021e-01,  3.6637e-01,  2.6840e-01,  3.2660e-01,  2.8381e-01,
        -8.9839e-02,  3.3979e-01,  3.3908e-01,  4.7641e-01,  5.8519e-02,
         1.1564e-01,  1.6770e-01,  4.0701e-01,  4.0845e-01,  1.7709e-01,
        -2.6860e-02,  2.3347e-01,  4.2799e-01,  2.8747e-01,  1.3739e-01,
         1.9337e-01,  1.6162e-01,  1.8437e-01,  1.9395e-01, -1.2552e-02,
         3.7076e-01,  3.8120e-01,  2.9707e-01,  1.5517e-01,  3.1149e-01,
         2.8220e-01,  2.7178e-01,  1.9091e-01,  3.0083e-01,  2.3988e-01,
         1.6567e-01,  1.1603e-02,  5.7233e-03,  2.4275e-01,  1.7525e-01,
         1.9294e-01, -3.2061e-01,  2.5951e-01,  4.9702e-01,  3.0997e-01,
         3.9270e-01,  3.1120e-01,  2.8588e-01,  4.4534e-01,  2.7308e-01,
         3.6316e-01,  1.4206e-01,  2.8333e-01,  3.4947e-01,  3.8692e-01,
        -4.5381e-03,  8.3828e-02, -9.1394e-02, -1.2156e-01,  3.2948e-01,
        -1.5617e-01,  3.4043e-01, -1.2757e-01,  2.6615e-01,  2.7897e-01,
         2.4311e-01,  2.3779e-01,  8.7858e-02,  8.0615e-02,  2.3915e-01,
         1.8191e-01,  3.2283e-01, -7.6736e-02,  5.9410e-02,  1.7378e-01,
         1.3190e-01, -1.1543e-01,  2.7914e-01,  2.0730e-01,  3.0022e-01,
         4.9115e-02,  3.9451e-02,  1.3157e-01,  5.1019e-02, -3.6979e-01,
         6.8919e-02, -4.5141e-02,  7.5592e-02, -7.3507e-02,  8.3476e-02,
         1.7039e-02, -2.6380e-01, -4.8133e-01, -1.6379e-01, -4.6006e-01,
        -2.4915e-01, -3.4753e-01, -1.8919e-01,  5.6853e-02, -9.4385e-02,
         2.9166e-02, -3.5431e-01, -5.0004e-01, -1.6702e-01,  9.4886e-02,
        -3.5210e-01, -4.4415e-01, -4.2631e-01, -2.9727e-01, -2.1070e-01,
        -1.5949e-01, -3.4952e-01, -6.4624e-01, -1.8080e-01, -2.3209e-01,
        -2.7855e-01, -1.9439e-01, -2.4215e-01, -2.5844e-01, -3.8224e-01,
        -3.7018e-01, -5.7578e-01, -8.4740e-01, -5.3957e-01, -6.7147e-01,
        -5.1606e-01, -4.2745e-01, -3.8485e-01, -9.9054e-01, -8.8325e-01,
        -5.3244e-01, -6.2418e-01, -1.0278e+00, -7.0587e-01, -8.1236e-01,
        -7.0561e-01, -6.7154e-01, -1.0512e+00, -9.5812e-01, -9.3400e-01,
        -9.1833e-01, -7.5390e-01, -8.4496e-01, -8.5777e-01], device='cuda:0',
       dtype=torch.float64)
t1: 1641198808007
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808019
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808024
t4: 1641198808024
surr1, surr2: tensor([-3.1615e+00, -9.8092e-01, -1.4778e+00, -2.4752e+00, -2.7462e+00,
        -2.9444e+00, -2.7830e+00, -2.7662e+00, -2.8246e+00, -2.6636e+00,
        -2.6193e+00, -2.6570e+00, -2.4892e+00, -2.5855e+00, -2.4523e+00,
        -2.6048e+00, -2.3961e+00, -2.5889e+00, -2.4114e+00, -2.3135e+00,
        -2.2174e+00, -2.1852e+00, -8.4450e-01, -1.7660e-01, -1.6352e+00,
        -1.8043e+00, -1.6616e+00, -1.6899e+00, -1.5999e+00, -1.5486e+00,
        -2.0575e+00, -1.7628e+00, -1.7357e+00, -1.9027e+00, -1.6202e+00,
        -1.5949e+00, -1.4881e+00, -1.8095e+00, -1.5354e+00, -1.5874e+00,
        -1.5212e+00, -1.4410e+00, -1.5038e+00, -1.6185e+00, -1.7448e+00,
        -1.4439e+00, -1.5348e+00, -1.5544e+00, -1.5355e+00, -1.7635e+00,
        -1.4352e+00, -1.5434e+00, -1.5402e+00, -1.8833e+00, -1.2460e+00,
        -1.3275e+00, -1.1556e+00, -1.4924e+00, -1.5686e+00, -1.4014e+00,
        -1.2445e+00, -1.5434e+00, -1.1109e+00, -1.2559e+00, -1.5638e+00,
        -1.1789e+00,  1.1452e+00,  1.2776e+00,  2.1438e+00,  2.4444e+00,
         4.6523e+00,  1.0474e-02,  2.8771e-01,  1.7367e-01, -6.3848e-02,
         1.7117e-01,  2.2146e-01,  2.7990e-01,  3.2377e-01,  3.0904e-01,
         2.7430e-01,  4.1969e-01,  3.4254e-01,  3.0532e-01,  4.6909e-01,
         1.3042e-01, -3.0982e-02,  2.1448e-01,  5.0619e-01,  2.8504e-01,
         1.7467e-01,  1.3580e-01,  2.3864e-01,  2.7261e-01,  3.5858e-01,
         1.0184e-01,  2.2260e-01,  2.9832e-01,  2.9356e-01,  2.1259e-01,
         2.8010e-01,  3.8535e-01,  2.3055e-01,  3.5069e-01, -1.1141e-01,
         8.0614e-02, -1.0784e-01,  4.2402e-02,  1.2995e-01,  1.8920e-01,
         2.5319e-01,  3.1468e-01,  1.8927e-01,  1.7323e+00,  2.6610e+00,
         3.6420e+00,  7.2813e-01,  9.7801e-01,  9.1777e-01,  9.8525e-01,
         9.7873e-01,  9.4142e-01,  8.7936e-01,  9.4109e-01,  8.2481e-01,
         8.5800e-01,  7.4453e-01,  7.6734e-01,  8.2345e-01,  6.2891e-01,
         6.0847e-01,  8.5305e-01,  5.2134e-01,  8.8542e-01,  3.0012e-01,
         8.1713e-01,  7.0029e-01,  8.8827e-01,  9.2447e-01,  8.6158e-01,
         1.0079e+00,  9.4329e-01,  6.1147e-01,  9.5901e-01,  1.0249e+00,
         1.0330e+00,  5.6020e-01,  8.3538e-01,  9.3672e-01,  1.0902e+00,
         5.4320e-01,  9.6635e-01,  6.8106e-01,  9.0960e-01,  9.7718e-01,
         4.9237e-01,  7.8851e-01,  8.5250e-01,  8.2784e-01,  8.9231e-01,
         7.2327e-01,  5.3448e-01,  9.5926e-01,  8.8361e-01,  5.1131e-01,
         9.0954e-01,  6.9554e-01,  6.6331e-01,  8.0064e-01,  9.3681e-01,
         9.3690e-01,  6.5049e-01,  7.0947e-01,  8.6724e-01,  8.5621e-01,
         1.0284e+00,  9.6384e-01,  9.4478e-01,  8.8538e-01,  1.0602e+00,
         8.9464e-01,  8.0631e-01,  3.7396e-01,  5.2964e-01,  8.2675e-01,
         9.4936e-01,  1.0017e+00,  8.2743e-01,  6.8320e-01,  5.9275e-01,
         7.0623e-01,  4.3329e-01,  4.0692e-01,  8.0686e-01,  5.2570e-01,
         6.6381e-01,  7.7640e-01,  6.4870e-01,  4.1695e-01,  6.7009e-01,
         3.2584e-01,  8.1245e-01,  6.8276e-01,  8.9653e-01,  7.6701e-01,
         7.3285e-01,  7.1318e-01,  3.2248e-01,  6.4222e-01,  7.2830e-01,
         5.7487e-01,  6.1880e-01,  7.3415e-01,  6.0131e-01,  5.4997e-01,
         6.0867e-01,  6.7783e-01,  7.0324e-01,  4.7699e-01,  5.9737e-01,
         7.4213e-01,  6.0788e-01,  6.8932e-01,  6.0997e-01,  7.8254e-01,
         5.3526e-01,  5.0102e-01,  6.2218e-01,  6.2831e-01,  7.2740e-01,
         6.5201e-01,  6.0252e-01,  2.0721e-01,  7.3303e-01,  3.0988e-01,
         7.1602e-01,  6.1213e-01,  6.7845e-01,  7.3528e-01,  8.1307e-01,
         7.2520e-01,  6.4737e-01,  7.5167e-01,  2.7705e-01,  6.4006e-01,
         4.0343e-01,  6.6658e-01,  5.6844e-01,  4.5535e-01,  1.6996e-01,
         5.1281e-01,  5.4318e-01,  5.9317e-01,  4.6494e-01,  1.8835e-01,
         1.1941e-01,  1.9809e-01,  1.1421e-03,  4.4087e-01,  4.1727e-01,
         4.5021e-01,  3.6637e-01,  2.6840e-01,  3.2660e-01,  2.8381e-01,
        -8.9839e-02,  3.3979e-01,  3.3908e-01,  4.7641e-01,  5.8519e-02,
         1.1564e-01,  1.6770e-01,  4.0701e-01,  4.0845e-01,  1.7709e-01,
        -2.6860e-02,  2.3347e-01,  4.2799e-01,  2.8747e-01,  1.3739e-01,
         1.9337e-01,  1.6162e-01,  1.8437e-01,  1.9395e-01, -1.2552e-02,
         3.7076e-01,  3.8120e-01,  2.9707e-01,  1.5517e-01,  3.1149e-01,
         2.8220e-01,  2.7178e-01,  1.9091e-01,  3.0083e-01,  2.3988e-01,
         1.6567e-01,  1.1603e-02,  5.7233e-03,  2.4275e-01,  1.7525e-01,
         1.9294e-01, -3.2061e-01,  2.5951e-01,  4.9702e-01,  3.0997e-01,
         3.9270e-01,  3.1120e-01,  2.8588e-01,  4.4534e-01,  2.7308e-01,
         3.6316e-01,  1.4206e-01,  2.8333e-01,  3.4947e-01,  3.8692e-01,
        -4.5381e-03,  8.3828e-02, -9.1394e-02, -1.2156e-01,  3.2948e-01,
        -1.5617e-01,  3.4043e-01, -1.2757e-01,  2.6615e-01,  2.7897e-01,
         2.4311e-01,  2.3779e-01,  8.7858e-02,  8.0615e-02,  2.3915e-01,
         1.8191e-01,  3.2283e-01, -7.6736e-02,  5.9410e-02,  1.7378e-01,
         1.3190e-01, -1.1543e-01,  2.7914e-01,  2.0730e-01,  3.0022e-01,
         4.9115e-02,  3.9451e-02,  1.3157e-01,  5.1019e-02, -3.6979e-01,
         6.8919e-02, -4.5141e-02,  7.5592e-02, -7.3507e-02,  8.3476e-02,
         1.7039e-02, -2.6380e-01, -4.8133e-01, -1.6379e-01, -4.6006e-01,
        -2.4915e-01, -3.4753e-01, -1.8919e-01,  5.6853e-02, -9.4385e-02,
         2.9166e-02, -3.5431e-01, -5.0004e-01, -1.6702e-01,  9.4886e-02,
        -3.5210e-01, -4.4415e-01, -4.2631e-01, -2.9727e-01, -2.1070e-01,
        -1.5949e-01, -3.4952e-01, -6.4624e-01, -1.8080e-01, -2.3209e-01,
        -2.7855e-01, -1.9439e-01, -2.4215e-01, -2.5844e-01, -3.8224e-01,
        -3.7018e-01, -5.7578e-01, -8.4740e-01, -5.3957e-01, -6.7147e-01,
        -5.1606e-01, -4.2745e-01, -3.8485e-01, -9.9054e-01, -8.8325e-01,
        -5.3244e-01, -6.2418e-01, -1.0278e+00, -7.0587e-01, -8.1236e-01,
        -7.0561e-01, -6.7154e-01, -1.0512e+00, -9.5812e-01, -9.3400e-01,
        -9.1833e-01, -7.5390e-01, -8.4496e-01, -8.5777e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1615e+00, -9.8092e-01, -1.4778e+00, -2.4752e+00, -2.7462e+00,
        -2.9444e+00, -2.7830e+00, -2.7662e+00, -2.8246e+00, -2.6636e+00,
        -2.6193e+00, -2.6570e+00, -2.4892e+00, -2.5855e+00, -2.4523e+00,
        -2.6048e+00, -2.3961e+00, -2.5889e+00, -2.4114e+00, -2.3135e+00,
        -2.2174e+00, -2.1852e+00, -8.4450e-01, -1.7660e-01, -1.6352e+00,
        -1.8043e+00, -1.6616e+00, -1.6899e+00, -1.5999e+00, -1.5486e+00,
        -2.0575e+00, -1.7628e+00, -1.7357e+00, -1.9027e+00, -1.6202e+00,
        -1.5949e+00, -1.4881e+00, -1.8095e+00, -1.5354e+00, -1.5874e+00,
        -1.5212e+00, -1.4410e+00, -1.5038e+00, -1.6185e+00, -1.7448e+00,
        -1.4439e+00, -1.5348e+00, -1.5544e+00, -1.5355e+00, -1.7635e+00,
        -1.4352e+00, -1.5434e+00, -1.5402e+00, -1.8833e+00, -1.2460e+00,
        -1.3275e+00, -1.1556e+00, -1.4924e+00, -1.5686e+00, -1.4014e+00,
        -1.2445e+00, -1.5434e+00, -1.1109e+00, -1.2559e+00, -1.5638e+00,
        -1.1789e+00,  1.1452e+00,  1.2776e+00,  2.1438e+00,  2.4444e+00,
         4.6523e+00,  1.0474e-02,  2.8771e-01,  1.7367e-01, -6.3848e-02,
         1.7117e-01,  2.2146e-01,  2.7990e-01,  3.2377e-01,  3.0904e-01,
         2.7430e-01,  4.1969e-01,  3.4254e-01,  3.0532e-01,  4.6909e-01,
         1.3042e-01, -3.0982e-02,  2.1448e-01,  5.0619e-01,  2.8504e-01,
         1.7467e-01,  1.3580e-01,  2.3864e-01,  2.7261e-01,  3.5858e-01,
         1.0184e-01,  2.2260e-01,  2.9832e-01,  2.9356e-01,  2.1259e-01,
         2.8010e-01,  3.8535e-01,  2.3055e-01,  3.5069e-01, -1.1141e-01,
         8.0614e-02, -1.0784e-01,  4.2402e-02,  1.2995e-01,  1.8920e-01,
         2.5319e-01,  3.1468e-01,  1.8927e-01,  1.7323e+00,  2.6610e+00,
         3.6420e+00,  7.2813e-01,  9.7801e-01,  9.1777e-01,  9.8525e-01,
         9.7873e-01,  9.4142e-01,  8.7936e-01,  9.4109e-01,  8.2481e-01,
         8.5800e-01,  7.4453e-01,  7.6734e-01,  8.2345e-01,  6.2891e-01,
         6.0847e-01,  8.5305e-01,  5.2134e-01,  8.8542e-01,  3.0012e-01,
         8.1713e-01,  7.0029e-01,  8.8827e-01,  9.2447e-01,  8.6158e-01,
         1.0079e+00,  9.4329e-01,  6.1147e-01,  9.5901e-01,  1.0249e+00,
         1.0330e+00,  5.6020e-01,  8.3538e-01,  9.3672e-01,  1.0902e+00,
         5.4320e-01,  9.6635e-01,  6.8106e-01,  9.0960e-01,  9.7718e-01,
         4.9237e-01,  7.8851e-01,  8.5250e-01,  8.2784e-01,  8.9231e-01,
         7.2327e-01,  5.3448e-01,  9.5926e-01,  8.8361e-01,  5.1131e-01,
         9.0954e-01,  6.9554e-01,  6.6331e-01,  8.0064e-01,  9.3681e-01,
         9.3690e-01,  6.5049e-01,  7.0947e-01,  8.6724e-01,  8.5621e-01,
         1.0284e+00,  9.6384e-01,  9.4478e-01,  8.8538e-01,  1.0602e+00,
         8.9464e-01,  8.0631e-01,  3.7396e-01,  5.2964e-01,  8.2675e-01,
         9.4936e-01,  1.0017e+00,  8.2743e-01,  6.8320e-01,  5.9275e-01,
         7.0623e-01,  4.3329e-01,  4.0692e-01,  8.0686e-01,  5.2570e-01,
         6.6381e-01,  7.7640e-01,  6.4870e-01,  4.1695e-01,  6.7009e-01,
         3.2584e-01,  8.1245e-01,  6.8276e-01,  8.9653e-01,  7.6701e-01,
         7.3285e-01,  7.1318e-01,  3.2248e-01,  6.4222e-01,  7.2830e-01,
         5.7487e-01,  6.1880e-01,  7.3415e-01,  6.0131e-01,  5.4997e-01,
         6.0867e-01,  6.7783e-01,  7.0324e-01,  4.7699e-01,  5.9737e-01,
         7.4213e-01,  6.0788e-01,  6.8932e-01,  6.0997e-01,  7.8254e-01,
         5.3526e-01,  5.0102e-01,  6.2218e-01,  6.2831e-01,  7.2740e-01,
         6.5201e-01,  6.0252e-01,  2.0721e-01,  7.3303e-01,  3.0988e-01,
         7.1602e-01,  6.1213e-01,  6.7845e-01,  7.3528e-01,  8.1307e-01,
         7.2520e-01,  6.4737e-01,  7.5167e-01,  2.7705e-01,  6.4006e-01,
         4.0343e-01,  6.6658e-01,  5.6844e-01,  4.5535e-01,  1.6996e-01,
         5.1281e-01,  5.4318e-01,  5.9317e-01,  4.6494e-01,  1.8835e-01,
         1.1941e-01,  1.9809e-01,  1.1421e-03,  4.4087e-01,  4.1727e-01,
         4.5021e-01,  3.6637e-01,  2.6840e-01,  3.2660e-01,  2.8381e-01,
        -8.9839e-02,  3.3979e-01,  3.3908e-01,  4.7641e-01,  5.8519e-02,
         1.1564e-01,  1.6770e-01,  4.0701e-01,  4.0845e-01,  1.7709e-01,
        -2.6860e-02,  2.3347e-01,  4.2799e-01,  2.8747e-01,  1.3739e-01,
         1.9337e-01,  1.6162e-01,  1.8437e-01,  1.9395e-01, -1.2552e-02,
         3.7076e-01,  3.8120e-01,  2.9707e-01,  1.5517e-01,  3.1149e-01,
         2.8220e-01,  2.7178e-01,  1.9091e-01,  3.0083e-01,  2.3988e-01,
         1.6567e-01,  1.1603e-02,  5.7233e-03,  2.4275e-01,  1.7525e-01,
         1.9294e-01, -3.2061e-01,  2.5951e-01,  4.9702e-01,  3.0997e-01,
         3.9270e-01,  3.1120e-01,  2.8588e-01,  4.4534e-01,  2.7308e-01,
         3.6316e-01,  1.4206e-01,  2.8333e-01,  3.4947e-01,  3.8692e-01,
        -4.5381e-03,  8.3828e-02, -9.1394e-02, -1.2156e-01,  3.2948e-01,
        -1.5617e-01,  3.4043e-01, -1.2757e-01,  2.6615e-01,  2.7897e-01,
         2.4311e-01,  2.3779e-01,  8.7858e-02,  8.0615e-02,  2.3915e-01,
         1.8191e-01,  3.2283e-01, -7.6736e-02,  5.9410e-02,  1.7378e-01,
         1.3190e-01, -1.1543e-01,  2.7914e-01,  2.0730e-01,  3.0022e-01,
         4.9115e-02,  3.9451e-02,  1.3157e-01,  5.1019e-02, -3.6979e-01,
         6.8919e-02, -4.5141e-02,  7.5592e-02, -7.3507e-02,  8.3476e-02,
         1.7039e-02, -2.6380e-01, -4.8133e-01, -1.6379e-01, -4.6006e-01,
        -2.4915e-01, -3.4753e-01, -1.8919e-01,  5.6853e-02, -9.4385e-02,
         2.9166e-02, -3.5431e-01, -5.0004e-01, -1.6702e-01,  9.4886e-02,
        -3.5210e-01, -4.4415e-01, -4.2631e-01, -2.9727e-01, -2.1070e-01,
        -1.5949e-01, -3.4952e-01, -6.4624e-01, -1.8080e-01, -2.3209e-01,
        -2.7855e-01, -1.9439e-01, -2.4215e-01, -2.5844e-01, -3.8224e-01,
        -3.7018e-01, -5.7578e-01, -8.4740e-01, -5.3957e-01, -6.7147e-01,
        -5.1606e-01, -4.2745e-01, -3.8485e-01, -9.9054e-01, -8.8325e-01,
        -5.3244e-01, -6.2418e-01, -1.0278e+00, -7.0587e-01, -8.1236e-01,
        -7.0561e-01, -6.7154e-01, -1.0512e+00, -9.5812e-01, -9.3400e-01,
        -9.1833e-01, -7.5390e-01, -8.4496e-01, -8.5777e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808033
t6: 1641198808033
state_values: tensor([ -2.7617,  -7.7867,  -5.6916,  -2.5776,  -1.8719,  -1.3302,  -1.8395,
         -1.9167,  -1.7975,  -2.2815,  -2.4606,  -2.4284,  -2.9816,  -2.7439,
         -3.1526,  -2.7770,  -3.4920,  -2.8906,  -3.3643,  -3.6653,  -3.9582,
         -4.1025,  -7.2181,  -9.3870,  -5.1009,  -4.6264,  -5.1001,  -5.0914,
         -5.4441,  -5.6996,  -4.2312,  -5.1603,  -5.2949,  -4.8633,  -5.7533,
         -5.8884,  -6.2791,  -5.3920,  -6.2850,  -6.1797,  -6.4149,  -6.7093,
         -6.5449,  -6.2619,  -5.9673,  -6.9260,  -6.6807,  -6.6661,  -6.8074,
         -6.2043,  -7.2395,  -6.9568,  -7.0612,  -5.0728,  -7.0201,  -6.8047,
         -7.3907,  -6.4800,  -6.3231,  -6.8484,  -7.3641,  -6.5101,  -7.8904,
         -7.5001,  -6.6511,  -7.8784, -13.6993, -13.1266, -14.5462, -14.4491,
        -20.9114,  -7.0246,  -7.8604,  -7.5202,  -6.8090,  -7.5134,  -7.6571,
         -7.8520,  -8.0059,  -7.9815,  -7.9041,  -8.3850,  -8.1688,  -8.0681,
         -8.6275,  -7.6375,  -7.2081,  -7.9417,  -8.9141,  -8.2905,  -7.9866,
         -7.8535,  -8.1482,  -8.2394,  -8.5390,  -7.8092,  -8.1852,  -8.4277,
         -8.4131,  -8.1625,  -8.3756,  -8.7436,  -8.3123,  -8.6894,  -7.3290,
         -7.9319,  -7.3879,  -7.8381,  -8.1187,  -8.3126,  -8.4441,  -8.7110,
         -8.3595, -12.0277, -13.7040, -16.7017,  -8.0362,  -8.8043,  -8.6475,
         -8.8818,  -8.8793,  -8.7846,  -8.5775,  -8.7736,  -8.4410,  -8.5328,
         -8.1899,  -8.2406,  -8.4371,  -7.8586,  -7.7656,  -8.4815,  -7.5244,
         -8.5872,  -6.8286,  -8.3697,  -8.0040,  -8.5598,  -8.6744,  -8.4650,
         -8.9150,  -8.7205,  -7.7492,  -8.7562,  -8.9681,  -9.0002,  -7.6043,
         -8.4118,  -8.7152,  -9.2042,  -7.5371,  -8.8388,  -8.0144,  -8.7426,
         -8.9632,  -7.5176,  -8.4196,  -8.6623,  -8.6332,  -8.8383,  -8.3594,
         -7.7839,  -9.0845,  -8.8495,  -7.8005,  -9.0058,  -8.3343,  -8.1792,
         -8.6361,  -9.0796,  -9.1009,  -8.2525,  -8.4049,  -8.8965,  -8.8555,
         -9.4121,  -9.2228,  -9.1932,  -9.0021,  -9.5861,  -9.1231,  -8.8520,
         -7.5495,  -7.9841,  -8.8752,  -9.2562,  -9.4577,  -8.9451,  -8.5393,
         -8.2403,  -8.5477,  -7.7293,  -7.6870,  -8.8948,  -8.0572,  -8.4678,
         -8.8395,  -8.4420,  -7.7641,  -8.5101,  -7.4547,  -8.9280,  -8.5281,
         -9.2226,  -8.8584,  -8.8059,  -8.7711,  -7.5756,  -8.5489,  -8.8360,
         -8.3916,  -8.5793,  -8.9368,  -8.5283,  -8.3744,  -8.5631,  -8.7687,
         -8.8853,  -8.2382,  -8.5959,  -9.0654,  -8.6770,  -8.9532,  -8.6708,
         -9.2007,  -8.4884,  -8.3765,  -8.7638,  -8.7940,  -9.1165,  -8.8873,
         -8.7131,  -7.5240,  -9.1194,  -7.8767,  -9.1037,  -8.8060,  -9.0297,
         -9.2174,  -9.5080,  -9.2589,  -9.0100,  -9.3649,  -7.9335,  -9.0516,
         -8.3657,  -9.1979,  -8.9277,  -8.5712,  -7.7177,  -8.7570,  -8.8873,
         -9.0934,  -8.7206,  -7.8581,  -7.7106,  -7.9835,  -7.4259,  -8.7571,
         -8.6942,  -8.8376,  -8.6175,  -8.3574,  -8.5427,  -8.4083,  -7.2721,
         -8.5758,  -8.5956,  -9.0534,  -7.8368,  -7.9675,  -8.2072,  -8.9213,
         -8.9327,  -8.2524,  -7.6623,  -8.3981,  -9.0054,  -8.6473,  -8.2376,
         -8.3886,  -8.2984,  -8.3134,  -8.3989,  -7.8476,  -9.0042,  -9.0456,
         -8.7792,  -8.3734,  -8.8569,  -8.7703,  -8.7584,  -8.5606,  -8.9624,
         -8.8294,  -8.6551,  -8.2285,  -8.2830,  -9.0476,  -8.8396,  -8.9142,
         -6.4828,  -8.2154,  -8.9778,  -8.4516,  -8.7258,  -8.4826,  -8.4161,
         -8.9503,  -8.4734,  -8.7813,  -8.1332,  -8.5766,  -8.7940,  -8.9353,
         -7.7668,  -8.0818,  -7.5729,  -7.4882,  -8.8632,  -7.3977,  -8.9282,
         -7.5333,  -8.6946,  -8.7532,  -8.6519,  -8.6798,  -8.2152,  -8.1818,
         -8.7305,  -8.5696,  -9.0518,  -7.8272,  -8.2512,  -8.5992,  -8.4902,
         -7.8010,  -9.0454,  -8.8436,  -9.1963,  -8.4748,  -8.4885,  -8.8167,
         -8.5845,  -7.3441,  -8.7030,  -8.4068,  -8.8182,  -8.4233,  -8.9484,
         -8.7661,  -7.8926,  -7.2844,  -8.2799,  -7.4395,  -8.1135,  -7.8364,
         -8.3061,  -9.1263,  -8.6710,  -9.0963,  -7.9796,  -7.5728,  -8.5829,
         -9.4359,  -8.1245,  -7.8576,  -7.9163,  -8.2939,  -8.6163,  -8.8188,
         -8.3113,  -7.4554,  -8.8653,  -8.7455,  -8.6347,  -8.9604,  -8.8642,
         -8.8601,  -8.5082,  -8.5490,  -7.9987,  -7.2141,  -8.1785,  -7.8635,
         -8.3714,  -8.6816,  -8.8917,  -7.1421,  -7.5397,  -8.6772,  -8.4529,
         -7.2886,  -8.3353,  -8.0930,  -8.4994,  -8.6704,  -7.5644,  -7.8807,
         -7.9935,  -8.0811,  -8.6674,  -8.4599,  -8.4735], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808038
t8: 1641198808038
t9: 1641198808038
t10: 1641198808050
t11: 1641198808052
t12: 1641198808052
t1: 1641198808052
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808063
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9864, 0.9299, 1.0906, 1.0519, 1.2958, 0.9769, 1.0666, 1.3350, 0.9321,
        0.7348, 1.2588, 0.9497, 1.2377, 0.8639, 0.5320, 0.8948, 1.2837, 0.8482,
        0.8102, 0.7499, 0.8414, 1.0615, 1.0007, 0.8651, 1.1246, 0.9785, 0.7382,
        1.0054, 0.9487, 0.5046, 0.8995, 0.8139, 1.2989, 1.0250, 1.0583, 0.9685,
        1.3276, 0.8664, 1.1127, 0.8891, 0.9227, 0.9633, 0.6733, 1.2759, 0.9451,
        1.0599, 0.7764, 1.1417, 1.3246, 0.9522, 0.8129, 1.1446, 0.8545, 0.9400,
        1.1305, 0.8292, 0.5492, 1.2022, 0.7604, 1.0458, 1.2407, 0.9834, 0.9073,
        1.3331, 0.9296, 1.1414, 1.2817, 1.1318, 0.8806, 0.6976, 1.3091, 0.9096,
        1.1101, 1.2742, 0.7462, 0.9312, 0.8915, 0.9374, 0.8924, 1.0793, 0.9759,
        0.9428, 1.0635, 0.8327, 0.6278, 1.2812, 1.0713, 0.7881, 0.7955, 1.1301,
        0.7879, 0.9860, 1.0163, 1.0553, 1.1993, 0.7524, 0.9308, 0.9632, 0.8072,
        0.9166, 1.0209, 1.1219, 0.9832, 1.5216, 1.1906, 1.3642, 1.1542, 1.1212,
        1.0852, 0.8461, 0.7480, 0.8618, 1.1275, 1.2117, 1.1815, 1.7774, 0.8003,
        1.0843, 0.8950, 0.9665, 1.0348, 1.0008, 1.0070, 0.7449, 1.0328, 0.7297,
        1.0925, 1.1280, 1.2493, 0.6388, 0.9474, 1.4428, 0.8885, 0.5271, 0.9725,
        0.7787, 0.9775, 0.9038, 0.8477, 0.9867, 1.0502, 1.3269, 0.9549, 0.9030,
        0.9199, 1.5655, 1.1200, 0.7620, 0.9810, 0.6068, 0.9884, 1.1737, 1.0811,
        1.0170, 0.6102, 0.9280, 0.8182, 1.1047, 1.0238, 1.1385, 1.2134, 0.9931,
        0.9454, 1.5001, 0.9238, 1.1192, 0.8014, 1.1293, 1.0249, 0.9617, 1.1853,
        1.1006, 0.8671, 1.0256, 0.9720, 0.9821, 0.8675, 0.9477, 0.9114, 1.0874,
        0.8247, 1.6925, 1.2269, 1.0708, 0.9491, 1.0011, 0.7820, 0.6499, 0.7703,
        1.0591, 1.2817, 1.6032, 0.7894, 1.2986, 0.7092, 0.8389, 0.7953, 1.2615,
        0.8626, 1.3714, 0.9857, 1.0896, 0.9107, 0.8255, 0.7582, 0.8842, 1.2909,
        1.0877, 0.8293, 0.6888, 1.1502, 0.9566, 0.8051, 0.7700, 1.0850, 1.0141,
        1.0749, 1.1975, 0.7983, 1.0042, 0.7360, 0.8507, 0.9689, 0.9860, 1.1780,
        1.1549, 0.7237, 1.1155, 0.8839, 0.9003, 0.8778, 1.6042, 0.8717, 1.4423,
        0.9200, 1.1221, 1.0819, 0.9205, 1.0097, 1.0016, 0.9622, 0.9193, 1.3159,
        1.0562, 1.1930, 1.0208, 0.8381, 1.0791, 1.3175, 0.8474, 0.8148, 0.8802,
        0.8608, 0.6543, 1.3215, 0.6693, 1.3450, 0.9220, 0.8855, 0.8624, 0.8079,
        0.7166, 0.9306, 0.8729, 1.5134, 0.8347, 1.1358, 0.8432, 1.4071, 0.6967,
        1.2766, 0.9496, 0.9096, 0.6742, 1.2795, 1.0567, 0.9142, 1.1685, 0.6510,
        0.9202, 0.7669, 1.0202, 1.1353, 1.4028, 1.0279, 1.0051, 1.0459, 1.1693,
        0.8588, 1.0416, 0.8739, 1.1333, 0.7852, 0.8495, 1.1060, 1.1700, 1.2057,
        1.0429, 0.9860, 0.9835, 1.1488, 0.7840, 0.9710, 1.1354, 1.0709, 1.1052,
        0.7306, 0.9340, 0.7158, 0.9480, 0.7168, 0.9565, 0.9710, 0.8740, 0.6215,
        1.1364, 1.3619, 1.6111, 0.8336, 1.8846, 0.8285, 1.5990, 1.0114, 1.0881,
        1.0898, 1.1329, 0.6935, 1.1462, 0.7177, 1.0971, 1.0589, 1.2239, 1.1472,
        0.8099, 1.1358, 1.3223, 1.0233, 0.9628, 0.9307, 0.7369, 0.7829, 0.9560,
        0.9387, 1.7050, 1.0608, 0.6941, 1.0085, 1.1158, 0.9276, 0.9767, 0.7126,
        1.5869, 0.7315, 0.6321, 1.0716, 1.1219, 1.0441, 0.9761, 0.9774, 1.0055,
        0.6454, 1.2264, 1.0893, 1.0047, 1.2030, 0.6519, 0.8116, 0.9639, 0.8271,
        0.9661, 1.1411, 1.4537, 0.9772, 1.0636, 1.0847, 0.8180, 0.9160, 0.9084,
        0.8316, 1.0092, 0.6292, 1.3973, 1.1037, 1.2305, 1.0881, 0.9091, 0.8609,
        0.5440, 1.1575, 0.8864, 0.8663, 1.4481, 0.7430, 0.7396, 0.8987, 0.9635,
        1.2143, 0.7386, 0.8152, 0.8624, 1.0276, 0.8666, 0.9310, 0.6274],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808067
t4: 1641198808067
surr1, surr2: tensor([-3.1185e+00, -9.1217e-01, -1.6118e+00, -2.6036e+00, -3.5586e+00,
        -2.8764e+00, -2.9684e+00, -3.6928e+00, -2.6329e+00, -1.9571e+00,
        -3.2972e+00, -2.5235e+00, -3.0809e+00, -2.2336e+00, -1.3046e+00,
        -2.3307e+00, -3.0759e+00, -2.1959e+00, -1.9536e+00, -1.7349e+00,
        -1.8658e+00, -2.3197e+00, -8.4510e-01, -1.5278e-01, -1.8389e+00,
        -1.7655e+00, -1.2266e+00, -1.6991e+00, -1.5179e+00, -7.8143e-01,
        -1.8506e+00, -1.4347e+00, -2.2545e+00, -1.9503e+00, -1.7147e+00,
        -1.5446e+00, -1.9756e+00, -1.5677e+00, -1.7085e+00, -1.4113e+00,
        -1.4036e+00, -1.3881e+00, -1.0125e+00, -2.0651e+00, -1.6490e+00,
        -1.5304e+00, -1.1916e+00, -1.7747e+00, -2.0339e+00, -1.6792e+00,
        -1.1666e+00, -1.7665e+00, -1.3161e+00, -1.7703e+00, -1.4086e+00,
        -1.1008e+00, -6.3471e-01, -1.7941e+00, -1.1928e+00, -1.4656e+00,
        -1.5441e+00, -1.5177e+00, -1.0079e+00, -1.6742e+00, -1.4537e+00,
        -1.3456e+00,  1.4678e+00,  1.4460e+00,  1.8878e+00,  1.7052e+00,
         6.0901e+00,  9.5274e-03,  3.1938e-01,  2.2129e-01, -4.7641e-02,
         1.5939e-01,  1.9742e-01,  2.6237e-01,  2.8894e-01,  3.3355e-01,
         2.6769e-01,  3.9570e-01,  3.6430e-01,  2.5424e-01,  2.9448e-01,
         1.6709e-01, -3.3190e-02,  1.6902e-01,  4.0267e-01,  3.2211e-01,
         1.3763e-01,  1.3391e-01,  2.4254e-01,  2.8770e-01,  4.3004e-01,
         7.6626e-02,  2.0718e-01,  2.8734e-01,  2.3697e-01,  1.9487e-01,
         2.8596e-01,  4.3230e-01,  2.2668e-01,  5.3362e-01, -1.3264e-01,
         1.0997e-01, -1.2447e-01,  4.7540e-02,  1.4102e-01,  1.6009e-01,
         1.8939e-01,  2.7119e-01,  2.1340e-01,  2.0990e+00,  3.1439e+00,
         6.4733e+00,  5.8270e-01,  1.0605e+00,  8.2140e-01,  9.5222e-01,
         1.0128e+00,  9.4218e-01,  8.8555e-01,  7.0100e-01,  8.5187e-01,
         6.2608e-01,  8.1336e-01,  8.6557e-01,  1.0287e+00,  4.0176e-01,
         5.7644e-01,  1.2308e+00,  4.6321e-01,  4.6667e-01,  2.9186e-01,
         6.3627e-01,  6.8451e-01,  8.0279e-01,  7.8364e-01,  8.5008e-01,
         1.0585e+00,  1.2517e+00,  5.8392e-01,  8.6598e-01,  9.4280e-01,
         1.6172e+00,  6.2742e-01,  6.3654e-01,  9.1895e-01,  6.6153e-01,
         5.3690e-01,  1.1342e+00,  7.3633e-01,  9.2506e-01,  5.9632e-01,
         4.5694e-01,  6.4518e-01,  9.4179e-01,  8.4758e-01,  1.0159e+00,
         8.7761e-01,  5.3079e-01,  9.0689e-01,  1.3255e+00,  4.7233e-01,
         1.0180e+00,  5.5738e-01,  7.4910e-01,  8.2053e-01,  9.0095e-01,
         1.1105e+00,  7.1592e-01,  6.1518e-01,  8.8942e-01,  8.3221e-01,
         1.0101e+00,  8.3618e-01,  8.9537e-01,  8.0696e-01,  1.1529e+00,
         7.3781e-01,  1.3647e+00,  4.5883e-01,  5.6717e-01,  7.8466e-01,
         9.5044e-01,  7.8331e-01,  5.3775e-01,  5.2626e-01,  6.2777e-01,
         9.0518e-01,  6.9465e-01,  3.2124e-01,  1.0478e+00,  3.7281e-01,
         5.5690e-01,  6.1750e-01,  8.1833e-01,  3.5966e-01,  9.1898e-01,
         3.2117e-01,  8.8522e-01,  6.2177e-01,  7.4009e-01,  5.8158e-01,
         6.4800e-01,  9.2062e-01,  3.5075e-01,  5.3262e-01,  5.0167e-01,
         6.6121e-01,  5.9193e-01,  5.9110e-01,  4.6303e-01,  5.9671e-01,
         6.1724e-01,  7.2858e-01,  8.4210e-01,  3.8076e-01,  5.9991e-01,
         5.4622e-01,  5.1715e-01,  6.6790e-01,  6.0145e-01,  9.2181e-01,
         6.1820e-01,  3.6260e-01,  6.9401e-01,  5.5538e-01,  6.5489e-01,
         5.7236e-01,  9.6654e-01,  1.8062e-01,  1.0573e+00,  2.8508e-01,
         8.0346e-01,  6.6226e-01,  6.2448e-01,  7.4242e-01,  8.1439e-01,
         6.9780e-01,  5.9516e-01,  9.8914e-01,  2.9261e-01,  7.6360e-01,
         4.1183e-01,  5.5868e-01,  6.1339e-01,  5.9993e-01,  1.4403e-01,
         4.1784e-01,  4.7809e-01,  5.1058e-01,  3.0420e-01,  2.4891e-01,
         7.9920e-02,  2.6642e-01,  1.0530e-03,  3.9039e-01,  3.5983e-01,
         3.6372e-01,  2.6255e-01,  2.4978e-01,  2.8509e-01,  4.2952e-01,
        -7.4992e-02,  3.8593e-01,  2.8591e-01,  6.7034e-01,  4.0769e-02,
         1.4763e-01,  1.5925e-01,  3.7021e-01,  2.7538e-01,  2.2658e-01,
        -2.8384e-02,  2.1344e-01,  5.0008e-01,  1.8714e-01,  1.2642e-01,
         1.4829e-01,  1.6489e-01,  2.0932e-01,  2.7207e-01, -1.2901e-02,
         3.7264e-01,  3.9868e-01,  3.4738e-01,  1.3326e-01,  3.2444e-01,
         2.4661e-01,  3.0801e-01,  1.4989e-01,  2.5556e-01,  2.6532e-01,
         1.9384e-01,  1.3990e-02,  5.9688e-03,  2.3936e-01,  1.7236e-01,
         2.2164e-01, -2.5137e-01,  2.5200e-01,  5.6432e-01,  3.3196e-01,
         4.3399e-01,  2.2735e-01,  2.6703e-01,  3.1877e-01,  2.5888e-01,
         2.6032e-01,  1.3589e-01,  2.7510e-01,  3.0543e-01,  2.4046e-01,
        -5.1573e-03,  1.1417e-01, -1.4724e-01, -1.0133e-01,  6.2094e-01,
        -1.2938e-01,  5.4436e-01, -1.2903e-01,  2.8959e-01,  3.0403e-01,
         2.7542e-01,  1.6492e-01,  1.0071e-01,  5.7861e-02,  2.6238e-01,
         1.9263e-01,  3.9511e-01, -8.8031e-02,  4.8117e-02,  1.9738e-01,
         1.7441e-01, -1.1812e-01,  2.6876e-01,  1.9295e-01,  2.2124e-01,
         3.8450e-02,  3.7715e-02,  1.2350e-01,  8.6986e-02, -3.9226e-01,
         4.7835e-02, -4.5526e-02,  8.4347e-02, -6.8189e-02,  8.1532e-02,
         1.2142e-02, -4.1861e-01, -3.5211e-01, -1.0354e-01, -4.9301e-01,
        -2.7952e-01, -3.6286e-01, -1.8466e-01,  5.5568e-02, -9.4907e-02,
         1.8823e-02, -4.3453e-01, -5.4467e-01, -1.6780e-01,  1.1414e-01,
        -2.2954e-01, -3.6046e-01, -4.1091e-01, -2.4588e-01, -2.0356e-01,
        -1.8200e-01, -5.0810e-01, -6.3152e-01, -1.9229e-01, -2.5175e-01,
        -2.2785e-01, -1.7807e-01, -2.1997e-01, -2.1491e-01, -3.8577e-01,
        -2.3291e-01, -8.0452e-01, -9.3526e-01, -6.6396e-01, -7.3060e-01,
        -4.6913e-01, -3.6801e-01, -2.0937e-01, -1.1466e+00, -7.8291e-01,
        -4.6123e-01, -9.0390e-01, -7.6366e-01, -5.2209e-01, -7.3005e-01,
        -6.7983e-01, -8.1547e-01, -7.7639e-01, -7.8110e-01, -8.0546e-01,
        -9.4365e-01, -6.5334e-01, -7.8664e-01, -5.3815e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1185e+00, -9.1217e-01, -1.6118e+00, -2.6036e+00, -3.0209e+00,
        -2.8764e+00, -2.9684e+00, -3.0429e+00, -2.6329e+00, -2.3972e+00,
        -2.8812e+00, -2.5235e+00, -2.7381e+00, -2.3270e+00, -2.2071e+00,
        -2.3443e+00, -2.6357e+00, -2.3300e+00, -2.1702e+00, -2.0821e+00,
        -1.9956e+00, -2.3197e+00, -8.4510e-01, -1.5894e-01, -1.7987e+00,
        -1.7655e+00, -1.4954e+00, -1.6991e+00, -1.5179e+00, -1.3938e+00,
        -1.8517e+00, -1.5865e+00, -1.9093e+00, -1.9503e+00, -1.7147e+00,
        -1.5446e+00, -1.6369e+00, -1.6286e+00, -1.6890e+00, -1.4286e+00,
        -1.4036e+00, -1.3881e+00, -1.3534e+00, -1.7803e+00, -1.6490e+00,
        -1.5304e+00, -1.3813e+00, -1.7098e+00, -1.6891e+00, -1.6792e+00,
        -1.2917e+00, -1.6978e+00, -1.3862e+00, -1.7703e+00, -1.3706e+00,
        -1.1948e+00, -1.0400e+00, -1.6416e+00, -1.4118e+00, -1.4656e+00,
        -1.3689e+00, -1.5177e+00, -1.0079e+00, -1.3815e+00, -1.4537e+00,
        -1.2968e+00,  1.2598e+00,  1.4053e+00,  1.9294e+00,  2.2000e+00,
         5.1175e+00,  9.5274e-03,  3.1648e-01,  1.9103e-01, -5.7464e-02,
         1.5939e-01,  1.9932e-01,  2.6237e-01,  2.9139e-01,  3.3355e-01,
         2.6769e-01,  3.9570e-01,  3.6430e-01,  2.7478e-01,  4.2218e-01,
         1.4347e-01, -3.3190e-02,  1.9303e-01,  4.5557e-01,  3.1354e-01,
         1.5720e-01,  1.3391e-01,  2.4254e-01,  2.8770e-01,  3.9444e-01,
         9.1655e-02,  2.0718e-01,  2.8734e-01,  2.6421e-01,  1.9487e-01,
         2.8596e-01,  4.2388e-01,  2.2668e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1863e-01,  4.6642e-02,  1.4102e-01,  1.7028e-01,
         2.2787e-01,  2.8321e-01,  2.0820e-01,  1.9055e+00,  2.9271e+00,
         4.0062e+00,  6.5532e-01,  1.0605e+00,  8.2600e-01,  9.5222e-01,
         1.0128e+00,  9.4218e-01,  8.8555e-01,  8.4698e-01,  8.5187e-01,
         7.7220e-01,  8.1336e-01,  8.4408e-01,  9.0579e-01,  5.6602e-01,
         5.7644e-01,  9.3836e-01,  4.6920e-01,  7.9688e-01,  2.9186e-01,
         7.3542e-01,  6.8451e-01,  8.0279e-01,  8.3202e-01,  8.5008e-01,
         1.0585e+00,  1.0376e+00,  5.8392e-01,  8.6598e-01,  9.4280e-01,
         1.1363e+00,  6.1622e-01,  7.5184e-01,  9.1895e-01,  9.8114e-01,
         5.3690e-01,  1.0630e+00,  7.3633e-01,  9.2506e-01,  8.7946e-01,
         4.5694e-01,  7.0966e-01,  9.3775e-01,  8.4758e-01,  9.8154e-01,
         7.9559e-01,  5.3079e-01,  9.0689e-01,  9.7197e-01,  4.7233e-01,
         1.0005e+00,  6.2599e-01,  7.2964e-01,  8.2053e-01,  9.0095e-01,
         1.0306e+00,  7.1553e-01,  6.3852e-01,  8.8942e-01,  8.3221e-01,
         1.0101e+00,  8.6746e-01,  8.9537e-01,  8.0696e-01,  1.1529e+00,
         8.0518e-01,  8.8694e-01,  4.1136e-01,  5.6717e-01,  7.8466e-01,
         9.5044e-01,  9.0154e-01,  7.4469e-01,  6.1488e-01,  6.2777e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.8754e-01,  4.7313e-01,
         5.9742e-01,  6.9876e-01,  7.1356e-01,  3.7526e-01,  7.3710e-01,
         3.2117e-01,  8.8522e-01,  6.2177e-01,  8.0687e-01,  6.9031e-01,
         6.5956e-01,  7.8450e-01,  3.5075e-01,  5.7800e-01,  6.5547e-01,
         6.3236e-01,  5.9193e-01,  6.6074e-01,  5.4118e-01,  5.9671e-01,
         6.1724e-01,  7.2858e-01,  7.7357e-01,  4.2929e-01,  5.9991e-01,
         6.6792e-01,  5.4709e-01,  6.6790e-01,  6.0145e-01,  8.6079e-01,
         5.8879e-01,  4.5092e-01,  6.8439e-01,  5.6548e-01,  6.5489e-01,
         5.8681e-01,  6.6277e-01,  1.8649e-01,  8.0634e-01,  2.8508e-01,
         7.8762e-01,  6.6226e-01,  6.2448e-01,  7.4242e-01,  8.1439e-01,
         6.9780e-01,  5.9516e-01,  8.2684e-01,  2.9261e-01,  7.0406e-01,
         4.1183e-01,  5.9992e-01,  6.1339e-01,  5.0088e-01,  1.5297e-01,
         4.6153e-01,  4.8887e-01,  5.3385e-01,  4.1844e-01,  2.0719e-01,
         1.0747e-01,  2.1790e-01,  1.0530e-03,  3.9678e-01,  3.7554e-01,
         4.0519e-01,  3.2973e-01,  2.4978e-01,  2.9394e-01,  3.1219e-01,
        -8.0855e-02,  3.7377e-01,  3.0517e-01,  5.2405e-01,  5.2668e-02,
         1.2721e-01,  1.5925e-01,  3.7021e-01,  3.6760e-01,  1.9479e-01,
        -2.8384e-02,  2.1344e-01,  4.7078e-01,  2.5872e-01,  1.2642e-01,
         1.7403e-01,  1.6489e-01,  2.0281e-01,  2.1334e-01, -1.2901e-02,
         3.7264e-01,  3.9868e-01,  3.2678e-01,  1.3965e-01,  3.2444e-01,
         2.5398e-01,  2.9896e-01,  1.7182e-01,  2.7075e-01,  2.6387e-01,
         1.8224e-01,  1.2764e-02,  5.9688e-03,  2.3936e-01,  1.7236e-01,
         2.1223e-01, -2.8855e-01,  2.5200e-01,  5.4673e-01,  3.3196e-01,
         4.3197e-01,  2.8008e-01,  2.6703e-01,  4.0081e-01,  2.5888e-01,
         3.2685e-01,  1.3589e-01,  2.7510e-01,  3.1452e-01,  3.4823e-01,
        -4.9920e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.6243e-01,
        -1.4055e-01,  3.7448e-01, -1.2903e-01,  2.8959e-01,  3.0403e-01,
         2.6742e-01,  2.1401e-01,  9.6643e-02,  7.2553e-02,  2.6238e-01,
         1.9263e-01,  3.5511e-01, -8.4409e-02,  5.3469e-02,  1.9115e-01,
         1.4509e-01, -1.1812e-01,  2.6876e-01,  1.9295e-01,  2.7020e-01,
         4.4204e-02,  3.7715e-02,  1.2350e-01,  5.6121e-02, -3.9226e-01,
         6.2027e-02, -4.5526e-02,  8.3151e-02, -6.8189e-02,  8.1532e-02,
         1.5335e-02, -2.9018e-01, -4.3320e-01, -1.4742e-01, -4.9301e-01,
        -2.7406e-01, -3.6286e-01, -1.8466e-01,  5.5568e-02, -9.4907e-02,
         2.6249e-02, -3.8974e-01, -5.4467e-01, -1.6780e-01,  1.0437e-01,
        -3.1689e-01, -3.9974e-01, -4.1091e-01, -2.6754e-01, -2.0356e-01,
        -1.7544e-01, -3.8447e-01, -6.3152e-01, -1.9229e-01, -2.5175e-01,
        -2.5070e-01, -1.7807e-01, -2.1997e-01, -2.3259e-01, -3.8577e-01,
        -3.3317e-01, -6.3336e-01, -9.3214e-01, -5.9353e-01, -7.3060e-01,
        -4.6913e-01, -3.8470e-01, -3.4637e-01, -1.0896e+00, -7.9492e-01,
        -4.7919e-01, -6.8660e-01, -9.2498e-01, -6.3528e-01, -7.3112e-01,
        -6.7983e-01, -7.3870e-01, -9.4607e-01, -8.6231e-01, -8.4060e-01,
        -9.4365e-01, -6.7851e-01, -7.8664e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808077
t6: 1641198808077
state_values: tensor([-0.4444, -0.9811, -0.7348, -0.4345, -0.3238, -0.2751, -0.3206, -0.3520,
        -0.3600, -0.4278, -0.4503, -0.4871, -0.5524, -0.5532, -0.5870, -0.5043,
        -0.6155, -0.5954, -0.6318, -0.6620, -0.6887, -0.7104, -1.1532, -1.5179,
        -0.9337, -0.8601, -0.9028, -0.8672, -0.9469, -0.9864, -0.7182, -0.8516,
        -0.8832, -0.8857, -0.9987, -1.0338, -1.0907, -0.9937, -1.0774, -1.0919,
        -1.1089, -1.1590, -1.1481, -1.0453, -1.0770, -1.1890, -1.1803, -1.1408,
        -1.1962, -1.1413, -1.2445, -1.1925, -1.2384, -0.9154, -1.1871, -1.1994,
        -1.2417, -1.0352, -1.1249, -1.1391, -1.2571, -1.1865, -1.3420, -1.2855,
        -1.2199, -1.3366, -2.3818, -2.4362, -2.8215, -2.7832, -4.5277, -1.5773,
        -1.4706, -1.4003, -1.2977, -1.3066, -1.3441, -1.3674, -1.3983, -1.3909,
        -1.4060, -1.4663, -1.4351, -1.4353, -1.4735, -1.2819, -1.3199, -1.4051,
        -1.4876, -1.4162, -1.4229, -1.3580, -1.4209, -1.4525, -1.4976, -1.4308,
        -1.4022, -1.4552, -1.4675, -1.4093, -1.4505, -1.5240, -1.4833, -1.5225,
        -1.3889, -1.4445, -1.3791, -1.4190, -1.4473, -1.4672, -1.4544, -1.4682,
        -1.4402, -2.0749, -2.5156, -3.3529, -1.6673, -1.5785, -1.5585, -1.5583,
        -1.5634, -1.5591, -1.5321, -1.5552, -1.4656, -1.5109, -1.4173, -1.4634,
        -1.5029, -1.4504, -1.2884, -1.4443, -1.3997, -1.4791, -1.1234, -1.3994,
        -1.3420, -1.4606, -1.4794, -1.4483, -1.5369, -1.5311, -1.4399, -1.5215,
        -1.5418, -1.5523, -1.4311, -1.5011, -1.4816, -1.5766, -1.2554, -1.4891,
        -1.4447, -1.5230, -1.5568, -1.2473, -1.4097, -1.4475, -1.4964, -1.5322,
        -1.4940, -1.4250, -1.5648, -1.5316, -1.4455, -1.5437, -1.4843, -1.4148,
        -1.4994, -1.5627, -1.5653, -1.4846, -1.4896, -1.5168, -1.5389, -1.6115,
        -1.5973, -1.5716, -1.5557, -1.6248, -1.5925, -1.5196, -1.4134, -1.4484,
        -1.5399, -1.5811, -1.6240, -1.5227, -1.4247, -1.3962, -1.4778, -1.4101,
        -1.4193, -1.4872, -1.4460, -1.4070, -1.4698, -1.4229, -1.3956, -1.4386,
        -1.3554, -1.5210, -1.4872, -1.5539, -1.4962, -1.4722, -1.4833, -1.3762,
        -1.4794, -1.4821, -1.3984, -1.4752, -1.5157, -1.4414, -1.4089, -1.4709,
        -1.5063, -1.5269, -1.4659, -1.4471, -1.5382, -1.4517, -1.4933, -1.4821,
        -1.5615, -1.4986, -1.4804, -1.4528, -1.5057, -1.5279, -1.5043, -1.4758,
        -1.3790, -1.5185, -1.4190, -1.5284, -1.5140, -1.5433, -1.5520, -1.6070,
        -1.5825, -1.5396, -1.5739, -1.4300, -1.5419, -1.4734, -1.5574, -1.4976,
        -1.4762, -1.3855, -1.4594, -1.4703, -1.5086, -1.4608, -1.2864, -1.3485,
        -1.2574, -1.3001, -1.4402, -1.4367, -1.4541, -1.4181, -1.3612, -1.4142,
        -1.3944, -1.3028, -1.4033, -1.4487, -1.4784, -1.3866, -1.2853, -1.4015,
        -1.4729, -1.4760, -1.3239, -1.3344, -1.4175, -1.4785, -1.4698, -1.2908,
        -1.3626, -1.3424, -1.3905, -1.4276, -1.3858, -1.4971, -1.5104, -1.4803,
        -1.4435, -1.4556, -1.4694, -1.4457, -1.4460, -1.4508, -1.4433, -1.4512,
        -1.4150, -1.4240, -1.5006, -1.4778, -1.4870, -1.1704, -1.3276, -1.4554,
        -1.4228, -1.4525, -1.4280, -1.3652, -1.4526, -1.3587, -1.4284, -1.3033,
        -1.3932, -1.4381, -1.4470, -1.2114, -1.3448, -1.3169, -1.3082, -1.4126,
        -1.2925, -1.4200, -1.3122, -1.4301, -1.4439, -1.4349, -1.4424, -1.3113,
        -1.3589, -1.3640, -1.4043, -1.4769, -1.3464, -1.3892, -1.3775, -1.4009,
        -1.3401, -1.4683, -1.4421, -1.4829, -1.3624, -1.3536, -1.4206, -1.3968,
        -1.2781, -1.4201, -1.3242, -1.4232, -1.3954, -1.4401, -1.4269, -1.2670,
        -1.2516, -1.2917, -1.1262, -1.2920, -1.2963, -1.3485, -1.4558, -1.4060,
        -1.4694, -1.2298, -1.2638, -1.3882, -1.5056, -1.3728, -1.2031, -1.2266,
        -1.3128, -1.3415, -1.3999, -1.3708, -1.2801, -1.4203, -1.4182, -1.4080,
        -1.4105, -1.4140, -1.4143, -1.3547, -1.3848, -1.2157, -1.2186, -1.3270,
        -1.3078, -1.3553, -1.3745, -1.3956, -1.0724, -1.2222, -1.3359, -1.3195,
        -1.2314, -1.2723, -1.2341, -1.3107, -1.3618, -1.2584, -1.2146, -1.2359,
        -1.2543, -1.3637, -1.3216, -1.3356], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808081
t8: 1641198808081
t9: 1641198808082
t10: 1641198808092
t11: 1641198808094
t12: 1641198808094
t1: 1641198808094
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808104
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9962, 0.9811, 1.0274, 1.0212, 1.1034, 0.9943, 1.0290, 1.1244, 0.9771,
        0.8889, 1.0768, 0.9836, 1.0980, 0.9471, 0.7830, 0.9602, 1.1064, 0.9289,
        0.9305, 0.8988, 0.9490, 1.0309, 1.0047, 0.9517, 1.0530, 0.9954, 0.8913,
        1.0066, 0.9820, 0.7576, 0.9678, 0.9198, 1.0822, 1.0146, 1.0258, 0.9913,
        1.1234, 0.9477, 1.0497, 0.9567, 0.9740, 0.9906, 0.8526, 1.0531, 0.9799,
        1.0308, 0.9035, 1.0517, 1.1349, 0.9846, 0.9231, 1.0613, 0.9397, 0.9810,
        1.0599, 0.9279, 0.7947, 1.0804, 0.8884, 1.0134, 1.1088, 0.9968, 0.9659,
        1.1326, 0.9744, 1.0632, 1.1178, 1.0616, 0.9541, 0.8822, 1.1257, 0.9605,
        1.0504, 1.1141, 0.8896, 0.9852, 0.9559, 0.9790, 0.9582, 1.0390, 0.9950,
        0.9816, 1.0329, 0.9304, 0.8370, 1.0643, 1.0358, 0.9063, 0.9321, 1.0537,
        0.9020, 1.0012, 1.0119, 1.0282, 1.0850, 0.8924, 0.9842, 0.9891, 0.9173,
        0.9733, 1.0149, 1.0560, 0.9983, 1.2002, 1.0833, 1.1477, 1.0699, 1.0573,
        1.0433, 0.9356, 0.8936, 0.9557, 1.0586, 1.0935, 1.0826, 1.2768, 0.9141,
        1.0385, 0.9596, 0.9929, 1.0213, 1.0068, 1.0096, 0.8876, 1.0176, 0.8775,
        1.0300, 1.0610, 1.1086, 0.8300, 0.9885, 1.1830, 0.9520, 0.7714, 0.9913,
        0.8935, 0.9973, 0.9532, 0.9393, 0.9991, 1.0250, 1.1271, 0.9857, 0.9649,
        0.9721, 1.2136, 1.0545, 0.8966, 1.0003, 0.8115, 0.9980, 1.0795, 1.0393,
        1.0119, 0.8191, 0.9793, 0.9170, 1.0427, 1.0146, 1.0588, 1.0875, 1.0018,
        0.9827, 1.1927, 0.9721, 1.0558, 0.9142, 1.0564, 1.0167, 0.9898, 1.0823,
        1.0484, 0.9458, 1.0182, 0.9943, 0.9992, 0.9475, 0.9852, 0.9675, 1.0435,
        0.9257, 1.2441, 1.0990, 1.0361, 0.9842, 1.0076, 0.9064, 0.8591, 0.9385,
        1.0270, 1.1296, 1.2289, 0.9092, 1.1002, 0.8673, 0.9691, 0.9148, 1.0966,
        0.9391, 1.1400, 0.9984, 1.0421, 0.9662, 0.9287, 0.9008, 0.9645, 1.1241,
        1.0422, 0.9286, 0.8690, 1.0441, 0.9844, 0.9170, 0.9094, 1.0357, 1.0111,
        1.0364, 1.0853, 0.9146, 1.0085, 0.8819, 0.9590, 0.9922, 0.9993, 1.0781,
        1.0685, 0.8772, 1.0332, 0.9517, 0.9634, 0.9516, 1.2279, 0.9474, 1.1693,
        0.9705, 1.0570, 1.0408, 0.9709, 1.0112, 1.0073, 0.9902, 0.9712, 1.1322,
        1.0303, 1.0868, 1.0154, 0.9332, 1.0400, 1.1348, 0.9361, 0.9260, 0.9570,
        0.9458, 0.8443, 1.0702, 0.8393, 1.0729, 0.9685, 0.9539, 0.9452, 0.9209,
        0.8864, 0.9828, 0.9421, 1.1902, 0.9303, 1.0558, 0.9346, 1.1490, 0.8628,
        1.0483, 0.9817, 0.9660, 0.8528, 1.0532, 1.0287, 0.9668, 1.0727, 0.8376,
        0.9799, 0.8921, 1.0110, 1.0649, 1.1564, 1.0158, 1.0072, 1.0242, 1.0740,
        0.9428, 1.0241, 0.9496, 1.0610, 0.9055, 0.9511, 1.0494, 1.0771, 1.0890,
        1.0239, 1.0005, 0.9996, 1.0681, 0.9055, 0.9968, 1.0626, 1.0369, 1.0513,
        0.8773, 0.9854, 0.8716, 0.9908, 0.8651, 0.9935, 0.9908, 0.9484, 0.8275,
        1.0323, 1.1596, 1.2352, 0.9307, 1.2803, 0.9270, 1.2008, 1.0098, 1.0420,
        1.0436, 1.0617, 0.8583, 1.0395, 0.8687, 1.0229, 1.0309, 1.0977, 1.0656,
        0.9191, 1.0576, 1.1362, 1.0157, 0.9903, 0.9763, 0.8839, 0.9282, 0.9893,
        0.9779, 1.2656, 1.0313, 0.8618, 1.0061, 1.0546, 0.9725, 0.9964, 0.8705,
        1.1440, 0.8750, 0.9097, 1.0314, 1.0691, 1.0220, 0.9943, 0.9956, 1.0076,
        0.8360, 1.0545, 1.0425, 1.0067, 1.0865, 0.8383, 0.9490, 0.9898, 0.9128,
        0.9931, 1.0647, 1.1755, 0.9949, 1.0313, 1.0404, 0.9232, 0.9734, 0.9658,
        0.9308, 1.0106, 0.8259, 1.1042, 1.0475, 1.0997, 1.0409, 0.9656, 0.9450,
        0.7842, 1.0602, 0.9518, 0.9458, 1.1740, 0.8845, 0.9412, 0.9664, 0.9860,
        1.0904, 0.8867, 0.9634, 0.9454, 1.0170, 0.9450, 0.9779, 0.8290],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808109
t4: 1641198808109
surr1, surr2: tensor([-3.1495e+00, -9.6236e-01, -1.5184e+00, -2.5276e+00, -3.0303e+00,
        -2.9277e+00, -2.8636e+00, -3.1103e+00, -2.7600e+00, -2.3675e+00,
        -2.8204e+00, -2.6133e+00, -2.7331e+00, -2.4487e+00, -1.9202e+00,
        -2.5011e+00, -2.6509e+00, -2.4049e+00, -2.2438e+00, -2.0793e+00,
        -2.1043e+00, -2.2527e+00, -8.4844e-01, -1.6806e-01, -1.7219e+00,
        -1.7959e+00, -1.4810e+00, -1.7010e+00, -1.5711e+00, -1.1732e+00,
        -1.9912e+00, -1.6215e+00, -1.8784e+00, -1.9304e+00, -1.6620e+00,
        -1.5810e+00, -1.6717e+00, -1.7149e+00, -1.6117e+00, -1.5187e+00,
        -1.4816e+00, -1.4274e+00, -1.2821e+00, -1.7045e+00, -1.7097e+00,
        -1.4884e+00, -1.3867e+00, -1.6347e+00, -1.7427e+00, -1.7364e+00,
        -1.3248e+00, -1.6380e+00, -1.4473e+00, -1.8475e+00, -1.3207e+00,
        -1.2318e+00, -9.1836e-01, -1.6123e+00, -1.3935e+00, -1.4202e+00,
        -1.3799e+00, -1.5384e+00, -1.0731e+00, -1.4224e+00, -1.5238e+00,
        -1.2534e+00,  1.2802e+00,  1.3562e+00,  2.0453e+00,  2.1565e+00,
         5.2370e+00,  1.0061e-02,  3.0223e-01,  1.9348e-01, -5.6798e-02,
         1.6864e-01,  2.1170e-01,  2.7403e-01,  3.1022e-01,  3.2110e-01,
         2.7293e-01,  4.1197e-01,  3.5382e-01,  2.8408e-01,  3.9265e-01,
         1.3881e-01, -3.2090e-02,  1.9439e-01,  4.7184e-01,  3.0033e-01,
         1.5756e-01,  1.3596e-01,  2.4147e-01,  2.8029e-01,  3.8908e-01,
         9.0878e-02,  2.1909e-01,  2.9508e-01,  2.6928e-01,  2.0692e-01,
         2.8428e-01,  4.0692e-01,  2.3015e-01,  4.2090e-01, -1.2069e-01,
         9.2524e-02, -1.1538e-01,  4.4833e-02,  1.3558e-01,  1.7702e-01,
         2.2624e-01,  3.0072e-01,  2.0036e-01,  1.8942e+00,  2.8807e+00,
         4.6502e+00,  6.6560e-01,  1.0157e+00,  8.8068e-01,  9.7829e-01,
         9.9958e-01,  9.4783e-01,  8.8784e-01,  8.3534e-01,  8.3932e-01,
         7.5293e-01,  7.6685e-01,  8.1412e-01,  9.1291e-01,  5.2198e-01,
         6.0147e-01,  1.0092e+00,  4.9634e-01,  6.8304e-01,  2.9751e-01,
         7.3013e-01,  6.9837e-01,  8.4668e-01,  8.6840e-01,  8.6083e-01,
         1.0331e+00,  1.0632e+00,  6.0273e-01,  9.2533e-01,  9.9636e-01,
         1.2536e+00,  5.9075e-01,  7.4899e-01,  9.3698e-01,  8.8471e-01,
         5.4214e-01,  1.0431e+00,  7.0780e-01,  9.2042e-01,  8.0042e-01,
         4.8218e-01,  7.2307e-01,  8.8891e-01,  8.3990e-01,  9.4477e-01,
         7.8655e-01,  5.3546e-01,  9.4270e-01,  1.0539e+00,  4.9702e-01,
         9.6032e-01,  6.3585e-01,  7.0075e-01,  8.1400e-01,  9.2722e-01,
         1.0140e+00,  6.8200e-01,  6.7098e-01,  8.8301e-01,  8.5137e-01,
         1.0276e+00,  9.1322e-01,  9.3083e-01,  8.5663e-01,  1.1063e+00,
         8.2818e-01,  1.0031e+00,  4.1100e-01,  5.4875e-01,  8.1372e-01,
         9.5660e-01,  9.0793e-01,  7.1085e-01,  6.4117e-01,  6.0875e-01,
         7.9774e-01,  5.3247e-01,  3.6998e-01,  8.8771e-01,  4.5595e-01,
         6.4331e-01,  7.1025e-01,  7.1135e-01,  3.9158e-01,  7.6388e-01,
         3.2531e-01,  8.4668e-01,  6.5965e-01,  8.3256e-01,  6.9094e-01,
         7.0682e-01,  8.0172e-01,  3.3608e-01,  5.9637e-01,  6.3290e-01,
         6.0024e-01,  6.0912e-01,  6.7322e-01,  5.4682e-01,  5.6959e-01,
         6.1541e-01,  7.0249e-01,  7.6326e-01,  4.3624e-01,  6.0248e-01,
         6.5450e-01,  5.8296e-01,  6.8396e-01,  6.0956e-01,  8.4362e-01,
         5.7195e-01,  4.3951e-01,  6.4282e-01,  5.9795e-01,  7.0081e-01,
         6.2043e-01,  7.3982e-01,  1.9632e-01,  8.5712e-01,  3.0073e-01,
         7.5685e-01,  6.3712e-01,  6.5870e-01,  7.4350e-01,  8.1901e-01,
         7.1807e-01,  6.2874e-01,  8.5102e-01,  2.8545e-01,  6.9560e-01,
         4.0965e-01,  6.2206e-01,  5.9118e-01,  5.1672e-01,  1.5911e-01,
         4.7487e-01,  5.1985e-01,  5.6104e-01,  3.9253e-01,  2.0158e-01,
         1.0022e-01,  2.1253e-01,  1.1061e-03,  4.2055e-01,  3.9439e-01,
         4.1459e-01,  3.2474e-01,  2.6377e-01,  3.0770e-01,  3.3781e-01,
        -8.3580e-02,  3.5873e-01,  3.1690e-01,  5.4741e-01,  5.0488e-02,
         1.2123e-01,  1.6464e-01,  3.9316e-01,  3.4834e-01,  1.8651e-01,
        -2.7632e-02,  2.2572e-01,  4.5909e-01,  2.4078e-01,  1.3463e-01,
         1.7251e-01,  1.6340e-01,  1.9634e-01,  2.2428e-01, -1.2750e-02,
         3.7345e-01,  3.9044e-01,  3.1905e-01,  1.4629e-01,  3.1901e-01,
         2.6798e-01,  2.8836e-01,  1.7287e-01,  2.8613e-01,  2.5174e-01,
         1.7843e-01,  1.2636e-02,  5.8601e-03,  2.4286e-01,  1.7517e-01,
         2.0607e-01, -2.9031e-01,  2.5868e-01,  5.2813e-01,  3.2142e-01,
         4.1285e-01,  2.7301e-01,  2.8169e-01,  3.8816e-01,  2.7056e-01,
         3.1416e-01,  1.4114e-01,  2.8073e-01,  3.3145e-01,  3.2016e-01,
        -4.6847e-03,  9.7209e-02, -1.1289e-01, -1.1315e-01,  4.2184e-01,
        -1.4477e-01,  4.0881e-01, -1.2882e-01,  2.7732e-01,  2.9113e-01,
         2.5811e-01,  2.0410e-01,  9.1331e-02,  7.0033e-02,  2.4461e-01,
         1.8753e-01,  3.5438e-01, -8.1769e-02,  5.4606e-02,  1.8379e-01,
         1.4987e-01, -1.1724e-01,  2.7642e-01,  2.0238e-01,  2.6536e-01,
         4.5589e-02,  3.9029e-02,  1.2865e-01,  6.4571e-02, -3.8137e-01,
         5.9392e-02, -4.5419e-02,  7.9718e-02, -7.1483e-02,  8.3172e-02,
         1.4832e-02, -3.0179e-01, -4.2118e-01, -1.4901e-01, -4.7450e-01,
        -2.6636e-01, -3.5517e-01, -1.8811e-01,  5.6605e-02, -9.5100e-02,
         2.4383e-02, -3.7362e-01, -5.2131e-01, -1.6814e-01,  1.0310e-01,
        -2.9517e-01, -4.2151e-01, -4.2197e-01, -2.7134e-01, -2.0924e-01,
        -1.6981e-01, -4.1088e-01, -6.4296e-01, -1.8645e-01, -2.4146e-01,
        -2.5717e-01, -1.8923e-01, -2.3388e-01, -2.4055e-01, -3.8629e-01,
        -3.0574e-01, -6.3580e-01, -8.8768e-01, -5.9334e-01, -6.9894e-01,
        -4.9828e-01, -4.0394e-01, -3.0181e-01, -1.0502e+00, -8.4065e-01,
        -5.0358e-01, -7.3276e-01, -9.0901e-01, -6.6439e-01, -7.8507e-01,
        -6.9571e-01, -7.3223e-01, -9.3209e-01, -9.2304e-01, -8.8298e-01,
        -9.3391e-01, -7.1241e-01, -8.2630e-01, -7.1109e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1495e+00, -9.6236e-01, -1.5184e+00, -2.5276e+00, -3.0209e+00,
        -2.9277e+00, -2.8636e+00, -3.0429e+00, -2.7600e+00, -2.3972e+00,
        -2.8204e+00, -2.6133e+00, -2.7331e+00, -2.4487e+00, -2.2071e+00,
        -2.5011e+00, -2.6357e+00, -2.4049e+00, -2.2438e+00, -2.0821e+00,
        -2.1043e+00, -2.2527e+00, -8.4844e-01, -1.6806e-01, -1.7219e+00,
        -1.7959e+00, -1.4954e+00, -1.7010e+00, -1.5711e+00, -1.3938e+00,
        -1.9912e+00, -1.6215e+00, -1.8784e+00, -1.9304e+00, -1.6620e+00,
        -1.5810e+00, -1.6369e+00, -1.7149e+00, -1.6117e+00, -1.5187e+00,
        -1.4816e+00, -1.4274e+00, -1.3534e+00, -1.7045e+00, -1.7097e+00,
        -1.4884e+00, -1.3867e+00, -1.6347e+00, -1.6891e+00, -1.7364e+00,
        -1.3248e+00, -1.6380e+00, -1.4473e+00, -1.8475e+00, -1.3207e+00,
        -1.2318e+00, -1.0400e+00, -1.6123e+00, -1.4118e+00, -1.4202e+00,
        -1.3689e+00, -1.5384e+00, -1.0731e+00, -1.3815e+00, -1.5238e+00,
        -1.2534e+00,  1.2598e+00,  1.3562e+00,  2.0453e+00,  2.2000e+00,
         5.1175e+00,  1.0061e-02,  3.0223e-01,  1.9103e-01, -5.7464e-02,
         1.6864e-01,  2.1170e-01,  2.7403e-01,  3.1022e-01,  3.2110e-01,
         2.7293e-01,  4.1197e-01,  3.5382e-01,  2.8408e-01,  4.2218e-01,
         1.3881e-01, -3.2090e-02,  1.9439e-01,  4.7184e-01,  3.0033e-01,
         1.5756e-01,  1.3596e-01,  2.4147e-01,  2.8029e-01,  3.8908e-01,
         9.1655e-02,  2.1909e-01,  2.9508e-01,  2.6928e-01,  2.0692e-01,
         2.8428e-01,  4.0692e-01,  2.3015e-01,  3.8576e-01, -1.2069e-01,
         8.8676e-02, -1.1538e-01,  4.4833e-02,  1.3558e-01,  1.7702e-01,
         2.2787e-01,  3.0072e-01,  2.0036e-01,  1.8942e+00,  2.8807e+00,
         4.0062e+00,  6.6560e-01,  1.0157e+00,  8.8068e-01,  9.7829e-01,
         9.9958e-01,  9.4783e-01,  8.8784e-01,  8.4698e-01,  8.3932e-01,
         7.7220e-01,  7.6685e-01,  8.1412e-01,  9.0579e-01,  5.6602e-01,
         6.0147e-01,  9.3836e-01,  4.9634e-01,  7.9688e-01,  2.9751e-01,
         7.3542e-01,  6.9837e-01,  8.4668e-01,  8.6840e-01,  8.6083e-01,
         1.0331e+00,  1.0376e+00,  6.0273e-01,  9.2533e-01,  9.9636e-01,
         1.1363e+00,  5.9075e-01,  7.5184e-01,  9.3698e-01,  9.8114e-01,
         5.4214e-01,  1.0431e+00,  7.0780e-01,  9.2042e-01,  8.7946e-01,
         4.8218e-01,  7.2307e-01,  8.8891e-01,  8.3990e-01,  9.4477e-01,
         7.8655e-01,  5.3546e-01,  9.4270e-01,  9.7197e-01,  4.9702e-01,
         9.6032e-01,  6.3585e-01,  7.0075e-01,  8.1400e-01,  9.2722e-01,
         1.0140e+00,  6.8200e-01,  6.7098e-01,  8.8301e-01,  8.5137e-01,
         1.0276e+00,  9.1322e-01,  9.3083e-01,  8.5663e-01,  1.1063e+00,
         8.2818e-01,  8.8694e-01,  4.1100e-01,  5.4875e-01,  8.1372e-01,
         9.5660e-01,  9.0793e-01,  7.4469e-01,  6.4117e-01,  6.0875e-01,
         7.7685e-01,  4.7662e-01,  3.6998e-01,  8.8754e-01,  4.7313e-01,
         6.4331e-01,  7.1025e-01,  7.1135e-01,  3.9158e-01,  7.3710e-01,
         3.2531e-01,  8.4668e-01,  6.5965e-01,  8.3256e-01,  6.9094e-01,
         7.0682e-01,  7.8450e-01,  3.3608e-01,  5.9637e-01,  6.5547e-01,
         6.0024e-01,  6.0912e-01,  6.7322e-01,  5.4682e-01,  5.6959e-01,
         6.1541e-01,  7.0249e-01,  7.6326e-01,  4.3624e-01,  6.0248e-01,
         6.6792e-01,  5.8296e-01,  6.8396e-01,  6.0956e-01,  8.4362e-01,
         5.7195e-01,  4.5092e-01,  6.4282e-01,  5.9795e-01,  7.0081e-01,
         6.2043e-01,  6.6277e-01,  1.9632e-01,  8.0634e-01,  3.0073e-01,
         7.5685e-01,  6.3712e-01,  6.5870e-01,  7.4350e-01,  8.1901e-01,
         7.1807e-01,  6.2874e-01,  8.2684e-01,  2.8545e-01,  6.9560e-01,
         4.0965e-01,  6.2206e-01,  5.9118e-01,  5.0088e-01,  1.5911e-01,
         4.7487e-01,  5.1985e-01,  5.6104e-01,  4.1844e-01,  2.0158e-01,
         1.0747e-01,  2.1253e-01,  1.1061e-03,  4.2055e-01,  3.9439e-01,
         4.1459e-01,  3.2973e-01,  2.6377e-01,  3.0770e-01,  3.1219e-01,
        -8.3580e-02,  3.5873e-01,  3.1690e-01,  5.2405e-01,  5.2668e-02,
         1.2123e-01,  1.6464e-01,  3.9316e-01,  3.6760e-01,  1.8651e-01,
        -2.7632e-02,  2.2572e-01,  4.5909e-01,  2.5872e-01,  1.3463e-01,
         1.7403e-01,  1.6340e-01,  1.9634e-01,  2.1334e-01, -1.2750e-02,
         3.7345e-01,  3.9044e-01,  3.1905e-01,  1.4629e-01,  3.1901e-01,
         2.6798e-01,  2.8836e-01,  1.7287e-01,  2.8613e-01,  2.5174e-01,
         1.7843e-01,  1.2636e-02,  5.8601e-03,  2.4286e-01,  1.7517e-01,
         2.0607e-01, -2.9031e-01,  2.5868e-01,  5.2813e-01,  3.2142e-01,
         4.1285e-01,  2.8008e-01,  2.8169e-01,  4.0081e-01,  2.7056e-01,
         3.2685e-01,  1.4114e-01,  2.8073e-01,  3.3145e-01,  3.4823e-01,
        -4.6847e-03,  9.2211e-02, -1.0053e-01, -1.1315e-01,  3.6243e-01,
        -1.4477e-01,  3.7448e-01, -1.2882e-01,  2.7732e-01,  2.9113e-01,
         2.5811e-01,  2.1401e-01,  9.1331e-02,  7.2553e-02,  2.4461e-01,
         1.8753e-01,  3.5438e-01, -8.1769e-02,  5.4606e-02,  1.8379e-01,
         1.4509e-01, -1.1724e-01,  2.7642e-01,  2.0238e-01,  2.7020e-01,
         4.5589e-02,  3.9029e-02,  1.2865e-01,  5.6121e-02, -3.8137e-01,
         6.2027e-02, -4.5419e-02,  7.9718e-02, -7.1483e-02,  8.3172e-02,
         1.5335e-02, -2.9018e-01, -4.3320e-01, -1.4901e-01, -4.7450e-01,
        -2.6636e-01, -3.5517e-01, -1.8811e-01,  5.6605e-02, -9.5100e-02,
         2.6249e-02, -3.7362e-01, -5.2131e-01, -1.6814e-01,  1.0310e-01,
        -3.1689e-01, -4.2151e-01, -4.2197e-01, -2.7134e-01, -2.0924e-01,
        -1.6981e-01, -3.8447e-01, -6.4296e-01, -1.8645e-01, -2.4146e-01,
        -2.5717e-01, -1.8923e-01, -2.3388e-01, -2.4055e-01, -3.8629e-01,
        -3.3317e-01, -6.3336e-01, -8.8768e-01, -5.9334e-01, -6.9894e-01,
        -4.9828e-01, -4.0394e-01, -3.4637e-01, -1.0502e+00, -8.4065e-01,
        -5.0358e-01, -6.8660e-01, -9.2498e-01, -6.6439e-01, -7.8507e-01,
        -6.9571e-01, -7.3223e-01, -9.4607e-01, -9.2304e-01, -8.8298e-01,
        -9.3391e-01, -7.1241e-01, -8.2630e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808118
t6: 1641198808118
state_values: tensor([-0.2129, -0.2395, -0.2171, -0.1843, -0.1658, -0.1537, -0.1591, -0.1622,
        -0.1633, -0.1696, -0.1694, -0.1758, -0.1832, -0.1839, -0.1870, -0.1683,
        -0.1861, -0.1864, -0.1890, -0.1923, -0.1931, -0.1971, -0.2443, -0.3138,
        -0.2207, -0.2153, -0.2167, -0.2106, -0.2172, -0.2195, -0.1894, -0.2052,
        -0.2062, -0.2097, -0.2166, -0.2187, -0.2213, -0.2171, -0.2200, -0.2213,
        -0.2215, -0.2239, -0.2235, -0.2127, -0.2180, -0.2238, -0.2241, -0.2202,
        -0.2242, -0.2222, -0.2301, -0.2231, -0.2287, -0.2017, -0.2197, -0.2218,
        -0.2235, -0.2048, -0.2155, -0.2119, -0.2219, -0.2197, -0.2331, -0.2246,
        -0.2221, -0.2321, -0.4423, -0.4806, -0.5520, -0.5517, -0.7411, -0.3321,
        -0.2961, -0.2766, -0.2539, -0.2348, -0.2456, -0.2482, -0.2526, -0.2498,
        -0.2531, -0.2617, -0.2555, -0.2562, -0.2594, -0.2194, -0.2271, -0.2430,
        -0.2542, -0.2417, -0.2456, -0.2262, -0.2411, -0.2487, -0.2567, -0.2446,
        -0.2300, -0.2434, -0.2469, -0.2348, -0.2422, -0.2563, -0.2505, -0.2566,
        -0.2356, -0.2414, -0.2307, -0.2346, -0.2389, -0.2424, -0.2378, -0.2376,
        -0.2331, -0.3643, -0.4677, -0.5860, -0.3094, -0.2808, -0.2735, -0.2684,
        -0.2673, -0.2656, -0.2605, -0.2630, -0.2383, -0.2506, -0.2234, -0.2372,
        -0.2450, -0.2355, -0.2131, -0.2240, -0.2242, -0.2311, -0.1989, -0.2190,
        -0.2134, -0.2225, -0.2242, -0.2230, -0.2344, -0.2362, -0.2250, -0.2342,
        -0.2368, -0.2392, -0.2246, -0.2314, -0.2244, -0.2410, -0.2071, -0.2226,
        -0.2221, -0.2280, -0.2347, -0.2053, -0.2176, -0.2200, -0.2236, -0.2262,
        -0.2243, -0.2214, -0.2327, -0.2269, -0.2231, -0.2284, -0.2242, -0.2197,
        -0.2241, -0.2312, -0.2317, -0.2238, -0.2242, -0.2246, -0.2275, -0.2396,
        -0.2384, -0.2328, -0.2306, -0.2423, -0.2385, -0.2250, -0.2210, -0.2218,
        -0.2258, -0.2317, -0.2410, -0.2244, -0.2153, -0.2141, -0.2208, -0.2187,
        -0.2194, -0.2208, -0.2196, -0.2128, -0.2183, -0.2160, -0.2161, -0.2176,
        -0.2148, -0.2219, -0.2209, -0.2234, -0.2206, -0.2187, -0.2197, -0.2157,
        -0.2201, -0.2194, -0.2115, -0.2183, -0.2205, -0.2168, -0.2143, -0.2186,
        -0.2205, -0.2216, -0.2188, -0.2169, -0.2217, -0.2152, -0.2185, -0.2189,
        -0.2227, -0.2202, -0.2194, -0.2144, -0.2193, -0.2203, -0.2195, -0.2181,
        -0.2146, -0.2197, -0.2163, -0.2203, -0.2202, -0.2215, -0.2216, -0.2247,
        -0.2238, -0.2217, -0.2229, -0.2170, -0.2216, -0.2184, -0.2219, -0.2187,
        -0.2185, -0.2144, -0.2164, -0.2165, -0.2183, -0.2161, -0.2027, -0.2095,
        -0.1984, -0.2058, -0.2123, -0.2125, -0.2134, -0.2117, -0.2054, -0.2107,
        -0.2102, -0.2070, -0.2105, -0.2135, -0.2143, -0.2116, -0.2005, -0.2100,
        -0.2136, -0.2140, -0.2020, -0.2067, -0.2114, -0.2138, -0.2142, -0.1992,
        -0.2063, -0.2032, -0.2085, -0.2110, -0.2103, -0.2147, -0.2157, -0.2147,
        -0.2130, -0.2128, -0.2140, -0.2124, -0.2129, -0.2122, -0.2119, -0.2130,
        -0.2113, -0.2116, -0.2150, -0.2142, -0.2146, -0.1983, -0.2051, -0.2114,
        -0.2107, -0.2122, -0.2113, -0.2055, -0.2109, -0.2035, -0.2091, -0.1994,
        -0.2067, -0.2097, -0.2103, -0.1923, -0.2037, -0.2043, -0.2040, -0.2075,
        -0.2032, -0.2078, -0.2042, -0.2094, -0.2102, -0.2101, -0.2106, -0.1995,
        -0.2051, -0.2017, -0.2069, -0.2111, -0.2056, -0.2073, -0.2060, -0.2079,
        -0.2057, -0.2109, -0.2097, -0.2114, -0.2042, -0.2044, -0.2080, -0.2072,
        -0.2024, -0.2083, -0.1993, -0.2068, -0.2067, -0.2085, -0.2084, -0.1963,
        -0.1990, -0.1965, -0.1829, -0.1977, -0.1993, -0.2027, -0.2074, -0.2059,
        -0.2092, -0.1916, -0.1979, -0.2044, -0.2106, -0.2048, -0.1896, -0.1904,
        -0.1987, -0.2005, -0.2040, -0.2036, -0.2002, -0.2059, -0.2063, -0.2062,
        -0.2055, -0.2060, -0.2061, -0.2032, -0.2052, -0.1903, -0.1951, -0.2008,
        -0.2003, -0.2026, -0.2032, -0.2040, -0.1788, -0.1927, -0.1990, -0.1988,
        -0.1957, -0.1932, -0.1894, -0.1967, -0.2003, -0.1965, -0.1890, -0.1916,
        -0.1942, -0.2004, -0.1986, -0.1997], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808123
t8: 1641198808123
t9: 1641198808123
t10: 1641198808133
t11: 1641198808135
t12: 1641198808135
t1: 1641198808135
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808146
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0049, 1.0317, 0.9689, 0.9939, 0.9628, 1.0009, 0.9890, 0.9759, 1.0028,
        1.0297, 0.9783, 0.9984, 0.9874, 0.9989, 1.0490, 0.9963, 0.9803, 0.9763,
        1.0681, 1.0445, 1.0511, 0.9788, 0.9972, 1.0386, 0.9715, 0.9998, 1.0076,
        0.9930, 0.9953, 1.0241, 1.0433, 1.1316, 0.7873, 0.9812, 0.9528, 1.0027,
        0.9737, 0.9920, 0.9915, 0.9941, 1.0015, 0.9998, 0.9890, 0.9753, 0.9934,
        0.9993, 0.9857, 0.9807, 1.0379, 0.9982, 1.0032, 0.9935, 0.9891, 1.0004,
        1.0043, 0.9924, 1.0373, 0.9899, 0.9544, 0.9596, 0.9666, 1.0000, 1.0036,
        1.0021, 0.9952, 0.9951, 1.0078, 1.0036, 1.0017, 1.0273, 1.0431, 0.9810,
        0.9879, 1.0200, 0.9874, 1.0048, 0.9901, 0.9989, 0.9949, 1.0024, 0.9997,
        0.9994, 1.0016, 0.9932, 1.0231, 1.0002, 1.0183, 0.9941, 1.0267, 0.9997,
        0.9620, 0.9941, 1.0036, 1.0046, 1.0105, 0.9858, 1.0039, 0.9982, 0.9881,
        1.0029, 1.0037, 1.0095, 1.0002, 1.0054, 1.0112, 1.0029, 1.0027, 0.9988,
        0.9990, 1.0016, 1.0170, 1.0097, 1.0058, 1.0105, 1.0032, 0.9861, 0.9895,
        0.9864, 0.9951, 1.0001, 1.0019, 1.0009, 1.0006, 0.9963, 0.9933, 0.9714,
        0.9883, 1.0215, 1.0166, 0.9748, 1.0067, 1.0418, 0.9835, 1.0271, 1.0041,
        1.0751, 0.9973, 1.0402, 1.0932, 0.9915, 0.9810, 0.9535, 0.9986, 1.0018,
        1.0000, 1.0104, 1.0105, 1.0009, 0.9986, 0.9622, 1.0001, 0.9926, 1.0070,
        1.0007, 0.9893, 1.0290, 1.0272, 0.9431, 0.9967, 0.9754, 1.0067, 1.0005,
        1.0004, 1.0002, 0.9970, 0.9939, 0.9950, 0.9868, 1.0029, 0.9999, 0.9995,
        1.0034, 0.9998, 0.9970, 0.9999, 0.9999, 0.9997, 1.0008, 0.9988, 0.9981,
        0.9921, 0.9678, 1.0193, 1.0021, 1.0001, 0.9993, 0.9997, 1.0387, 0.9873,
        1.0058, 1.0540, 1.0320, 0.9899, 0.9619, 0.9658, 1.0248, 0.9891, 0.9493,
        0.9767, 0.9263, 1.0002, 1.0015, 0.9963, 1.0016, 1.0018, 1.0039, 1.0311,
        1.0123, 0.9967, 1.0152, 1.0013, 0.9942, 0.9980, 1.0099, 0.9953, 1.0051,
        1.0033, 1.0098, 0.9905, 0.9964, 0.9765, 1.0096, 0.9989, 1.0006, 1.0088,
        1.0106, 0.9916, 0.9877, 0.9866, 1.0003, 0.9944, 1.0164, 0.9877, 0.9819,
        0.9950, 0.9957, 1.0046, 0.9990, 0.9990, 1.0006, 0.9999, 0.9999, 0.9944,
        1.0044, 0.9996, 1.0007, 1.0019, 0.9942, 1.0103, 0.9954, 1.0110, 1.0027,
        0.9985, 0.9979, 0.9945, 0.9272, 0.8521, 1.0150, 1.0221, 1.0118, 1.0158,
        1.0557, 1.0141, 1.0088, 0.8570, 0.9912, 0.9542, 0.9833, 0.9593, 0.9550,
        0.9026, 0.9966, 1.0019, 0.9899, 0.9060, 1.0098, 1.0002, 0.9978, 0.9684,
        1.0336, 1.0225, 0.9797, 0.9864, 0.9553, 1.0024, 0.9999, 1.0007, 1.0037,
        0.9976, 0.9962, 0.9954, 0.9949, 0.9894, 1.0148, 1.0058, 1.0148, 1.0068,
        1.0014, 1.0000, 1.0000, 0.9995, 0.9937, 0.9991, 1.0063, 1.0038, 1.0014,
        0.9963, 1.0040, 0.9812, 1.0018, 0.9556, 1.0059, 0.9996, 1.0068, 1.0482,
        0.9028, 0.9549, 0.9588, 0.9922, 0.9302, 0.9817, 0.9494, 1.0031, 1.0005,
        1.0028, 1.0023, 0.9964, 0.9924, 0.9694, 0.9867, 1.0113, 1.0146, 1.0107,
        0.9965, 0.9865, 1.0238, 1.0021, 1.0001, 1.0002, 1.0047, 1.0165, 0.9999,
        0.9956, 1.0185, 1.0069, 0.9988, 0.9979, 1.0167, 0.9960, 1.0000, 0.9849,
        0.9650, 0.9535, 1.1241, 0.9722, 1.0191, 0.9844, 1.0002, 1.0000, 1.0009,
        0.9883, 0.9381, 1.0135, 1.0012, 1.0044, 0.9769, 1.0595, 0.9995, 0.9803,
        1.0005, 0.9841, 1.0009, 0.9997, 0.9986, 1.0028, 0.9970, 1.0045, 0.9972,
        0.9983, 0.9995, 0.9675, 0.8962, 1.0083, 1.0129, 1.0062, 0.9990, 1.0026,
        1.0037, 0.9876, 0.9988, 1.0311, 0.9607, 0.9645, 1.1282, 1.0162, 1.0027,
        0.9338, 1.0148, 1.1132, 1.0444, 0.9829, 1.0286, 1.0157, 1.0630],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808150
t4: 1641198808150
surr1, surr2: tensor([-3.1770e+00, -1.0120e+00, -1.4318e+00, -2.4602e+00, -2.6442e+00,
        -2.9469e+00, -2.7525e+00, -2.6997e+00, -2.8325e+00, -2.7427e+00,
        -2.5624e+00, -2.6527e+00, -2.4579e+00, -2.5826e+00, -2.5726e+00,
        -2.5953e+00, -2.3488e+00, -2.5274e+00, -2.5757e+00, -2.4164e+00,
        -2.3306e+00, -2.1389e+00, -8.4210e-01, -1.8342e-01, -1.5886e+00,
        -1.8039e+00, -1.6742e+00, -1.6781e+00, -1.5925e+00, -1.5859e+00,
        -2.1465e+00, -1.9948e+00, -1.3666e+00, -1.8669e+00, -1.5438e+00,
        -1.5991e+00, -1.4490e+00, -1.7950e+00, -1.5224e+00, -1.5780e+00,
        -1.5234e+00, -1.4406e+00, -1.4872e+00, -1.5784e+00, -1.7332e+00,
        -1.4429e+00, -1.5129e+00, -1.5243e+00, -1.5937e+00, -1.7603e+00,
        -1.4398e+00, -1.5334e+00, -1.5234e+00, -1.8840e+00, -1.2513e+00,
        -1.3174e+00, -1.1987e+00, -1.4774e+00, -1.4971e+00, -1.3448e+00,
        -1.2029e+00, -1.5434e+00, -1.1149e+00, -1.2585e+00, -1.5563e+00,
        -1.1731e+00,  1.1542e+00,  1.2822e+00,  2.1475e+00,  2.5111e+00,
         4.8529e+00,  1.0275e-02,  2.8423e-01,  1.7715e-01, -6.3047e-02,
         1.7199e-01,  2.1928e-01,  2.7960e-01,  3.2212e-01,  3.0980e-01,
         2.7422e-01,  4.1945e-01,  3.4309e-01,  3.0324e-01,  4.7993e-01,
         1.3044e-01, -3.1549e-02,  2.1322e-01,  5.1971e-01,  2.8496e-01,
         1.6804e-01,  1.3500e-01,  2.3949e-01,  2.7386e-01,  3.6236e-01,
         1.0039e-01,  2.2346e-01,  2.9777e-01,  2.9008e-01,  2.1322e-01,
         2.8114e-01,  3.8899e-01,  2.3060e-01,  3.5258e-01, -1.1265e-01,
         8.0852e-02, -1.0813e-01,  4.2350e-02,  1.2982e-01,  1.8951e-01,
         2.5749e-01,  3.1774e-01,  1.9037e-01,  1.7505e+00,  2.6696e+00,
         3.5915e+00,  7.2048e-01,  9.6474e-01,  9.1325e-01,  9.8536e-01,
         9.8063e-01,  9.4226e-01,  8.7987e-01,  9.3762e-01,  8.1931e-01,
         8.3348e-01,  7.3585e-01,  7.8382e-01,  8.3714e-01,  6.1308e-01,
         6.1255e-01,  8.8867e-01,  5.1271e-01,  9.0940e-01,  3.0136e-01,
         8.7847e-01,  6.9842e-01,  9.2398e-01,  1.0106e+00,  8.5429e-01,
         9.8872e-01,  8.9939e-01,  6.1061e-01,  9.6078e-01,  1.0249e+00,
         1.0438e+00,  5.6606e-01,  8.3609e-01,  9.3544e-01,  1.0490e+00,
         5.4327e-01,  9.5924e-01,  6.8581e-01,  9.1023e-01,  9.6673e-01,
         5.0663e-01,  8.0997e-01,  8.0403e-01,  8.2508e-01,  8.7035e-01,
         7.2813e-01,  5.3472e-01,  9.5962e-01,  8.8375e-01,  5.0975e-01,
         9.0401e-01,  6.9209e-01,  6.5458e-01,  8.0292e-01,  9.3669e-01,
         9.3639e-01,  6.5269e-01,  7.0936e-01,  8.6465e-01,  8.5616e-01,
         1.0283e+00,  9.6357e-01,  9.4551e-01,  8.8432e-01,  1.0582e+00,
         8.8761e-01,  7.8037e-01,  3.8118e-01,  5.3076e-01,  8.2679e-01,
         9.4874e-01,  1.0014e+00,  8.5949e-01,  6.7451e-01,  5.9619e-01,
         7.4438e-01,  4.4715e-01,  4.0283e-01,  7.7615e-01,  5.0775e-01,
         6.8029e-01,  7.6795e-01,  6.1583e-01,  4.0725e-01,  6.2069e-01,
         3.2591e-01,  8.1367e-01,  6.8020e-01,  8.9800e-01,  7.6839e-01,
         7.3568e-01,  7.3533e-01,  3.2643e-01,  6.4013e-01,  7.3937e-01,
         5.7561e-01,  6.1519e-01,  7.3265e-01,  6.0725e-01,  5.4739e-01,
         6.1176e-01,  6.8010e-01,  7.1012e-01,  4.7245e-01,  5.9519e-01,
         7.2467e-01,  6.1372e-01,  6.8856e-01,  6.1034e-01,  7.8944e-01,
         5.4092e-01,  4.9680e-01,  6.1449e-01,  6.1990e-01,  7.2764e-01,
         6.4833e-01,  6.1239e-01,  2.0467e-01,  7.1974e-01,  3.0832e-01,
         7.1296e-01,  6.1493e-01,  6.7779e-01,  7.3456e-01,  8.1352e-01,
         7.2511e-01,  6.4731e-01,  7.4748e-01,  2.7826e-01,  6.3983e-01,
         4.0372e-01,  6.6786e-01,  5.6512e-01,  4.6005e-01,  1.6917e-01,
         5.1844e-01,  5.4466e-01,  5.9229e-01,  4.6397e-01,  1.8731e-01,
         1.1072e-01,  1.6880e-01,  1.1592e-03,  4.5063e-01,  4.2221e-01,
         4.5730e-01,  3.8676e-01,  2.7219e-01,  3.2949e-01,  2.4322e-01,
        -8.9044e-02,  3.2422e-01,  3.3341e-01,  4.5703e-01,  5.5887e-02,
         1.0437e-01,  1.6712e-01,  4.0776e-01,  4.0432e-01,  1.6044e-01,
        -2.7122e-02,  2.3351e-01,  4.2703e-01,  2.7840e-01,  1.4201e-01,
         1.9772e-01,  1.5834e-01,  1.8187e-01,  1.8527e-01, -1.2582e-02,
         3.7073e-01,  3.8148e-01,  2.9816e-01,  1.5480e-01,  3.1030e-01,
         2.8090e-01,  2.7040e-01,  1.8888e-01,  3.0527e-01,  2.4127e-01,
         1.6812e-01,  1.1682e-02,  5.7314e-03,  2.4274e-01,  1.7524e-01,
         1.9284e-01, -3.1859e-01,  2.5928e-01,  5.0013e-01,  3.1115e-01,
         3.9327e-01,  3.1006e-01,  2.8701e-01,  4.3696e-01,  2.7356e-01,
         3.4705e-01,  1.4290e-01,  2.8321e-01,  3.5184e-01,  4.0559e-01,
        -4.0972e-03,  8.0047e-02, -8.7626e-02, -1.2062e-01,  3.0648e-01,
        -1.5332e-01,  3.2320e-01, -1.2796e-01,  2.6629e-01,  2.7974e-01,
         2.4367e-01,  2.3693e-01,  8.7188e-02,  7.8147e-02,  2.3596e-01,
         1.8397e-01,  3.2753e-01, -7.7559e-02,  5.9199e-02,  1.7144e-01,
         1.3505e-01, -1.1567e-01,  2.7916e-01,  2.0734e-01,  3.0163e-01,
         4.9925e-02,  3.9446e-02,  1.3098e-01,  5.1960e-02, -3.7233e-01,
         6.8835e-02, -4.5049e-02,  7.6851e-02, -7.3210e-02,  8.3476e-02,
         1.6781e-02, -2.5456e-01, -4.5896e-01, -1.8411e-01, -4.4726e-01,
        -2.5391e-01, -3.4209e-01, -1.8922e-01,  5.6852e-02, -9.4474e-02,
         2.8824e-02, -3.3236e-01, -5.0680e-01, -1.6722e-01,  9.5303e-02,
        -3.4397e-01, -4.7056e-01, -4.2609e-01, -2.9141e-01, -2.1081e-01,
        -1.5695e-01, -3.4984e-01, -6.4604e-01, -1.8055e-01, -2.3274e-01,
        -2.7772e-01, -1.9526e-01, -2.4147e-01, -2.5799e-01, -3.8205e-01,
        -3.5816e-01, -5.1601e-01, -8.5447e-01, -5.4654e-01, -6.7561e-01,
        -5.1556e-01, -4.2856e-01, -3.8630e-01, -9.7821e-01, -8.8219e-01,
        -5.4898e-01, -5.9962e-01, -9.9131e-01, -7.9637e-01, -8.2554e-01,
        -7.0754e-01, -6.2708e-01, -1.0667e+00, -1.0666e+00, -9.7546e-01,
        -9.0264e-01, -7.7545e-01, -8.5821e-01, -9.1183e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1770e+00, -1.0120e+00, -1.4318e+00, -2.4602e+00, -2.6442e+00,
        -2.9469e+00, -2.7525e+00, -2.6997e+00, -2.8325e+00, -2.7427e+00,
        -2.5624e+00, -2.6527e+00, -2.4579e+00, -2.5826e+00, -2.5726e+00,
        -2.5953e+00, -2.3488e+00, -2.5274e+00, -2.5757e+00, -2.4164e+00,
        -2.3306e+00, -2.1389e+00, -8.4210e-01, -1.8342e-01, -1.5886e+00,
        -1.8039e+00, -1.6742e+00, -1.6781e+00, -1.5925e+00, -1.5859e+00,
        -2.1465e+00, -1.9391e+00, -1.5622e+00, -1.8669e+00, -1.5438e+00,
        -1.5991e+00, -1.4490e+00, -1.7950e+00, -1.5224e+00, -1.5780e+00,
        -1.5234e+00, -1.4406e+00, -1.4872e+00, -1.5784e+00, -1.7332e+00,
        -1.4429e+00, -1.5129e+00, -1.5243e+00, -1.5937e+00, -1.7603e+00,
        -1.4398e+00, -1.5334e+00, -1.5234e+00, -1.8840e+00, -1.2513e+00,
        -1.3174e+00, -1.1987e+00, -1.4774e+00, -1.4971e+00, -1.3448e+00,
        -1.2029e+00, -1.5434e+00, -1.1149e+00, -1.2585e+00, -1.5563e+00,
        -1.1731e+00,  1.1542e+00,  1.2822e+00,  2.1475e+00,  2.5111e+00,
         4.8529e+00,  1.0275e-02,  2.8423e-01,  1.7715e-01, -6.3047e-02,
         1.7199e-01,  2.1928e-01,  2.7960e-01,  3.2212e-01,  3.0980e-01,
         2.7422e-01,  4.1945e-01,  3.4309e-01,  3.0324e-01,  4.7993e-01,
         1.3044e-01, -3.1549e-02,  2.1322e-01,  5.1971e-01,  2.8496e-01,
         1.6804e-01,  1.3500e-01,  2.3949e-01,  2.7386e-01,  3.6236e-01,
         1.0039e-01,  2.2346e-01,  2.9777e-01,  2.9008e-01,  2.1322e-01,
         2.8114e-01,  3.8899e-01,  2.3060e-01,  3.5258e-01, -1.1265e-01,
         8.0852e-02, -1.0813e-01,  4.2350e-02,  1.2982e-01,  1.8951e-01,
         2.5749e-01,  3.1774e-01,  1.9037e-01,  1.7505e+00,  2.6696e+00,
         3.5915e+00,  7.2048e-01,  9.6474e-01,  9.1325e-01,  9.8536e-01,
         9.8063e-01,  9.4226e-01,  8.7987e-01,  9.3762e-01,  8.1931e-01,
         8.3348e-01,  7.3585e-01,  7.8382e-01,  8.3714e-01,  6.1308e-01,
         6.1255e-01,  8.8867e-01,  5.1271e-01,  9.0940e-01,  3.0136e-01,
         8.7847e-01,  6.9842e-01,  9.2398e-01,  1.0106e+00,  8.5429e-01,
         9.8872e-01,  8.9939e-01,  6.1061e-01,  9.6078e-01,  1.0249e+00,
         1.0438e+00,  5.6606e-01,  8.3609e-01,  9.3544e-01,  1.0490e+00,
         5.4327e-01,  9.5924e-01,  6.8581e-01,  9.1023e-01,  9.6673e-01,
         5.0663e-01,  8.0997e-01,  8.0403e-01,  8.2508e-01,  8.7035e-01,
         7.2813e-01,  5.3472e-01,  9.5962e-01,  8.8375e-01,  5.0975e-01,
         9.0401e-01,  6.9209e-01,  6.5458e-01,  8.0292e-01,  9.3669e-01,
         9.3639e-01,  6.5269e-01,  7.0936e-01,  8.6465e-01,  8.5616e-01,
         1.0283e+00,  9.6357e-01,  9.4551e-01,  8.8432e-01,  1.0582e+00,
         8.8761e-01,  7.8037e-01,  3.8118e-01,  5.3076e-01,  8.2679e-01,
         9.4874e-01,  1.0014e+00,  8.5949e-01,  6.7451e-01,  5.9619e-01,
         7.4438e-01,  4.4715e-01,  4.0283e-01,  7.7615e-01,  5.0775e-01,
         6.8029e-01,  7.6795e-01,  6.1583e-01,  4.0725e-01,  6.2069e-01,
         3.2591e-01,  8.1367e-01,  6.8020e-01,  8.9800e-01,  7.6839e-01,
         7.3568e-01,  7.3533e-01,  3.2643e-01,  6.4013e-01,  7.3937e-01,
         5.7561e-01,  6.1519e-01,  7.3265e-01,  6.0725e-01,  5.4739e-01,
         6.1176e-01,  6.8010e-01,  7.1012e-01,  4.7245e-01,  5.9519e-01,
         7.2467e-01,  6.1372e-01,  6.8856e-01,  6.1034e-01,  7.8944e-01,
         5.4092e-01,  4.9680e-01,  6.1449e-01,  6.1990e-01,  7.2764e-01,
         6.4833e-01,  6.1239e-01,  2.0467e-01,  7.1974e-01,  3.0832e-01,
         7.1296e-01,  6.1493e-01,  6.7779e-01,  7.3456e-01,  8.1352e-01,
         7.2511e-01,  6.4731e-01,  7.4748e-01,  2.7826e-01,  6.3983e-01,
         4.0372e-01,  6.6786e-01,  5.6512e-01,  4.6005e-01,  1.6917e-01,
         5.1844e-01,  5.4466e-01,  5.9229e-01,  4.6397e-01,  1.8731e-01,
         1.1072e-01,  1.7828e-01,  1.1592e-03,  4.5063e-01,  4.2221e-01,
         4.5730e-01,  3.8676e-01,  2.7219e-01,  3.2949e-01,  2.5543e-01,
        -8.9044e-02,  3.2422e-01,  3.3341e-01,  4.5703e-01,  5.5887e-02,
         1.0437e-01,  1.6712e-01,  4.0776e-01,  4.0432e-01,  1.6044e-01,
        -2.7122e-02,  2.3351e-01,  4.2703e-01,  2.7840e-01,  1.4201e-01,
         1.9772e-01,  1.5834e-01,  1.8187e-01,  1.8527e-01, -1.2582e-02,
         3.7073e-01,  3.8148e-01,  2.9816e-01,  1.5480e-01,  3.1030e-01,
         2.8090e-01,  2.7040e-01,  1.8888e-01,  3.0527e-01,  2.4127e-01,
         1.6812e-01,  1.1682e-02,  5.7314e-03,  2.4274e-01,  1.7524e-01,
         1.9284e-01, -3.1859e-01,  2.5928e-01,  5.0013e-01,  3.1115e-01,
         3.9327e-01,  3.1006e-01,  2.8701e-01,  4.3696e-01,  2.7356e-01,
         3.4705e-01,  1.4290e-01,  2.8321e-01,  3.5184e-01,  4.0559e-01,
        -4.0972e-03,  8.0047e-02, -8.7626e-02, -1.2062e-01,  3.0648e-01,
        -1.5332e-01,  3.2320e-01, -1.2796e-01,  2.6629e-01,  2.7974e-01,
         2.4367e-01,  2.3693e-01,  8.7188e-02,  7.8147e-02,  2.3596e-01,
         1.8397e-01,  3.2753e-01, -7.7559e-02,  5.9199e-02,  1.7144e-01,
         1.3505e-01, -1.1567e-01,  2.7916e-01,  2.0734e-01,  3.0163e-01,
         4.9925e-02,  3.9446e-02,  1.3098e-01,  5.1960e-02, -3.7233e-01,
         6.8835e-02, -4.5049e-02,  7.6851e-02, -7.3210e-02,  8.3476e-02,
         1.6781e-02, -2.5456e-01, -4.5896e-01, -1.8017e-01, -4.4726e-01,
        -2.5391e-01, -3.4209e-01, -1.8922e-01,  5.6852e-02, -9.4474e-02,
         2.8824e-02, -3.3236e-01, -5.0680e-01, -1.6722e-01,  9.5303e-02,
        -3.4397e-01, -4.7056e-01, -4.2609e-01, -2.9141e-01, -2.1081e-01,
        -1.5695e-01, -3.4984e-01, -6.4604e-01, -1.8055e-01, -2.3274e-01,
        -2.7772e-01, -1.9526e-01, -2.4147e-01, -2.5799e-01, -3.8205e-01,
        -3.5816e-01, -5.1820e-01, -8.5447e-01, -5.4654e-01, -6.7561e-01,
        -5.1556e-01, -4.2856e-01, -3.8630e-01, -9.7821e-01, -8.8219e-01,
        -5.4898e-01, -5.9962e-01, -9.9131e-01, -7.7646e-01, -8.2554e-01,
        -7.0754e-01, -6.2708e-01, -1.0667e+00, -1.0539e+00, -9.7546e-01,
        -9.0264e-01, -7.7545e-01, -8.5821e-01, -9.1183e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808159
t6: 1641198808159
state_values: tensor([-0.1482, -0.1710, -0.1485, -0.1172, -0.0951, -0.0799, -0.0810, -0.0809,
        -0.0794, -0.0815, -0.0742, -0.0817, -0.0869, -0.0866, -0.0860, -0.0559,
        -0.0752, -0.0788, -0.0760, -0.0768, -0.0745, -0.0781, -0.1142, -0.1320,
        -0.1047, -0.0999, -0.1002, -0.0880, -0.0970, -0.0989, -0.0583, -0.0752,
        -0.0744, -0.0824, -0.0894, -0.0914, -0.0934, -0.0897, -0.0901, -0.0920,
        -0.0910, -0.0933, -0.0927, -0.0753, -0.0848, -0.0903, -0.0910, -0.0823,
        -0.0890, -0.0878, -0.0911, -0.0847, -0.0894, -0.0600, -0.0802, -0.0833,
        -0.0828, -0.0568, -0.0733, -0.0640, -0.0784, -0.0780, -0.0849, -0.0817,
        -0.0804, -0.0840, -0.1258, -0.1351, -0.1474, -0.1479, -0.1729, -0.1183,
        -0.1072, -0.1020, -0.0959, -0.0832, -0.0880, -0.0882, -0.0893, -0.0876,
        -0.0889, -0.0907, -0.0884, -0.0888, -0.0876, -0.0668, -0.0774, -0.0821,
        -0.0816, -0.0766, -0.0808, -0.0713, -0.0781, -0.0810, -0.0833, -0.0808,
        -0.0711, -0.0772, -0.0790, -0.0726, -0.0763, -0.0809, -0.0799, -0.0812,
        -0.0762, -0.0776, -0.0750, -0.0755, -0.0762, -0.0769, -0.0741, -0.0707,
        -0.0710, -0.1016, -0.1205, -0.1381, -0.0982, -0.0862, -0.0862, -0.0838,
        -0.0835, -0.0831, -0.0815, -0.0818, -0.0701, -0.0761, -0.0623, -0.0708,
        -0.0736, -0.0719, -0.0482, -0.0631, -0.0651, -0.0666, -0.0281, -0.0548,
        -0.0449, -0.0584, -0.0607, -0.0580, -0.0650, -0.0665, -0.0639, -0.0660,
        -0.0663, -0.0670, -0.0634, -0.0658, -0.0585, -0.0665, -0.0359, -0.0568,
        -0.0585, -0.0626, -0.0646, -0.0327, -0.0482, -0.0494, -0.0569, -0.0602,
        -0.0595, -0.0572, -0.0629, -0.0611, -0.0587, -0.0613, -0.0597, -0.0508,
        -0.0581, -0.0618, -0.0620, -0.0593, -0.0594, -0.0587, -0.0610, -0.0642,
        -0.0641, -0.0617, -0.0616, -0.0644, -0.0640, -0.0576, -0.0553, -0.0565,
        -0.0601, -0.0613, -0.0640, -0.0551, -0.0416, -0.0392, -0.0508, -0.0506,
        -0.0511, -0.0479, -0.0504, -0.0371, -0.0438, -0.0402, -0.0448, -0.0443,
        -0.0442, -0.0512, -0.0506, -0.0527, -0.0482, -0.0436, -0.0470, -0.0452,
        -0.0495, -0.0464, -0.0337, -0.0451, -0.0482, -0.0411, -0.0367, -0.0450,
        -0.0480, -0.0496, -0.0476, -0.0410, -0.0489, -0.0375, -0.0430, -0.0452,
        -0.0501, -0.0484, -0.0478, -0.0366, -0.0455, -0.0464, -0.0459, -0.0443,
        -0.0419, -0.0458, -0.0439, -0.0470, -0.0476, -0.0492, -0.0489, -0.0524,
        -0.0520, -0.0497, -0.0507, -0.0457, -0.0498, -0.0468, -0.0500, -0.0449,
        -0.0459, -0.0426, -0.0423, -0.0412, -0.0438, -0.0414, -0.0212, -0.0333,
        -0.0132, -0.0274, -0.0342, -0.0346, -0.0353, -0.0312, -0.0213, -0.0308,
        -0.0302, -0.0293, -0.0294, -0.0352, -0.0348, -0.0344, -0.0147, -0.0303,
        -0.0348, -0.0355, -0.0162, -0.0273, -0.0329, -0.0351, -0.0364, -0.0119,
        -0.0234, -0.0162, -0.0273, -0.0314, -0.0311, -0.0359, -0.0375, -0.0370,
        -0.0358, -0.0342, -0.0365, -0.0341, -0.0354, -0.0311, -0.0320, -0.0347,
        -0.0338, -0.0343, -0.0375, -0.0366, -0.0372, -0.0193, -0.0221, -0.0317,
        -0.0320, -0.0339, -0.0332, -0.0215, -0.0303, -0.0169, -0.0274, -0.0101,
        -0.0236, -0.0283, -0.0287,  0.0012, -0.0194, -0.0218, -0.0222, -0.0230,
        -0.0210, -0.0236, -0.0222, -0.0282, -0.0297, -0.0299, -0.0306, -0.0110,
        -0.0225, -0.0124, -0.0239, -0.0292, -0.0250, -0.0268, -0.0214, -0.0262,
        -0.0251, -0.0301, -0.0290, -0.0308, -0.0180, -0.0170, -0.0251, -0.0250,
        -0.0207, -0.0269, -0.0091, -0.0230, -0.0244, -0.0262, -0.0265, -0.0052,
        -0.0131, -0.0034,  0.0266, -0.0066, -0.0115, -0.0164, -0.0228, -0.0214,
        -0.0263,  0.0037, -0.0100, -0.0192, -0.0270, -0.0218,  0.0066,  0.0079,
        -0.0079, -0.0084, -0.0161, -0.0174, -0.0139, -0.0206, -0.0221, -0.0224,
        -0.0191, -0.0211, -0.0216, -0.0164, -0.0210,  0.0059, -0.0053, -0.0135,
        -0.0140, -0.0167, -0.0169, -0.0176,  0.0331, -0.0004, -0.0081, -0.0081,
        -0.0066,  0.0038,  0.0114, -0.0020, -0.0093, -0.0069,  0.0105,  0.0080,
         0.0033, -0.0084, -0.0053, -0.0083], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808164
t8: 1641198808164
t9: 1641198808165
t10: 1641198808175
t11: 1641198808176
t12: 1641198808176
t1: 1641198808176
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808187
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0092, 1.0505, 0.9546, 0.9806, 0.9046, 1.0019, 0.9707, 0.9224, 1.0107,
        1.0870, 0.9648, 1.0026, 0.9475, 1.0164, 1.1521, 0.9971, 0.9200, 1.0214,
        1.1266, 1.1264, 1.0798, 0.9516, 0.9875, 1.0736, 0.9222, 0.9996, 1.1090,
        0.9830, 1.0067, 1.3088, 1.0414, 1.1808, 0.7464, 0.9664, 0.9182, 1.0037,
        0.8655, 1.0119, 0.9536, 1.0057, 1.0093, 1.0010, 1.0368, 0.9147, 1.0023,
        0.9695, 1.0196, 0.9411, 0.9967, 1.0013, 1.0319, 0.9721, 1.0042, 1.0046,
        0.9859, 1.0125, 1.1236, 0.9660, 1.0360, 0.9514, 0.8797, 0.9983, 1.0282,
        0.9191, 1.0007, 0.9665, 0.9739, 0.9823, 1.0161, 1.0549, 0.9985, 0.9907,
        0.9390, 0.9755, 1.0196, 1.0122, 1.0067, 1.0078, 1.0142, 0.9775, 1.0001,
        1.0042, 0.9899, 1.0120, 1.0870, 0.9407, 1.0048, 1.0521, 1.0755, 0.9575,
        1.0187, 0.9855, 0.9900, 0.9793, 0.9687, 1.0193, 1.0107, 1.0007, 1.0233,
        1.0161, 0.9944, 0.9852, 0.9995, 0.9450, 0.9884, 0.9502, 0.9775, 0.9736,
        0.9792, 1.0256, 1.0612, 1.0150, 0.9885, 0.9833, 0.9755, 0.9113, 1.0137,
        0.9701, 1.0054, 1.0000, 0.9941, 0.9973, 0.9955, 1.0340, 0.9896, 1.0009,
        0.9635, 1.0028, 0.9805, 1.0254, 1.0121, 0.9142, 0.9999, 1.2320, 1.0039,
        1.1468, 0.9962, 1.0669, 1.1343, 0.9825, 0.9536, 0.8494, 1.0057, 1.0211,
        1.0094, 0.9432, 0.9964, 1.0405, 0.9974, 1.0172, 1.0000, 0.9274, 0.9714,
        0.9878, 1.0774, 1.0311, 1.0909, 0.9113, 0.9811, 0.9172, 0.9402, 0.9984,
        1.0049, 0.9391, 1.0041, 0.9693, 1.0239, 0.9645, 0.9967, 1.0013, 0.9695,
        0.9860, 1.0179, 0.9871, 0.9999, 0.9975, 1.0160, 1.0031, 1.0070, 0.9829,
        1.0119, 0.8965, 0.9966, 0.9880, 1.0033, 0.9936, 1.0318, 1.0820, 0.9982,
        0.9956, 0.9972, 0.9437, 1.0165, 0.9226, 1.0003, 1.0593, 1.0517, 0.8870,
        1.0184, 0.8143, 0.9994, 0.9741, 1.0050, 1.0258, 1.0338, 1.0166, 0.9742,
        1.0013, 1.0206, 1.0691, 0.9702, 0.9985, 1.0548, 1.0756, 0.9649, 0.9975,
        0.9769, 0.9801, 1.0171, 0.9905, 1.0075, 1.0368, 1.0007, 0.9983, 0.9629,
        0.9903, 1.0327, 0.9646, 0.9975, 1.0173, 1.0113, 0.9258, 0.9981, 0.9140,
        1.0014, 0.9732, 0.9913, 1.0069, 0.9923, 0.9963, 1.0010, 1.0078, 0.9517,
        0.9947, 0.9675, 0.9931, 1.0258, 0.9766, 0.9692, 1.0147, 1.0380, 1.0140,
        1.0130, 1.0475, 0.9381, 0.9728, 0.8384, 1.0379, 1.0549, 1.0527, 1.0812,
        1.1604, 1.0221, 1.0485, 0.7275, 1.0482, 0.8981, 1.0331, 0.8378, 1.0106,
        0.8565, 1.0099, 1.0258, 1.1095, 0.8516, 0.9844, 1.0241, 0.9408, 1.0243,
        1.0370, 1.1166, 0.9678, 0.9359, 0.8290, 0.9896, 0.9940, 0.9908, 0.9781,
        1.0173, 0.9840, 1.0102, 0.9714, 1.0184, 1.0248, 0.9917, 0.9954, 0.9782,
        0.9914, 0.9974, 0.9976, 0.9731, 1.0259, 0.9975, 0.9864, 0.9911, 0.9825,
        1.0402, 1.0057, 1.0154, 1.0057, 1.0175, 1.0072, 1.0018, 1.0475, 1.2250,
        0.8899, 0.8456, 0.7945, 1.0337, 0.7638, 0.9986, 0.8448, 1.0001, 0.9839,
        0.9869, 0.9793, 1.0514, 0.9871, 1.0051, 0.9593, 0.9927, 0.9636, 0.9917,
        1.0246, 0.9638, 0.9874, 0.9960, 1.0012, 1.0066, 1.0469, 1.0264, 1.0013,
        1.0005, 0.9207, 0.9994, 1.0491, 0.9928, 1.0023, 1.0028, 0.9989, 1.0234,
        0.8741, 0.9785, 1.2455, 0.9576, 0.9793, 0.9552, 0.9997, 0.9988, 0.9956,
        1.0465, 0.9002, 0.9788, 0.9928, 0.9634, 1.0298, 1.0835, 1.0036, 1.0395,
        0.9967, 0.9300, 0.8629, 1.0003, 0.9813, 0.9890, 1.0227, 1.0115, 1.0056,
        1.0187, 0.9938, 1.0142, 0.8660, 0.9625, 0.9365, 0.9911, 1.0095, 1.0202,
        1.0819, 0.9845, 1.0348, 1.0690, 0.8411, 1.0341, 1.1819, 1.0349, 1.0061,
        0.8582, 1.1189, 1.1362, 1.0757, 0.9633, 1.0712, 1.0237, 1.2434],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808192
t4: 1641198808192
surr1, surr2: tensor([-3.1906e+00, -1.0304e+00, -1.4108e+00, -2.4273e+00, -2.4842e+00,
        -2.9500e+00, -2.7014e+00, -2.5517e+00, -2.8548e+00, -2.8952e+00,
        -2.5271e+00, -2.6640e+00, -2.3585e+00, -2.6281e+00, -2.8253e+00,
        -2.5972e+00, -2.2043e+00, -2.6444e+00, -2.7167e+00, -2.6060e+00,
        -2.3943e+00, -2.0794e+00, -8.3391e-01, -1.8959e-01, -1.5080e+00,
        -1.8035e+00, -1.8428e+00, -1.6612e+00, -1.6106e+00, -2.0269e+00,
        -2.1427e+00, -2.0816e+00, -1.2955e+00, -1.8388e+00, -1.4877e+00,
        -1.6008e+00, -1.2879e+00, -1.8311e+00, -1.4643e+00, -1.5964e+00,
        -1.5354e+00, -1.4425e+00, -1.5592e+00, -1.4804e+00, -1.7488e+00,
        -1.3998e+00, -1.5649e+00, -1.4628e+00, -1.5304e+00, -1.7658e+00,
        -1.4809e+00, -1.5003e+00, -1.5467e+00, -1.8920e+00, -1.2284e+00,
        -1.3441e+00, -1.2984e+00, -1.4416e+00, -1.6251e+00, -1.3333e+00,
        -1.0947e+00, -1.5408e+00, -1.1423e+00, -1.1543e+00, -1.5648e+00,
        -1.1394e+00,  1.1153e+00,  1.2550e+00,  2.1782e+00,  2.5786e+00,
         4.6453e+00,  1.0377e-02,  2.7017e-01,  1.6941e-01, -6.5102e-02,
         1.7326e-01,  2.2294e-01,  2.8209e-01,  3.2838e-01,  3.0209e-01,
         2.7432e-01,  4.2144e-01,  3.3908e-01,  3.0897e-01,  5.0988e-01,
         1.2269e-01, -3.1131e-02,  2.2566e-01,  5.4442e-01,  2.7292e-01,
         1.7794e-01,  1.3383e-01,  2.3625e-01,  2.6697e-01,  3.4737e-01,
         1.0381e-01,  2.2497e-01,  2.9853e-01,  3.0040e-01,  2.1602e-01,
         2.7852e-01,  3.7964e-01,  2.3043e-01,  3.3139e-01, -1.1012e-01,
         7.6602e-02, -1.0542e-01,  4.1284e-02,  1.2725e-01,  1.9404e-01,
         2.6869e-01,  3.1941e-01,  1.8709e-01,  1.7034e+00,  2.5958e+00,
         3.3191e+00,  7.3813e-01,  9.4879e-01,  9.2272e-01,  9.8522e-01,
         9.7292e-01,  9.3885e-01,  8.7536e-01,  9.7309e-01,  8.1621e-01,
         8.5876e-01,  7.1739e-01,  7.6949e-01,  8.0737e-01,  6.4486e-01,
         6.1580e-01,  7.7983e-01,  5.2128e-01,  1.0908e+00,  3.0130e-01,
         9.3712e-01,  6.9761e-01,  9.4772e-01,  1.0486e+00,  8.4646e-01,
         9.6112e-01,  8.0122e-01,  6.1495e-01,  9.7925e-01,  1.0345e+00,
         9.7430e-01,  5.5820e-01,  8.6920e-01,  9.3430e-01,  1.1089e+00,
         5.4322e-01,  8.9616e-01,  6.6161e-01,  8.9848e-01,  1.0529e+00,
         5.0770e-01,  8.6017e-01,  7.7686e-01,  8.1217e-01,  8.1838e-01,
         6.8005e-01,  5.3364e-01,  9.6393e-01,  8.2980e-01,  5.1338e-01,
         8.8159e-01,  7.1216e-01,  6.3980e-01,  7.9798e-01,  9.3801e-01,
         9.0831e-01,  6.4140e-01,  7.2220e-01,  8.5601e-01,  8.5610e-01,
         1.0259e+00,  9.7925e-01,  9.4772e-01,  8.9162e-01,  1.0421e+00,
         9.0526e-01,  7.2288e-01,  3.7268e-01,  5.2330e-01,  8.2944e-01,
         9.4326e-01,  1.0336e+00,  8.9528e-01,  6.8198e-01,  5.9013e-01,
         7.0428e-01,  4.0887e-01,  4.1366e-01,  7.4444e-01,  5.2588e-01,
         7.0314e-01,  8.1653e-01,  5.7540e-01,  4.2462e-01,  5.4564e-01,
         3.2564e-01,  7.9139e-01,  6.8615e-01,  9.1963e-01,  7.9297e-01,
         7.4503e-01,  6.9479e-01,  3.2288e-01,  6.5546e-01,  7.7863e-01,
         5.5771e-01,  6.1789e-01,  7.7438e-01,  6.4679e-01,  5.3068e-01,
         6.0715e-01,  6.6220e-01,  6.8927e-01,  4.8514e-01,  5.9170e-01,
         7.4773e-01,  6.3022e-01,  6.8980e-01,  6.0891e-01,  7.5353e-01,
         5.3010e-01,  5.1739e-01,  6.0015e-01,  6.2673e-01,  7.3998e-01,
         6.5936e-01,  5.5778e-01,  2.0681e-01,  6.7001e-01,  3.1031e-01,
         6.9686e-01,  6.0679e-01,  6.8310e-01,  7.2965e-01,  8.1006e-01,
         7.2596e-01,  6.5240e-01,  7.1539e-01,  2.7559e-01,  6.1923e-01,
         4.0063e-01,  6.8378e-01,  5.5512e-01,  4.4133e-01,  1.7246e-01,
         5.3227e-01,  5.5081e-01,  6.0086e-01,  4.8702e-01,  1.7670e-01,
         1.1616e-01,  1.6607e-01,  1.1854e-03,  4.6505e-01,  4.3926e-01,
         4.8676e-01,  4.2513e-01,  2.7433e-01,  3.4244e-01,  2.0648e-01,
        -9.4166e-02,  3.0517e-01,  3.5030e-01,  3.9912e-01,  5.9141e-02,
         9.9046e-02,  1.6935e-01,  4.1752e-01,  4.5317e-01,  1.5081e-01,
        -2.6440e-02,  2.3911e-01,  4.0267e-01,  2.9445e-01,  1.4247e-01,
         2.1591e-01,  1.5642e-01,  1.7254e-01,  1.6079e-01, -1.2421e-02,
         3.6855e-01,  3.7769e-01,  2.9057e-01,  1.5786e-01,  3.0650e-01,
         2.8507e-01,  2.6400e-01,  1.9442e-01,  3.0828e-01,  2.3790e-01,
         1.6490e-01,  1.1351e-02,  5.6743e-03,  2.4212e-01,  1.7482e-01,
         1.8775e-01, -3.2890e-01,  2.5885e-01,  4.9027e-01,  3.0722e-01,
         3.8582e-01,  3.2371e-01,  2.8751e-01,  4.5221e-01,  2.7465e-01,
         3.6950e-01,  1.4309e-01,  2.8384e-01,  3.6607e-01,  4.7396e-01,
        -4.0385e-03,  7.0886e-02, -7.2611e-02, -1.2566e-01,  2.5165e-01,
        -1.5595e-01,  2.8759e-01, -1.2758e-01,  2.6187e-01,  2.7533e-01,
         2.3808e-01,  2.5001e-01,  8.6721e-02,  8.1028e-02,  2.2941e-01,
         1.8058e-01,  3.1109e-01, -7.6099e-02,  6.0873e-02,  1.6748e-01,
         1.3025e-01, -1.1497e-01,  2.7949e-01,  2.0867e-01,  3.1430e-01,
         5.0411e-02,  3.9502e-02,  1.3164e-01,  4.6973e-02, -3.6955e-01,
         7.2301e-02, -4.4814e-02,  7.5764e-02, -7.3714e-02,  8.3382e-02,
         1.7438e-02, -2.3058e-01, -4.7098e-01, -2.0400e-01, -4.4053e-01,
        -2.4398e-01, -3.3195e-01, -1.8913e-01,  5.6783e-02, -9.3974e-02,
         3.0522e-02, -3.1895e-01, -4.8947e-01, -1.6581e-01,  9.1413e-02,
        -3.6260e-01, -4.8126e-01, -4.2784e-01, -3.0900e-01, -2.0999e-01,
        -1.4833e-01, -3.0159e-01, -6.4641e-01, -1.7742e-01, -2.2954e-01,
        -2.8489e-01, -1.9662e-01, -2.4351e-01, -2.6327e-01, -3.7988e-01,
        -3.7543e-01, -4.9860e-01, -8.1561e-01, -5.0530e-01, -6.6550e-01,
        -5.2098e-01, -4.3609e-01, -4.1638e-01, -9.7520e-01, -9.1402e-01,
        -5.6917e-01, -5.2497e-01, -1.0628e+00, -8.3424e-01, -8.4070e-01,
        -7.0991e-01, -5.7631e-01, -1.1762e+00, -1.0887e+00, -1.0047e+00,
        -8.8460e-01, -8.0754e-01, -8.6495e-01, -1.0666e+00], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1906e+00, -1.0304e+00, -1.4108e+00, -2.4273e+00, -2.4842e+00,
        -2.9500e+00, -2.7014e+00, -2.5517e+00, -2.8548e+00, -2.8952e+00,
        -2.5271e+00, -2.6640e+00, -2.3585e+00, -2.6281e+00, -2.6975e+00,
        -2.5972e+00, -2.2043e+00, -2.6444e+00, -2.6525e+00, -2.5448e+00,
        -2.3943e+00, -2.0794e+00, -8.3391e-01, -1.8959e-01, -1.5080e+00,
        -1.8035e+00, -1.8277e+00, -1.6612e+00, -1.6106e+00, -1.7035e+00,
        -2.1427e+00, -1.9391e+00, -1.5622e+00, -1.8388e+00, -1.4877e+00,
        -1.6008e+00, -1.3393e+00, -1.8311e+00, -1.4643e+00, -1.5964e+00,
        -1.5354e+00, -1.4425e+00, -1.5592e+00, -1.4804e+00, -1.7488e+00,
        -1.3998e+00, -1.5649e+00, -1.4628e+00, -1.5304e+00, -1.7658e+00,
        -1.4809e+00, -1.5003e+00, -1.5467e+00, -1.8920e+00, -1.2284e+00,
        -1.3441e+00, -1.2712e+00, -1.4416e+00, -1.6251e+00, -1.3333e+00,
        -1.1200e+00, -1.5408e+00, -1.1423e+00, -1.1543e+00, -1.5648e+00,
        -1.1394e+00,  1.1153e+00,  1.2550e+00,  2.1782e+00,  2.5786e+00,
         4.6453e+00,  1.0377e-02,  2.7017e-01,  1.6941e-01, -6.5102e-02,
         1.7326e-01,  2.2294e-01,  2.8209e-01,  3.2838e-01,  3.0209e-01,
         2.7432e-01,  4.2144e-01,  3.3908e-01,  3.0897e-01,  5.0988e-01,
         1.2269e-01, -3.1131e-02,  2.2566e-01,  5.4442e-01,  2.7292e-01,
         1.7794e-01,  1.3383e-01,  2.3625e-01,  2.6697e-01,  3.4737e-01,
         1.0381e-01,  2.2497e-01,  2.9853e-01,  3.0040e-01,  2.1602e-01,
         2.7852e-01,  3.7964e-01,  2.3043e-01,  3.3139e-01, -1.1012e-01,
         7.6602e-02, -1.0542e-01,  4.1284e-02,  1.2725e-01,  1.9404e-01,
         2.6869e-01,  3.1941e-01,  1.8709e-01,  1.7034e+00,  2.5958e+00,
         3.3191e+00,  7.3813e-01,  9.4879e-01,  9.2272e-01,  9.8522e-01,
         9.7292e-01,  9.3885e-01,  8.7536e-01,  9.7309e-01,  8.1621e-01,
         8.5876e-01,  7.1739e-01,  7.6949e-01,  8.0737e-01,  6.4486e-01,
         6.1580e-01,  7.7983e-01,  5.2128e-01,  9.7396e-01,  3.0130e-01,
         8.9884e-01,  6.9761e-01,  9.4772e-01,  1.0169e+00,  8.4646e-01,
         9.6112e-01,  8.4896e-01,  6.1495e-01,  9.7925e-01,  1.0345e+00,
         9.7430e-01,  5.5820e-01,  8.6920e-01,  9.3430e-01,  1.1089e+00,
         5.4322e-01,  8.9616e-01,  6.6161e-01,  8.9848e-01,  1.0529e+00,
         5.0770e-01,  8.6017e-01,  7.7686e-01,  8.1217e-01,  8.1838e-01,
         6.8005e-01,  5.3364e-01,  9.6393e-01,  8.2980e-01,  5.1338e-01,
         8.8159e-01,  7.1216e-01,  6.3980e-01,  7.9798e-01,  9.3801e-01,
         9.0831e-01,  6.4140e-01,  7.2220e-01,  8.5601e-01,  8.5610e-01,
         1.0259e+00,  9.7925e-01,  9.4772e-01,  8.9162e-01,  1.0421e+00,
         9.0526e-01,  7.2568e-01,  3.7268e-01,  5.2330e-01,  8.2944e-01,
         9.4326e-01,  1.0336e+00,  8.9528e-01,  6.8198e-01,  5.9013e-01,
         7.0428e-01,  4.0887e-01,  4.1366e-01,  7.4444e-01,  5.2588e-01,
         7.0314e-01,  8.1653e-01,  5.8383e-01,  4.2462e-01,  6.0308e-01,
         3.2564e-01,  7.9139e-01,  6.8615e-01,  9.1963e-01,  7.9297e-01,
         7.4503e-01,  6.9479e-01,  3.2288e-01,  6.5546e-01,  7.7863e-01,
         5.5771e-01,  6.1789e-01,  7.7438e-01,  6.4679e-01,  5.3068e-01,
         6.0715e-01,  6.6220e-01,  6.8927e-01,  4.8514e-01,  5.9170e-01,
         7.4773e-01,  6.3022e-01,  6.8980e-01,  6.0891e-01,  7.5353e-01,
         5.3010e-01,  5.1739e-01,  6.0015e-01,  6.2673e-01,  7.3998e-01,
         6.5936e-01,  5.5778e-01,  2.0681e-01,  6.7001e-01,  3.1031e-01,
         6.9686e-01,  6.0679e-01,  6.8310e-01,  7.2965e-01,  8.1006e-01,
         7.2596e-01,  6.5240e-01,  7.1539e-01,  2.7559e-01,  6.1923e-01,
         4.0063e-01,  6.8378e-01,  5.5512e-01,  4.4133e-01,  1.7246e-01,
         5.3227e-01,  5.5081e-01,  6.0086e-01,  4.8702e-01,  1.7670e-01,
         1.1616e-01,  1.7828e-01,  1.1854e-03,  4.6505e-01,  4.3926e-01,
         4.8676e-01,  4.0300e-01,  2.7433e-01,  3.4244e-01,  2.5543e-01,
        -9.4166e-02,  3.0581e-01,  3.5030e-01,  4.2877e-01,  5.9141e-02,
         1.0408e-01,  1.6935e-01,  4.1752e-01,  4.4929e-01,  1.5938e-01,
        -2.6440e-02,  2.3911e-01,  4.0267e-01,  2.9445e-01,  1.4247e-01,
         2.1270e-01,  1.5642e-01,  1.7254e-01,  1.7455e-01, -1.2421e-02,
         3.6855e-01,  3.7769e-01,  2.9057e-01,  1.5786e-01,  3.0650e-01,
         2.8507e-01,  2.6400e-01,  1.9442e-01,  3.0828e-01,  2.3790e-01,
         1.6490e-01,  1.1351e-02,  5.6743e-03,  2.4212e-01,  1.7482e-01,
         1.8775e-01, -3.2890e-01,  2.5885e-01,  4.9027e-01,  3.0722e-01,
         3.8582e-01,  3.2371e-01,  2.8751e-01,  4.5221e-01,  2.7465e-01,
         3.6950e-01,  1.4309e-01,  2.8384e-01,  3.6607e-01,  4.2562e-01,
        -4.0843e-03,  7.5445e-02, -8.2255e-02, -1.2566e-01,  2.9653e-01,
        -1.5595e-01,  3.0639e-01, -1.2758e-01,  2.6187e-01,  2.7533e-01,
         2.3808e-01,  2.5001e-01,  8.6721e-02,  8.1028e-02,  2.2941e-01,
         1.8058e-01,  3.1109e-01, -7.6099e-02,  6.0873e-02,  1.6748e-01,
         1.3025e-01, -1.1497e-01,  2.7949e-01,  2.0867e-01,  3.1430e-01,
         5.0411e-02,  3.9502e-02,  1.3164e-01,  4.6973e-02, -3.6955e-01,
         7.2301e-02, -4.4814e-02,  7.5764e-02, -7.3714e-02,  8.3382e-02,
         1.7438e-02, -2.3742e-01, -4.7098e-01, -1.8017e-01, -4.4053e-01,
        -2.4398e-01, -3.3195e-01, -1.8913e-01,  5.6783e-02, -9.3974e-02,
         3.0522e-02, -3.1895e-01, -4.8947e-01, -1.6581e-01,  9.1413e-02,
        -3.6260e-01, -4.8126e-01, -4.2784e-01, -3.0900e-01, -2.0999e-01,
        -1.4833e-01, -3.1457e-01, -6.4641e-01, -1.7742e-01, -2.2954e-01,
        -2.8489e-01, -1.9662e-01, -2.4351e-01, -2.6327e-01, -3.7988e-01,
        -3.7543e-01, -5.1820e-01, -8.1561e-01, -5.0530e-01, -6.6550e-01,
        -5.2098e-01, -4.3609e-01, -4.1638e-01, -9.7520e-01, -9.1402e-01,
        -5.6917e-01, -5.6176e-01, -1.0628e+00, -7.7646e-01, -8.4070e-01,
        -7.0991e-01, -6.0439e-01, -1.1563e+00, -1.0539e+00, -1.0047e+00,
        -8.8460e-01, -8.0754e-01, -8.6495e-01, -9.4354e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808201
t6: 1641198808201
state_values: tensor([ 0.1170, -0.1069, -0.0819, -0.0433,  0.0170,  0.0789,  0.0908,  0.0979,
         0.1138,  0.1249,  0.2365,  0.1573,  0.1346,  0.1406,  0.1879,  0.5644,
         0.3521,  0.2896,  0.3580,  0.3751,  0.4349,  0.4029,  0.0911,  0.0319,
         0.1557,  0.1789,  0.1827,  0.3384,  0.2357,  0.2187,  0.8632,  0.5826,
         0.6206,  0.4319,  0.3502,  0.3294,  0.3166,  0.3426,  0.3822,  0.3392,
         0.3729,  0.3420,  0.3460,  0.6718,  0.4593,  0.3934,  0.3772,  0.5568,
         0.4189,  0.4196,  0.3967,  0.5289,  0.4248,  0.9874,  0.5960,  0.5225,
         0.5862,  1.2197,  0.7480,  1.0377,  0.6806,  0.6518,  0.5509,  0.6302,
         0.6160,  0.5747,  0.1274,  0.0818,  0.0429,  0.0427, -0.0188,  0.1376,
         0.2624,  0.2995,  0.3609,  0.6312,  0.5407,  0.5525,  0.5210,  0.5700,
         0.5242,  0.5021,  0.5424,  0.5287,  0.6007,  1.0920,  0.7637,  0.6677,
         0.7472,  0.8679,  0.7186,  0.9822,  0.7911,  0.7171,  0.6691,  0.6995,
         0.9994,  0.8268,  0.7723,  0.9580,  0.8539,  0.7396,  0.7400,  0.7251,
         0.7911,  0.7718,  0.8136,  0.8116,  0.7975,  0.7874,  0.9112,  1.0352,
         1.0297,  0.4207,  0.2027,  0.1038,  0.4100,  0.6735,  0.6372,  0.6996,
         0.6948,  0.6956,  0.7264,  0.7231,  1.0607,  0.8704,  1.2968,  1.0030,
         0.9113,  0.9355,  1.7392,  1.2683,  1.1325,  1.1611,  2.4088,  1.5451,
         1.9191,  1.4172,  1.3753,  1.4758,  1.2173,  1.1467,  1.1784,  1.1553,
         1.1672,  1.1395,  1.1744,  1.1231,  1.4276,  1.1582,  2.1997,  1.4713,
         1.3557,  1.2345,  1.1861,  2.3083,  1.8241,  1.8019,  1.4621,  1.3451,
         1.3345,  1.3845,  1.2438,  1.2968,  1.3187,  1.2888,  1.3002,  1.6577,
         1.3738,  1.2659,  1.2630,  1.3066,  1.3000,  1.3961,  1.2897,  1.1996,
         1.1991,  1.3028,  1.2820,  1.2039,  1.1921,  1.4474,  1.4154,  1.3806,
         1.2893,  1.2702,  1.1917,  1.5204,  2.0266,  2.1318,  1.6488,  1.5924,
         1.5504,  1.7820,  1.6093,  2.1674,  1.9689,  2.0941,  1.8267,  1.9215,
         1.8214,  1.6156,  1.5997,  1.5667,  1.7705,  1.9666,  1.8409,  1.7906,
         1.6446,  1.8366,  2.3152,  1.8339,  1.7239,  2.0216,  2.2035,  1.8407,
         1.7196,  1.6482,  1.6874,  2.0083,  1.6998,  2.1621,  1.9907,  1.8425,
         1.6611,  1.6723,  1.6773,  2.1790,  1.8010,  1.8129,  1.8242,  1.8986,
         1.8621,  1.8264,  1.8033,  1.7353,  1.6795,  1.6255,  1.6522,  1.5368,
         1.5383,  1.6109,  1.5865,  1.6939,  1.5883,  1.6568,  1.5804,  1.8175,
         1.7191,  1.7831,  1.9033,  1.9753,  1.8700,  1.9752,  2.7007,  2.1772,
         2.9413,  2.3743,  2.1889,  2.2079,  2.2073,  2.3624,  2.7386,  2.3742,
         2.4054,  2.2988,  2.4079,  2.1270,  2.2210,  2.1280,  2.8843,  2.2882,
         2.1418,  2.1319,  2.8456,  2.3797,  2.1751,  2.1291,  2.0354,  2.9497,
         2.6200,  2.8623,  2.4248,  2.2277,  2.1990,  2.0668,  2.0100,  2.0081,
         2.0226,  2.1722,  2.0381,  2.1716,  2.0559,  2.3039,  2.2880,  2.1075,
         2.1009,  2.0706,  1.9753,  2.0038,  1.9853,  2.5010,  2.5592,  2.1943,
         2.1351,  2.0706,  2.0767,  2.5901,  2.2476,  2.7540,  2.3688,  2.9649,
         2.5134,  2.3186,  2.3560,  3.3092,  2.6031,  2.4631,  2.4225,  2.5339,
         2.4655,  2.5182,  2.4337,  2.2683,  2.1987,  2.1775,  2.1415,  2.8694,
         2.4419,  2.8602,  2.4302,  2.2349,  2.3318,  2.2668,  2.5411,  2.3132,
         2.3081,  2.1724,  2.2131,  2.1595,  2.6592,  2.7132,  2.4022,  2.3862,
         2.4275,  2.2690,  2.9225,  2.4533,  2.3559,  2.3182,  2.2983,  3.0220,
         2.6621,  3.0903,  3.8028,  2.9312,  2.7331,  2.5881,  2.4180,  2.4500,
         2.3040,  3.2950,  2.7735,  2.4938,  2.2771,  2.3915,  3.3628,  3.4488,
         2.8986,  2.9426,  2.6512,  2.5568,  2.6053,  2.4613,  2.4034,  2.3808,
         2.5772,  2.4801,  2.4585,  2.6617,  2.4660,  3.3602,  2.8840,  2.6383,
         2.5995,  2.5241,  2.5714,  2.5949,  3.8666,  3.0449,  2.8984,  2.9148,
         2.8292,  3.2795,  3.5471,  3.1372,  2.8397,  2.8361,  3.4782,  3.4384,
         3.2868,  2.8609,  3.0069,  2.8746], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808206
t8: 1641198808206
t9: 1641198808206
t10: 1641198808217
t11: 1641198808218
t12: 1641198808218
t1: 1641198808218
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808229
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0108, 1.0474, 0.9580, 0.9751, 0.8923, 1.0022, 0.9649, 0.8911, 1.0157,
        1.1049, 0.9646, 1.0062, 0.9380, 1.0302, 1.1737, 0.9578, 0.9668, 1.0015,
        1.0825, 1.0674, 1.0435, 0.9751, 0.9946, 1.0431, 0.9596, 1.0002, 1.0526,
        0.9936, 1.0012, 1.1198, 0.9857, 1.1086, 0.8268, 0.9812, 0.9488, 1.0035,
        0.9335, 1.0195, 0.9687, 1.0169, 1.0100, 1.0014, 1.0678, 0.9867, 1.0015,
        0.9850, 1.0348, 0.9740, 0.9752, 1.0038, 1.0406, 0.9677, 1.0201, 1.0052,
        0.9742, 1.0303, 1.1301, 1.0481, 0.9999, 0.9684, 0.9350, 0.9997, 1.0158,
        0.9503, 1.0070, 0.9635, 0.9478, 0.9657, 1.0229, 1.0715, 1.1012, 0.9960,
        0.9695, 0.9638, 1.0495, 1.0053, 1.0073, 1.0046, 1.0103, 0.9858, 0.9999,
        1.0052, 0.9838, 1.0285, 1.0916, 1.0126, 1.0032, 1.0376, 1.0367, 0.9829,
        1.0036, 0.9935, 0.9975, 0.9911, 0.9708, 1.0470, 1.0053, 1.0010, 1.0220,
        1.0091, 0.9966, 0.9839, 0.9985, 0.9169, 0.9641, 0.9159, 0.9569, 0.9572,
        0.9663, 1.0394, 1.0748, 1.0192, 0.9782, 0.9639, 0.9588, 0.8835, 1.0373,
        0.9684, 1.0144, 0.9997, 0.9887, 0.9940, 0.9919, 1.0573, 0.9889, 1.0322,
        0.9885, 0.9959, 0.9678, 1.0718, 1.0011, 0.9775, 1.0024, 1.1286, 0.9934,
        1.0805, 0.9991, 1.0421, 1.0901, 0.9920, 0.9756, 0.9170, 1.0034, 1.0163,
        1.0098, 0.9233, 0.9786, 1.0649, 0.9973, 1.0638, 1.0000, 0.9671, 0.9888,
        0.9929, 1.0910, 1.0026, 1.0445, 0.9461, 0.9908, 0.9528, 0.9660, 0.9970,
        1.0064, 0.9090, 1.0105, 0.9614, 1.0464, 0.9607, 0.9904, 1.0015, 0.9551,
        0.9718, 1.0287, 0.9835, 0.9992, 0.9961, 1.0246, 1.0037, 1.0116, 0.9763,
        1.0291, 0.8877, 0.9680, 0.9778, 1.0046, 0.9905, 1.0506, 1.0945, 0.9836,
        1.0041, 1.0074, 0.9436, 1.0374, 0.9396, 1.0399, 1.0173, 1.0150, 0.9401,
        0.9998, 0.8947, 0.9997, 0.9805, 1.0118, 1.0314, 1.0416, 1.0085, 0.9878,
        0.9917, 1.0324, 1.0666, 1.0002, 1.0001, 1.0306, 1.0359, 0.9859, 0.9995,
        0.9857, 0.9678, 1.0385, 0.9907, 1.0349, 1.0122, 1.0003, 0.9994, 0.9769,
        0.9747, 1.0636, 0.9885, 1.0082, 1.0118, 1.0117, 0.9389, 1.0125, 0.9185,
        1.0086, 0.9662, 0.9789, 1.0117, 0.9892, 0.9929, 1.0012, 1.0114, 0.9328,
        0.9844, 0.9487, 0.9868, 1.0383, 0.9708, 0.9366, 1.0297, 1.0444, 1.0186,
        1.0201, 1.0664, 1.0027, 0.9930, 0.9342, 1.0227, 1.0330, 1.0275, 1.0387,
        1.0824, 1.0117, 1.0201, 0.8362, 1.0156, 0.9428, 1.0113, 0.9230, 1.0244,
        0.9336, 1.0034, 1.0120, 1.0482, 0.9385, 0.9977, 1.0126, 0.9694, 1.0625,
        1.0114, 1.0518, 0.9836, 0.9689, 0.9026, 0.9935, 0.9930, 0.9844, 0.9595,
        1.0311, 0.9805, 1.0215, 0.9640, 1.0426, 1.0259, 0.9840, 0.9765, 0.9579,
        0.9839, 0.9953, 0.9958, 0.9590, 1.0504, 0.9969, 0.9728, 0.9810, 0.9703,
        1.0695, 1.0055, 1.0405, 1.0015, 1.0033, 1.0053, 1.0011, 1.0240, 1.1089,
        0.9415, 0.9216, 0.8952, 1.0259, 0.8590, 1.0212, 0.8886, 0.9956, 0.9745,
        0.9747, 0.9634, 1.0890, 0.9915, 1.0412, 0.9893, 0.9981, 0.9728, 0.9756,
        1.0432, 0.9629, 0.9531, 0.9904, 1.0013, 1.0092, 1.0651, 1.0293, 1.0017,
        1.0025, 0.9279, 0.9898, 1.0778, 0.9986, 0.9922, 1.0070, 0.9987, 1.0497,
        0.9605, 1.0112, 1.1052, 0.9927, 1.0011, 0.9741, 1.0005, 0.9997, 0.9949,
        1.0766, 0.9776, 0.9962, 0.9961, 0.9644, 1.0788, 1.0143, 0.9997, 1.0073,
        1.0005, 0.9655, 0.9326, 1.0003, 0.9799, 0.9774, 1.0406, 1.0122, 1.0107,
        1.0268, 0.9929, 1.0579, 0.9679, 0.9867, 0.9672, 0.9829, 1.0158, 1.0266,
        1.1168, 1.0459, 1.0151, 1.0416, 0.9151, 1.0159, 1.1023, 1.0107, 1.0036,
        0.9178, 1.0567, 1.0879, 1.0402, 0.9819, 1.0399, 1.0167, 1.1130],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808234
t4: 1641198808234
surr1, surr2: tensor([-3.1956e+00, -1.0274e+00, -1.4157e+00, -2.4137e+00, -2.4505e+00,
        -2.9507e+00, -2.6852e+00, -2.4651e+00, -2.8689e+00, -2.9430e+00,
        -2.5267e+00, -2.6735e+00, -2.3348e+00, -2.6635e+00, -2.8783e+00,
        -2.4949e+00, -2.3164e+00, -2.5928e+00, -2.6102e+00, -2.4695e+00,
        -2.3137e+00, -2.1307e+00, -8.3993e-01, -1.8421e-01, -1.5691e+00,
        -1.8047e+00, -1.7490e+00, -1.6790e+00, -1.6018e+00, -1.7342e+00,
        -2.0280e+00, -1.9542e+00, -1.4350e+00, -1.8670e+00, -1.5372e+00,
        -1.6004e+00, -1.3891e+00, -1.8447e+00, -1.4873e+00, -1.6142e+00,
        -1.5364e+00, -1.4430e+00, -1.6058e+00, -1.5969e+00, -1.7473e+00,
        -1.4222e+00, -1.5882e+00, -1.5140e+00, -1.4974e+00, -1.7702e+00,
        -1.4934e+00, -1.4936e+00, -1.5711e+00, -1.8931e+00, -1.2139e+00,
        -1.3677e+00, -1.3059e+00, -1.5641e+00, -1.5684e+00, -1.3572e+00,
        -1.1636e+00, -1.5429e+00, -1.1284e+00, -1.1935e+00, -1.5748e+00,
        -1.1359e+00,  1.0854e+00,  1.2338e+00,  2.1929e+00,  2.6192e+00,
         5.1229e+00,  1.0433e-02,  2.7895e-01,  1.6739e-01, -6.7010e-02,
         1.7208e-01,  2.2309e-01,  2.8119e-01,  3.2711e-01,  3.0465e-01,
         2.7428e-01,  4.2187e-01,  3.3701e-01,  3.1401e-01,  5.1205e-01,
         1.3207e-01, -3.1082e-02,  2.2255e-01,  5.2479e-01,  2.8018e-01,
         1.7530e-01,  1.3492e-01,  2.3805e-01,  2.7019e-01,  3.4812e-01,
         1.0662e-01,  2.2376e-01,  2.9861e-01,  3.0003e-01,  2.1453e-01,
         2.7915e-01,  3.7915e-01,  2.3020e-01,  3.2155e-01, -1.0741e-01,
         7.3838e-02, -1.0319e-01,  4.0587e-02,  1.2557e-01,  1.9666e-01,
         2.7213e-01,  3.2073e-01,  1.8515e-01,  1.6698e+00,  2.5513e+00,
         3.2178e+00,  7.5529e-01,  9.4711e-01,  9.3095e-01,  9.8494e-01,
         9.6762e-01,  9.3581e-01,  8.7221e-01,  9.9501e-01,  8.1562e-01,
         8.8563e-01,  7.3597e-01,  7.6418e-01,  7.9697e-01,  6.7404e-01,
         6.0912e-01,  8.3389e-01,  5.2259e-01,  9.9928e-01,  2.9814e-01,
         8.8287e-01,  6.9966e-01,  9.2565e-01,  1.0078e+00,  8.5466e-01,
         9.8322e-01,  8.6504e-01,  6.1357e-01,  9.7465e-01,  1.0350e+00,
         9.5378e-01,  5.4821e-01,  8.8959e-01,  9.3421e-01,  1.1597e+00,
         5.4318e-01,  9.3458e-01,  6.7345e-01,  9.0317e-01,  1.0661e+00,
         4.9366e-01,  8.2359e-01,  8.0654e-01,  8.2019e-01,  8.5015e-01,
         6.9868e-01,  5.3289e-01,  9.6539e-01,  8.0321e-01,  5.1669e-01,
         8.7447e-01,  7.2780e-01,  6.3722e-01,  7.9294e-01,  9.3826e-01,
         8.9487e-01,  6.3214e-01,  7.2983e-01,  8.5291e-01,  8.5549e-01,
         1.0244e+00,  9.8759e-01,  9.4825e-01,  8.9568e-01,  1.0350e+00,
         9.2066e-01,  7.1580e-01,  3.6200e-01,  5.1787e-01,  8.3057e-01,
         9.4038e-01,  1.0524e+00,  9.0564e-01,  6.7200e-01,  5.9520e-01,
         7.1143e-01,  4.0883e-01,  4.2215e-01,  7.5812e-01,  5.4667e-01,
         6.7531e-01,  7.8803e-01,  6.0981e-01,  4.1688e-01,  5.9952e-01,
         3.2574e-01,  7.9659e-01,  6.9084e-01,  9.2464e-01,  7.9889e-01,
         7.3908e-01,  7.0448e-01,  3.1980e-01,  6.6305e-01,  7.7679e-01,
         5.7501e-01,  6.1887e-01,  7.5661e-01,  6.2288e-01,  5.4221e-01,
         6.0835e-01,  6.6817e-01,  6.8061e-01,  4.9538e-01,  5.9180e-01,
         7.6807e-01,  6.1530e-01,  6.8952e-01,  6.0959e-01,  7.6448e-01,
         5.2170e-01,  5.3290e-01,  6.1503e-01,  6.3348e-01,  7.3601e-01,
         6.5964e-01,  5.6569e-01,  2.0981e-01,  6.7331e-01,  3.1256e-01,
         6.9185e-01,  5.9919e-01,  6.8637e-01,  7.2735e-01,  8.0731e-01,
         7.2607e-01,  6.5472e-01,  7.0114e-01,  2.7273e-01,  6.0719e-01,
         3.9811e-01,  6.9210e-01,  5.5185e-01,  4.2646e-01,  1.7501e-01,
         5.3556e-01,  5.5329e-01,  6.0511e-01,  4.9579e-01,  1.8885e-01,
         1.1857e-01,  1.8506e-01,  1.1680e-03,  4.5541e-01,  4.2873e-01,
         4.6762e-01,  3.9657e-01,  2.7153e-01,  3.3317e-01,  2.3733e-01,
        -9.1242e-02,  3.2035e-01,  3.4291e-01,  4.3974e-01,  5.9949e-02,
         1.0796e-01,  1.6827e-01,  4.1187e-01,  4.2814e-01,  1.6620e-01,
        -2.6798e-02,  2.3641e-01,  4.1491e-01,  3.0545e-01,  1.3895e-01,
         2.0339e-01,  1.5896e-01,  1.7863e-01,  1.7506e-01, -1.2471e-02,
         3.6816e-01,  3.7525e-01,  2.8503e-01,  1.5999e-01,  3.0543e-01,
         2.8827e-01,  2.6199e-01,  1.9904e-01,  3.0861e-01,  2.3604e-01,
         1.6178e-01,  1.1115e-02,  5.6312e-03,  2.4161e-01,  1.7451e-01,
         1.8502e-01, -3.3676e-01,  2.5870e-01,  4.8350e-01,  3.0409e-01,
         3.8102e-01,  3.3282e-01,  2.8747e-01,  4.6338e-01,  2.7350e-01,
         3.6437e-01,  1.4282e-01,  2.8365e-01,  3.5784e-01,  4.2905e-01,
        -4.2726e-03,  7.7255e-02, -8.1818e-02, -1.2472e-01,  2.8301e-01,
        -1.5949e-01,  3.0249e-01, -1.2701e-01,  2.5937e-01,  2.7191e-01,
         2.3420e-01,  2.5895e-01,  8.7112e-02,  8.3939e-02,  2.3658e-01,
         1.8157e-01,  3.1405e-01, -7.4861e-02,  6.1975e-02,  1.6734e-01,
         1.2572e-01, -1.1431e-01,  2.7950e-01,  2.0921e-01,  3.1976e-01,
         5.0554e-02,  3.9520e-02,  1.3190e-01,  4.7340e-02, -3.6602e-01,
         7.4283e-02, -4.5080e-02,  7.5000e-02, -7.4019e-02,  8.3367e-02,
         1.7885e-02, -2.5339e-01, -4.8674e-01, -1.8102e-01, -4.5669e-01,
        -2.4941e-01, -3.3852e-01, -1.8928e-01,  5.6837e-02, -9.3902e-02,
         3.1399e-02, -3.4636e-01, -4.9813e-01, -1.6637e-01,  9.1511e-02,
        -3.7985e-01, -4.5050e-01, -4.2616e-01, -2.9943e-01, -2.1080e-01,
        -1.5399e-01, -3.2597e-01, -6.4641e-01, -1.7716e-01, -2.2685e-01,
        -2.8987e-01, -1.9676e-01, -2.4474e-01, -2.6536e-01, -3.7953e-01,
        -3.9163e-01, -5.5729e-01, -8.3616e-01, -5.2185e-01, -6.5999e-01,
        -5.2423e-01, -4.3883e-01, -4.2982e-01, -1.0360e+00, -8.9656e-01,
        -5.5461e-01, -5.7121e-01, -1.0441e+00, -7.7809e-01, -8.2105e-01,
        -7.0817e-01, -6.1635e-01, -1.1108e+00, -1.0423e+00, -9.7159e-01,
        -9.0173e-01, -7.8398e-01, -8.5906e-01, -9.5467e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1956e+00, -1.0274e+00, -1.4157e+00, -2.4137e+00, -2.4716e+00,
        -2.9507e+00, -2.6852e+00, -2.4896e+00, -2.8689e+00, -2.9300e+00,
        -2.5267e+00, -2.6735e+00, -2.3348e+00, -2.6635e+00, -2.6975e+00,
        -2.4949e+00, -2.3164e+00, -2.5928e+00, -2.6102e+00, -2.4695e+00,
        -2.3137e+00, -2.1307e+00, -8.3993e-01, -1.8421e-01, -1.5691e+00,
        -1.8047e+00, -1.7490e+00, -1.6790e+00, -1.6018e+00, -1.7035e+00,
        -2.0280e+00, -1.9391e+00, -1.5622e+00, -1.8670e+00, -1.5372e+00,
        -1.6004e+00, -1.3891e+00, -1.8447e+00, -1.4873e+00, -1.6142e+00,
        -1.5364e+00, -1.4430e+00, -1.6058e+00, -1.5969e+00, -1.7473e+00,
        -1.4222e+00, -1.5882e+00, -1.5140e+00, -1.4974e+00, -1.7702e+00,
        -1.4934e+00, -1.4936e+00, -1.5711e+00, -1.8931e+00, -1.2139e+00,
        -1.3677e+00, -1.2712e+00, -1.5641e+00, -1.5684e+00, -1.3572e+00,
        -1.1636e+00, -1.5429e+00, -1.1284e+00, -1.1935e+00, -1.5748e+00,
        -1.1359e+00,  1.0854e+00,  1.2338e+00,  2.1929e+00,  2.6192e+00,
         5.1175e+00,  1.0433e-02,  2.7895e-01,  1.6739e-01, -6.7010e-02,
         1.7208e-01,  2.2309e-01,  2.8119e-01,  3.2711e-01,  3.0465e-01,
         2.7428e-01,  4.2187e-01,  3.3701e-01,  3.1401e-01,  5.1205e-01,
         1.3207e-01, -3.1082e-02,  2.2255e-01,  5.2479e-01,  2.8018e-01,
         1.7530e-01,  1.3492e-01,  2.3805e-01,  2.7019e-01,  3.4812e-01,
         1.0662e-01,  2.2376e-01,  2.9861e-01,  3.0003e-01,  2.1453e-01,
         2.7915e-01,  3.7915e-01,  2.3020e-01,  3.2155e-01, -1.0741e-01,
         7.3838e-02, -1.0319e-01,  4.0587e-02,  1.2557e-01,  1.9666e-01,
         2.7213e-01,  3.2073e-01,  1.8515e-01,  1.6698e+00,  2.5513e+00,
         3.2778e+00,  7.5529e-01,  9.4711e-01,  9.3095e-01,  9.8494e-01,
         9.6762e-01,  9.3581e-01,  8.7221e-01,  9.9501e-01,  8.1562e-01,
         8.8563e-01,  7.3597e-01,  7.6418e-01,  7.9697e-01,  6.7404e-01,
         6.0912e-01,  8.3389e-01,  5.2259e-01,  9.7396e-01,  2.9814e-01,
         8.8287e-01,  6.9966e-01,  9.2565e-01,  1.0078e+00,  8.5466e-01,
         9.8322e-01,  8.6504e-01,  6.1357e-01,  9.7465e-01,  1.0350e+00,
         9.5378e-01,  5.4821e-01,  8.8959e-01,  9.3421e-01,  1.1597e+00,
         5.4318e-01,  9.3458e-01,  6.7345e-01,  9.0317e-01,  1.0661e+00,
         4.9366e-01,  8.2359e-01,  8.0654e-01,  8.2019e-01,  8.5015e-01,
         6.9868e-01,  5.3289e-01,  9.6539e-01,  8.0321e-01,  5.1669e-01,
         8.7447e-01,  7.2780e-01,  6.3722e-01,  7.9294e-01,  9.3826e-01,
         8.9487e-01,  6.3214e-01,  7.2983e-01,  8.5291e-01,  8.5549e-01,
         1.0244e+00,  9.8759e-01,  9.4825e-01,  8.9568e-01,  1.0350e+00,
         9.2066e-01,  7.2568e-01,  3.6200e-01,  5.1787e-01,  8.3057e-01,
         9.4038e-01,  1.0524e+00,  9.0564e-01,  6.7200e-01,  5.9520e-01,
         7.1143e-01,  4.0883e-01,  4.2215e-01,  7.5812e-01,  5.4667e-01,
         6.7531e-01,  7.8803e-01,  6.0981e-01,  4.1688e-01,  6.0308e-01,
         3.2574e-01,  7.9659e-01,  6.9084e-01,  9.2464e-01,  7.9889e-01,
         7.3908e-01,  7.0448e-01,  3.1980e-01,  6.6305e-01,  7.7679e-01,
         5.7501e-01,  6.1887e-01,  7.5661e-01,  6.2288e-01,  5.4221e-01,
         6.0835e-01,  6.6817e-01,  6.8061e-01,  4.9538e-01,  5.9180e-01,
         7.6807e-01,  6.1530e-01,  6.8952e-01,  6.0959e-01,  7.6448e-01,
         5.2170e-01,  5.3290e-01,  6.1503e-01,  6.3348e-01,  7.3601e-01,
         6.5964e-01,  5.6569e-01,  2.0981e-01,  6.7331e-01,  3.1256e-01,
         6.9185e-01,  5.9919e-01,  6.8637e-01,  7.2735e-01,  8.0731e-01,
         7.2607e-01,  6.5472e-01,  7.0114e-01,  2.7273e-01,  6.0719e-01,
         3.9811e-01,  6.9210e-01,  5.5185e-01,  4.2646e-01,  1.7501e-01,
         5.3556e-01,  5.5329e-01,  6.0511e-01,  4.9579e-01,  1.8885e-01,
         1.1857e-01,  1.8506e-01,  1.1680e-03,  4.5541e-01,  4.2873e-01,
         4.6762e-01,  3.9657e-01,  2.7153e-01,  3.3317e-01,  2.5543e-01,
        -9.1242e-02,  3.2035e-01,  3.4291e-01,  4.3974e-01,  5.9949e-02,
         1.0796e-01,  1.6827e-01,  4.1187e-01,  4.2814e-01,  1.6620e-01,
        -2.6798e-02,  2.3641e-01,  4.1491e-01,  3.0545e-01,  1.3895e-01,
         2.0339e-01,  1.5896e-01,  1.7863e-01,  1.7506e-01, -1.2471e-02,
         3.6816e-01,  3.7525e-01,  2.8503e-01,  1.5999e-01,  3.0543e-01,
         2.8827e-01,  2.6199e-01,  1.9904e-01,  3.0861e-01,  2.3604e-01,
         1.6178e-01,  1.1115e-02,  5.6312e-03,  2.4161e-01,  1.7451e-01,
         1.8502e-01, -3.3676e-01,  2.5870e-01,  4.8350e-01,  3.0409e-01,
         3.8102e-01,  3.3282e-01,  2.8747e-01,  4.6338e-01,  2.7350e-01,
         3.6437e-01,  1.4282e-01,  2.8365e-01,  3.5784e-01,  4.2562e-01,
        -4.2726e-03,  7.7255e-02, -8.2255e-02, -1.2472e-01,  2.9653e-01,
        -1.5949e-01,  3.0639e-01, -1.2701e-01,  2.5937e-01,  2.7191e-01,
         2.3420e-01,  2.5895e-01,  8.7112e-02,  8.3939e-02,  2.3658e-01,
         1.8157e-01,  3.1405e-01, -7.4861e-02,  6.1975e-02,  1.6734e-01,
         1.2572e-01, -1.1431e-01,  2.7950e-01,  2.0921e-01,  3.1976e-01,
         5.0554e-02,  3.9520e-02,  1.3190e-01,  4.7340e-02, -3.6602e-01,
         7.4283e-02, -4.5080e-02,  7.5000e-02, -7.4019e-02,  8.3367e-02,
         1.7885e-02, -2.5339e-01, -4.8674e-01, -1.8017e-01, -4.5669e-01,
        -2.4941e-01, -3.3852e-01, -1.8928e-01,  5.6837e-02, -9.3902e-02,
         3.1399e-02, -3.4636e-01, -4.9813e-01, -1.6637e-01,  9.1511e-02,
        -3.7985e-01, -4.5050e-01, -4.2616e-01, -2.9943e-01, -2.1080e-01,
        -1.5399e-01, -3.2597e-01, -6.4641e-01, -1.7716e-01, -2.2685e-01,
        -2.8987e-01, -1.9676e-01, -2.4474e-01, -2.6536e-01, -3.7953e-01,
        -3.9163e-01, -5.5729e-01, -8.3616e-01, -5.2185e-01, -6.5999e-01,
        -5.2423e-01, -4.3883e-01, -4.2334e-01, -1.0360e+00, -8.9656e-01,
        -5.5461e-01, -5.7121e-01, -1.0441e+00, -7.7646e-01, -8.2105e-01,
        -7.0817e-01, -6.1635e-01, -1.1108e+00, -1.0423e+00, -9.7159e-01,
        -9.0173e-01, -7.8398e-01, -8.5906e-01, -9.4354e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808243
t6: 1641198808243
state_values: tensor([ 1.1722,  0.1031,  0.1448,  0.3985,  0.8248,  1.3015,  1.5648,  1.6911,
         1.8498,  2.1344,  2.8016,  2.4828,  2.5244,  2.5546,  2.9512,  4.3553,
         3.9776,  3.6203,  4.0504,  4.2325,  4.5432,  4.5232,  2.9961,  2.3367,
         3.4710,  3.5603,  3.6525,  4.4429,  4.0442,  4.0230,  5.8224,  5.4519,
         5.6128,  5.0173,  4.7269,  4.6317,  4.6538,  4.6828,  4.9852,  4.7724,
         5.0074,  4.9478,  4.9623,  5.9261,  5.3413,  5.2243,  5.1009,  5.7231,
         5.2947,  5.2341,  5.2620,  5.7203,  5.3682,  6.6100,  5.9596,  5.6879,
         5.9626,  7.2394,  6.3249,  6.9898,  6.2293,  6.0726,  5.8742,  6.1429,
         6.0053,  6.0222,  4.3180,  3.8107,  3.2867,  3.2766,  2.4496,  4.1471,
         4.9350,  5.0719,  5.2817,  6.1965,  6.0675,  6.1416,  6.0775,  6.2223,
         6.0045,  5.9623,  6.1122,  6.0006,  6.2871,  7.3308,  6.6245,  6.3676,
         6.6770,  6.9746,  6.5528,  7.1784,  6.8042,  6.5593,  6.4060,  6.4312,
         7.1839,  6.9202,  6.7822,  7.1961,  7.0258,  6.6845,  6.6136,  6.6074,
         6.6397,  6.6041,  6.6552,  6.6657,  6.6417,  6.6247,  6.9982,  7.3134,
         7.3472,  5.8757,  4.9881,  4.3014,  5.6005,  6.4895,  6.3754,  6.6379,
         6.6290,  6.5628,  6.6393,  6.6176,  7.3893,  6.9911,  7.8166,  7.2624,
         7.0319,  7.0267,  8.3886,  7.8231,  7.4220,  7.5879,  9.3219,  8.2908,
         8.8240,  8.1228,  8.0600,  8.2211,  7.7576,  7.5374,  7.4960,  7.5586,
         7.6435,  7.6269,  7.5057,  7.4073,  8.0308,  7.6024,  9.1092,  8.1824,
         7.8607,  7.6282,  7.5292,  9.1952,  8.6816,  8.6862,  8.0666,  7.8276,
         7.7576,  7.7901,  7.6252,  7.7866,  7.6831,  7.7777,  7.7124,  8.3443,
         7.8598,  7.6634,  7.7145,  7.7071,  7.6818,  7.9620,  7.7305,  7.6077,
         7.6065,  7.8604,  7.8363,  7.7415,  7.6138,  8.1151,  7.9089,  7.8343,
         7.7023,  7.7550,  7.5968,  8.2125,  8.9926,  9.1993,  8.4131,  8.2221,
         8.0751,  8.5809,  8.2344,  9.1393,  8.9512,  9.1457,  8.6194,  8.8501,
         8.5553,  8.3027,  8.2242,  8.2972,  8.6402,  8.9587,  8.8089,  8.5643,
         8.3233,  8.7145,  9.4322,  8.6575,  8.5489,  9.0126,  9.3051,  8.6847,
         8.4644,  8.3206,  8.3291,  8.9215,  8.4665,  9.1983,  9.0164,  8.7631,
         8.4529,  8.3721,  8.3413,  9.1945,  8.5905,  8.7008,  8.7469,  8.8701,
         8.6314,  8.7214,  8.5433,  8.5741,  8.3968,  8.2874,  8.4391,  8.2138,
         8.1982,  8.3501,  8.3815,  8.4038,  8.2482,  8.3102,  8.2149,  8.6834,
         8.4695,  8.4877,  8.8095,  8.9801,  8.8669,  9.0261, 10.0219,  9.1798,
        10.3064,  9.3911,  9.2529,  9.3164,  9.3493,  9.5855, 10.1231,  9.6352,
         9.6708,  9.3075,  9.5919,  9.0899,  9.3277,  9.0427, 10.1854,  9.2799,
         9.1541,  9.1819, 10.1924,  9.4194,  9.0948,  9.1370,  8.8952, 10.2574,
         9.8915, 10.2536,  9.5683,  9.1818,  9.0354,  8.8768,  8.8104,  8.7911,
         8.7863,  9.1445,  8.8864,  9.1780,  8.9212,  9.3913,  9.4243,  9.0456,
         8.9680,  8.8902,  8.7661,  8.8473,  8.8423,  9.4965,  9.7379,  9.2319,
         9.0442,  8.9255,  8.9112,  9.7650,  9.3618, 10.0914,  9.5858, 10.4044,
         9.8045,  9.4816,  9.5668, 10.8507,  9.7829,  9.4595,  9.3331,  9.6796,
         9.4226,  9.6692,  9.3879,  9.1795,  9.0644,  9.0248,  8.9629, 10.1019,
         9.4568, 10.1675,  9.4798,  9.1578,  9.2311,  9.1356,  9.6706,  9.2735,
         9.1970,  9.0280,  9.1578,  9.1542,  9.9118, 10.0385,  9.5971,  9.5742,
         9.4329,  9.2177, 10.2474,  9.5944,  9.3648,  9.4183,  9.3653, 10.4100,
         9.7629, 10.5001, 11.4903, 10.2067,  9.8454,  9.6255,  9.4452,  9.5067,
         9.2755, 10.7167,  9.9094,  9.4959,  9.2106,  9.3096, 10.7601, 10.9880,
        10.2560, 10.3408,  9.9035,  9.6352,  9.5873,  9.4834,  9.3641,  9.3137,
         9.7333,  9.6486,  9.6429,  9.9477,  9.5903, 10.8834, 10.0668,  9.7049,
         9.6030,  9.5013,  9.6994,  9.8042, 11.4980, 10.3000, 10.2164, 10.2711,
         9.9514, 10.7276, 11.1559, 10.6248, 10.1643,  9.9983, 10.9898, 11.0123,
        10.8194, 10.0948, 10.3732, 10.2065], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808248
t8: 1641198808248
t9: 1641198808248
t10: 1641198808258
t11: 1641198808259
t12: 1641198808260
t1: 1641198808260
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808270
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0110, 1.0393, 0.9620, 0.9721, 0.8897, 1.0022, 0.9643, 0.8799, 1.0177,
        1.1058, 0.9594, 1.0079, 0.9388, 1.0364, 1.1643, 0.9289, 1.0001, 1.0070,
        1.0313, 1.0218, 1.0096, 0.9960, 0.9989, 1.0100, 0.9874, 1.0002, 1.0557,
        0.9991, 1.0030, 1.0908, 0.9477, 1.0495, 0.9148, 0.9930, 0.9772, 1.0023,
        0.9431, 1.0289, 0.9720, 1.0232, 1.0100, 1.0015, 1.0821, 1.0102, 1.0046,
        0.9863, 1.0477, 0.9762, 0.9523, 1.0050, 1.0431, 0.9676, 1.0289, 1.0052,
        0.9691, 1.0393, 1.1228, 1.1050, 1.0044, 0.9877, 0.9871, 0.9999, 1.0138,
        0.9474, 1.0106, 0.9631, 0.9363, 0.9581, 1.0251, 1.0762, 1.1131, 1.0032,
        0.9794, 0.9493, 1.0659, 1.0049, 1.0115, 1.0043, 1.0128, 0.9854, 0.9996,
        1.0058, 0.9811, 1.0364, 1.0876, 1.0297, 0.9946, 1.0406, 1.0232, 0.9912,
        1.0202, 0.9984, 0.9973, 0.9888, 0.9617, 1.0622, 1.0049, 1.0015, 1.0272,
        1.0075, 0.9947, 0.9776, 0.9978, 0.9047, 0.9506, 0.9014, 0.9466, 0.9505,
        0.9609, 1.0445, 1.0735, 1.0230, 0.9750, 0.9557, 0.9525, 0.8817, 1.0502,
        0.9706, 1.0190, 0.9997, 0.9860, 0.9924, 0.9902, 1.0671, 0.9881, 1.0507,
        0.9895, 0.9865, 0.9584, 1.0956, 0.9920, 0.9796, 1.0090, 1.1031, 0.9852,
        1.0235, 1.0010, 1.0174, 1.0410, 0.9980, 0.9943, 0.9565, 1.0050, 1.0175,
        1.0108, 0.9102, 0.9677, 1.0755, 0.9970, 1.0911, 0.9982, 0.9973, 0.9905,
        0.9919, 1.1108, 0.9800, 1.0031, 0.9795, 0.9988, 0.9782, 0.9568, 0.9958,
        1.0070, 0.8983, 1.0138, 0.9607, 1.0569, 0.9629, 0.9869, 1.0016, 0.9506,
        0.9652, 1.0329, 0.9837, 0.9986, 0.9956, 1.0277, 1.0037, 1.0134, 0.9756,
        1.0371, 0.8929, 0.9510, 0.9729, 1.0050, 0.9895, 1.0584, 1.0909, 0.9876,
        1.0017, 0.9921, 0.9269, 1.0478, 0.9443, 1.0644, 0.9986, 1.0080, 0.9917,
        1.0050, 0.9613, 0.9990, 0.9778, 1.0154, 1.0326, 1.0437, 1.0105, 0.9789,
        0.9837, 1.0377, 1.0639, 0.9993, 1.0023, 1.0293, 1.0291, 0.9910, 0.9967,
        0.9831, 0.9584, 1.0496, 0.9916, 1.0518, 1.0109, 1.0004, 0.9990, 0.9719,
        0.9650, 1.0788, 0.9888, 1.0155, 1.0104, 1.0144, 0.9337, 1.0213, 0.9238,
        1.0128, 0.9650, 0.9723, 1.0136, 0.9884, 0.9912, 1.0011, 1.0125, 0.9285,
        0.9788, 0.9421, 0.9839, 1.0428, 0.9713, 0.9216, 1.0367, 1.0426, 1.0194,
        1.0219, 1.0694, 1.0095, 1.0270, 1.0013, 1.0055, 1.0090, 1.0094, 1.0139,
        1.0267, 0.9998, 0.9954, 0.9624, 1.0171, 0.9787, 1.0239, 0.9548, 1.0597,
        1.0075, 1.0035, 1.0095, 1.0546, 1.0061, 0.9966, 1.0118, 0.9700, 1.0932,
        0.9881, 1.0052, 0.9956, 0.9982, 0.9538, 0.9906, 0.9917, 0.9811, 0.9511,
        1.0373, 0.9813, 1.0269, 0.9643, 1.0546, 1.0261, 0.9811, 0.9662, 0.9486,
        0.9803, 0.9944, 0.9949, 0.9535, 1.0615, 0.9972, 0.9661, 0.9763, 0.9656,
        1.0816, 1.0060, 1.0526, 1.0013, 1.0163, 1.0015, 0.9997, 1.0070, 1.0355,
        0.9929, 0.9926, 0.9540, 1.0349, 0.8858, 1.0361, 0.9057, 0.9924, 0.9704,
        0.9688, 0.9561, 1.1059, 0.9905, 1.0625, 1.0024, 0.9937, 0.9666, 0.9668,
        1.0511, 0.9656, 0.9341, 0.9872, 1.0012, 1.0100, 1.0696, 1.0335, 1.0019,
        1.0038, 0.9223, 0.9842, 1.0884, 0.9988, 0.9843, 1.0089, 0.9986, 1.0616,
        0.9617, 1.0369, 1.0068, 1.0194, 1.0187, 0.9908, 1.0005, 0.9996, 0.9936,
        1.0934, 1.0357, 0.9912, 0.9953, 0.9585, 1.1038, 0.9616, 0.9968, 0.9945,
        1.0015, 0.9944, 0.9469, 1.0001, 0.9780, 0.9718, 1.0485, 1.0116, 1.0129,
        1.0288, 0.9929, 1.0840, 1.0697, 0.9925, 0.9627, 0.9764, 1.0186, 1.0280,
        1.1211, 1.0941, 1.0024, 1.0104, 0.9735, 1.0399, 1.0243, 0.9900, 0.9992,
        0.9764, 1.0385, 1.0311, 1.0089, 0.9973, 1.0099, 1.0053, 1.0438],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808274
t4: 1641198808275
surr1, surr2: tensor([-3.1963e+00, -1.0194e+00, -1.4217e+00, -2.4061e+00, -2.4433e+00,
        -2.9507e+00, -2.6838e+00, -2.4340e+00, -2.8747e+00, -2.9455e+00,
        -2.5129e+00, -2.6780e+00, -2.3367e+00, -2.6797e+00, -2.8552e+00,
        -2.4197e+00, -2.3962e+00, -2.6071e+00, -2.4869e+00, -2.3639e+00,
        -2.2387e+00, -2.1765e+00, -8.4353e-01, -1.7837e-01, -1.6146e+00,
        -1.8046e+00, -1.7542e+00, -1.6884e+00, -1.6047e+00, -1.6892e+00,
        -1.9499e+00, -1.8500e+00, -1.5878e+00, -1.8893e+00, -1.5833e+00,
        -1.5986e+00, -1.4034e+00, -1.8619e+00, -1.4925e+00, -1.6242e+00,
        -1.5364e+00, -1.4431e+00, -1.6272e+00, -1.6350e+00, -1.7528e+00,
        -1.4242e+00, -1.6080e+00, -1.5173e+00, -1.4623e+00, -1.7724e+00,
        -1.4971e+00, -1.4934e+00, -1.5847e+00, -1.8930e+00, -1.2075e+00,
        -1.3796e+00, -1.2975e+00, -1.6491e+00, -1.5755e+00, -1.3842e+00,
        -1.2284e+00, -1.5432e+00, -1.1263e+00, -1.1899e+00, -1.5804e+00,
        -1.1354e+00,  1.0723e+00,  1.2241e+00,  2.1976e+00,  2.6306e+00,
         5.1786e+00,  1.0508e-02,  2.8179e-01,  1.6487e-01, -6.8058e-02,
         1.7201e-01,  2.2402e-01,  2.8110e-01,  3.2790e-01,  3.0454e-01,
         2.7420e-01,  4.2211e-01,  3.3607e-01,  3.1642e-01,  5.1020e-01,
         1.3429e-01, -3.0814e-02,  2.2319e-01,  5.1792e-01,  2.8252e-01,
         1.7819e-01,  1.3558e-01,  2.3799e-01,  2.6955e-01,  3.4484e-01,
         1.0817e-01,  2.2368e-01,  2.9877e-01,  3.0156e-01,  2.1419e-01,
         2.7861e-01,  3.7670e-01,  2.3004e-01,  3.1726e-01, -1.0590e-01,
         7.2665e-02, -1.0208e-01,  4.0302e-02,  1.2488e-01,  1.9762e-01,
         2.7180e-01,  3.2191e-01,  1.8453e-01,  1.6556e+00,  2.5347e+00,
         3.2113e+00,  7.6469e-01,  9.4924e-01,  9.3519e-01,  9.8493e-01,
         9.6505e-01,  9.3428e-01,  8.7074e-01,  1.0043e+00,  8.1503e-01,
         9.0148e-01,  7.3668e-01,  7.5699e-01,  7.8915e-01,  6.8901e-01,
         6.0361e-01,  8.3568e-01,  5.2605e-01,  9.7672e-01,  2.9567e-01,
         8.3630e-01,  7.0098e-01,  9.0370e-01,  9.6241e-01,  8.5983e-01,
         1.0021e+00,  9.0231e-01,  6.1451e-01,  9.7578e-01,  1.0360e+00,
         9.4018e-01,  5.4214e-01,  8.9849e-01,  9.3389e-01,  1.1895e+00,
         5.4223e-01,  9.6377e-01,  6.7459e-01,  9.0219e-01,  1.0855e+00,
         4.8250e-01,  7.9096e-01,  8.3500e-01,  8.2686e-01,  8.7290e-01,
         6.9199e-01,  5.3225e-01,  9.6597e-01,  7.9373e-01,  5.1835e-01,
         8.7379e-01,  7.3514e-01,  6.3868e-01,  7.9017e-01,  9.3827e-01,
         8.9059e-01,  6.2785e-01,  7.3278e-01,  8.5312e-01,  8.5505e-01,
         1.0239e+00,  9.9056e-01,  9.4824e-01,  8.9722e-01,  1.0343e+00,
         9.2781e-01,  7.1994e-01,  3.5565e-01,  5.1530e-01,  8.3089e-01,
         9.3943e-01,  1.0602e+00,  9.0261e-01,  6.7472e-01,  5.9375e-01,
         7.0063e-01,  4.0160e-01,  4.2638e-01,  7.6192e-01,  5.5957e-01,
         6.6288e-01,  7.8263e-01,  6.4328e-01,  4.1905e-01,  6.4415e-01,
         3.2553e-01,  7.9443e-01,  6.9329e-01,  9.2575e-01,  8.0055e-01,
         7.4055e-01,  6.9811e-01,  3.1723e-01,  6.6642e-01,  7.7481e-01,
         5.7446e-01,  6.2024e-01,  7.5568e-01,  6.1882e-01,  5.4502e-01,
         6.0669e-01,  6.6636e-01,  6.7399e-01,  5.0063e-01,  5.9238e-01,
         7.8054e-01,  6.1450e-01,  6.8960e-01,  6.0933e-01,  7.6051e-01,
         5.1655e-01,  5.4049e-01,  6.1522e-01,  6.3803e-01,  7.3494e-01,
         6.6141e-01,  5.6258e-01,  2.1162e-01,  6.7715e-01,  3.1385e-01,
         6.9099e-01,  5.9519e-01,  6.8769e-01,  7.2676e-01,  8.0594e-01,
         7.2603e-01,  6.5544e-01,  6.9793e-01,  2.7117e-01,  6.0299e-01,
         3.9695e-01,  6.9511e-01,  5.5215e-01,  4.1964e-01,  1.7621e-01,
         5.3468e-01,  5.5374e-01,  6.0615e-01,  4.9721e-01,  1.9013e-01,
         1.2264e-01,  1.9834e-01,  1.1484e-03,  4.4484e-01,  4.2121e-01,
         4.5645e-01,  3.7616e-01,  2.6834e-01,  3.2508e-01,  2.7314e-01,
        -9.1372e-02,  3.3254e-01,  3.4718e-01,  4.5489e-01,  6.2014e-02,
         1.1651e-01,  1.6829e-01,  4.1089e-01,  4.3073e-01,  1.7817e-01,
        -2.6770e-02,  2.3622e-01,  4.1513e-01,  3.1425e-01,  1.3575e-01,
         1.9436e-01,  1.6092e-01,  1.8404e-01,  1.8498e-01, -1.2434e-02,
         3.6769e-01,  3.7399e-01,  2.8255e-01,  1.6096e-01,  3.0568e-01,
         2.8980e-01,  2.6209e-01,  2.0133e-01,  3.0869e-01,  2.3534e-01,
         1.6007e-01,  1.1007e-02,  5.6104e-03,  2.4138e-01,  1.7436e-01,
         1.8397e-01, -3.4031e-01,  2.5878e-01,  4.8018e-01,  3.0264e-01,
         3.7919e-01,  3.3660e-01,  2.8759e-01,  4.6878e-01,  2.7343e-01,
         3.6908e-01,  1.4228e-01,  2.8325e-01,  3.5191e-01,  4.0065e-01,
        -4.5059e-03,  8.3207e-02, -8.7192e-02, -1.2580e-01,  2.9184e-01,
        -1.6181e-01,  3.0833e-01, -1.2660e-01,  2.5827e-01,  2.7026e-01,
         2.3245e-01,  2.6297e-01,  8.7019e-02,  8.5656e-02,  2.3972e-01,
         1.8076e-01,  3.1205e-01, -7.4187e-02,  6.2446e-02,  1.6779e-01,
         1.2321e-01, -1.1395e-01,  2.7949e-01,  2.0937e-01,  3.2113e-01,
         5.0760e-02,  3.9525e-02,  1.3206e-01,  4.7054e-02, -3.6394e-01,
         7.5008e-02, -4.5086e-02,  7.4406e-02, -7.4163e-02,  8.3362e-02,
         1.8088e-02, -2.5368e-01, -4.9909e-01, -1.6491e-01, -4.6897e-01,
        -2.5380e-01, -3.4433e-01, -1.8929e-01,  5.6833e-02, -9.3785e-02,
         3.1891e-02, -3.6694e-01, -4.9563e-01, -1.6623e-01,  9.0946e-02,
        -3.8864e-01, -4.2710e-01, -4.2494e-01, -2.9563e-01, -2.1101e-01,
        -1.5860e-01, -3.3097e-01, -6.4634e-01, -1.7681e-01, -2.2553e-01,
        -2.9207e-01, -1.9666e-01, -2.4527e-01, -2.6588e-01, -3.7955e-01,
        -4.0129e-01, -6.1591e-01, -8.4102e-01, -5.1947e-01, -6.5562e-01,
        -5.2564e-01, -4.3941e-01, -4.3147e-01, -1.0837e+00, -8.8541e-01,
        -5.3800e-01, -6.0762e-01, -1.0687e+00, -7.2305e-01, -8.0424e-01,
        -7.0507e-01, -6.5566e-01, -1.0917e+00, -9.8790e-01, -9.4233e-01,
        -9.1588e-01, -7.6133e-01, -8.4941e-01, -8.9532e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1963e+00, -1.0194e+00, -1.4217e+00, -2.4061e+00, -2.4716e+00,
        -2.9507e+00, -2.6838e+00, -2.4896e+00, -2.8747e+00, -2.9300e+00,
        -2.5129e+00, -2.6780e+00, -2.3367e+00, -2.6797e+00, -2.6975e+00,
        -2.4197e+00, -2.3962e+00, -2.6071e+00, -2.4869e+00, -2.3639e+00,
        -2.2387e+00, -2.1765e+00, -8.4353e-01, -1.7837e-01, -1.6146e+00,
        -1.8046e+00, -1.7542e+00, -1.6884e+00, -1.6047e+00, -1.6892e+00,
        -1.9499e+00, -1.8500e+00, -1.5878e+00, -1.8893e+00, -1.5833e+00,
        -1.5986e+00, -1.4034e+00, -1.8619e+00, -1.4925e+00, -1.6242e+00,
        -1.5364e+00, -1.4431e+00, -1.6272e+00, -1.6350e+00, -1.7528e+00,
        -1.4242e+00, -1.6080e+00, -1.5173e+00, -1.4623e+00, -1.7724e+00,
        -1.4971e+00, -1.4934e+00, -1.5847e+00, -1.8930e+00, -1.2075e+00,
        -1.3796e+00, -1.2712e+00, -1.6416e+00, -1.5755e+00, -1.3842e+00,
        -1.2284e+00, -1.5432e+00, -1.1263e+00, -1.1899e+00, -1.5804e+00,
        -1.1354e+00,  1.0723e+00,  1.2241e+00,  2.1976e+00,  2.6306e+00,
         5.1175e+00,  1.0508e-02,  2.8179e-01,  1.6487e-01, -6.8058e-02,
         1.7201e-01,  2.2402e-01,  2.8110e-01,  3.2790e-01,  3.0454e-01,
         2.7420e-01,  4.2211e-01,  3.3607e-01,  3.1642e-01,  5.1020e-01,
         1.3429e-01, -3.0814e-02,  2.2319e-01,  5.1792e-01,  2.8252e-01,
         1.7819e-01,  1.3558e-01,  2.3799e-01,  2.6955e-01,  3.4484e-01,
         1.0817e-01,  2.2368e-01,  2.9877e-01,  3.0156e-01,  2.1419e-01,
         2.7861e-01,  3.7670e-01,  2.3004e-01,  3.1726e-01, -1.0590e-01,
         7.2665e-02, -1.0208e-01,  4.0302e-02,  1.2488e-01,  1.9762e-01,
         2.7180e-01,  3.2191e-01,  1.8453e-01,  1.6556e+00,  2.5347e+00,
         3.2778e+00,  7.6469e-01,  9.4924e-01,  9.3519e-01,  9.8493e-01,
         9.6505e-01,  9.3428e-01,  8.7074e-01,  1.0043e+00,  8.1503e-01,
         9.0148e-01,  7.3668e-01,  7.5699e-01,  7.8915e-01,  6.8901e-01,
         6.0361e-01,  8.3568e-01,  5.2605e-01,  9.7396e-01,  2.9567e-01,
         8.3630e-01,  7.0098e-01,  9.0370e-01,  9.6241e-01,  8.5983e-01,
         1.0021e+00,  9.0231e-01,  6.1451e-01,  9.7578e-01,  1.0360e+00,
         9.4018e-01,  5.4214e-01,  8.9849e-01,  9.3389e-01,  1.1895e+00,
         5.4223e-01,  9.6377e-01,  6.7459e-01,  9.0219e-01,  1.0749e+00,
         4.8250e-01,  7.9096e-01,  8.3500e-01,  8.2686e-01,  8.7290e-01,
         6.9199e-01,  5.3225e-01,  9.6597e-01,  7.9525e-01,  5.1835e-01,
         8.7379e-01,  7.3514e-01,  6.3868e-01,  7.9017e-01,  9.3827e-01,
         8.9059e-01,  6.2785e-01,  7.3278e-01,  8.5312e-01,  8.5505e-01,
         1.0239e+00,  9.9056e-01,  9.4824e-01,  8.9722e-01,  1.0343e+00,
         9.2781e-01,  7.2568e-01,  3.5565e-01,  5.1530e-01,  8.3089e-01,
         9.3943e-01,  1.0602e+00,  9.0261e-01,  6.7472e-01,  5.9375e-01,
         7.0063e-01,  4.0160e-01,  4.2638e-01,  7.6192e-01,  5.5957e-01,
         6.6288e-01,  7.8263e-01,  6.4328e-01,  4.1905e-01,  6.4415e-01,
         3.2553e-01,  7.9443e-01,  6.9329e-01,  9.2575e-01,  8.0055e-01,
         7.4055e-01,  6.9811e-01,  3.1723e-01,  6.6642e-01,  7.7481e-01,
         5.7446e-01,  6.2024e-01,  7.5568e-01,  6.1882e-01,  5.4502e-01,
         6.0669e-01,  6.6636e-01,  6.7399e-01,  5.0063e-01,  5.9238e-01,
         7.8054e-01,  6.1450e-01,  6.8960e-01,  6.0933e-01,  7.6051e-01,
         5.1655e-01,  5.4049e-01,  6.1522e-01,  6.3803e-01,  7.3494e-01,
         6.6141e-01,  5.6258e-01,  2.1162e-01,  6.7715e-01,  3.1385e-01,
         6.9099e-01,  5.9519e-01,  6.8769e-01,  7.2676e-01,  8.0594e-01,
         7.2603e-01,  6.5544e-01,  6.9793e-01,  2.7117e-01,  6.0299e-01,
         3.9695e-01,  6.9511e-01,  5.5215e-01,  4.1964e-01,  1.7621e-01,
         5.3468e-01,  5.5374e-01,  6.0615e-01,  4.9721e-01,  1.9013e-01,
         1.2264e-01,  1.9834e-01,  1.1484e-03,  4.4484e-01,  4.2121e-01,
         4.5645e-01,  3.7616e-01,  2.6834e-01,  3.2508e-01,  2.7314e-01,
        -9.1372e-02,  3.3254e-01,  3.4718e-01,  4.5489e-01,  6.2014e-02,
         1.1651e-01,  1.6829e-01,  4.1089e-01,  4.3073e-01,  1.7817e-01,
        -2.6770e-02,  2.3622e-01,  4.1513e-01,  3.1425e-01,  1.3575e-01,
         1.9436e-01,  1.6092e-01,  1.8404e-01,  1.8498e-01, -1.2434e-02,
         3.6769e-01,  3.7399e-01,  2.8255e-01,  1.6096e-01,  3.0568e-01,
         2.8980e-01,  2.6209e-01,  2.0133e-01,  3.0869e-01,  2.3534e-01,
         1.6007e-01,  1.1007e-02,  5.6104e-03,  2.4138e-01,  1.7436e-01,
         1.8397e-01, -3.4031e-01,  2.5878e-01,  4.8018e-01,  3.0264e-01,
         3.7919e-01,  3.3660e-01,  2.8759e-01,  4.6878e-01,  2.7343e-01,
         3.6908e-01,  1.4228e-01,  2.8325e-01,  3.5191e-01,  4.0065e-01,
        -4.5059e-03,  8.3207e-02, -8.7192e-02, -1.2580e-01,  2.9653e-01,
        -1.6181e-01,  3.0833e-01, -1.2660e-01,  2.5827e-01,  2.7026e-01,
         2.3245e-01,  2.6157e-01,  8.7019e-02,  8.5656e-02,  2.3972e-01,
         1.8076e-01,  3.1205e-01, -7.4187e-02,  6.2446e-02,  1.6779e-01,
         1.2321e-01, -1.1395e-01,  2.7949e-01,  2.0937e-01,  3.2113e-01,
         5.0760e-02,  3.9525e-02,  1.3206e-01,  4.7054e-02, -3.6394e-01,
         7.5008e-02, -4.5086e-02,  7.4406e-02, -7.4163e-02,  8.3362e-02,
         1.8088e-02, -2.5368e-01, -4.9909e-01, -1.6491e-01, -4.6897e-01,
        -2.5380e-01, -3.4433e-01, -1.8929e-01,  5.6833e-02, -9.3785e-02,
         3.1891e-02, -3.6694e-01, -4.9563e-01, -1.6623e-01,  9.0946e-02,
        -3.8731e-01, -4.2710e-01, -4.2494e-01, -2.9563e-01, -2.1101e-01,
        -1.5860e-01, -3.3097e-01, -6.4634e-01, -1.7681e-01, -2.2553e-01,
        -2.9207e-01, -1.9666e-01, -2.4527e-01, -2.6588e-01, -3.7955e-01,
        -4.0129e-01, -6.1591e-01, -8.4102e-01, -5.1947e-01, -6.5562e-01,
        -5.2564e-01, -4.3941e-01, -4.2334e-01, -1.0837e+00, -8.8541e-01,
        -5.3800e-01, -6.0762e-01, -1.0687e+00, -7.2305e-01, -8.0424e-01,
        -7.0507e-01, -6.5566e-01, -1.0917e+00, -9.8790e-01, -9.4233e-01,
        -9.1588e-01, -7.6133e-01, -8.4941e-01, -8.9532e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808284
t6: 1641198808284
state_values: tensor([1.0527, 0.1048, 0.1449, 0.3548, 0.6794, 1.0230, 1.1961, 1.2933, 1.4104,
        1.6134, 2.1642, 1.8789, 1.8788, 1.9056, 2.2092, 3.3988, 3.0292, 2.7085,
        3.0536, 3.1905, 3.4415, 3.4023, 2.1282, 1.6095, 2.5389, 2.6028, 2.6445,
        3.3105, 2.9411, 2.9082, 4.4604, 4.0996, 4.2221, 3.7154, 3.4792, 3.4054,
        3.4010, 3.4369, 3.6798, 3.5045, 3.6864, 3.6201, 3.6179, 4.4469, 3.9482,
        3.8318, 3.7438, 4.2653, 3.9058, 3.8592, 3.8631, 4.2490, 3.9611, 5.0012,
        4.4229, 4.2163, 4.4459, 5.5338, 4.7500, 5.3143, 4.6718, 4.5514, 4.3793,
        4.5935, 4.4924, 4.4916, 3.0960, 2.6948, 2.2766, 2.2722, 1.6266, 3.0110,
        3.6294, 3.7327, 3.8994, 4.6608, 4.5167, 4.5699, 4.5107, 4.6327, 4.4713,
        4.4239, 4.5454, 4.4729, 4.7094, 5.5950, 4.9952, 4.7920, 5.0498, 5.2956,
        4.9488, 5.4711, 5.1301, 4.9482, 4.8327, 4.8653, 5.4900, 5.2437, 5.1184,
        5.4756, 5.3225, 5.0559, 5.0139, 4.9942, 5.0365, 5.0200, 5.0637, 5.0766,
        5.0598, 5.0466, 5.3455, 5.6066, 5.6151, 4.4067, 3.6853, 3.1309, 4.2300,
        4.9528, 4.8427, 5.0383, 5.0131, 4.9753, 5.0302, 5.0202, 5.6670, 5.3271,
        6.0254, 5.5590, 5.3811, 5.3873, 6.5361, 6.0273, 5.7052, 5.8387, 7.3260,
        6.4258, 6.8879, 6.2817, 6.2391, 6.3784, 5.9824, 5.8229, 5.7989, 5.8300,
        5.9003, 5.8790, 5.7964, 5.7295, 6.2476, 5.8631, 7.1531, 6.3428, 6.1039,
        5.9240, 5.8458, 7.2555, 6.7936, 6.8007, 6.2940, 6.1033, 6.0557, 6.0840,
        5.9364, 6.0480, 5.9768, 6.0409, 5.9977, 6.5195, 6.1220, 5.9636, 5.9852,
        5.9985, 5.9807, 6.1967, 6.0120, 5.8981, 5.8914, 6.1043, 6.0697, 5.9920,
        5.9051, 6.3171, 6.1514, 6.1006, 5.9958, 6.0161, 5.8991, 6.4093, 7.0635,
        7.2270, 6.5738, 6.4284, 6.3146, 6.7354, 6.4508, 7.2103, 7.0384, 7.1985,
        6.7710, 6.9541, 6.7173, 6.4967, 6.4473, 6.4911, 6.7789, 7.0487, 6.9070,
        6.7221, 6.5362, 6.8553, 7.4597, 6.8163, 6.7072, 7.1014, 7.3467, 6.8381,
        6.6589, 6.5489, 6.5636, 7.0465, 6.6519, 7.2697, 7.1080, 6.8762, 6.6305,
        6.5876, 6.5708, 7.2787, 6.7722, 6.8542, 6.8795, 6.9778, 6.7947, 6.8682,
        6.7301, 6.7397, 6.6094, 6.5250, 6.6292, 6.4532, 6.4401, 6.5419, 6.5630,
        6.6002, 6.4794, 6.5395, 6.4583, 6.8319, 6.6569, 6.6785, 6.9320, 7.0725,
        6.9678, 7.0938, 7.9492, 7.2527, 8.2126, 7.4462, 7.3180, 7.3652, 7.3965,
        7.5976, 8.0570, 7.6298, 7.6613, 7.3781, 7.6198, 7.2153, 7.4088, 7.1786,
        8.1438, 7.3849, 7.2614, 7.2808, 8.1450, 7.5014, 7.2369, 7.2586, 7.0733,
        8.2208, 7.8859, 8.1969, 7.6007, 7.3120, 7.1983, 7.0701, 7.0111, 6.9933,
        6.9952, 7.2727, 7.0608, 7.2847, 7.0863, 7.4761, 7.4891, 7.1873, 7.1340,
        7.0751, 6.9701, 7.0061, 6.9900, 7.5272, 7.7384, 7.3015, 7.1750, 7.0863,
        7.0777, 7.7860, 7.4219, 8.0466, 7.6068, 8.3133, 7.7929, 7.5196, 7.6051,
        8.7031, 7.8160, 7.5552, 7.4569, 7.7393, 7.5247, 7.7273, 7.4974, 7.3269,
        7.2362, 7.2049, 7.1552, 8.1041, 7.5565, 8.1536, 7.5792, 7.3171, 7.3816,
        7.3071, 7.7373, 7.4097, 7.3498, 7.2085, 7.2867, 7.2727, 7.9154, 8.0177,
        7.6337, 7.6096, 7.5155, 7.3519, 8.2152, 7.6395, 7.4736, 7.5006, 7.4415,
        8.3374, 7.8014, 8.4263, 9.2697, 8.2006, 7.9114, 7.7285, 7.5572, 7.5887,
        7.4159, 8.6314, 7.9539, 7.6189, 7.3824, 7.4696, 8.6856, 8.8640, 8.2246,
        8.3110, 7.9314, 7.7360, 7.7024, 7.5921, 7.5076, 7.4706, 7.8082, 7.7169,
        7.7068, 7.9606, 7.6641, 8.7655, 8.0826, 7.7968, 7.7200, 7.6359, 7.7737,
        7.8555, 9.3037, 8.2993, 8.2219, 8.2566, 8.0036, 8.6587, 9.0181, 8.5646,
        8.1685, 8.0538, 8.8925, 8.9010, 8.7324, 8.1432, 8.3685, 8.2138],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808288
t8: 1641198808288
t9: 1641198808289
t10: 1641198808299
t11: 1641198808300
t12: 1641198808300
t1: 1641198808300
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808311
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0102, 1.0303, 0.9717, 0.9750, 0.9076, 1.0022, 0.9699, 0.8934, 1.0163,
        1.0879, 0.9624, 1.0071, 0.9546, 1.0319, 1.1195, 0.9226, 1.0178, 0.9991,
        1.0079, 1.0011, 0.9914, 1.0062, 0.9998, 0.9974, 0.9954, 1.0002, 1.0427,
        1.0001, 1.0022, 1.0491, 0.9338, 1.0127, 0.9696, 0.9992, 0.9932, 1.0018,
        0.9589, 1.0255, 0.9816, 1.0204, 1.0075, 1.0013, 1.0654, 1.0105, 1.0035,
        0.9925, 1.0378, 0.9876, 0.9608, 1.0043, 1.0330, 0.9790, 1.0251, 1.0037,
        0.9763, 1.0335, 1.0782, 1.1131, 0.9949, 0.9962, 1.0092, 1.0000, 1.0096,
        0.9645, 1.0093, 0.9729, 0.9480, 0.9639, 1.0210, 1.0588, 1.1096, 1.0011,
        0.9909, 0.9606, 1.0549, 1.0033, 1.0077, 1.0021, 1.0082, 0.9913, 0.9998,
        1.0046, 0.9860, 1.0303, 1.0545, 1.0284, 0.9966, 1.0265, 1.0087, 1.0003,
        1.0128, 0.9997, 0.9985, 0.9926, 0.9706, 1.0522, 1.0034, 1.0011, 1.0179,
        1.0036, 0.9967, 0.9834, 0.9984, 0.9237, 0.9565, 0.9144, 0.9523, 0.9566,
        0.9658, 1.0392, 1.0581, 1.0175, 0.9825, 0.9660, 0.9623, 0.9155, 1.0446,
        0.9788, 1.0166, 1.0000, 0.9890, 0.9940, 0.9922, 1.0553, 0.9918, 1.0405,
        0.9935, 0.9914, 0.9704, 1.0775, 0.9917, 0.9999, 1.0052, 1.0555, 0.9819,
        0.9862, 1.0007, 1.0019, 1.0108, 0.9999, 0.9991, 0.9704, 1.0043, 1.0137,
        1.0083, 0.9312, 0.9714, 1.0644, 0.9978, 1.0728, 0.9976, 1.0080, 0.9958,
        0.9944, 1.0851, 0.9727, 0.9854, 1.0008, 1.0006, 0.9880, 0.9665, 0.9966,
        1.0060, 0.9185, 1.0124, 0.9692, 1.0499, 0.9742, 0.9888, 1.0015, 0.9602,
        0.9702, 1.0282, 0.9878, 0.9991, 0.9968, 1.0230, 1.0030, 1.0106, 0.9824,
        1.0303, 0.9299, 0.9572, 0.9770, 1.0046, 0.9918, 1.0495, 1.0654, 0.9882,
        1.0053, 1.0043, 0.9525, 1.0381, 0.9655, 1.0550, 0.9979, 0.9969, 1.0134,
        1.0015, 0.9936, 0.9993, 0.9833, 1.0132, 1.0240, 1.0278, 1.0058, 0.9905,
        0.9872, 1.0299, 1.0388, 1.0016, 1.0018, 1.0164, 1.0128, 0.9977, 0.9977,
        0.9882, 0.9670, 1.0421, 0.9947, 1.0430, 1.0063, 1.0001, 0.9994, 0.9812,
        0.9717, 1.0655, 0.9901, 1.0124, 1.0054, 1.0093, 0.9595, 1.0182, 0.9490,
        1.0114, 0.9736, 0.9763, 1.0117, 0.9911, 0.9929, 1.0012, 1.0104, 0.9447,
        0.9818, 0.9520, 0.9864, 1.0364, 0.9787, 0.9330, 1.0319, 1.0321, 1.0141,
        1.0161, 1.0462, 1.0089, 1.0153, 1.0359, 0.9968, 0.9990, 1.0017, 1.0011,
        1.0028, 0.9943, 0.9902, 1.0165, 1.0130, 0.9931, 1.0205, 0.9814, 1.0546,
        1.0127, 1.0027, 1.0054, 1.0360, 1.0223, 0.9981, 1.0080, 0.9806, 1.0778,
        0.9806, 0.9890, 1.0015, 1.0069, 0.9744, 0.9926, 0.9935, 0.9844, 0.9586,
        1.0325, 0.9864, 1.0232, 0.9736, 1.0465, 1.0175, 0.9876, 0.9734, 0.9582,
        0.9835, 0.9956, 0.9960, 0.9612, 1.0529, 0.9983, 0.9726, 0.9806, 0.9719,
        1.0690, 1.0053, 1.0387, 1.0007, 1.0021, 0.9992, 0.9990, 0.9998, 1.0018,
        1.0132, 1.0284, 0.9801, 1.0280, 0.9314, 1.0327, 0.9398, 0.9931, 0.9756,
        0.9731, 0.9622, 1.0916, 0.9884, 1.0515, 1.0028, 0.9969, 0.9793, 0.9734,
        1.0424, 0.9769, 0.9431, 0.9892, 1.0013, 1.0086, 1.0548, 1.0254, 1.0011,
        1.0020, 0.9543, 0.9870, 1.0711, 0.9987, 0.9888, 1.0066, 0.9993, 1.0478,
        0.9776, 1.0291, 1.0015, 1.0275, 1.0240, 0.9973, 1.0005, 0.9999, 0.9953,
        1.0727, 1.0401, 0.9953, 0.9971, 0.9695, 1.0861, 0.9571, 0.9960, 0.9844,
        1.0002, 1.0034, 0.9654, 1.0002, 0.9827, 0.9762, 1.0415, 1.0083, 1.0102,
        1.0207, 0.9959, 1.0674, 1.0909, 0.9977, 0.9756, 0.9808, 1.0156, 1.0216,
        1.0856, 1.1040, 0.9963, 0.9962, 1.0008, 1.0308, 0.9978, 0.9828, 0.9967,
        1.0036, 1.0261, 1.0052, 0.9916, 1.0048, 1.0027, 0.9998, 1.0241],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808315
t4: 1641198808315
surr1, surr2: tensor([-3.1938e+00, -1.0107e+00, -1.4361e+00, -2.4134e+00, -2.4925e+00,
        -2.9508e+00, -2.6994e+00, -2.4713e+00, -2.8707e+00, -2.8977e+00,
        -2.5209e+00, -2.6759e+00, -2.3762e+00, -2.6680e+00, -2.7453e+00,
        -2.4031e+00, -2.4386e+00, -2.5865e+00, -2.4303e+00, -2.3160e+00,
        -2.1982e+00, -2.1987e+00, -8.4436e-01, -1.7614e-01, -1.6277e+00,
        -1.8047e+00, -1.7325e+00, -1.6900e+00, -1.6035e+00, -1.6247e+00,
        -1.9212e+00, -1.7852e+00, -1.6830e+00, -1.9012e+00, -1.6091e+00,
        -1.5977e+00, -1.4269e+00, -1.8557e+00, -1.5071e+00, -1.6198e+00,
        -1.5325e+00, -1.4429e+00, -1.6022e+00, -1.6354e+00, -1.7508e+00,
        -1.4330e+00, -1.5928e+00, -1.5352e+00, -1.4754e+00, -1.7711e+00,
        -1.4826e+00, -1.5111e+00, -1.5788e+00, -1.8903e+00, -1.2164e+00,
        -1.3720e+00, -1.2460e+00, -1.6612e+00, -1.5606e+00, -1.3961e+00,
        -1.2559e+00, -1.5433e+00, -1.1216e+00, -1.2113e+00, -1.5784e+00,
        -1.1470e+00,  1.0857e+00,  1.2315e+00,  2.1887e+00,  2.5882e+00,
         5.1623e+00,  1.0486e-02,  2.8508e-01,  1.6682e-01, -6.7355e-02,
         1.7173e-01,  2.2318e-01,  2.8050e-01,  3.2644e-01,  3.0636e-01,
         2.7426e-01,  4.2161e-01,  3.3775e-01,  3.1457e-01,  4.9468e-01,
         1.3412e-01, -3.0877e-02,  2.2017e-01,  5.1059e-01,  2.8511e-01,
         1.7691e-01,  1.3576e-01,  2.3829e-01,  2.7058e-01,  3.4805e-01,
         1.0715e-01,  2.2334e-01,  2.9865e-01,  2.9883e-01,  2.1336e-01,
         2.7918e-01,  3.7895e-01,  2.3017e-01,  3.2392e-01, -1.0656e-01,
         7.3717e-02, -1.0270e-01,  4.0560e-02,  1.2551e-01,  1.9663e-01,
         2.6791e-01,  3.2019e-01,  1.8595e-01,  1.6735e+00,  2.5607e+00,
         3.3342e+00,  7.6057e-01,  9.5729e-01,  9.3302e-01,  9.8527e-01,
         9.6792e-01,  9.3576e-01,  8.7249e-01,  9.9314e-01,  8.1805e-01,
         8.9272e-01,  7.3973e-01,  7.6077e-01,  7.9908e-01,  6.7766e-01,
         6.0344e-01,  8.5296e-01,  5.2404e-01,  9.3455e-01,  2.9468e-01,
         8.0587e-01,  7.0075e-01,  8.8998e-01,  9.3447e-01,  8.6151e-01,
         1.0069e+00,  9.1540e-01,  6.1409e-01,  9.7214e-01,  1.0334e+00,
         9.6194e-01,  5.4417e-01,  8.8921e-01,  9.3468e-01,  1.1695e+00,
         5.4187e-01,  9.7405e-01,  6.7821e-01,  9.0446e-01,  1.0604e+00,
         4.7890e-01,  7.7697e-01,  8.5320e-01,  8.2837e-01,  8.8158e-01,
         6.9906e-01,  5.3269e-01,  9.6497e-01,  8.1160e-01,  5.1766e-01,
         8.8154e-01,  7.3027e-01,  6.4622e-01,  7.9166e-01,  9.3825e-01,
         8.9958e-01,  6.3111e-01,  7.2950e-01,  8.5662e-01,  8.5543e-01,
         1.0251e+00,  9.8598e-01,  9.4759e-01,  8.9477e-01,  1.0415e+00,
         9.2179e-01,  7.4980e-01,  3.5796e-01,  5.1747e-01,  8.3052e-01,
         9.4155e-01,  1.0513e+00,  8.8157e-01,  6.7513e-01,  5.9587e-01,
         7.0924e-01,  4.1271e-01,  4.2244e-01,  7.7904e-01,  5.5459e-01,
         6.6239e-01,  7.7397e-01,  6.5742e-01,  4.1759e-01,  6.6581e-01,
         3.2560e-01,  7.9892e-01,  6.9176e-01,  9.1807e-01,  7.8836e-01,
         7.3709e-01,  7.0638e-01,  3.1834e-01,  6.6139e-01,  7.5659e-01,
         5.7579e-01,  6.1988e-01,  7.4622e-01,  6.0902e-01,  5.4873e-01,
         6.0729e-01,  6.6983e-01,  6.8004e-01,  4.9708e-01,  5.9419e-01,
         7.7407e-01,  6.1169e-01,  6.8936e-01,  6.0963e-01,  7.6780e-01,
         5.2014e-01,  5.3385e-01,  6.1600e-01,  6.3612e-01,  7.3137e-01,
         6.5810e-01,  5.7812e-01,  2.1099e-01,  6.9563e-01,  3.1342e-01,
         6.9713e-01,  5.9759e-01,  6.8642e-01,  7.2876e-01,  8.0727e-01,
         7.2607e-01,  6.5412e-01,  7.1009e-01,  2.7200e-01,  6.0933e-01,
         3.9794e-01,  6.9082e-01,  5.5631e-01,  4.2484e-01,  1.7538e-01,
         5.2929e-01,  5.5082e-01,  6.0272e-01,  4.8643e-01,  1.9003e-01,
         1.2124e-01,  2.0521e-01,  1.1384e-03,  4.4042e-01,  4.1796e-01,
         4.5072e-01,  3.6738e-01,  2.6688e-01,  3.2341e-01,  2.8849e-01,
        -9.1009e-02,  3.3743e-01,  3.4602e-01,  4.6753e-01,  6.1715e-02,
         1.1711e-01,  1.6815e-01,  4.0921e-01,  4.2315e-01,  1.8103e-01,
        -2.6808e-02,  2.3534e-01,  4.1970e-01,  3.0983e-01,  1.3472e-01,
         1.9123e-01,  1.6187e-01,  1.8565e-01,  1.8899e-01, -1.2459e-02,
         3.6836e-01,  3.7525e-01,  2.8478e-01,  1.6021e-01,  3.0726e-01,
         2.8874e-01,  2.6462e-01,  1.9979e-01,  3.0611e-01,  2.3691e-01,
         1.6126e-01,  1.1118e-02,  5.6290e-03,  2.4167e-01,  1.7454e-01,
         1.8545e-01, -3.3758e-01,  2.5907e-01,  4.8343e-01,  3.0397e-01,
         3.8166e-01,  3.3266e-01,  2.8740e-01,  4.6258e-01,  2.7327e-01,
         3.6393e-01,  1.4196e-01,  2.8306e-01,  3.4940e-01,  3.8763e-01,
        -4.5979e-03,  8.6208e-02, -8.9578e-02, -1.2496e-01,  3.0689e-01,
        -1.6128e-01,  3.1994e-01, -1.2669e-01,  2.5965e-01,  2.7146e-01,
         2.3392e-01,  2.5958e-01,  8.6841e-02,  8.4764e-02,  2.3983e-01,
         1.8135e-01,  3.1614e-01, -7.4693e-02,  6.1928e-02,  1.6977e-01,
         1.2439e-01, -1.1417e-01,  2.7951e-01,  2.0908e-01,  3.1668e-01,
         5.0360e-02,  3.9496e-02,  1.3183e-01,  4.8688e-02, -3.6497e-01,
         7.3822e-02, -4.5080e-02,  7.4747e-02, -7.3992e-02,  8.3416e-02,
         1.7853e-02, -2.5789e-01, -4.9536e-01, -1.6404e-01, -4.7272e-01,
        -2.5512e-01, -3.4658e-01, -1.8928e-01,  5.6848e-02, -9.3939e-02,
         3.1287e-02, -3.6852e-01, -4.9770e-01, -1.6653e-01,  9.1994e-02,
        -3.8242e-01, -4.2511e-01, -4.2460e-01, -2.9265e-01, -2.1073e-01,
        -1.6003e-01, -3.3742e-01, -6.4640e-01, -1.7767e-01, -2.2657e-01,
        -2.9010e-01, -1.9600e-01, -2.4462e-01, -2.6378e-01, -3.8068e-01,
        -3.9513e-01, -6.2811e-01, -8.4542e-01, -5.2640e-01, -6.5861e-01,
        -5.2413e-01, -4.3666e-01, -4.1779e-01, -1.0935e+00, -8.8002e-01,
        -5.3043e-01, -6.2469e-01, -1.0594e+00, -7.0430e-01, -7.9835e-01,
        -7.0324e-01, -6.7397e-01, -1.0786e+00, -9.6307e-01, -9.2614e-01,
        -9.2276e-01, -7.5590e-01, -8.4480e-01, -8.7844e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1938e+00, -1.0107e+00, -1.4361e+00, -2.4134e+00, -2.4925e+00,
        -2.9508e+00, -2.6994e+00, -2.4896e+00, -2.8707e+00, -2.8977e+00,
        -2.5209e+00, -2.6759e+00, -2.3762e+00, -2.6680e+00, -2.6975e+00,
        -2.4031e+00, -2.4386e+00, -2.5865e+00, -2.4303e+00, -2.3160e+00,
        -2.1982e+00, -2.1987e+00, -8.4436e-01, -1.7614e-01, -1.6277e+00,
        -1.8047e+00, -1.7325e+00, -1.6900e+00, -1.6035e+00, -1.6247e+00,
        -1.9212e+00, -1.7852e+00, -1.6830e+00, -1.9012e+00, -1.6091e+00,
        -1.5977e+00, -1.4269e+00, -1.8557e+00, -1.5071e+00, -1.6198e+00,
        -1.5325e+00, -1.4429e+00, -1.6022e+00, -1.6354e+00, -1.7508e+00,
        -1.4330e+00, -1.5928e+00, -1.5352e+00, -1.4754e+00, -1.7711e+00,
        -1.4826e+00, -1.5111e+00, -1.5788e+00, -1.8903e+00, -1.2164e+00,
        -1.3720e+00, -1.2460e+00, -1.6416e+00, -1.5606e+00, -1.3961e+00,
        -1.2559e+00, -1.5433e+00, -1.1216e+00, -1.2113e+00, -1.5784e+00,
        -1.1470e+00,  1.0857e+00,  1.2315e+00,  2.1887e+00,  2.5882e+00,
         5.1175e+00,  1.0486e-02,  2.8508e-01,  1.6682e-01, -6.7355e-02,
         1.7173e-01,  2.2318e-01,  2.8050e-01,  3.2644e-01,  3.0636e-01,
         2.7426e-01,  4.2161e-01,  3.3775e-01,  3.1457e-01,  4.9468e-01,
         1.3412e-01, -3.0877e-02,  2.2017e-01,  5.1059e-01,  2.8511e-01,
         1.7691e-01,  1.3576e-01,  2.3829e-01,  2.7058e-01,  3.4805e-01,
         1.0715e-01,  2.2334e-01,  2.9865e-01,  2.9883e-01,  2.1336e-01,
         2.7918e-01,  3.7895e-01,  2.3017e-01,  3.2392e-01, -1.0656e-01,
         7.3717e-02, -1.0270e-01,  4.0560e-02,  1.2551e-01,  1.9663e-01,
         2.6791e-01,  3.2019e-01,  1.8595e-01,  1.6735e+00,  2.5607e+00,
         3.3342e+00,  7.6057e-01,  9.5729e-01,  9.3302e-01,  9.8527e-01,
         9.6792e-01,  9.3576e-01,  8.7249e-01,  9.9314e-01,  8.1805e-01,
         8.9272e-01,  7.3973e-01,  7.6077e-01,  7.9908e-01,  6.7766e-01,
         6.0344e-01,  8.5296e-01,  5.2404e-01,  9.3455e-01,  2.9468e-01,
         8.0587e-01,  7.0075e-01,  8.8998e-01,  9.3447e-01,  8.6151e-01,
         1.0069e+00,  9.1540e-01,  6.1409e-01,  9.7214e-01,  1.0334e+00,
         9.6194e-01,  5.4417e-01,  8.8921e-01,  9.3468e-01,  1.1695e+00,
         5.4187e-01,  9.7405e-01,  6.7821e-01,  9.0446e-01,  1.0604e+00,
         4.7890e-01,  7.7697e-01,  8.5320e-01,  8.2837e-01,  8.8158e-01,
         6.9906e-01,  5.3269e-01,  9.6497e-01,  8.1160e-01,  5.1766e-01,
         8.8154e-01,  7.3027e-01,  6.4622e-01,  7.9166e-01,  9.3825e-01,
         8.9958e-01,  6.3111e-01,  7.2950e-01,  8.5662e-01,  8.5543e-01,
         1.0251e+00,  9.8598e-01,  9.4759e-01,  8.9477e-01,  1.0415e+00,
         9.2179e-01,  7.4980e-01,  3.5796e-01,  5.1747e-01,  8.3052e-01,
         9.4155e-01,  1.0513e+00,  8.8157e-01,  6.7513e-01,  5.9587e-01,
         7.0924e-01,  4.1271e-01,  4.2244e-01,  7.7904e-01,  5.5459e-01,
         6.6239e-01,  7.7397e-01,  6.5742e-01,  4.1759e-01,  6.6581e-01,
         3.2560e-01,  7.9892e-01,  6.9176e-01,  9.1807e-01,  7.8836e-01,
         7.3709e-01,  7.0638e-01,  3.1834e-01,  6.6139e-01,  7.5659e-01,
         5.7579e-01,  6.1988e-01,  7.4622e-01,  6.0902e-01,  5.4873e-01,
         6.0729e-01,  6.6983e-01,  6.8004e-01,  4.9708e-01,  5.9419e-01,
         7.7407e-01,  6.1169e-01,  6.8936e-01,  6.0963e-01,  7.6780e-01,
         5.2014e-01,  5.3385e-01,  6.1600e-01,  6.3612e-01,  7.3137e-01,
         6.5810e-01,  5.7812e-01,  2.1099e-01,  6.9563e-01,  3.1342e-01,
         6.9713e-01,  5.9759e-01,  6.8642e-01,  7.2876e-01,  8.0727e-01,
         7.2607e-01,  6.5412e-01,  7.1009e-01,  2.7200e-01,  6.0933e-01,
         3.9794e-01,  6.9082e-01,  5.5631e-01,  4.2484e-01,  1.7538e-01,
         5.2929e-01,  5.5082e-01,  6.0272e-01,  4.8643e-01,  1.9003e-01,
         1.2124e-01,  2.0521e-01,  1.1384e-03,  4.4042e-01,  4.1796e-01,
         4.5072e-01,  3.6738e-01,  2.6688e-01,  3.2341e-01,  2.8849e-01,
        -9.1009e-02,  3.3743e-01,  3.4602e-01,  4.6753e-01,  6.1715e-02,
         1.1711e-01,  1.6815e-01,  4.0921e-01,  4.2315e-01,  1.8103e-01,
        -2.6808e-02,  2.3534e-01,  4.1970e-01,  3.0983e-01,  1.3472e-01,
         1.9123e-01,  1.6187e-01,  1.8565e-01,  1.8899e-01, -1.2459e-02,
         3.6836e-01,  3.7525e-01,  2.8478e-01,  1.6021e-01,  3.0726e-01,
         2.8874e-01,  2.6462e-01,  1.9979e-01,  3.0611e-01,  2.3691e-01,
         1.6126e-01,  1.1118e-02,  5.6290e-03,  2.4167e-01,  1.7454e-01,
         1.8545e-01, -3.3758e-01,  2.5907e-01,  4.8343e-01,  3.0397e-01,
         3.8166e-01,  3.3266e-01,  2.8740e-01,  4.6258e-01,  2.7327e-01,
         3.6393e-01,  1.4196e-01,  2.8306e-01,  3.4940e-01,  3.8763e-01,
        -4.5979e-03,  8.6208e-02, -8.9578e-02, -1.2496e-01,  3.0689e-01,
        -1.6128e-01,  3.1994e-01, -1.2669e-01,  2.5965e-01,  2.7146e-01,
         2.3392e-01,  2.5958e-01,  8.6841e-02,  8.4764e-02,  2.3983e-01,
         1.8135e-01,  3.1614e-01, -7.4693e-02,  6.1928e-02,  1.6977e-01,
         1.2439e-01, -1.1417e-01,  2.7951e-01,  2.0908e-01,  3.1668e-01,
         5.0360e-02,  3.9496e-02,  1.3183e-01,  4.8688e-02, -3.6497e-01,
         7.3822e-02, -4.5080e-02,  7.4747e-02, -7.3992e-02,  8.3416e-02,
         1.7853e-02, -2.5789e-01, -4.9536e-01, -1.6404e-01, -4.7272e-01,
        -2.5512e-01, -3.4658e-01, -1.8928e-01,  5.6848e-02, -9.3939e-02,
         3.1287e-02, -3.6852e-01, -4.9770e-01, -1.6653e-01,  9.1994e-02,
        -3.8242e-01, -4.2511e-01, -4.2460e-01, -2.9265e-01, -2.1073e-01,
        -1.6003e-01, -3.3742e-01, -6.4640e-01, -1.7767e-01, -2.2657e-01,
        -2.9010e-01, -1.9600e-01, -2.4462e-01, -2.6378e-01, -3.8068e-01,
        -3.9513e-01, -6.2811e-01, -8.4542e-01, -5.2640e-01, -6.5861e-01,
        -5.2413e-01, -4.3666e-01, -4.1779e-01, -1.0896e+00, -8.8002e-01,
        -5.3043e-01, -6.2469e-01, -1.0594e+00, -7.0430e-01, -7.9835e-01,
        -7.0324e-01, -6.7397e-01, -1.0786e+00, -9.6307e-01, -9.2614e-01,
        -9.2276e-01, -7.5590e-01, -8.4480e-01, -8.7844e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808325
t6: 1641198808325
state_values: tensor([6.4042e-01, 4.8574e-03, 3.5788e-02, 1.5796e-01, 3.0618e-01, 4.3872e-01,
        4.9296e-01, 5.1969e-01, 5.5804e-01, 6.0904e-01, 8.6932e-01, 6.9888e-01,
        6.7477e-01, 6.8266e-01, 8.0542e-01, 1.5647e+00, 1.2214e+00, 1.0229e+00,
        1.2027e+00, 1.2571e+00, 1.4021e+00, 1.3409e+00, 6.4138e-01, 4.5165e-01,
        8.3954e-01, 8.7208e-01, 8.8323e-01, 1.2453e+00, 1.0136e+00, 9.7867e-01,
        2.0512e+00, 1.6980e+00, 1.7594e+00, 1.4011e+00, 1.2537e+00, 1.2109e+00,
        1.2008e+00, 1.2296e+00, 1.3495e+00, 1.2454e+00, 1.3339e+00, 1.2772e+00,
        1.2774e+00, 1.8482e+00, 1.4769e+00, 1.3826e+00, 1.3384e+00, 1.6769e+00,
        1.4256e+00, 1.4049e+00, 1.3921e+00, 1.6383e+00, 1.4433e+00, 2.2059e+00,
        1.7295e+00, 1.5963e+00, 1.7385e+00, 2.5556e+00, 1.9550e+00, 2.3577e+00,
        1.8813e+00, 1.8095e+00, 1.6860e+00, 1.8167e+00, 1.7635e+00, 1.7362e+00,
        8.7976e-01, 7.1461e-01, 5.4688e-01, 5.4655e-01, 3.3483e-01, 9.0319e-01,
        1.2098e+00, 1.2638e+00, 1.3586e+00, 1.8555e+00, 1.7066e+00, 1.7298e+00,
        1.6778e+00, 1.7613e+00, 1.6726e+00, 1.6479e+00, 1.7177e+00, 1.6849e+00,
        1.8338e+00, 2.4929e+00, 2.0449e+00, 1.9051e+00, 2.0716e+00, 2.2416e+00,
        2.0054e+00, 2.3752e+00, 2.1218e+00, 2.0051e+00, 1.9315e+00, 1.9673e+00,
        2.4027e+00, 2.1886e+00, 2.1068e+00, 2.3636e+00, 2.2309e+00, 2.0643e+00,
        2.0517e+00, 2.0420e+00, 2.0894e+00, 2.0790e+00, 2.1180e+00, 2.1269e+00,
        2.1138e+00, 2.1042e+00, 2.2932e+00, 2.4674e+00, 2.4552e+00, 1.5928e+00,
        1.1546e+00, 8.6618e-01, 1.5571e+00, 2.0323e+00, 1.9465e+00, 2.0534e+00,
        2.0388e+00, 2.0227e+00, 2.0657e+00, 2.0614e+00, 2.5128e+00, 2.2661e+00,
        2.7657e+00, 2.4270e+00, 2.3095e+00, 2.3270e+00, 3.1775e+00, 2.7404e+00,
        2.5379e+00, 2.6148e+00, 3.7615e+00, 3.0178e+00, 3.3717e+00, 2.9063e+00,
        2.8756e+00, 2.9845e+00, 2.7158e+00, 2.6200e+00, 2.6223e+00, 2.6262e+00,
        2.6574e+00, 2.6307e+00, 2.6100e+00, 2.5701e+00, 2.9294e+00, 2.6506e+00,
        3.6249e+00, 2.9758e+00, 2.8250e+00, 2.7076e+00, 2.6604e+00, 3.7181e+00,
        3.3103e+00, 3.3063e+00, 2.9569e+00, 2.8352e+00, 2.8155e+00, 2.8483e+00,
        2.7371e+00, 2.7948e+00, 2.7693e+00, 2.7819e+00, 2.7754e+00, 3.1424e+00,
        2.8560e+00, 2.7495e+00, 2.7607e+00, 2.7848e+00, 2.7765e+00, 2.9034e+00,
        2.7820e+00, 2.7045e+00, 2.7044e+00, 2.8314e+00, 2.8024e+00, 2.7372e+00,
        2.6985e+00, 2.9819e+00, 2.8835e+00, 2.8542e+00, 2.7801e+00, 2.7845e+00,
        2.7067e+00, 3.0624e+00, 3.5378e+00, 3.6332e+00, 3.1606e+00, 3.0771e+00,
        3.0141e+00, 3.2993e+00, 3.1035e+00, 3.6545e+00, 3.4939e+00, 3.6072e+00,
        3.3161e+00, 3.4343e+00, 3.2905e+00, 3.1327e+00, 3.1087e+00, 3.1077e+00,
        3.3104e+00, 3.5034e+00, 3.3819e+00, 3.2815e+00, 3.1576e+00, 3.3681e+00,
        3.8144e+00, 3.3481e+00, 3.2588e+00, 3.5433e+00, 3.7180e+00, 3.3612e+00,
        3.2446e+00, 3.1766e+00, 3.1989e+00, 3.5282e+00, 3.2434e+00, 3.6860e+00,
        3.5428e+00, 3.3768e+00, 3.2181e+00, 3.2060e+00, 3.2031e+00, 3.7046e+00,
        3.3346e+00, 3.3705e+00, 3.3772e+00, 3.4454e+00, 3.3482e+00, 3.3767e+00,
        3.3071e+00, 3.2848e+00, 3.2166e+00, 3.1652e+00, 3.2145e+00, 3.1040e+00,
        3.1023e+00, 3.1688e+00, 3.1645e+00, 3.2179e+00, 3.1326e+00, 3.1853e+00,
        3.1248e+00, 3.3701e+00, 3.2563e+00, 3.2858e+00, 3.4395e+00, 3.5238e+00,
        3.4337e+00, 3.5250e+00, 4.1746e+00, 3.6664e+00, 4.3828e+00, 3.8213e+00,
        3.7010e+00, 3.7322e+00, 3.7482e+00, 3.8957e+00, 4.2375e+00, 3.9021e+00,
        3.9297e+00, 3.7650e+00, 3.9227e+00, 3.6498e+00, 3.7755e+00, 3.6361e+00,
        4.3465e+00, 3.7843e+00, 3.6773e+00, 3.6794e+00, 4.3315e+00, 3.8665e+00,
        3.6861e+00, 3.6753e+00, 3.5680e+00, 4.4200e+00, 4.1302e+00, 4.3546e+00,
        3.9320e+00, 3.7428e+00, 3.6822e+00, 3.5921e+00, 3.5506e+00, 3.5415e+00,
        3.5467e+00, 3.7170e+00, 3.5744e+00, 3.7133e+00, 3.5884e+00, 3.8531e+00,
        3.8416e+00, 3.6481e+00, 3.6262e+00, 3.5931e+00, 3.5187e+00, 3.5450e+00,
        3.5319e+00, 3.9414e+00, 4.0534e+00, 3.7333e+00, 3.6585e+00, 3.6019e+00,
        3.6022e+00, 4.1000e+00, 3.7980e+00, 4.2625e+00, 3.9120e+00, 4.4489e+00,
        4.0395e+00, 3.8628e+00, 3.9186e+00, 4.7551e+00, 4.1006e+00, 3.9348e+00,
        3.8816e+00, 4.0589e+00, 3.9282e+00, 4.0476e+00, 3.9051e+00, 3.7827e+00,
        3.7230e+00, 3.7036e+00, 3.6701e+00, 4.3515e+00, 3.9447e+00, 4.3641e+00,
        3.9511e+00, 3.7707e+00, 3.8342e+00, 3.7828e+00, 4.0704e+00, 3.8416e+00,
        3.8116e+00, 3.7084e+00, 3.7526e+00, 3.7261e+00, 4.1889e+00, 4.2443e+00,
        3.9582e+00, 3.9299e+00, 3.9070e+00, 3.7967e+00, 4.4176e+00, 3.9926e+00,
        3.8857e+00, 3.8766e+00, 3.8475e+00, 4.5012e+00, 4.1198e+00, 4.5624e+00,
        5.1884e+00, 4.3988e+00, 4.2085e+00, 4.0865e+00, 3.9638e+00, 3.9825e+00,
        3.8618e+00, 4.7561e+00, 4.2479e+00, 4.0081e+00, 3.8395e+00, 3.9155e+00,
        4.8064e+00, 4.8983e+00, 4.3950e+00, 4.4614e+00, 4.1963e+00, 4.0840e+00,
        4.0829e+00, 3.9995e+00, 3.9428e+00, 3.9191e+00, 4.1354e+00, 4.0432e+00,
        4.0258e+00, 4.2118e+00, 4.0157e+00, 4.8331e+00, 4.3369e+00, 4.1372e+00,
        4.0953e+00, 4.0385e+00, 4.1053e+00, 4.1493e+00, 5.2491e+00, 4.4976e+00,
        4.4137e+00, 4.4335e+00, 4.2887e+00, 4.7550e+00, 5.0067e+00, 4.6476e+00,
        4.3734e+00, 4.3266e+00, 4.9352e+00, 4.9151e+00, 4.7777e+00, 4.3783e+00,
        4.5287e+00, 4.4032e+00], device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808329
t8: 1641198808329
t9: 1641198808330
t10: 1641198808340
t11: 1641198808341
t12: 1641198808341
t1: 1641198808341
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808352
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0087, 1.0200, 0.9849, 0.9820, 0.9389, 1.0020, 0.9797, 0.9228, 1.0124,
        1.0582, 0.9835, 1.0046, 0.9797, 1.0201, 1.0519, 0.9259, 1.0511, 0.9801,
        0.9823, 0.9679, 0.9759, 1.0152, 1.0017, 0.9821, 1.0098, 1.0001, 1.0140,
        1.0023, 0.9991, 0.9760, 0.9312, 0.9815, 1.0140, 1.0042, 1.0042, 1.0003,
        0.9904, 1.0136, 0.9961, 1.0116, 1.0025, 1.0006, 1.0286, 1.0157, 0.9995,
        1.0021, 1.0139, 1.0052, 0.9895, 1.0021, 1.0152, 0.9965, 1.0125, 1.0008,
        0.9911, 1.0180, 1.0167, 1.1109, 0.9638, 1.0035, 1.0364, 0.9999, 1.0013,
        0.9976, 1.0047, 0.9901, 0.9739, 0.9785, 1.0124, 1.0238, 1.1072, 0.9931,
        1.0067, 0.9880, 1.0272, 0.9995, 0.9985, 0.9980, 0.9991, 1.0020, 1.0001,
        1.0018, 0.9950, 1.0153, 1.0080, 1.0319, 1.0055, 1.0010, 0.9851, 1.0164,
        0.9903, 1.0009, 1.0019, 1.0005, 0.9911, 1.0266, 0.9995, 0.9998, 0.9992,
        0.9968, 1.0011, 0.9971, 0.9995, 0.9652, 0.9746, 0.9447, 0.9680, 0.9704,
        0.9769, 1.0271, 1.0331, 1.0072, 0.9971, 0.9875, 0.9816, 0.9745, 1.0271,
        0.9915, 1.0095, 1.0002, 0.9952, 0.9972, 0.9962, 1.0297, 0.9984, 1.0125,
        1.0038, 1.0053, 0.9962, 1.0347, 0.9915, 1.0434, 0.9946, 0.9848, 0.9811,
        0.9530, 0.9997, 0.9886, 0.9871, 1.0004, 1.0059, 1.0031, 1.0019, 1.0062,
        1.0029, 0.9772, 0.9841, 1.0393, 0.9994, 1.0273, 0.9976, 1.0297, 1.0069,
        0.9989, 1.0324, 0.9722, 0.9624, 1.0180, 1.0047, 1.0056, 0.9896, 0.9985,
        1.0034, 0.9584, 1.0078, 0.9843, 1.0313, 0.9911, 0.9940, 1.0011, 0.9793,
        0.9824, 1.0176, 0.9943, 0.9998, 0.9986, 1.0126, 1.0013, 1.0049, 0.9946,
        1.0143, 0.9863, 0.9783, 0.9868, 1.0032, 0.9956, 1.0290, 1.0215, 0.9833,
        1.0128, 1.0342, 1.0074, 1.0156, 0.9972, 1.0238, 0.9939, 0.9739, 1.0445,
        0.9885, 1.0330, 0.9999, 0.9947, 1.0069, 1.0079, 1.0032, 0.9965, 1.0179,
        0.9976, 1.0131, 1.0029, 1.0122, 0.9992, 0.9957, 0.9855, 1.0090, 1.0010,
        0.9984, 0.9873, 1.0228, 0.9990, 1.0171, 0.9944, 0.9991, 1.0003, 1.0005,
        0.9887, 1.0347, 0.9999, 1.0025, 0.9972, 0.9987, 1.0098, 1.0078, 0.9893,
        1.0065, 0.9888, 0.9865, 1.0072, 0.9958, 0.9963, 1.0010, 1.0059, 0.9749,
        0.9898, 0.9721, 0.9918, 1.0229, 0.9901, 0.9608, 1.0194, 1.0152, 1.0050,
        1.0043, 1.0050, 1.0171, 0.9759, 1.0421, 0.9879, 0.9862, 0.9880, 0.9812,
        0.9662, 0.9895, 0.9777, 1.0707, 0.9967, 1.0114, 1.0064, 1.0204, 1.0235,
        1.0167, 0.9992, 0.9978, 0.9970, 1.0325, 1.0051, 1.0002, 1.0001, 1.0352,
        0.9804, 0.9610, 1.0055, 1.0225, 1.0166, 0.9978, 0.9970, 0.9917, 0.9762,
        1.0206, 0.9943, 1.0135, 0.9897, 1.0257, 1.0038, 1.0001, 0.9914, 0.9792,
        0.9906, 0.9977, 0.9980, 0.9773, 1.0326, 0.9996, 0.9872, 0.9904, 0.9848,
        1.0406, 1.0021, 1.0078, 0.9984, 0.9699, 0.9970, 0.9973, 0.9872, 0.9535,
        1.0175, 1.0673, 1.0385, 1.0103, 1.0068, 1.0176, 0.9952, 0.9963, 0.9866,
        0.9839, 0.9766, 1.0582, 0.9921, 1.0204, 1.0063, 1.0048, 1.0038, 0.9896,
        1.0233, 0.9951, 0.9705, 0.9941, 1.0011, 1.0052, 1.0268, 1.0045, 0.9992,
        0.9980, 1.0128, 0.9951, 1.0361, 1.0000, 1.0015, 1.0012, 0.9999, 1.0167,
        1.0166, 1.0009, 0.9721, 1.0296, 1.0387, 1.0053, 1.0000, 1.0000, 0.9985,
        1.0288, 1.0409, 1.0066, 1.0001, 0.9917, 1.0428, 0.9560, 0.9937, 0.9637,
        0.9980, 1.0195, 1.0094, 1.0002, 0.9920, 0.9869, 1.0245, 1.0024, 1.0038,
        1.0050, 1.0000, 1.0236, 1.0885, 1.0108, 1.0018, 0.9920, 1.0087, 1.0094,
        1.0252, 1.0999, 0.9821, 0.9799, 1.0469, 1.0007, 0.9751, 0.9753, 0.9934,
        1.0302, 0.9932, 0.9870, 0.9755, 1.0097, 0.9881, 0.9937, 0.9760],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808356
t4: 1641198808356
surr1, surr2: tensor([-3.1890e+00, -1.0005e+00, -1.4556e+00, -2.4307e+00, -2.5785e+00,
        -2.9501e+00, -2.7266e+00, -2.5526e+00, -2.8598e+00, -2.8185e+00,
        -2.5760e+00, -2.6692e+00, -2.4386e+00, -2.6376e+00, -2.5795e+00,
        -2.4119e+00, -2.5185e+00, -2.5373e+00, -2.3688e+00, -2.2391e+00,
        -2.1639e+00, -2.2184e+00, -8.4592e-01, -1.7344e-01, -1.6511e+00,
        -1.8045e+00, -1.6848e+00, -1.6938e+00, -1.5985e+00, -1.5114e+00,
        -1.9159e+00, -1.7302e+00, -1.7600e+00, -1.9107e+00, -1.6270e+00,
        -1.5953e+00, -1.4738e+00, -1.8341e+00, -1.5294e+00, -1.6057e+00,
        -1.5250e+00, -1.4419e+00, -1.5469e+00, -1.6439e+00, -1.7439e+00,
        -1.4469e+00, -1.5561e+00, -1.5624e+00, -1.5194e+00, -1.7672e+00,
        -1.4570e+00, -1.5380e+00, -1.5595e+00, -1.8847e+00, -1.2350e+00,
        -1.3513e+00, -1.1748e+00, -1.6579e+00, -1.5118e+00, -1.4063e+00,
        -1.2898e+00, -1.5432e+00, -1.1124e+00, -1.2528e+00, -1.5711e+00,
        -1.1672e+00,  1.1153e+00,  1.2501e+00,  2.1703e+00,  2.5026e+00,
         5.1510e+00,  1.0402e-02,  2.8965e-01,  1.7158e-01, -6.5585e-02,
         1.7109e-01,  2.2113e-01,  2.7933e-01,  3.2348e-01,  3.0965e-01,
         2.7432e-01,  4.2043e-01,  3.4082e-01,  3.1000e-01,  4.7286e-01,
         1.3459e-01, -3.1153e-02,  2.1470e-01,  4.9866e-01,  2.8971e-01,
         1.7297e-01,  1.3593e-01,  2.3910e-01,  2.7274e-01,  3.5540e-01,
         1.0455e-01,  2.2249e-01,  2.9825e-01,  2.9333e-01,  2.1192e-01,
         2.8042e-01,  3.8421e-01,  2.3043e-01,  3.3848e-01, -1.0858e-01,
         7.6157e-02, -1.0439e-01,  4.1148e-02,  1.2695e-01,  1.9433e-01,
         2.6156e-01,  3.1694e-01,  1.8872e-01,  1.7106e+00,  2.6122e+00,
         3.5492e+00,  7.4784e-01,  9.6966e-01,  9.2654e-01,  9.8543e-01,
         9.7400e-01,  9.3883e-01,  8.7601e-01,  9.6903e-01,  8.2346e-01,
         8.6874e-01,  7.4733e-01,  7.7140e-01,  8.2029e-01,  6.5070e-01,
         6.0331e-01,  8.9009e-01,  5.1853e-01,  8.7192e-01,  2.9443e-01,
         7.7869e-01,  7.0005e-01,  8.7819e-01,  9.1251e-01,  8.6190e-01,
         1.0138e+00,  9.4620e-01,  6.1262e-01,  9.6494e-01,  1.0278e+00,
         1.0095e+00,  5.5132e-01,  8.6823e-01,  9.3619e-01,  1.1199e+00,
         5.4190e-01,  9.9501e-01,  6.8577e-01,  9.0862e-01,  1.0088e+00,
         4.7867e-01,  7.5884e-01,  8.6780e-01,  8.3177e-01,  8.9734e-01,
         7.1571e-01,  5.3367e-01,  9.6252e-01,  8.4686e-01,  5.1533e-01,
         8.9522e-01,  7.1733e-01,  6.5740e-01,  7.9581e-01,  9.3785e-01,
         9.1751e-01,  6.3902e-01,  7.2196e-01,  8.6227e-01,  8.5604e-01,
         1.0270e+00,  9.7596e-01,  9.4602e-01,  8.8970e-01,  1.0545e+00,
         9.0747e-01,  7.9528e-01,  3.6585e-01,  5.2264e-01,  8.2940e-01,
         9.4514e-01,  1.0308e+00,  8.4525e-01,  6.7176e-01,  6.0036e-01,
         7.3038e-01,  4.3649e-01,  4.1326e-01,  8.0462e-01,  5.3823e-01,
         6.5975e-01,  7.5610e-01,  6.7756e-01,  4.1215e-01,  6.9218e-01,
         3.2580e-01,  8.0813e-01,  6.8747e-01,  9.0359e-01,  7.6948e-01,
         7.3027e-01,  7.2592e-01,  3.2170e-01,  6.5062e-01,  7.3041e-01,
         5.8190e-01,  6.1831e-01,  7.3100e-01,  5.9258e-01,  5.5493e-01,
         6.0928e-01,  6.7675e-01,  6.9430e-01,  4.8785e-01,  5.9680e-01,
         7.5479e-01,  6.0446e-01,  6.8867e-01,  6.1016e-01,  7.8294e-01,
         5.2920e-01,  5.1840e-01,  6.2211e-01,  6.2990e-01,  7.2533e-01,
         6.5119e-01,  6.0840e-01,  2.0883e-01,  7.2521e-01,  3.1190e-01,
         7.0797e-01,  6.0385e-01,  6.8333e-01,  7.3217e-01,  8.1006e-01,
         7.2590e-01,  6.5117e-01,  7.3284e-01,  2.7421e-01,  6.2218e-01,
         4.0010e-01,  6.8184e-01,  5.6283e-01,  4.3749e-01,  1.7327e-01,
         5.2059e-01,  5.4591e-01,  5.9570e-01,  4.6726e-01,  1.9158e-01,
         1.1653e-01,  2.0642e-01,  1.1283e-03,  4.3479e-01,  4.1225e-01,
         4.4174e-01,  3.5398e-01,  2.6558e-01,  3.1933e-01,  3.0389e-01,
        -8.9545e-02,  3.4367e-01,  3.4127e-01,  4.8612e-01,  5.9895e-02,
         1.1757e-01,  1.6756e-01,  4.0609e-01,  4.0723e-01,  1.8283e-01,
        -2.6998e-02,  2.3351e-01,  4.2804e-01,  2.9760e-01,  1.3470e-01,
         1.8583e-01,  1.6250e-01,  1.8851e-01,  1.9717e-01, -1.2524e-02,
         3.6965e-01,  3.7805e-01,  2.8999e-01,  1.5837e-01,  3.0971e-01,
         2.8601e-01,  2.6899e-01,  1.9582e-01,  3.0196e-01,  2.3992e-01,
         1.6424e-01,  1.1362e-02,  5.6697e-03,  2.4220e-01,  1.7489e-01,
         1.8856e-01, -3.3105e-01,  2.5940e-01,  4.9066e-01,  3.0699e-01,
         3.8674e-01,  3.2384e-01,  2.8649e-01,  4.4882e-01,  2.7264e-01,
         3.5224e-01,  1.4163e-01,  2.8257e-01,  3.4498e-01,  3.6892e-01,
        -4.6175e-03,  8.9468e-02, -9.4914e-02, -1.2282e-01,  3.3171e-01,
        -1.5891e-01,  3.3880e-01, -1.2710e-01,  2.6259e-01,  2.7448e-01,
         2.3743e-01,  2.5163e-01,  8.7166e-02,  8.2257e-02,  2.4065e-01,
         1.8278e-01,  3.2405e-01, -7.5934e-02,  6.0793e-02,  1.7292e-01,
         1.2801e-01, -1.1475e-01,  2.7945e-01,  2.0839e-01,  3.0828e-01,
         4.9334e-02,  3.9419e-02,  1.3131e-01,  5.1669e-02, -3.6798e-01,
         7.1410e-02, -4.5140e-02,  7.5708e-02, -7.3595e-02,  8.3472e-02,
         1.7323e-02, -2.6817e-01, -4.8175e-01, -1.5923e-01, -4.7366e-01,
        -2.5878e-01, -3.4938e-01, -1.8919e-01,  5.6855e-02, -9.4246e-02,
         3.0006e-02, -3.6881e-01, -5.0337e-01, -1.6703e-01,  9.4100e-02,
        -3.6717e-01, -4.2462e-01, -4.2364e-01, -2.8648e-01, -2.1028e-01,
        -1.6260e-01, -3.5279e-01, -6.4638e-01, -1.7935e-01, -2.2905e-01,
        -2.8538e-01, -1.9487e-01, -2.4307e-01, -2.5973e-01, -3.8225e-01,
        -3.7893e-01, -6.2672e-01, -8.5651e-01, -5.4055e-01, -6.6609e-01,
        -5.2057e-01, -4.3148e-01, -3.9454e-01, -1.0895e+00, -8.6747e-01,
        -5.2172e-01, -6.5346e-01, -1.0285e+00, -6.8827e-01, -7.9230e-01,
        -7.0098e-01, -6.9182e-01, -1.0440e+00, -9.4568e-01, -9.1114e-01,
        -9.2723e-01, -7.4491e-01, -8.3961e-01, -8.3720e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1890e+00, -1.0005e+00, -1.4556e+00, -2.4307e+00, -2.5785e+00,
        -2.9501e+00, -2.7266e+00, -2.5526e+00, -2.8598e+00, -2.8185e+00,
        -2.5760e+00, -2.6692e+00, -2.4386e+00, -2.6376e+00, -2.5795e+00,
        -2.4119e+00, -2.5185e+00, -2.5373e+00, -2.3688e+00, -2.2391e+00,
        -2.1639e+00, -2.2184e+00, -8.4592e-01, -1.7344e-01, -1.6511e+00,
        -1.8045e+00, -1.6848e+00, -1.6938e+00, -1.5985e+00, -1.5114e+00,
        -1.9159e+00, -1.7302e+00, -1.7600e+00, -1.9107e+00, -1.6270e+00,
        -1.5953e+00, -1.4738e+00, -1.8341e+00, -1.5294e+00, -1.6057e+00,
        -1.5250e+00, -1.4419e+00, -1.5469e+00, -1.6439e+00, -1.7439e+00,
        -1.4469e+00, -1.5561e+00, -1.5624e+00, -1.5194e+00, -1.7672e+00,
        -1.4570e+00, -1.5380e+00, -1.5595e+00, -1.8847e+00, -1.2350e+00,
        -1.3513e+00, -1.1748e+00, -1.6416e+00, -1.5118e+00, -1.4063e+00,
        -1.2898e+00, -1.5432e+00, -1.1124e+00, -1.2528e+00, -1.5711e+00,
        -1.1672e+00,  1.1153e+00,  1.2501e+00,  2.1703e+00,  2.5026e+00,
         5.1175e+00,  1.0402e-02,  2.8965e-01,  1.7158e-01, -6.5585e-02,
         1.7109e-01,  2.2113e-01,  2.7933e-01,  3.2348e-01,  3.0965e-01,
         2.7432e-01,  4.2043e-01,  3.4082e-01,  3.1000e-01,  4.7286e-01,
         1.3459e-01, -3.1153e-02,  2.1470e-01,  4.9866e-01,  2.8971e-01,
         1.7297e-01,  1.3593e-01,  2.3910e-01,  2.7274e-01,  3.5540e-01,
         1.0455e-01,  2.2249e-01,  2.9825e-01,  2.9333e-01,  2.1192e-01,
         2.8042e-01,  3.8421e-01,  2.3043e-01,  3.3848e-01, -1.0858e-01,
         7.6157e-02, -1.0439e-01,  4.1148e-02,  1.2695e-01,  1.9433e-01,
         2.6156e-01,  3.1694e-01,  1.8872e-01,  1.7106e+00,  2.6122e+00,
         3.5492e+00,  7.4784e-01,  9.6966e-01,  9.2654e-01,  9.8543e-01,
         9.7400e-01,  9.3883e-01,  8.7601e-01,  9.6903e-01,  8.2346e-01,
         8.6874e-01,  7.4733e-01,  7.7140e-01,  8.2029e-01,  6.5070e-01,
         6.0331e-01,  8.9009e-01,  5.1853e-01,  8.7192e-01,  2.9443e-01,
         7.7869e-01,  7.0005e-01,  8.7819e-01,  9.1251e-01,  8.6190e-01,
         1.0138e+00,  9.4620e-01,  6.1262e-01,  9.6494e-01,  1.0278e+00,
         1.0095e+00,  5.5132e-01,  8.6823e-01,  9.3619e-01,  1.1199e+00,
         5.4190e-01,  9.9501e-01,  6.8577e-01,  9.0862e-01,  1.0088e+00,
         4.7867e-01,  7.5884e-01,  8.6780e-01,  8.3177e-01,  8.9734e-01,
         7.1571e-01,  5.3367e-01,  9.6252e-01,  8.4686e-01,  5.1533e-01,
         8.9522e-01,  7.1733e-01,  6.5740e-01,  7.9581e-01,  9.3785e-01,
         9.1751e-01,  6.3902e-01,  7.2196e-01,  8.6227e-01,  8.5604e-01,
         1.0270e+00,  9.7596e-01,  9.4602e-01,  8.8970e-01,  1.0545e+00,
         9.0747e-01,  7.9528e-01,  3.6585e-01,  5.2264e-01,  8.2940e-01,
         9.4514e-01,  1.0308e+00,  8.4525e-01,  6.7176e-01,  6.0036e-01,
         7.3038e-01,  4.3649e-01,  4.1326e-01,  8.0462e-01,  5.3823e-01,
         6.5975e-01,  7.5610e-01,  6.7756e-01,  4.1215e-01,  6.9218e-01,
         3.2580e-01,  8.0813e-01,  6.8747e-01,  9.0359e-01,  7.6948e-01,
         7.3027e-01,  7.2592e-01,  3.2170e-01,  6.5062e-01,  7.3041e-01,
         5.8190e-01,  6.1831e-01,  7.3100e-01,  5.9258e-01,  5.5493e-01,
         6.0928e-01,  6.7675e-01,  6.9430e-01,  4.8785e-01,  5.9680e-01,
         7.5479e-01,  6.0446e-01,  6.8867e-01,  6.1016e-01,  7.8294e-01,
         5.2920e-01,  5.1840e-01,  6.2211e-01,  6.2990e-01,  7.2533e-01,
         6.5119e-01,  6.0840e-01,  2.0883e-01,  7.2521e-01,  3.1190e-01,
         7.0797e-01,  6.0385e-01,  6.8333e-01,  7.3217e-01,  8.1006e-01,
         7.2590e-01,  6.5117e-01,  7.3284e-01,  2.7421e-01,  6.2218e-01,
         4.0010e-01,  6.8184e-01,  5.6283e-01,  4.3749e-01,  1.7327e-01,
         5.2059e-01,  5.4591e-01,  5.9570e-01,  4.6726e-01,  1.9158e-01,
         1.1653e-01,  2.0642e-01,  1.1283e-03,  4.3479e-01,  4.1225e-01,
         4.4174e-01,  3.5398e-01,  2.6558e-01,  3.1933e-01,  3.0389e-01,
        -8.9545e-02,  3.4367e-01,  3.4127e-01,  4.8612e-01,  5.9895e-02,
         1.1757e-01,  1.6756e-01,  4.0609e-01,  4.0723e-01,  1.8283e-01,
        -2.6998e-02,  2.3351e-01,  4.2804e-01,  2.9760e-01,  1.3470e-01,
         1.8583e-01,  1.6250e-01,  1.8851e-01,  1.9717e-01, -1.2524e-02,
         3.6965e-01,  3.7805e-01,  2.8999e-01,  1.5837e-01,  3.0971e-01,
         2.8601e-01,  2.6899e-01,  1.9582e-01,  3.0196e-01,  2.3992e-01,
         1.6424e-01,  1.1362e-02,  5.6697e-03,  2.4220e-01,  1.7489e-01,
         1.8856e-01, -3.3105e-01,  2.5940e-01,  4.9066e-01,  3.0699e-01,
         3.8674e-01,  3.2384e-01,  2.8649e-01,  4.4882e-01,  2.7264e-01,
         3.5224e-01,  1.4163e-01,  2.8257e-01,  3.4498e-01,  3.6892e-01,
        -4.6175e-03,  8.9468e-02, -9.4914e-02, -1.2282e-01,  3.3171e-01,
        -1.5891e-01,  3.3880e-01, -1.2710e-01,  2.6259e-01,  2.7448e-01,
         2.3743e-01,  2.5163e-01,  8.7166e-02,  8.2257e-02,  2.4065e-01,
         1.8278e-01,  3.2405e-01, -7.5934e-02,  6.0793e-02,  1.7292e-01,
         1.2801e-01, -1.1475e-01,  2.7945e-01,  2.0839e-01,  3.0828e-01,
         4.9334e-02,  3.9419e-02,  1.3131e-01,  5.1669e-02, -3.6798e-01,
         7.1410e-02, -4.5140e-02,  7.5708e-02, -7.3595e-02,  8.3472e-02,
         1.7323e-02, -2.6817e-01, -4.8175e-01, -1.5923e-01, -4.7366e-01,
        -2.5878e-01, -3.4938e-01, -1.8919e-01,  5.6855e-02, -9.4246e-02,
         3.0006e-02, -3.6881e-01, -5.0337e-01, -1.6703e-01,  9.4100e-02,
        -3.6717e-01, -4.2462e-01, -4.2364e-01, -2.8648e-01, -2.1028e-01,
        -1.6260e-01, -3.5279e-01, -6.4638e-01, -1.7935e-01, -2.2905e-01,
        -2.8538e-01, -1.9487e-01, -2.4307e-01, -2.5973e-01, -3.8225e-01,
        -3.7893e-01, -6.2672e-01, -8.5651e-01, -5.4055e-01, -6.6609e-01,
        -5.2057e-01, -4.3148e-01, -3.9454e-01, -1.0895e+00, -8.6747e-01,
        -5.2172e-01, -6.5346e-01, -1.0285e+00, -6.8827e-01, -7.9230e-01,
        -7.0098e-01, -6.9182e-01, -1.0440e+00, -9.4568e-01, -9.1114e-01,
        -9.2723e-01, -7.4491e-01, -8.3961e-01, -8.3720e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808366
t6: 1641198808366
state_values: tensor([ 0.3391, -0.0426, -0.0183,  0.0570,  0.1439,  0.2191,  0.2388,  0.2456,
         0.2575,  0.2721,  0.3601,  0.2910,  0.2755,  0.2731,  0.3066,  0.5988,
         0.4429,  0.3667,  0.4259,  0.4379,  0.4856,  0.4542,  0.1916,  0.1308,
         0.2691,  0.2815,  0.2826,  0.4032,  0.3154,  0.3030,  0.7333,  0.5475,
         0.5630,  0.4340,  0.3782,  0.3627,  0.3582,  0.3692,  0.4034,  0.3667,
         0.3921,  0.3740,  0.3736,  0.5854,  0.4397,  0.4038,  0.3862,  0.5051,
         0.4118,  0.4056,  0.3991,  0.4814,  0.4110,  0.7188,  0.5114,  0.4617,
         0.5046,  0.8668,  0.5883,  0.7545,  0.5513,  0.5276,  0.4802,  0.5238,
         0.5085,  0.4978,  0.1999,  0.1615,  0.1194,  0.1197,  0.0766,  0.2188,
         0.3064,  0.3229,  0.3524,  0.5251,  0.4593,  0.4599,  0.4473,  0.4702,
         0.4429,  0.4357,  0.4618,  0.4496,  0.4943,  0.7740,  0.5795,  0.5236,
         0.5828,  0.6505,  0.5585,  0.7097,  0.6041,  0.5585,  0.5299,  0.5470,
         0.7219,  0.6299,  0.5981,  0.6985,  0.6427,  0.5759,  0.5735,  0.5698,
         0.5923,  0.5869,  0.6044,  0.6069,  0.6005,  0.5958,  0.6641,  0.7340,
         0.7222,  0.3849,  0.2449,  0.1803,  0.3908,  0.5584,  0.5232,  0.5607,
         0.5558,  0.5490,  0.5665,  0.5641,  0.7464,  0.6412,  0.8616,  0.7056,
         0.6576,  0.6686,  1.0869,  0.8370,  0.7528,  0.7771,  1.4313,  0.9717,
         1.1621,  0.9130,  0.8883,  0.9440,  0.8200,  0.7815,  0.7893,  0.7922,
         0.8029,  0.7941,  0.7881,  0.7686,  0.9351,  0.7983,  1.3398,  0.9523,
         0.8761,  0.8224,  0.8029,  1.3949,  1.1164,  1.1077,  0.9297,  0.8729,
         0.8690,  0.8907,  0.8357,  0.8657,  0.8564,  0.8618,  0.8590,  1.0414,
         0.8900,  0.8403,  0.8474,  0.8609,  0.8574,  0.9068,  0.8532,  0.8192,
         0.8211,  0.8701,  0.8632,  0.8341,  0.8172,  0.9454,  0.9035,  0.8903,
         0.8541,  0.8575,  0.8213,  0.9905,  1.2583,  1.3011,  1.0295,  0.9921,
         0.9653,  1.1128,  1.0106,  1.3254,  1.2127,  1.2783,  1.1155,  1.1749,
         1.1060,  1.0206,  1.0129,  1.0139,  1.1139,  1.2215,  1.1398,  1.0981,
         1.0331,  1.1404,  1.4060,  1.1301,  1.0848,  1.2437,  1.3412,  1.1354,
         1.0750,  1.0424,  1.0600,  1.2408,  1.0768,  1.3307,  1.2309,  1.1458,
         1.0594,  1.0589,  1.0610,  1.3456,  1.1264,  1.1353,  1.1421,  1.1753,
         1.1353,  1.1370,  1.1122,  1.1034,  1.0665,  1.0400,  1.0701,  1.0079,
         1.0086,  1.0475,  1.0453,  1.0771,  1.0270,  1.0587,  1.0238,  1.1481,
         1.0900,  1.1120,  1.1848,  1.2263,  1.1719,  1.2211,  1.6410,  1.3117,
         1.7925,  1.4054,  1.3276,  1.3395,  1.3462,  1.4359,  1.6700,  1.4326,
         1.4479,  1.3682,  1.4553,  1.2999,  1.3643,  1.2977,  1.7655,  1.3809,
         1.3232,  1.3224,  1.7546,  1.4357,  1.3287,  1.3230,  1.2647,  1.8310,
         1.5958,  1.7490,  1.4673,  1.3568,  1.3312,  1.2795,  1.2578,  1.2554,
         1.2612,  1.3455,  1.2689,  1.3384,  1.2743,  1.4230,  1.4054,  1.3012,
         1.2962,  1.2809,  1.2376,  1.2559,  1.2489,  1.5177,  1.5631,  1.3587,
         1.3193,  1.2884,  1.2922,  1.5963,  1.4016,  1.7039,  1.4683,  1.8310,
         1.5447,  1.4361,  1.4583,  2.0600,  1.5877,  1.4882,  1.4635,  1.5609,
         1.4930,  1.5530,  1.4770,  1.3999,  1.3650,  1.3553,  1.3360,  1.7830,
         1.5004,  1.7754,  1.4953,  1.3846,  1.4352,  1.4040,  1.5756,  1.4336,
         1.4217,  1.3556,  1.3869,  1.3707,  1.6601,  1.6844,  1.4994,  1.4885,
         1.4832,  1.4110,  1.8226,  1.5269,  1.4636,  1.4617,  1.4448,  1.8816,
         1.6191,  1.9161,  2.3770,  1.7921,  1.6692,  1.5906,  1.5142,  1.5320,
         1.4554,  2.0786,  1.7081,  1.5434,  1.4367,  1.4940,  2.1180,  2.1570,
         1.7963,  1.8295,  1.6575,  1.5891,  1.6003,  1.5437,  1.5097,  1.4966,
         1.6215,  1.5683,  1.5565,  1.6671,  1.5433,  2.1220,  1.7654,  1.6270,
         1.6047,  1.5698,  1.6088,  1.6237,  2.4382,  1.8724,  1.7944,  1.8046,
         1.7288,  2.0490,  2.2252,  1.9451,  1.7720,  1.7548,  2.1840,  2.1495,
         2.0403,  1.7734,  1.8702,  1.7974], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808370
t8: 1641198808370
t9: 1641198808370
t10: 1641198808381
t11: 1641198808382
t12: 1641198808382
t1: 1641198808382
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808393
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0066, 1.0089, 1.0011, 0.9921, 0.9826, 1.0011, 0.9926, 0.9666, 1.0060,
        1.0190, 1.0158, 1.0000, 1.0132, 1.0021, 0.9695, 0.9326, 1.0877, 0.9519,
        0.9541, 0.9282, 0.9571, 1.0243, 1.0036, 0.9621, 1.0298, 0.9992, 0.9724,
        1.0052, 0.9934, 0.8845, 0.9360, 0.9507, 1.0579, 1.0089, 1.0148, 0.9972,
        1.0373, 0.9941, 1.0146, 0.9973, 0.9949, 0.9989, 0.9764, 1.0406, 0.9926,
        1.0136, 0.9784, 1.0288, 1.0355, 0.9979, 0.9898, 1.0185, 0.9928, 0.9959,
        1.0117, 0.9943, 0.9409, 1.0996, 0.9172, 1.0091, 1.0719, 0.9990, 0.9890,
        1.0437, 0.9966, 1.0125, 1.0127, 0.9999, 0.9996, 0.9787, 1.1091, 0.9794,
        1.0260, 1.0293, 0.9854, 0.9927, 0.9845, 0.9915, 0.9857, 1.0159, 0.9996,
        0.9969, 1.0070, 0.9927, 0.9500, 1.0476, 1.0191, 0.9660, 0.9545, 1.0377,
        0.9559, 1.0014, 1.0065, 1.0113, 1.0213, 0.9877, 0.9930, 0.9970, 0.9718,
        0.9876, 1.0069, 1.0171, 1.0003, 1.0278, 1.0028, 0.9907, 0.9914, 0.9900,
        0.9922, 1.0090, 0.9997, 0.9912, 1.0170, 1.0185, 1.0092, 1.0576, 0.9997,
        1.0066, 0.9980, 0.9995, 1.0035, 1.0012, 1.0012, 0.9927, 1.0066, 0.9709,
        1.0186, 1.0262, 1.0331, 0.9717, 0.9915, 1.1078, 0.9779, 0.8993, 0.9825,
        0.9201, 0.9984, 0.9759, 0.9662, 1.0001, 1.0156, 1.0528, 0.9974, 0.9948,
        0.9945, 1.0443, 1.0040, 1.0036, 1.0008, 0.9612, 0.9980, 1.0598, 1.0226,
        1.0045, 0.9601, 0.9752, 0.9365, 1.0335, 1.0101, 1.0300, 1.0237, 1.0006,
        0.9989, 1.0177, 1.0001, 1.0039, 1.0036, 1.0114, 1.0012, 0.9996, 1.0066,
        0.9998, 1.0017, 1.0020, 1.0000, 1.0003, 0.9970, 0.9981, 0.9959, 1.0103,
        0.9906, 1.0631, 1.0110, 1.0003, 1.0004, 1.0000, 0.9996, 0.9670, 0.9608,
        1.0229, 1.0788, 1.0909, 0.9817, 1.0398, 0.9765, 0.9766, 0.9425, 1.0854,
        0.9671, 1.0843, 1.0002, 1.0103, 0.9967, 0.9852, 0.9699, 0.9817, 1.0578,
        1.0132, 0.9887, 0.9569, 1.0300, 0.9945, 0.9672, 0.9522, 1.0242, 1.0055,
        1.0123, 1.0173, 0.9931, 1.0035, 0.9779, 0.9761, 0.9968, 1.0007, 1.0280,
        1.0136, 0.9902, 1.0155, 0.9867, 0.9854, 0.9833, 1.0822, 0.9906, 1.0436,
        0.9981, 1.0083, 1.0012, 0.9997, 1.0012, 1.0006, 0.9998, 0.9985, 1.0166,
        1.0012, 1.0004, 0.9988, 1.0032, 1.0045, 1.0025, 1.0005, 0.9926, 0.9922,
        0.9875, 0.9515, 1.0508, 0.9169, 1.0309, 0.9762, 0.9704, 0.9685, 0.9535,
        0.9215, 0.9845, 0.9585, 1.1406, 0.9700, 1.0326, 0.9833, 1.0726, 0.9724,
        1.0405, 0.9929, 0.9864, 0.9438, 1.0503, 1.0163, 0.9883, 1.0265, 0.9730,
        0.9833, 0.9218, 1.0093, 1.0448, 1.0796, 1.0050, 1.0012, 1.0018, 1.0021,
        1.0025, 1.0038, 0.9984, 1.0107, 0.9947, 0.9848, 1.0172, 1.0179, 1.0101,
        1.0005, 1.0000, 0.9999, 1.0002, 1.0025, 1.0001, 1.0082, 1.0039, 1.0029,
        1.0001, 0.9972, 0.9645, 0.9940, 0.9246, 0.9943, 0.9940, 0.9686, 0.8913,
        1.0164, 1.1204, 1.1302, 0.9833, 1.1037, 0.9920, 1.0655, 1.0011, 1.0023,
        0.9997, 0.9978, 1.0103, 1.0094, 0.9735, 1.0190, 1.0162, 1.0388, 1.0136,
        0.9952, 1.0164, 1.0140, 1.0010, 1.0000, 0.9996, 0.9891, 0.9752, 0.9956,
        0.9915, 1.0972, 1.0073, 0.9868, 1.0033, 1.0206, 0.9927, 0.9999, 0.9721,
        1.0822, 0.9560, 0.9239, 1.0292, 1.0570, 1.0147, 0.9985, 0.9994, 1.0024,
        0.9677, 1.0382, 1.0235, 1.0034, 1.0232, 0.9798, 0.9617, 0.9908, 0.9344,
        0.9953, 1.0426, 1.0769, 0.9994, 1.0045, 1.0023, 0.9991, 0.9944, 0.9938,
        0.9827, 1.0047, 0.9605, 1.0722, 1.0301, 1.0388, 1.0082, 0.9980, 0.9924,
        0.9471, 1.0918, 0.9613, 0.9614, 1.1108, 0.9548, 0.9498, 0.9678, 0.9894,
        1.0630, 0.9448, 0.9687, 0.9574, 1.0140, 0.9662, 0.9851, 0.9103],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808397
t4: 1641198808397
surr1, surr2: tensor([-3.1823e+00, -9.8965e-01, -1.4794e+00, -2.4558e+00, -2.6986e+00,
        -2.9476e+00, -2.7624e+00, -2.6739e+00, -2.8417e+00, -2.7143e+00,
        -2.6606e+00, -2.6570e+00, -2.5219e+00, -2.5909e+00, -2.3776e+00,
        -2.4292e+00, -2.6061e+00, -2.4644e+00, -2.3008e+00, -2.1473e+00,
        -2.1223e+00, -2.2384e+00, -8.4757e-01, -1.6990e-01, -1.6840e+00,
        -1.8029e+00, -1.6157e+00, -1.6987e+00, -1.5894e+00, -1.3697e+00,
        -1.9258e+00, -1.6760e+00, -1.8362e+00, -1.9196e+00, -1.6442e+00,
        -1.5904e+00, -1.5436e+00, -1.7988e+00, -1.5578e+00, -1.5831e+00,
        -1.5134e+00, -1.4393e+00, -1.4683e+00, -1.6842e+00, -1.7318e+00,
        -1.4636e+00, -1.5017e+00, -1.5992e+00, -1.5901e+00, -1.7598e+00,
        -1.4205e+00, -1.5720e+00, -1.5292e+00, -1.8755e+00, -1.2606e+00,
        -1.3199e+00, -1.0873e+00, -1.6410e+00, -1.4387e+00, -1.4142e+00,
        -1.3340e+00, -1.5418e+00, -1.0987e+00, -1.3108e+00, -1.5585e+00,
        -1.1936e+00,  1.1597e+00,  1.2774e+00,  2.1429e+00,  2.3925e+00,
         5.1599e+00,  1.0259e-02,  2.9520e-01,  1.7876e-01, -6.2916e-02,
         1.6993e-01,  2.1804e-01,  2.7751e-01,  3.1913e-01,  3.1397e-01,
         2.7419e-01,  4.1840e-01,  3.4493e-01,  3.0308e-01,  4.4562e-01,
         1.3663e-01, -3.1574e-02,  2.0719e-01,  4.8316e-01,  2.9579e-01,
         1.6696e-01,  1.3599e-01,  2.4018e-01,  2.7569e-01,  3.6622e-01,
         1.0059e-01,  2.2104e-01,  2.9744e-01,  2.8530e-01,  2.0996e-01,
         2.8203e-01,  3.9194e-01,  2.3062e-01,  3.6046e-01, -1.1172e-01,
         7.9864e-02, -1.0692e-01,  4.1977e-02,  1.2895e-01,  1.9091e-01,
         2.5311e-01,  3.1190e-01,  1.9248e-01,  1.7644e+00,  2.6855e+00,
         3.8520e+00,  7.2792e-01,  9.8450e-01,  9.1591e-01,  9.8473e-01,
         9.8216e-01,  9.4255e-01,  8.8037e-01,  9.3419e-01,  8.3022e-01,
         8.3307e-01,  7.5841e-01,  7.8745e-01,  8.5069e-01,  6.1111e-01,
         6.0329e-01,  9.4497e-01,  5.0980e-01,  7.9625e-01,  2.9488e-01,
         7.5182e-01,  6.9914e-01,  8.6690e-01,  8.9322e-01,  8.6167e-01,
         1.0235e+00,  9.9309e-01,  6.0986e-01,  9.5400e-01,  1.0192e+00,
         1.0788e+00,  5.6244e-01,  8.3838e-01,  9.3742e-01,  1.0478e+00,
         5.4213e-01,  1.0242e+00,  6.9646e-01,  9.1373e-01,  9.3823e-01,
         4.8014e-01,  7.3843e-01,  8.8102e-01,  8.3624e-01,  9.1910e-01,
         7.4037e-01,  5.3478e-01,  9.5824e-01,  8.9927e-01,  5.1135e-01,
         9.1305e-01,  6.9801e-01,  6.7088e-01,  8.0161e-01,  9.3644e-01,
         9.4305e-01,  6.5034e-01,  7.1066e-01,  8.6900e-01,  8.5620e-01,
         1.0287e+00,  9.6095e-01,  9.4301e-01,  8.8176e-01,  1.0711e+00,
         8.8619e-01,  8.5719e-01,  3.7809e-01,  5.2979e-01,  8.2709e-01,
         9.4940e-01,  1.0013e+00,  8.0016e-01,  6.5643e-01,  6.0632e-01,
         7.6191e-01,  4.7265e-01,  3.9949e-01,  8.3894e-01,  5.1334e-01,
         6.4827e-01,  7.3172e-01,  7.0406e-01,  4.0322e-01,  7.2656e-01,
         3.2591e-01,  8.2084e-01,  6.8048e-01,  8.8326e-01,  7.4390e-01,
         7.1946e-01,  7.5438e-01,  3.2673e-01,  6.3495e-01,  6.9690e-01,
         5.9211e-01,  6.1540e-01,  7.1004e-01,  5.7257e-01,  5.6327e-01,
         6.1201e-01,  6.8618e-01,  7.1541e-01,  4.7369e-01,  5.9947e-01,
         7.2571e-01,  5.9333e-01,  6.8713e-01,  6.1043e-01,  8.0445e-01,
         5.4252e-01,  4.9610e-01,  6.3180e-01,  6.1993e-01,  7.1678e-01,
         6.4112e-01,  6.5205e-01,  2.0525e-01,  7.6502e-01,  3.0928e-01,
         7.2200e-01,  6.1289e-01,  6.7827e-01,  7.3616e-01,  8.1352e-01,
         7.2502e-01,  6.4639e-01,  7.6413e-01,  2.7738e-01,  6.4033e-01,
         4.0293e-01,  6.6872e-01,  5.7098e-01,  4.5647e-01,  1.7005e-01,
         5.0901e-01,  5.3893e-01,  5.8573e-01,  4.4238e-01,  1.9792e-01,
         1.0949e-01,  2.0420e-01,  1.1149e-03,  4.2783e-01,  4.0410e-01,
         4.2929e-01,  3.3759e-01,  2.6425e-01,  3.1304e-01,  3.2371e-01,
        -8.7148e-02,  3.5087e-01,  3.3342e-01,  5.1098e-01,  5.6902e-02,
         1.2032e-01,  1.6650e-01,  4.0148e-01,  3.8550e-01,  1.8600e-01,
        -2.7298e-02,  2.3074e-01,  4.3931e-01,  2.7972e-01,  1.3509e-01,
         1.7824e-01,  1.6312e-01,  1.9263e-01,  2.0938e-01, -1.2614e-02,
         3.7121e-01,  3.8188e-01,  2.9770e-01,  1.5555e-01,  3.1268e-01,
         2.8174e-01,  2.7470e-01,  1.8989e-01,  2.9625e-01,  2.4400e-01,
         1.6863e-01,  1.1720e-02,  5.7259e-03,  2.4274e-01,  1.7524e-01,
         1.9298e-01, -3.2142e-01,  2.5953e-01,  5.0108e-01,  3.1116e-01,
         3.9383e-01,  3.1123e-01,  2.8507e-01,  4.2955e-01,  2.7145e-01,
         3.3579e-01,  1.4126e-01,  2.8164e-01,  3.3850e-01,  3.4487e-01,
        -4.6125e-03,  9.3922e-02, -1.0329e-01, -1.1954e-01,  3.6364e-01,
        -1.5492e-01,  3.6272e-01, -1.2772e-01,  2.6676e-01,  2.7890e-01,
         2.4257e-01,  2.4024e-01,  8.8687e-02,  7.8481e-02,  2.4368e-01,
         1.8485e-01,  3.3534e-01, -7.7782e-02,  5.9127e-02,  1.7663e-01,
         1.3375e-01, -1.1554e-01,  2.7913e-01,  2.0722e-01,  2.9695e-01,
         4.7896e-02,  3.9278e-02,  1.3045e-01,  5.5980e-02, -3.7249e-01,
         6.8006e-02, -4.5289e-02,  7.7146e-02, -7.2967e-02,  8.3468e-02,
         1.6564e-02, -2.8548e-01, -4.6018e-01, -1.5134e-01, -4.7348e-01,
        -2.6336e-01, -3.5263e-01, -1.8889e-01,  5.6817e-02, -9.4614e-02,
         2.8224e-02, -3.6784e-01, -5.1181e-01, -1.6759e-01,  9.7090e-02,
        -3.4498e-01, -4.2713e-01, -4.2239e-01, -2.7776e-01, -2.0970e-01,
        -1.6628e-01, -3.7638e-01, -6.4584e-01, -1.8162e-01, -2.3262e-01,
        -2.7829e-01, -1.9331e-01, -2.4065e-01, -2.5395e-01, -3.8403e-01,
        -3.5557e-01, -6.1734e-01, -8.7295e-01, -5.6052e-01, -6.7699e-01,
        -5.1500e-01, -4.2419e-01, -3.6450e-01, -1.0815e+00, -8.4906e-01,
        -5.1190e-01, -6.9336e-01, -9.8131e-01, -6.7045e-01, -7.8620e-01,
        -6.9816e-01, -7.1382e-01, -9.9320e-01, -9.2818e-01, -8.9420e-01,
        -9.3121e-01, -7.2841e-01, -8.3238e-01, -7.8081e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1823e+00, -9.8965e-01, -1.4794e+00, -2.4558e+00, -2.6986e+00,
        -2.9476e+00, -2.7624e+00, -2.6739e+00, -2.8417e+00, -2.7143e+00,
        -2.6606e+00, -2.6570e+00, -2.5219e+00, -2.5909e+00, -2.3776e+00,
        -2.4292e+00, -2.6061e+00, -2.4644e+00, -2.3008e+00, -2.1473e+00,
        -2.1223e+00, -2.2384e+00, -8.4757e-01, -1.6990e-01, -1.6840e+00,
        -1.8029e+00, -1.6157e+00, -1.6987e+00, -1.5894e+00, -1.3938e+00,
        -1.9258e+00, -1.6760e+00, -1.8362e+00, -1.9196e+00, -1.6442e+00,
        -1.5904e+00, -1.5436e+00, -1.7988e+00, -1.5578e+00, -1.5831e+00,
        -1.5134e+00, -1.4393e+00, -1.4683e+00, -1.6842e+00, -1.7318e+00,
        -1.4636e+00, -1.5017e+00, -1.5992e+00, -1.5901e+00, -1.7598e+00,
        -1.4205e+00, -1.5720e+00, -1.5292e+00, -1.8755e+00, -1.2606e+00,
        -1.3199e+00, -1.0873e+00, -1.6410e+00, -1.4387e+00, -1.4142e+00,
        -1.3340e+00, -1.5418e+00, -1.0987e+00, -1.3108e+00, -1.5585e+00,
        -1.1936e+00,  1.1597e+00,  1.2774e+00,  2.1429e+00,  2.3925e+00,
         5.1175e+00,  1.0259e-02,  2.9520e-01,  1.7876e-01, -6.2916e-02,
         1.6993e-01,  2.1804e-01,  2.7751e-01,  3.1913e-01,  3.1397e-01,
         2.7419e-01,  4.1840e-01,  3.4493e-01,  3.0308e-01,  4.4562e-01,
         1.3663e-01, -3.1574e-02,  2.0719e-01,  4.8316e-01,  2.9579e-01,
         1.6696e-01,  1.3599e-01,  2.4018e-01,  2.7569e-01,  3.6622e-01,
         1.0059e-01,  2.2104e-01,  2.9744e-01,  2.8530e-01,  2.0996e-01,
         2.8203e-01,  3.9194e-01,  2.3062e-01,  3.6046e-01, -1.1172e-01,
         7.9864e-02, -1.0692e-01,  4.1977e-02,  1.2895e-01,  1.9091e-01,
         2.5311e-01,  3.1190e-01,  1.9248e-01,  1.7644e+00,  2.6855e+00,
         3.8520e+00,  7.2792e-01,  9.8450e-01,  9.1591e-01,  9.8473e-01,
         9.8216e-01,  9.4255e-01,  8.8037e-01,  9.3419e-01,  8.3022e-01,
         8.3307e-01,  7.5841e-01,  7.8745e-01,  8.5069e-01,  6.1111e-01,
         6.0329e-01,  9.3836e-01,  5.0980e-01,  7.9688e-01,  2.9488e-01,
         7.5182e-01,  6.9914e-01,  8.6690e-01,  8.9322e-01,  8.6167e-01,
         1.0235e+00,  9.9309e-01,  6.0986e-01,  9.5400e-01,  1.0192e+00,
         1.0788e+00,  5.6244e-01,  8.3838e-01,  9.3742e-01,  1.0478e+00,
         5.4213e-01,  1.0242e+00,  6.9646e-01,  9.1373e-01,  9.3823e-01,
         4.8014e-01,  7.3843e-01,  8.8102e-01,  8.3624e-01,  9.1910e-01,
         7.4037e-01,  5.3478e-01,  9.5824e-01,  8.9927e-01,  5.1135e-01,
         9.1305e-01,  6.9801e-01,  6.7088e-01,  8.0161e-01,  9.3644e-01,
         9.4305e-01,  6.5034e-01,  7.1066e-01,  8.6900e-01,  8.5620e-01,
         1.0287e+00,  9.6095e-01,  9.4301e-01,  8.8176e-01,  1.0711e+00,
         8.8619e-01,  8.5719e-01,  3.7809e-01,  5.2979e-01,  8.2709e-01,
         9.4940e-01,  1.0013e+00,  8.0016e-01,  6.5643e-01,  6.0632e-01,
         7.6191e-01,  4.7265e-01,  3.9949e-01,  8.3894e-01,  5.1334e-01,
         6.4827e-01,  7.3172e-01,  7.0406e-01,  4.0322e-01,  7.2656e-01,
         3.2591e-01,  8.2084e-01,  6.8048e-01,  8.8326e-01,  7.4390e-01,
         7.1946e-01,  7.5438e-01,  3.2673e-01,  6.3495e-01,  6.9690e-01,
         5.9211e-01,  6.1540e-01,  7.1004e-01,  5.7257e-01,  5.6327e-01,
         6.1201e-01,  6.8618e-01,  7.1541e-01,  4.7369e-01,  5.9947e-01,
         7.2571e-01,  5.9333e-01,  6.8713e-01,  6.1043e-01,  8.0445e-01,
         5.4252e-01,  4.9610e-01,  6.3180e-01,  6.1993e-01,  7.1678e-01,
         6.4112e-01,  6.5205e-01,  2.0525e-01,  7.6502e-01,  3.0928e-01,
         7.2200e-01,  6.1289e-01,  6.7827e-01,  7.3616e-01,  8.1352e-01,
         7.2502e-01,  6.4639e-01,  7.6413e-01,  2.7738e-01,  6.4033e-01,
         4.0293e-01,  6.6872e-01,  5.7098e-01,  4.5647e-01,  1.7005e-01,
         5.0901e-01,  5.3893e-01,  5.8573e-01,  4.4238e-01,  1.9792e-01,
         1.0949e-01,  2.0420e-01,  1.1149e-03,  4.2783e-01,  4.0410e-01,
         4.2929e-01,  3.3759e-01,  2.6425e-01,  3.1304e-01,  3.1219e-01,
        -8.7148e-02,  3.5087e-01,  3.3342e-01,  5.1098e-01,  5.6902e-02,
         1.2032e-01,  1.6650e-01,  4.0148e-01,  3.8550e-01,  1.8600e-01,
        -2.7298e-02,  2.3074e-01,  4.3931e-01,  2.7972e-01,  1.3509e-01,
         1.7824e-01,  1.6312e-01,  1.9263e-01,  2.0938e-01, -1.2614e-02,
         3.7121e-01,  3.8188e-01,  2.9770e-01,  1.5555e-01,  3.1268e-01,
         2.8174e-01,  2.7470e-01,  1.8989e-01,  2.9625e-01,  2.4400e-01,
         1.6863e-01,  1.1720e-02,  5.7259e-03,  2.4274e-01,  1.7524e-01,
         1.9298e-01, -3.2142e-01,  2.5953e-01,  5.0108e-01,  3.1116e-01,
         3.9383e-01,  3.1123e-01,  2.8507e-01,  4.2955e-01,  2.7145e-01,
         3.3579e-01,  1.4126e-01,  2.8164e-01,  3.3850e-01,  3.4823e-01,
        -4.6125e-03,  9.2211e-02, -1.0053e-01, -1.1954e-01,  3.6243e-01,
        -1.5492e-01,  3.6272e-01, -1.2772e-01,  2.6676e-01,  2.7890e-01,
         2.4257e-01,  2.4024e-01,  8.8687e-02,  7.8481e-02,  2.4368e-01,
         1.8485e-01,  3.3534e-01, -7.7782e-02,  5.9127e-02,  1.7663e-01,
         1.3375e-01, -1.1554e-01,  2.7913e-01,  2.0722e-01,  2.9695e-01,
         4.7896e-02,  3.9278e-02,  1.3045e-01,  5.5980e-02, -3.7249e-01,
         6.8006e-02, -4.5289e-02,  7.7146e-02, -7.2967e-02,  8.3468e-02,
         1.6564e-02, -2.8548e-01, -4.6018e-01, -1.5134e-01, -4.7348e-01,
        -2.6336e-01, -3.5263e-01, -1.8889e-01,  5.6817e-02, -9.4614e-02,
         2.8224e-02, -3.6784e-01, -5.1181e-01, -1.6759e-01,  9.7090e-02,
        -3.4498e-01, -4.2713e-01, -4.2239e-01, -2.7776e-01, -2.0970e-01,
        -1.6628e-01, -3.7638e-01, -6.4584e-01, -1.8162e-01, -2.3262e-01,
        -2.7829e-01, -1.9331e-01, -2.4065e-01, -2.5395e-01, -3.8403e-01,
        -3.5557e-01, -6.1734e-01, -8.7295e-01, -5.6052e-01, -6.7699e-01,
        -5.1500e-01, -4.2419e-01, -3.6450e-01, -1.0815e+00, -8.4906e-01,
        -5.1190e-01, -6.8660e-01, -9.8131e-01, -6.7045e-01, -7.8620e-01,
        -6.9816e-01, -7.1382e-01, -9.9320e-01, -9.2818e-01, -8.9420e-01,
        -9.3121e-01, -7.2841e-01, -8.3238e-01, -7.8081e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808406
t6: 1641198808406
state_values: tensor([ 0.2213, -0.0458, -0.0195,  0.0269,  0.0955,  0.1495,  0.1593,  0.1599,
         0.1645,  0.1703,  0.2291,  0.1737,  0.1585,  0.1541,  0.1713,  0.3540,
         0.2549,  0.2048,  0.2368,  0.2393,  0.2641,  0.2413,  0.1013,  0.0725,
         0.1317,  0.1388,  0.1390,  0.2001,  0.1539,  0.1466,  0.3721,  0.2724,
         0.2761,  0.2061,  0.1783,  0.1710,  0.1685,  0.1738,  0.1858,  0.1706,
         0.1818,  0.1734,  0.1726,  0.2738,  0.1995,  0.1831,  0.1752,  0.2281,
         0.1844,  0.1821,  0.1789,  0.2128,  0.1821,  0.3249,  0.2257,  0.2007,
         0.2183,  0.3949,  0.2556,  0.3318,  0.2362,  0.2260,  0.2034,  0.2253,
         0.2172,  0.2117,  0.0921,  0.0735,  0.0534,  0.0550,  0.0319,  0.1037,
         0.1349,  0.1414,  0.1538,  0.2200,  0.1905,  0.1903,  0.1845,  0.1934,
         0.1824,  0.1798,  0.1886,  0.1841,  0.1988,  0.3243,  0.2337,  0.2093,
         0.2325,  0.2599,  0.2217,  0.2863,  0.2405,  0.2209,  0.2086,  0.2163,
         0.2914,  0.2508,  0.2369,  0.2779,  0.2547,  0.2256,  0.2247,  0.2230,
         0.2330,  0.2303,  0.2380,  0.2388,  0.2356,  0.2333,  0.2616,  0.2920,
         0.2858,  0.1573,  0.1049,  0.0801,  0.1612,  0.2147,  0.2020,  0.2164,
         0.2130,  0.2096,  0.2164,  0.2148,  0.2933,  0.2456,  0.3435,  0.2729,
         0.2516,  0.2572,  0.4592,  0.3346,  0.2948,  0.3072,  0.6485,  0.3979,
         0.4935,  0.3665,  0.3566,  0.3787,  0.3240,  0.3069,  0.3115,  0.3134,
         0.3191,  0.3149,  0.3118,  0.3025,  0.3762,  0.3139,  0.5927,  0.3853,
         0.3514,  0.3257,  0.3160,  0.6205,  0.4617,  0.4546,  0.3708,  0.3476,
         0.3471,  0.3581,  0.3315,  0.3469,  0.3431,  0.3458,  0.3442,  0.4262,
         0.3556,  0.3325,  0.3364,  0.3438,  0.3421,  0.3642,  0.3390,  0.3217,
         0.3225,  0.3481,  0.3441,  0.3292,  0.3204,  0.3759,  0.3605,  0.3555,
         0.3378,  0.3396,  0.3213,  0.3962,  0.5326,  0.5517,  0.4136,  0.3979,
         0.3865,  0.4535,  0.4068,  0.5667,  0.5020,  0.5355,  0.4551,  0.4820,
         0.4517,  0.4108,  0.4085,  0.4101,  0.4539,  0.5073,  0.4683,  0.4490,
         0.4176,  0.4658,  0.6044,  0.4615,  0.4409,  0.5179,  0.5677,  0.4631,
         0.4353,  0.4204,  0.4302,  0.5171,  0.4358,  0.5637,  0.5074,  0.4685,
         0.4270,  0.4279,  0.4297,  0.5724,  0.4592,  0.4665,  0.4709,  0.4871,
         0.4670,  0.4688,  0.4560,  0.4519,  0.4330,  0.4201,  0.4354,  0.4045,
         0.4046,  0.4239,  0.4230,  0.4388,  0.4135,  0.4292,  0.4116,  0.4688,
         0.4422,  0.4551,  0.4869,  0.5052,  0.4822,  0.5055,  0.7217,  0.5517,
         0.7993,  0.6011,  0.5620,  0.5678,  0.5678,  0.6128,  0.7291,  0.6150,
         0.6177,  0.5819,  0.6217,  0.5446,  0.5748,  0.5445,  0.7843,  0.5877,
         0.5589,  0.5606,  0.7793,  0.6161,  0.5617,  0.5606,  0.5297,  0.8223,
         0.6912,  0.7688,  0.6281,  0.5745,  0.5628,  0.5354,  0.5249,  0.5246,
         0.5283,  0.5700,  0.5308,  0.5684,  0.5340,  0.6058,  0.5960,  0.5445,
         0.5436,  0.5358,  0.5135,  0.5238,  0.5202,  0.6648,  0.6781,  0.5753,
         0.5556,  0.5395,  0.5425,  0.6955,  0.5972,  0.7478,  0.6283,  0.8123,
         0.6668,  0.6119,  0.6236,  0.9353,  0.6885,  0.6382,  0.6270,  0.6728,
         0.6423,  0.6684,  0.6335,  0.5958,  0.5793,  0.5752,  0.5661,  0.7943,
         0.6461,  0.7847,  0.6404,  0.5860,  0.6132,  0.5980,  0.6824,  0.6103,
         0.6060,  0.5735,  0.5901,  0.5827,  0.7278,  0.7358,  0.6418,  0.6385,
         0.6368,  0.5995,  0.8097,  0.6564,  0.6244,  0.6250,  0.6167,  0.8429,
         0.7074,  0.8546,  1.1116,  0.7898,  0.7289,  0.6904,  0.6499,  0.6621,
         0.6211,  0.9454,  0.7528,  0.6668,  0.6102,  0.6421,  0.9668,  0.9791,
         0.7952,  0.8061,  0.7223,  0.6894,  0.6978,  0.6672,  0.6491,  0.6427,
         0.7052,  0.6817,  0.6769,  0.7284,  0.6655,  0.9651,  0.7801,  0.7092,
         0.6992,  0.6814,  0.7050,  0.7124,  1.1574,  0.8353,  0.7911,  0.7944,
         0.7610,  0.9191,  1.0119,  0.8615,  0.7788,  0.7737,  0.9910,  0.9654,
         0.9068,  0.7775,  0.8244,  0.7940], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808411
t8: 1641198808411
t9: 1641198808411
t10: 1641198808421
t11: 1641198808423
t12: 1641198808423
t1: 1641198808423
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808433
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0040, 0.9980, 1.0171, 1.0043, 1.0366, 0.9990, 1.0071, 1.0215, 0.9968,
        0.9734, 1.0555, 0.9930, 1.0534, 0.9784, 0.8797, 0.9404, 1.1227, 0.9196,
        0.9343, 0.8944, 0.9400, 1.0315, 1.0048, 0.9405, 1.0532, 0.9968, 0.9222,
        1.0076, 0.9849, 0.7822, 0.9435, 0.9248, 1.0968, 1.0132, 1.0231, 0.9924,
        1.0973, 0.9680, 1.0355, 0.9785, 0.9846, 0.9953, 0.9141, 1.0691, 0.9822,
        1.0264, 0.9345, 1.0543, 1.0958, 0.9911, 0.9584, 1.0439, 0.9674, 0.9886,
        1.0362, 0.9640, 0.8577, 1.0839, 0.8698, 1.0132, 1.1011, 0.9967, 0.9732,
        1.1011, 0.9850, 1.0385, 1.0619, 1.0260, 0.9825, 0.9288, 1.1165, 0.9606,
        1.0465, 1.0820, 0.9330, 0.9837, 0.9660, 0.9823, 0.9679, 1.0317, 0.9976,
        0.9896, 1.0206, 0.9638, 0.8845, 1.0719, 1.0357, 0.9240, 0.9221, 1.0608,
        0.9121, 1.0008, 1.0111, 1.0237, 1.0590, 0.9384, 0.9837, 0.9924, 0.9375,
        0.9756, 1.0127, 1.0412, 0.9999, 1.1089, 1.0394, 1.0510, 1.0208, 1.0137,
        1.0102, 0.9859, 0.9602, 0.9717, 1.0403, 1.0572, 1.0431, 1.1644, 0.9639,
        1.0226, 0.9823, 0.9972, 1.0126, 1.0047, 1.0058, 0.9478, 1.0149, 0.9200,
        1.0352, 1.0521, 1.0799, 0.8956, 0.9921, 1.1893, 0.9551, 0.8069, 0.9850,
        0.8910, 0.9971, 0.9637, 0.9482, 0.9994, 1.0242, 1.1161, 0.9902, 0.9797,
        0.9832, 1.1313, 1.0293, 0.9593, 1.0008, 0.8830, 0.9986, 1.0836, 1.0405,
        1.0100, 0.8769, 0.9818, 0.9135, 1.0452, 1.0141, 1.0572, 1.0676, 1.0019,
        0.9922, 1.0945, 0.9887, 1.0261, 0.9687, 1.0343, 1.0089, 0.9964, 1.0394,
        1.0208, 0.9810, 1.0099, 0.9988, 1.0009, 0.9770, 0.9929, 0.9837, 1.0276,
        0.9606, 1.1591, 1.0535, 1.0162, 0.9956, 1.0041, 0.9636, 0.9083, 0.9291,
        1.0330, 1.1356, 1.2044, 0.9391, 1.0878, 0.9173, 0.9599, 0.9052, 1.1157,
        0.9383, 1.1429, 0.9993, 1.0285, 0.9827, 0.9575, 0.9298, 0.9638, 1.1085,
        1.0324, 0.9582, 0.9038, 1.0540, 0.9872, 0.9321, 0.9132, 1.0403, 1.0099,
        1.0283, 1.0546, 0.9550, 1.0071, 0.9291, 0.9524, 0.9927, 0.9999, 1.0614,
        1.0448, 0.9352, 1.0351, 0.9656, 0.9701, 0.9635, 1.1757, 0.9670, 1.1101,
        0.9859, 1.0309, 1.0186, 0.9892, 1.0064, 1.0044, 0.9969, 0.9881, 1.0672,
        1.0147, 1.0348, 1.0063, 0.9784, 1.0202, 1.0551, 0.9755, 0.9654, 0.9758,
        0.9663, 0.8899, 1.0845, 0.8446, 1.0263, 0.9661, 0.9568, 0.9456, 0.9207,
        0.8745, 0.9785, 0.9400, 1.1991, 0.9341, 1.0557, 0.9531, 1.1365, 0.9075,
        1.0601, 0.9834, 0.9713, 0.8815, 1.0698, 1.0298, 0.9725, 1.0576, 0.8971,
        0.9866, 0.8886, 1.0121, 1.0625, 1.1579, 1.0129, 1.0050, 1.0130, 1.0340,
        0.9784, 1.0138, 0.9786, 1.0349, 0.9560, 0.9625, 1.0371, 1.0503, 1.0488,
        1.0117, 1.0013, 1.0010, 1.0275, 0.9644, 0.9992, 1.0335, 1.0195, 1.0241,
        0.9509, 0.9895, 0.9125, 0.9880, 0.8696, 0.9919, 0.9903, 0.9476, 0.8241,
        1.0198, 1.1724, 1.2512, 0.9484, 1.2261, 0.9584, 1.1502, 1.0062, 1.0204,
        1.0184, 1.0234, 0.9518, 1.0315, 0.9163, 1.0274, 1.0294, 1.0820, 1.0440,
        0.9597, 1.0401, 1.0699, 1.0085, 0.9972, 0.9914, 0.9443, 0.9398, 0.9900,
        0.9821, 1.2086, 1.0217, 0.9285, 1.0065, 1.0436, 0.9808, 0.9984, 0.9185,
        1.1688, 0.8990, 0.8923, 1.0276, 1.0747, 1.0223, 0.9954, 0.9972, 1.0059,
        0.8964, 1.0437, 1.0440, 1.0061, 1.0616, 0.9037, 0.9644, 0.9876, 0.9076,
        0.9926, 1.0632, 1.1654, 0.9969, 1.0186, 1.0204, 0.9672, 0.9838, 0.9804,
        0.9552, 1.0088, 0.8863, 1.0622, 1.0529, 1.0847, 1.0279, 0.9835, 0.9710,
        0.8596, 1.0794, 0.9434, 0.9484, 1.1720, 0.8968, 0.9311, 0.9610, 0.9854,
        1.0879, 0.8871, 0.9561, 0.9409, 1.0174, 0.9451, 0.9766, 0.8347],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808438
t4: 1641198808438
surr1, surr2: tensor([-3.1742e+00, -9.7895e-01, -1.5031e+00, -2.4858e+00, -2.8467e+00,
        -2.9415e+00, -2.8027e+00, -2.8257e+00, -2.8157e+00, -2.5928e+00,
        -2.7647e+00, -2.6384e+00, -2.6221e+00, -2.5298e+00, -2.1572e+00,
        -2.4496e+00, -2.6900e+00, -2.3808e+00, -2.2529e+00, -2.0692e+00,
        -2.0843e+00, -2.2541e+00, -8.4852e-01, -1.6609e-01, -1.7222e+00,
        -1.7985e+00, -1.5323e+00, -1.7028e+00, -1.5757e+00, -1.2114e+00,
        -1.9411e+00, -1.6303e+00, -1.9037e+00, -1.9278e+00, -1.6577e+00,
        -1.5827e+00, -1.6329e+00, -1.7517e+00, -1.5900e+00, -1.5532e+00,
        -1.4977e+00, -1.4342e+00, -1.3746e+00, -1.7304e+00, -1.7138e+00,
        -1.4820e+00, -1.4344e+00, -1.6387e+00, -1.6827e+00, -1.7478e+00,
        -1.3755e+00, -1.6111e+00, -1.4900e+00, -1.8619e+00, -1.2911e+00,
        -1.2797e+00, -9.9121e-01, -1.6176e+00, -1.3644e+00, -1.4200e+00,
        -1.3703e+00, -1.5382e+00, -1.0811e+00, -1.3829e+00, -1.5403e+00,
        -1.2243e+00,  1.2161e+00,  1.3109e+00,  2.1062e+00,  2.2703e+00,
         5.1941e+00,  1.0061e-02,  3.0110e-01,  1.8791e-01, -5.9568e-02,
         1.6839e-01,  2.1393e-01,  2.7494e-01,  3.1339e-01,  3.1883e-01,
         2.7366e-01,  4.1533e-01,  3.4960e-01,  2.9427e-01,  4.1493e-01,
         1.3980e-01, -3.2087e-02,  1.9818e-01,  4.6678e-01,  3.0238e-01,
         1.5932e-01,  1.3591e-01,  2.4129e-01,  2.7907e-01,  3.7975e-01,
         9.5570e-02,  2.1896e-01,  2.9605e-01,  2.7521e-01,  2.0740e-01,
         2.8365e-01,  4.0121e-01,  2.3053e-01,  3.8888e-01, -1.1580e-01,
         8.4726e-02, -1.1008e-01,  4.2984e-02,  1.3128e-01,  1.8653e-01,
         2.4312e-01,  3.0579e-01,  1.9690e-01,  1.8314e+00,  2.7757e+00,
         4.2408e+00,  7.0187e-01,  1.0001e+00,  9.0155e-01,  9.8252e-01,
         9.9102e-01,  9.4582e-01,  8.8450e-01,  8.9197e-01,  8.3707e-01,
         7.8934e-01,  7.7075e-01,  8.0733e-01,  8.8922e-01,  5.6324e-01,
         6.0368e-01,  1.0146e+00,  4.9792e-01,  7.1449e-01,  2.9561e-01,
         7.2806e-01,  6.9824e-01,  8.5607e-01,  8.7658e-01,  8.6107e-01,
         1.0322e+00,  1.0528e+00,  6.0549e-01,  9.3950e-01,  1.0077e+00,
         1.1686e+00,  5.7662e-01,  8.0138e-01,  9.3746e-01,  9.6263e-01,
         5.4245e-01,  1.0472e+00,  7.0864e-01,  9.1869e-01,  8.5694e-01,
         4.8341e-01,  7.2030e-01,  8.9105e-01,  8.3950e-01,  9.4335e-01,
         7.7218e-01,  5.3549e-01,  9.5175e-01,  9.6713e-01,  5.0552e-01,
         9.3332e-01,  6.7375e-01,  6.8604e-01,  8.0777e-01,  9.3341e-01,
         9.7379e-01,  6.6403e-01,  6.9598e-01,  8.7579e-01,  8.5520e-01,
         1.0294e+00,  9.4169e-01,  9.3807e-01,  8.7091e-01,  1.0894e+00,
         8.5941e-01,  9.3461e-01,  3.9398e-01,  5.3820e-01,  8.2311e-01,
         9.5328e-01,  9.6528e-01,  7.5155e-01,  6.3473e-01,  6.1230e-01,
         8.0201e-01,  5.2186e-01,  3.8212e-01,  8.7766e-01,  4.8222e-01,
         6.3716e-01,  7.0279e-01,  7.2377e-01,  3.9124e-01,  7.6583e-01,
         3.2561e-01,  8.3562e-01,  6.7095e-01,  8.5839e-01,  7.1318e-01,
         7.0631e-01,  7.9056e-01,  3.3292e-01,  6.1537e-01,  6.5824e-01,
         6.0590e-01,  6.1086e-01,  6.8428e-01,  5.4912e-01,  5.7216e-01,
         6.1472e-01,  6.9702e-01,  7.4163e-01,  4.5551e-01,  6.0161e-01,
         6.8952e-01,  5.7895e-01,  6.8431e-01,  6.0992e-01,  8.3062e-01,
         5.5923e-01,  4.6858e-01,  6.4401e-01,  6.0672e-01,  7.0568e-01,
         6.2821e-01,  7.0839e-01,  2.0038e-01,  8.1376e-01,  3.0552e-01,
         7.3814e-01,  6.2350e-01,  6.7112e-01,  7.3997e-01,  8.1666e-01,
         7.2294e-01,  6.3969e-01,  8.0218e-01,  2.8111e-01,  6.6230e-01,
         4.0598e-01,  6.5215e-01,  5.7994e-01,  4.8043e-01,  1.6579e-01,
         4.9505e-01,  5.3005e-01,  5.7318e-01,  4.1373e-01,  2.0426e-01,
         1.0085e-01,  2.0329e-01,  1.1034e-03,  4.2183e-01,  3.9458e-01,
         4.1453e-01,  3.2040e-01,  2.6264e-01,  3.0700e-01,  3.4032e-01,
        -8.3921e-02,  3.5871e-01,  3.2318e-01,  5.4142e-01,  5.3104e-02,
         1.2259e-01,  1.6492e-01,  3.9532e-01,  3.6006e-01,  1.8945e-01,
        -2.7660e-02,  2.2705e-01,  4.5265e-01,  2.5790e-01,  1.3554e-01,
         1.7183e-01,  1.6358e-01,  1.9590e-01,  2.2457e-01, -1.2714e-02,
         3.7262e-01,  3.8615e-01,  3.0717e-01,  1.5183e-01,  3.1580e-01,
         2.7616e-01,  2.8128e-01,  1.8251e-01,  2.8954e-01,  2.4878e-01,
         1.7401e-01,  1.2170e-02,  5.7900e-03,  2.4307e-01,  1.7542e-01,
         1.9824e-01, -3.0921e-01,  2.5930e-01,  5.1367e-01,  3.1602e-01,
         4.0216e-01,  2.9591e-01,  2.8289e-01,  4.0637e-01,  2.6980e-01,
         3.1580e-01,  1.4091e-01,  2.8057e-01,  3.3116e-01,  3.1884e-01,
        -4.6278e-03,  9.8281e-02, -1.1435e-01, -1.1529e-01,  4.0397e-01,
        -1.4967e-01,  3.9157e-01, -1.2836e-01,  2.7159e-01,  2.8412e-01,
         2.4879e-01,  2.2633e-01,  9.0627e-02,  7.3864e-02,  2.4569e-01,
         1.8725e-01,  3.4931e-01, -8.0111e-02,  5.7017e-02,  1.8075e-01,
         1.4113e-01, -1.1641e-01,  2.7836e-01,  2.0551e-01,  2.8348e-01,
         4.6159e-02,  3.9058e-02,  1.2921e-01,  6.1661e-02, -3.7781e-01,
         6.3988e-02, -4.5435e-02,  7.8887e-02, -7.2093e-02,  8.3346e-02,
         1.5650e-02, -3.0833e-01, -4.3273e-01, -1.4616e-01, -4.7277e-01,
        -2.6775e-01, -3.5529e-01, -1.8831e-01,  5.6694e-02, -9.4945e-02,
         2.6144e-02, -3.6980e-01, -5.2206e-01, -1.6805e-01,  1.0073e-01,
        -3.1818e-01, -4.2834e-01, -4.2102e-01, -2.6979e-01, -2.0913e-01,
        -1.6956e-01, -4.0734e-01, -6.4422e-01, -1.8416e-01, -2.3682e-01,
        -2.6941e-01, -1.9125e-01, -2.3741e-01, -2.4685e-01, -3.8562e-01,
        -3.2809e-01, -6.1162e-01, -8.9227e-01, -5.8530e-01, -6.9020e-01,
        -5.0755e-01, -4.1503e-01, -3.3084e-01, -1.0691e+00, -8.3322e-01,
        -5.0497e-01, -7.3151e-01, -9.2170e-01, -6.5722e-01, -7.8065e-01,
        -6.9527e-01, -7.3055e-01, -9.3251e-01, -9.1610e-01, -8.7881e-01,
        -9.3434e-01, -7.1249e-01, -8.2521e-01, -7.1598e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1742e+00, -9.7895e-01, -1.5031e+00, -2.4858e+00, -2.8467e+00,
        -2.9415e+00, -2.8027e+00, -2.8257e+00, -2.8157e+00, -2.5928e+00,
        -2.7647e+00, -2.6384e+00, -2.6221e+00, -2.5298e+00, -2.2071e+00,
        -2.4496e+00, -2.6357e+00, -2.3808e+00, -2.2529e+00, -2.0821e+00,
        -2.0843e+00, -2.2541e+00, -8.4852e-01, -1.6609e-01, -1.7222e+00,
        -1.7985e+00, -1.5323e+00, -1.7028e+00, -1.5757e+00, -1.3938e+00,
        -1.9411e+00, -1.6303e+00, -1.9037e+00, -1.9278e+00, -1.6577e+00,
        -1.5827e+00, -1.6329e+00, -1.7517e+00, -1.5900e+00, -1.5532e+00,
        -1.4977e+00, -1.4342e+00, -1.3746e+00, -1.7304e+00, -1.7138e+00,
        -1.4820e+00, -1.4344e+00, -1.6387e+00, -1.6827e+00, -1.7478e+00,
        -1.3755e+00, -1.6111e+00, -1.4900e+00, -1.8619e+00, -1.2911e+00,
        -1.2797e+00, -1.0400e+00, -1.6176e+00, -1.4118e+00, -1.4200e+00,
        -1.3689e+00, -1.5382e+00, -1.0811e+00, -1.3815e+00, -1.5403e+00,
        -1.2243e+00,  1.2161e+00,  1.3109e+00,  2.1062e+00,  2.2703e+00,
         5.1175e+00,  1.0061e-02,  3.0110e-01,  1.8791e-01, -5.9568e-02,
         1.6839e-01,  2.1393e-01,  2.7494e-01,  3.1339e-01,  3.1883e-01,
         2.7366e-01,  4.1533e-01,  3.4960e-01,  2.9427e-01,  4.2218e-01,
         1.3980e-01, -3.2087e-02,  1.9818e-01,  4.6678e-01,  3.0238e-01,
         1.5932e-01,  1.3591e-01,  2.4129e-01,  2.7907e-01,  3.7975e-01,
         9.5570e-02,  2.1896e-01,  2.9605e-01,  2.7521e-01,  2.0740e-01,
         2.8365e-01,  4.0121e-01,  2.3053e-01,  3.8576e-01, -1.1580e-01,
         8.4726e-02, -1.1008e-01,  4.2984e-02,  1.3128e-01,  1.8653e-01,
         2.4312e-01,  3.0579e-01,  1.9690e-01,  1.8314e+00,  2.7757e+00,
         4.0062e+00,  7.0187e-01,  1.0001e+00,  9.0155e-01,  9.8252e-01,
         9.9102e-01,  9.4582e-01,  8.8450e-01,  8.9197e-01,  8.3707e-01,
         7.8934e-01,  7.7075e-01,  8.0733e-01,  8.8922e-01,  5.6602e-01,
         6.0368e-01,  9.3836e-01,  4.9792e-01,  7.9688e-01,  2.9561e-01,
         7.3542e-01,  6.9824e-01,  8.5607e-01,  8.7658e-01,  8.6107e-01,
         1.0322e+00,  1.0376e+00,  6.0549e-01,  9.3950e-01,  1.0077e+00,
         1.1363e+00,  5.7662e-01,  8.0138e-01,  9.3746e-01,  9.8114e-01,
         5.4245e-01,  1.0472e+00,  7.0864e-01,  9.1869e-01,  8.7946e-01,
         4.8341e-01,  7.2030e-01,  8.9105e-01,  8.3950e-01,  9.4335e-01,
         7.7218e-01,  5.3549e-01,  9.5175e-01,  9.6713e-01,  5.0552e-01,
         9.3332e-01,  6.7375e-01,  6.8604e-01,  8.0777e-01,  9.3341e-01,
         9.7379e-01,  6.6403e-01,  6.9598e-01,  8.7579e-01,  8.5520e-01,
         1.0294e+00,  9.4169e-01,  9.3807e-01,  8.7091e-01,  1.0894e+00,
         8.5941e-01,  8.8694e-01,  3.9398e-01,  5.3820e-01,  8.2311e-01,
         9.5328e-01,  9.6528e-01,  7.5155e-01,  6.3473e-01,  6.1230e-01,
         7.7685e-01,  4.7662e-01,  3.8212e-01,  8.7766e-01,  4.8222e-01,
         6.3716e-01,  7.0279e-01,  7.1356e-01,  3.9124e-01,  7.3710e-01,
         3.2561e-01,  8.3562e-01,  6.7095e-01,  8.5839e-01,  7.1318e-01,
         7.0631e-01,  7.8450e-01,  3.3292e-01,  6.1537e-01,  6.5824e-01,
         6.0590e-01,  6.1086e-01,  6.8428e-01,  5.4912e-01,  5.7216e-01,
         6.1472e-01,  6.9702e-01,  7.4163e-01,  4.5551e-01,  6.0161e-01,
         6.8952e-01,  5.7895e-01,  6.8431e-01,  6.0992e-01,  8.3062e-01,
         5.5923e-01,  4.6858e-01,  6.4401e-01,  6.0672e-01,  7.0568e-01,
         6.2821e-01,  6.6277e-01,  2.0038e-01,  8.0634e-01,  3.0552e-01,
         7.3814e-01,  6.2350e-01,  6.7112e-01,  7.3997e-01,  8.1666e-01,
         7.2294e-01,  6.3969e-01,  8.0218e-01,  2.8111e-01,  6.6230e-01,
         4.0598e-01,  6.5215e-01,  5.7994e-01,  4.8043e-01,  1.6579e-01,
         4.9505e-01,  5.3005e-01,  5.7318e-01,  4.1844e-01,  2.0426e-01,
         1.0747e-01,  2.0329e-01,  1.1034e-03,  4.2183e-01,  3.9458e-01,
         4.1453e-01,  3.2973e-01,  2.6264e-01,  3.0700e-01,  3.1219e-01,
        -8.3921e-02,  3.5871e-01,  3.2318e-01,  5.2405e-01,  5.3104e-02,
         1.2259e-01,  1.6492e-01,  3.9532e-01,  3.6760e-01,  1.8945e-01,
        -2.7660e-02,  2.2705e-01,  4.5265e-01,  2.5872e-01,  1.3554e-01,
         1.7403e-01,  1.6358e-01,  1.9590e-01,  2.1334e-01, -1.2714e-02,
         3.7262e-01,  3.8615e-01,  3.0717e-01,  1.5183e-01,  3.1580e-01,
         2.7616e-01,  2.8128e-01,  1.8251e-01,  2.8954e-01,  2.4878e-01,
         1.7401e-01,  1.2170e-02,  5.7900e-03,  2.4307e-01,  1.7542e-01,
         1.9824e-01, -3.0921e-01,  2.5930e-01,  5.1367e-01,  3.1602e-01,
         4.0216e-01,  2.9591e-01,  2.8289e-01,  4.0637e-01,  2.6980e-01,
         3.2685e-01,  1.4091e-01,  2.8057e-01,  3.3116e-01,  3.4823e-01,
        -4.6278e-03,  9.2211e-02, -1.0053e-01, -1.1529e-01,  3.6243e-01,
        -1.4967e-01,  3.7448e-01, -1.2836e-01,  2.7159e-01,  2.8412e-01,
         2.4879e-01,  2.2633e-01,  9.0627e-02,  7.3864e-02,  2.4569e-01,
         1.8725e-01,  3.4931e-01, -8.0111e-02,  5.7017e-02,  1.8075e-01,
         1.4113e-01, -1.1641e-01,  2.7836e-01,  2.0551e-01,  2.8348e-01,
         4.6159e-02,  3.9058e-02,  1.2921e-01,  5.6121e-02, -3.7781e-01,
         6.3988e-02, -4.5435e-02,  7.8887e-02, -7.2093e-02,  8.3346e-02,
         1.5650e-02, -2.9018e-01, -4.3320e-01, -1.4742e-01, -4.7277e-01,
        -2.6775e-01, -3.5529e-01, -1.8831e-01,  5.6694e-02, -9.4945e-02,
         2.6249e-02, -3.6980e-01, -5.2206e-01, -1.6805e-01,  1.0073e-01,
        -3.1818e-01, -4.2834e-01, -4.2102e-01, -2.6979e-01, -2.0913e-01,
        -1.6956e-01, -3.8447e-01, -6.4422e-01, -1.8416e-01, -2.3682e-01,
        -2.6941e-01, -1.9125e-01, -2.3741e-01, -2.4685e-01, -3.8562e-01,
        -3.3317e-01, -6.1162e-01, -8.9227e-01, -5.8530e-01, -6.9020e-01,
        -5.0755e-01, -4.1503e-01, -3.4637e-01, -1.0691e+00, -8.3322e-01,
        -5.0497e-01, -6.8660e-01, -9.2498e-01, -6.5722e-01, -7.8065e-01,
        -6.9527e-01, -7.3055e-01, -9.4607e-01, -9.1610e-01, -8.7881e-01,
        -9.3434e-01, -7.1249e-01, -8.2521e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808447
t6: 1641198808447
state_values: tensor([ 0.1942, -0.0411, -0.0151,  0.0341,  0.0968,  0.1437,  0.1509,  0.1495,
         0.1516,  0.1545,  0.1967,  0.1516,  0.1385,  0.1350,  0.1471,  0.2922,
         0.2033,  0.1648,  0.1834,  0.1835,  0.2016,  0.1822,  0.0876,  0.0643,
         0.1114,  0.1160,  0.1154,  0.1529,  0.1232,  0.1186,  0.2810,  0.1990,
         0.1998,  0.1550,  0.1375,  0.1322,  0.1301,  0.1339,  0.1427,  0.1314,
         0.1395,  0.1332,  0.1323,  0.1944,  0.1499,  0.1391,  0.1333,  0.1639,
         0.1392,  0.1376,  0.1353,  0.1551,  0.1370,  0.2307,  0.1635,  0.1501,
         0.1594,  0.2731,  0.1803,  0.2308,  0.1691,  0.1639,  0.1513,  0.1637,
         0.1593,  0.1563,  0.0719,  0.0568,  0.0432,  0.0448,  0.0274,  0.0831,
         0.1053,  0.1082,  0.1144,  0.1559,  0.1407,  0.1408,  0.1365,  0.1429,
         0.1343,  0.1317,  0.1391,  0.1353,  0.1465,  0.2189,  0.1653,  0.1525,
         0.1637,  0.1780,  0.1588,  0.1911,  0.1691,  0.1588,  0.1523,  0.1565,
         0.1932,  0.1751,  0.1678,  0.1873,  0.1773,  0.1618,  0.1614,  0.1604,
         0.1661,  0.1647,  0.1691,  0.1697,  0.1679,  0.1666,  0.1815,  0.1937,
         0.1923,  0.1095,  0.0788,  0.0582,  0.1146,  0.1537,  0.1469,  0.1563,
         0.1544,  0.1524,  0.1563,  0.1555,  0.1932,  0.1716,  0.2212,  0.1846,
         0.1745,  0.1774,  0.2882,  0.2158,  0.1954,  0.2012,  0.3976,  0.2516,
         0.3031,  0.2318,  0.2268,  0.2380,  0.2083,  0.2010,  0.2031,  0.2041,
         0.2070,  0.2051,  0.2037,  0.1995,  0.2369,  0.2042,  0.3600,  0.2414,
         0.2222,  0.2088,  0.2049,  0.3766,  0.2851,  0.2796,  0.2314,  0.2187,
         0.2187,  0.2250,  0.2112,  0.2194,  0.2168,  0.2188,  0.2180,  0.2643,
         0.2235,  0.2117,  0.2136,  0.2172,  0.2164,  0.2295,  0.2152,  0.2072,
         0.2074,  0.2204,  0.2179,  0.2108,  0.2067,  0.2355,  0.2260,  0.2230,
         0.2140,  0.2150,  0.2069,  0.2459,  0.3201,  0.3290,  0.2547,  0.2466,
         0.2401,  0.2764,  0.2514,  0.3393,  0.3005,  0.3184,  0.2771,  0.2915,
         0.2760,  0.2536,  0.2530,  0.2545,  0.2778,  0.3046,  0.2861,  0.2754,
         0.2583,  0.2835,  0.3617,  0.2806,  0.2705,  0.3096,  0.3388,  0.2811,
         0.2672,  0.2595,  0.2651,  0.3097,  0.2677,  0.3367,  0.3058,  0.2850,
         0.2635,  0.2641,  0.2654,  0.3427,  0.2797,  0.2844,  0.2870,  0.2961,
         0.2847,  0.2862,  0.2795,  0.2780,  0.2681,  0.2612,  0.2700,  0.2533,
         0.2533,  0.2639,  0.2637,  0.2718,  0.2582,  0.2668,  0.2573,  0.2873,
         0.2738,  0.2799,  0.2970,  0.3061,  0.2938,  0.3072,  0.4360,  0.3315,
         0.4868,  0.3601,  0.3383,  0.3428,  0.3434,  0.3671,  0.4371,  0.3697,
         0.3717,  0.3505,  0.3720,  0.3274,  0.3464,  0.3284,  0.4778,  0.3535,
         0.3373,  0.3392,  0.4744,  0.3709,  0.3389,  0.3392,  0.3204,  0.5042,
         0.4160,  0.4642,  0.3771,  0.3457,  0.3397,  0.3238,  0.3183,  0.3183,
         0.3206,  0.3468,  0.3223,  0.3457,  0.3242,  0.3643,  0.3610,  0.3295,
         0.3292,  0.3249,  0.3134,  0.3187,  0.3169,  0.4026,  0.4064,  0.3471,
         0.3357,  0.3270,  0.3290,  0.4161,  0.3603,  0.4499,  0.3781,  0.4942,
         0.3999,  0.3682,  0.3767,  0.5816,  0.4126,  0.3836,  0.3783,  0.4022,
         0.3868,  0.4001,  0.3819,  0.3605,  0.3508,  0.3488,  0.3438,  0.4840,
         0.3894,  0.4760,  0.3852,  0.3540,  0.3712,  0.3623,  0.4078,  0.3687,
         0.3668,  0.3481,  0.3579,  0.3536,  0.4372,  0.4412,  0.3861,  0.3853,
         0.3845,  0.3630,  0.4931,  0.3939,  0.3770,  0.3782,  0.3734,  0.5167,
         0.4246,  0.5229,  0.6922,  0.4786,  0.4376,  0.4142,  0.3916,  0.3990,
         0.3764,  0.5891,  0.4555,  0.4011,  0.3698,  0.3884,  0.6048,  0.6102,
         0.4844,  0.4884,  0.4334,  0.4138,  0.4196,  0.4020,  0.3920,  0.3889,
         0.4240,  0.4108,  0.4084,  0.4410,  0.4009,  0.6028,  0.4745,  0.4263,
         0.4203,  0.4102,  0.4255,  0.4312,  0.7293,  0.5128,  0.4839,  0.4873,
         0.4632,  0.5703,  0.6304,  0.5312,  0.4743,  0.4712,  0.6195,  0.6012,
         0.5586,  0.4712,  0.5051,  0.4856], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808451
t8: 1641198808451
t9: 1641198808452
t10: 1641198808462
t11: 1641198808464
t12: 1641198808464
t1: 1641198808464
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808474
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0026, 0.9919, 1.0252, 1.0096, 1.0650, 0.9974, 1.0147, 1.0520, 0.9910,
        0.9484, 1.0783, 0.9889, 1.0763, 0.9648, 0.8322, 0.9464, 1.1310, 0.9080,
        0.9252, 0.8830, 0.9319, 1.0339, 1.0048, 0.9328, 1.0617, 0.9949, 0.8943,
        1.0082, 0.9801, 0.7335, 0.9464, 0.9178, 1.1183, 1.0152, 1.0274, 0.9897,
        1.1290, 0.9528, 1.0467, 0.9679, 0.9783, 0.9928, 0.8805, 1.0760, 0.9766,
        1.0325, 0.9105, 1.0663, 1.1287, 0.9864, 0.9406, 1.0573, 0.9533, 0.9840,
        1.0494, 0.9468, 0.8137, 1.0719, 0.8568, 1.0161, 1.1116, 0.9950, 0.9647,
        1.1321, 0.9778, 1.0526, 1.0897, 1.0407, 0.9721, 0.9020, 1.1025, 0.9510,
        1.0562, 1.1105, 0.9034, 0.9786, 0.9562, 0.9769, 0.9577, 1.0397, 0.9959,
        0.9848, 1.0277, 0.9476, 0.8496, 1.0781, 1.0432, 0.9017, 0.9113, 1.0669,
        0.8902, 1.0003, 1.0128, 1.0297, 1.0800, 0.9111, 0.9783, 0.9894, 0.9182,
        0.9688, 1.0152, 1.0542, 0.9990, 1.1569, 1.0605, 1.0860, 1.0376, 1.0266,
        1.0198, 0.9725, 0.9382, 0.9606, 1.0524, 1.0792, 1.0624, 1.2285, 0.9434,
        1.0308, 0.9735, 0.9953, 1.0170, 1.0059, 1.0078, 0.9228, 1.0188, 0.8933,
        1.0413, 1.0651, 1.1067, 0.8538, 0.9918, 1.2104, 0.9419, 0.7640, 0.9861,
        0.8836, 0.9960, 0.9585, 0.9384, 0.9989, 1.0271, 1.1484, 0.9854, 0.9707,
        0.9765, 1.1810, 1.0434, 0.9345, 1.0002, 0.8437, 0.9989, 1.0901, 1.0473,
        1.0122, 0.8329, 0.9849, 0.9081, 1.0514, 1.0155, 1.0666, 1.0908, 1.0020,
        0.9878, 1.1387, 0.9815, 1.0382, 0.9494, 1.0465, 1.0125, 0.9939, 1.0576,
        1.0321, 0.9688, 1.0136, 0.9975, 1.0006, 0.9653, 0.9893, 0.9762, 1.0366,
        0.9437, 1.2161, 1.0779, 1.0247, 0.9922, 1.0058, 0.9436, 0.8772, 0.9209,
        1.0353, 1.1574, 1.2707, 0.9142, 1.1142, 0.8865, 0.9545, 0.8966, 1.1283,
        0.9240, 1.1668, 0.9981, 1.0382, 0.9745, 0.9418, 0.9081, 0.9537, 1.1356,
        1.0428, 0.9405, 0.8748, 1.0629, 0.9828, 0.9130, 0.8941, 1.0455, 1.0117,
        1.0367, 1.0753, 0.9334, 1.0084, 0.9031, 0.9412, 0.9902, 0.9988, 1.0796,
        1.0620, 0.9048, 1.0457, 0.9544, 0.9611, 0.9522, 1.2295, 0.9530, 1.1488,
        0.9784, 1.0430, 1.0276, 0.9827, 1.0086, 1.0059, 0.9946, 0.9817, 1.0958,
        1.0217, 1.0539, 1.0099, 0.9639, 1.0285, 1.0844, 0.9605, 0.9496, 0.9664,
        0.9544, 0.8569, 1.0930, 0.8098, 1.0295, 0.9623, 0.9512, 0.9384, 0.9074,
        0.8608, 0.9754, 0.9344, 1.2241, 0.9150, 1.0655, 0.9366, 1.1711, 0.8719,
        1.0678, 0.9786, 0.9630, 0.8499, 1.0754, 1.0348, 0.9640, 1.0737, 0.8573,
        0.9866, 0.8792, 1.0135, 1.0691, 1.1894, 1.0168, 1.0065, 1.0186, 1.0512,
        0.9647, 1.0188, 0.9673, 1.0479, 0.9345, 0.9494, 1.0473, 1.0681, 1.0709,
        1.0174, 1.0014, 1.0009, 1.0424, 0.9429, 0.9980, 1.0469, 1.0278, 1.0356,
        0.9234, 0.9847, 0.8857, 0.9856, 0.8442, 0.9902, 0.9891, 0.9405, 0.8013,
        1.0221, 1.1876, 1.3165, 0.9281, 1.2952, 0.9397, 1.1987, 1.0083, 1.0301,
        1.0283, 1.0373, 0.9198, 1.0451, 0.8875, 1.0314, 1.0348, 1.1040, 1.0610,
        0.9395, 1.0528, 1.1009, 1.0122, 0.9950, 0.9862, 0.9196, 0.9205, 0.9866,
        0.9763, 1.2741, 1.0294, 0.8965, 1.0079, 1.0549, 0.9735, 0.9969, 0.8893,
        1.2072, 0.8689, 0.8752, 1.0268, 1.0793, 1.0256, 0.9932, 0.9955, 1.0072,
        0.8587, 1.0466, 1.0513, 1.0069, 1.0825, 0.8632, 0.9634, 0.9862, 0.8987,
        0.9910, 1.0702, 1.2132, 0.9947, 1.0259, 1.0297, 0.9493, 0.9774, 0.9727,
        0.9396, 1.0105, 0.8470, 1.0614, 1.0599, 1.1083, 1.0386, 0.9748, 0.9586,
        0.8146, 1.0694, 0.9386, 0.9417, 1.1912, 0.8669, 0.9192, 0.9580, 0.9838,
        1.1000, 0.8584, 0.9484, 0.9355, 1.0188, 0.9374, 0.9733, 0.8061],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808478
t4: 1641198808478
surr1, surr2: tensor([-3.1697e+00, -9.7299e-01, -1.5150e+00, -2.4989e+00, -2.9247e+00,
        -2.9366e+00, -2.8239e+00, -2.9102e+00, -2.7992e+00, -2.5263e+00,
        -2.8243e+00, -2.6274e+00, -2.6792e+00, -2.4944e+00, -2.0409e+00,
        -2.4651e+00, -2.7100e+00, -2.3508e+00, -2.2309e+00, -2.0427e+00,
        -2.0664e+00, -2.2593e+00, -8.4859e-01, -1.6472e-01, -1.7361e+00,
        -1.7951e+00, -1.4859e+00, -1.7037e+00, -1.5682e+00, -1.1359e+00,
        -1.9471e+00, -1.6180e+00, -1.9411e+00, -1.9317e+00, -1.6646e+00,
        -1.5784e+00, -1.6801e+00, -1.7242e+00, -1.6071e+00, -1.5364e+00,
        -1.4881e+00, -1.4306e+00, -1.3241e+00, -1.7414e+00, -1.7039e+00,
        -1.4908e+00, -1.3975e+00, -1.6575e+00, -1.7331e+00, -1.7396e+00,
        -1.3499e+00, -1.6318e+00, -1.4683e+00, -1.8531e+00, -1.3075e+00,
        -1.2569e+00, -9.4026e-01, -1.5997e+00, -1.3441e+00, -1.4239e+00,
        -1.3834e+00, -1.5357e+00, -1.0717e+00, -1.4219e+00, -1.5290e+00,
        -1.2409e+00,  1.2479e+00,  1.3296e+00,  2.0841e+00,  2.2048e+00,
         5.1293e+00,  9.9611e-03,  3.0388e-01,  1.9285e-01, -5.7679e-02,
         1.6752e-01,  2.1177e-01,  2.7343e-01,  3.1009e-01,  3.2132e-01,
         2.7318e-01,  4.1333e-01,  3.5202e-01,  2.8932e-01,  3.9854e-01,
         1.4061e-01, -3.2320e-02,  1.9340e-01,  4.6128e-01,  3.0412e-01,
         1.5549e-01,  1.3584e-01,  2.4170e-01,  2.8072e-01,  3.8726e-01,
         9.2787e-02,  2.1777e-01,  2.9516e-01,  2.6955e-01,  2.0596e-01,
         2.8436e-01,  4.0624e-01,  2.3032e-01,  4.0570e-01, -1.1815e-01,
         8.7547e-02, -1.1189e-01,  4.3532e-02,  1.3252e-01,  1.8400e-01,
         2.3755e-01,  3.0229e-01,  1.9918e-01,  1.8696e+00,  2.8270e+00,
         4.4741e+00,  6.8690e-01,  1.0081e+00,  8.9345e-01,  9.8064e-01,
         9.9533e-01,  9.4702e-01,  8.8623e-01,  8.6848e-01,  8.4031e-01,
         7.6643e-01,  7.7526e-01,  8.1728e-01,  9.1135e-01,  5.3695e-01,
         6.0346e-01,  1.0326e+00,  4.9106e-01,  6.7648e-01,  2.9596e-01,
         7.2199e-01,  6.9750e-01,  8.5143e-01,  8.6756e-01,  8.6062e-01,
         1.0352e+00,  1.0833e+00,  6.0251e-01,  9.3092e-01,  1.0008e+00,
         1.2200e+00,  5.8450e-01,  7.8063e-01,  9.3688e-01,  9.1979e-01,
         5.4262e-01,  1.0534e+00,  7.1326e-01,  9.2068e-01,  8.1391e-01,
         4.8492e-01,  7.1604e-01,  8.9630e-01,  8.4064e-01,  9.5178e-01,
         7.8897e-01,  5.3552e-01,  9.4752e-01,  1.0062e+00,  5.0186e-01,
         9.4425e-01,  6.6036e-01,  6.9416e-01,  8.1061e-01,  9.3107e-01,
         9.9083e-01,  6.7136e-01,  6.8730e-01,  8.7901e-01,  8.5409e-01,
         1.0290e+00,  9.3041e-01,  9.3467e-01,  8.6435e-01,  1.0990e+00,
         8.4430e-01,  9.8055e-01,  4.0309e-01,  5.4270e-01,  8.2033e-01,
         9.5484e-01,  9.4523e-01,  7.2581e-01,  6.2919e-01,  6.1371e-01,
         8.1737e-01,  5.5059e-01,  3.7200e-01,  8.9901e-01,  4.6603e-01,
         6.3361e-01,  6.9609e-01,  7.3193e-01,  3.8527e-01,  7.8187e-01,
         3.2523e-01,  8.4350e-01,  6.6532e-01,  8.4431e-01,  6.9650e-01,
         6.9894e-01,  8.0989e-01,  3.3629e-01,  6.0402e-01,  6.3712e-01,
         6.1103e-01,  6.0816e-01,  6.7031e-01,  5.3763e-01,  5.7498e-01,
         6.1577e-01,  7.0270e-01,  7.5622e-01,  4.4523e-01,  6.0241e-01,
         6.7025e-01,  5.7214e-01,  6.8254e-01,  6.0927e-01,  8.4483e-01,
         5.6845e-01,  4.5330e-01,  6.5064e-01,  5.9966e-01,  6.9912e-01,
         6.2083e-01,  7.4081e-01,  1.9747e-01,  8.4210e-01,  3.0318e-01,
         7.4681e-01,  6.2903e-01,  6.6670e-01,  7.4163e-01,  8.1787e-01,
         7.2128e-01,  6.3552e-01,  8.2367e-01,  2.8305e-01,  6.7455e-01,
         4.0743e-01,  6.4252e-01,  5.8462e-01,  4.9380e-01,  1.6325e-01,
         4.8698e-01,  5.2491e-01,  5.6610e-01,  3.9841e-01,  2.0586e-01,
         9.6699e-02,  2.0392e-01,  1.0991e-03,  4.1934e-01,  3.9157e-01,
         4.0853e-01,  3.1537e-01,  2.6180e-01,  3.0516e-01,  3.4741e-01,
        -8.2205e-02,  3.6203e-01,  3.1759e-01,  5.5793e-01,  5.1026e-02,
         1.2349e-01,  1.6410e-01,  3.9196e-01,  3.4714e-01,  1.9044e-01,
        -2.7794e-02,  2.2508e-01,  4.5952e-01,  2.4643e-01,  1.3554e-01,
         1.7000e-01,  1.6381e-01,  1.9711e-01,  2.3069e-01, -1.2763e-02,
         3.7317e-01,  3.8828e-01,  3.1228e-01,  1.4969e-01,  3.1736e-01,
         2.7297e-01,  2.8480e-01,  1.7840e-01,  2.8561e-01,  2.5123e-01,
         1.7696e-01,  1.2426e-02,  5.8229e-03,  2.4310e-01,  1.7541e-01,
         2.0111e-01, -3.0230e-01,  2.5900e-01,  5.2034e-01,  3.1858e-01,
         4.0667e-01,  2.8736e-01,  2.8149e-01,  3.9445e-01,  2.6916e-01,
         3.0658e-01,  1.4068e-01,  2.8024e-01,  3.2867e-01,  3.1005e-01,
        -4.6385e-03,  9.9554e-02, -1.2032e-01, -1.1282e-01,  4.2675e-01,
        -1.4676e-01,  4.0809e-01, -1.2864e-01,  2.7416e-01,  2.8688e-01,
         2.5218e-01,  2.1872e-01,  9.1822e-02,  7.1547e-02,  2.4665e-01,
         1.8823e-01,  3.5641e-01, -8.1419e-02,  5.5817e-02,  1.8295e-01,
         1.4521e-01, -1.1684e-01,  2.7773e-01,  2.0443e-01,  2.7608e-01,
         4.5211e-02,  3.8921e-02,  1.2845e-01,  6.5002e-02, -3.8065e-01,
         6.1787e-02, -4.5498e-02,  7.9745e-02, -7.1558e-02,  8.3220e-02,
         1.5152e-02, -3.1845e-01, -4.1824e-01, -1.4335e-01, -4.7240e-01,
        -2.6891e-01, -3.5642e-01, -1.8791e-01,  5.6597e-02, -9.5068e-02,
         2.5044e-02, -3.7080e-01, -5.2572e-01, -1.6818e-01,  1.0271e-01,
        -3.0392e-01, -4.2791e-01, -4.2043e-01, -2.6717e-01, -2.0881e-01,
        -1.7068e-01, -4.2403e-01, -6.4279e-01, -1.8549e-01, -2.3898e-01,
        -2.6443e-01, -1.9000e-01, -2.3553e-01, -2.4281e-01, -3.8625e-01,
        -3.1356e-01, -6.1113e-01, -8.9814e-01, -5.9804e-01, -6.9739e-01,
        -5.0307e-01, -4.0977e-01, -3.1349e-01, -1.0593e+00, -8.2902e-01,
        -5.0138e-01, -7.4351e-01, -8.9096e-01, -6.4884e-01, -7.7823e-01,
        -6.9417e-01, -7.3868e-01, -9.0239e-01, -9.0865e-01, -8.7374e-01,
        -9.3556e-01, -7.0667e-01, -8.2239e-01, -6.9145e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1697e+00, -9.7299e-01, -1.5150e+00, -2.4989e+00, -2.9247e+00,
        -2.9366e+00, -2.8239e+00, -2.9102e+00, -2.7992e+00, -2.5263e+00,
        -2.8243e+00, -2.6274e+00, -2.6792e+00, -2.4944e+00, -2.2071e+00,
        -2.4651e+00, -2.6357e+00, -2.3508e+00, -2.2309e+00, -2.0821e+00,
        -2.0664e+00, -2.2593e+00, -8.4859e-01, -1.6472e-01, -1.7361e+00,
        -1.7951e+00, -1.4954e+00, -1.7037e+00, -1.5682e+00, -1.3938e+00,
        -1.9471e+00, -1.6180e+00, -1.9093e+00, -1.9317e+00, -1.6646e+00,
        -1.5784e+00, -1.6369e+00, -1.7242e+00, -1.6071e+00, -1.5364e+00,
        -1.4881e+00, -1.4306e+00, -1.3534e+00, -1.7414e+00, -1.7039e+00,
        -1.4908e+00, -1.3975e+00, -1.6575e+00, -1.6891e+00, -1.7396e+00,
        -1.3499e+00, -1.6318e+00, -1.4683e+00, -1.8531e+00, -1.3075e+00,
        -1.2569e+00, -1.0400e+00, -1.5997e+00, -1.4118e+00, -1.4239e+00,
        -1.3689e+00, -1.5357e+00, -1.0717e+00, -1.3815e+00, -1.5290e+00,
        -1.2409e+00,  1.2479e+00,  1.3296e+00,  2.0841e+00,  2.2048e+00,
         5.1175e+00,  9.9611e-03,  3.0388e-01,  1.9103e-01, -5.7679e-02,
         1.6752e-01,  2.1177e-01,  2.7343e-01,  3.1009e-01,  3.2132e-01,
         2.7318e-01,  4.1333e-01,  3.5202e-01,  2.8932e-01,  4.2218e-01,
         1.4061e-01, -3.2320e-02,  1.9340e-01,  4.6128e-01,  3.0412e-01,
         1.5720e-01,  1.3584e-01,  2.4170e-01,  2.8072e-01,  3.8726e-01,
         9.2787e-02,  2.1777e-01,  2.9516e-01,  2.6955e-01,  2.0596e-01,
         2.8436e-01,  4.0624e-01,  2.3032e-01,  3.8576e-01, -1.1815e-01,
         8.7547e-02, -1.1189e-01,  4.3532e-02,  1.3252e-01,  1.8400e-01,
         2.3755e-01,  3.0229e-01,  1.9918e-01,  1.8696e+00,  2.8270e+00,
         4.0062e+00,  6.8690e-01,  1.0081e+00,  8.9345e-01,  9.8064e-01,
         9.9533e-01,  9.4702e-01,  8.8623e-01,  8.6848e-01,  8.4031e-01,
         7.7220e-01,  7.7526e-01,  8.1728e-01,  9.0579e-01,  5.6602e-01,
         6.0346e-01,  9.3836e-01,  4.9106e-01,  7.9688e-01,  2.9596e-01,
         7.3542e-01,  6.9750e-01,  8.5143e-01,  8.6756e-01,  8.6062e-01,
         1.0352e+00,  1.0376e+00,  6.0251e-01,  9.3092e-01,  1.0008e+00,
         1.1363e+00,  5.8450e-01,  7.8063e-01,  9.3688e-01,  9.8114e-01,
         5.4262e-01,  1.0534e+00,  7.1326e-01,  9.2068e-01,  8.7946e-01,
         4.8492e-01,  7.1604e-01,  8.9630e-01,  8.4064e-01,  9.5178e-01,
         7.8897e-01,  5.3552e-01,  9.4752e-01,  9.7197e-01,  5.0186e-01,
         9.4425e-01,  6.6036e-01,  6.9416e-01,  8.1061e-01,  9.3107e-01,
         9.9083e-01,  6.7136e-01,  6.8730e-01,  8.7901e-01,  8.5409e-01,
         1.0290e+00,  9.3041e-01,  9.3467e-01,  8.6435e-01,  1.0990e+00,
         8.4430e-01,  8.8694e-01,  4.0309e-01,  5.4270e-01,  8.2033e-01,
         9.5484e-01,  9.4523e-01,  7.4469e-01,  6.2919e-01,  6.1371e-01,
         7.7685e-01,  4.7662e-01,  3.7200e-01,  8.8754e-01,  4.7313e-01,
         6.3361e-01,  6.9876e-01,  7.1356e-01,  3.8527e-01,  7.3710e-01,
         3.2523e-01,  8.4350e-01,  6.6532e-01,  8.4431e-01,  6.9650e-01,
         6.9894e-01,  7.8450e-01,  3.3629e-01,  6.0402e-01,  6.5547e-01,
         6.1103e-01,  6.0816e-01,  6.7031e-01,  5.4118e-01,  5.7498e-01,
         6.1577e-01,  7.0270e-01,  7.5622e-01,  4.4523e-01,  6.0241e-01,
         6.7025e-01,  5.7214e-01,  6.8254e-01,  6.0927e-01,  8.4483e-01,
         5.6845e-01,  4.5330e-01,  6.5064e-01,  5.9966e-01,  6.9912e-01,
         6.2083e-01,  6.6277e-01,  1.9747e-01,  8.0634e-01,  3.0318e-01,
         7.4681e-01,  6.2903e-01,  6.6670e-01,  7.4163e-01,  8.1787e-01,
         7.2128e-01,  6.3552e-01,  8.2367e-01,  2.8305e-01,  6.7455e-01,
         4.0743e-01,  6.4252e-01,  5.8462e-01,  4.9380e-01,  1.6325e-01,
         4.8698e-01,  5.2491e-01,  5.6610e-01,  4.1844e-01,  2.0586e-01,
         1.0747e-01,  2.0392e-01,  1.0991e-03,  4.1934e-01,  3.9157e-01,
         4.0853e-01,  3.2973e-01,  2.6180e-01,  3.0516e-01,  3.1219e-01,
        -8.2205e-02,  3.6203e-01,  3.1759e-01,  5.2405e-01,  5.2668e-02,
         1.2349e-01,  1.6410e-01,  3.9196e-01,  3.6760e-01,  1.9044e-01,
        -2.7794e-02,  2.2508e-01,  4.5952e-01,  2.5872e-01,  1.3554e-01,
         1.7403e-01,  1.6381e-01,  1.9711e-01,  2.1334e-01, -1.2763e-02,
         3.7317e-01,  3.8828e-01,  3.1228e-01,  1.4969e-01,  3.1736e-01,
         2.7297e-01,  2.8480e-01,  1.7840e-01,  2.8561e-01,  2.5123e-01,
         1.7696e-01,  1.2426e-02,  5.8229e-03,  2.4310e-01,  1.7541e-01,
         2.0111e-01, -3.0230e-01,  2.5900e-01,  5.2034e-01,  3.1858e-01,
         4.0667e-01,  2.8736e-01,  2.8149e-01,  4.0081e-01,  2.6916e-01,
         3.2685e-01,  1.4068e-01,  2.8024e-01,  3.2867e-01,  3.4823e-01,
        -4.6385e-03,  9.2211e-02, -1.0053e-01, -1.1282e-01,  3.6243e-01,
        -1.4676e-01,  3.7448e-01, -1.2864e-01,  2.7416e-01,  2.8688e-01,
         2.5218e-01,  2.1872e-01,  9.1822e-02,  7.2553e-02,  2.4665e-01,
         1.8823e-01,  3.5511e-01, -8.1419e-02,  5.5817e-02,  1.8295e-01,
         1.4509e-01, -1.1684e-01,  2.7773e-01,  2.0443e-01,  2.7608e-01,
         4.5211e-02,  3.8921e-02,  1.2845e-01,  5.6121e-02, -3.8065e-01,
         6.2027e-02, -4.5498e-02,  7.9745e-02, -7.1558e-02,  8.3220e-02,
         1.5335e-02, -2.9018e-01, -4.3320e-01, -1.4742e-01, -4.7240e-01,
        -2.6891e-01, -3.5642e-01, -1.8791e-01,  5.6597e-02, -9.5068e-02,
         2.6249e-02, -3.7080e-01, -5.2572e-01, -1.6818e-01,  1.0271e-01,
        -3.1689e-01, -4.2791e-01, -4.2043e-01, -2.6754e-01, -2.0881e-01,
        -1.7068e-01, -3.8447e-01, -6.4279e-01, -1.8549e-01, -2.3898e-01,
        -2.6443e-01, -1.9000e-01, -2.3553e-01, -2.4281e-01, -3.8625e-01,
        -3.3317e-01, -6.1113e-01, -8.9814e-01, -5.9353e-01, -6.9739e-01,
        -5.0307e-01, -4.0977e-01, -3.4637e-01, -1.0593e+00, -8.2902e-01,
        -5.0138e-01, -6.8660e-01, -9.2498e-01, -6.4884e-01, -7.7823e-01,
        -6.9417e-01, -7.3868e-01, -9.4607e-01, -9.0865e-01, -8.7374e-01,
        -9.3556e-01, -7.0667e-01, -8.2239e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808488
t6: 1641198808488
state_values: tensor([ 2.0931e-01, -2.3113e-02,  3.2693e-04,  6.4271e-02,  1.2501e-01,
         1.7283e-01,  1.7962e-01,  1.7807e-01,  1.7903e-01,  1.8098e-01,
         2.1767e-01,  1.7494e-01,  1.6000e-01,  1.5439e-01,  1.6632e-01,
         2.9724e-01,  2.1420e-01,  1.7893e-01,  1.9425e-01,  1.9307e-01,
         2.0783e-01,  1.9031e-01,  9.9789e-02,  7.7682e-02,  1.2364e-01,
         1.2750e-01,  1.2669e-01,  1.6053e-01,  1.3430e-01,  1.2918e-01,
         2.7162e-01,  1.9867e-01,  1.9841e-01,  1.6100e-01,  1.4528e-01,
         1.4039e-01,  1.3845e-01,  1.4164e-01,  1.4993e-01,  1.3935e-01,
         1.4679e-01,  1.4088e-01,  1.3981e-01,  1.9143e-01,  1.5467e-01,
         1.4507e-01,  1.3977e-01,  1.6630e-01,  1.4441e-01,  1.4293e-01,
         1.4100e-01,  1.5914e-01,  1.4234e-01,  2.2088e-01,  1.6579e-01,
         1.5355e-01,  1.6231e-01,  2.5760e-01,  1.7922e-01,  2.1857e-01,
         1.6950e-01,  1.6489e-01,  1.5364e-01,  1.6515e-01,  1.6089e-01,
         1.5841e-01,  7.9785e-02,  6.4896e-02,  5.0118e-02,  5.1848e-02,
         3.3324e-02,  9.0540e-02,  1.1076e-01,  1.1304e-01,  1.1844e-01,
         1.5666e-01,  1.4353e-01,  1.4360e-01,  1.3945e-01,  1.4518e-01,
         1.3704e-01,  1.3450e-01,  1.4122e-01,  1.3765e-01,  1.4798e-01,
         2.0447e-01,  1.6383e-01,  1.5240e-01,  1.6246e-01,  1.7425e-01,
         1.5755e-01,  1.8598e-01,  1.6679e-01,  1.5762e-01,  1.5173e-01,
         1.5556e-01,  1.8767e-01,  1.7220e-01,  1.6565e-01,  1.8280e-01,
         1.7425e-01,  1.6011e-01,  1.5962e-01,  1.5881e-01,  1.6383e-01,
         1.6264e-01,  1.6649e-01,  1.6713e-01,  1.6558e-01,  1.6441e-01,
         1.7818e-01,  1.8773e-01,  1.8740e-01,  1.1215e-01,  8.3664e-02,
         6.4776e-02,  1.1722e-01,  1.5294e-01,  1.4672e-01,  1.5529e-01,
         1.5327e-01,  1.5119e-01,  1.5464e-01,  1.5376e-01,  1.8670e-01,
         1.6767e-01,  2.0579e-01,  1.7901e-01,  1.6995e-01,  1.7252e-01,
         2.5960e-01,  2.0431e-01,  1.8829e-01,  1.9403e-01,  3.4366e-01,
         2.3067e-01,  2.6959e-01,  2.1500e-01,  2.1215e-01,  2.2100e-01,
         1.9979e-01,  1.9335e-01,  1.9518e-01,  1.9636e-01,  1.9911e-01,
         1.9758e-01,  1.9598e-01,  1.9233e-01,  2.1888e-01,  1.9618e-01,
         3.1509e-01,  2.2339e-01,  2.0940e-01,  2.0000e-01,  1.9669e-01,
         3.2627e-01,  2.5470e-01,  2.4889e-01,  2.1525e-01,  2.0732e-01,
         2.0738e-01,  2.1162e-01,  2.0201e-01,  2.0805e-01,  2.0633e-01,
         2.0787e-01,  2.0730e-01,  2.3831e-01,  2.1038e-01,  2.0221e-01,
         2.0381e-01,  2.0661e-01,  2.0609e-01,  2.1481e-01,  2.0519e-01,
         1.9882e-01,  1.9889e-01,  2.0883e-01,  2.0717e-01,  2.0191e-01,
         1.9821e-01,  2.1867e-01,  2.1251e-01,  2.1066e-01,  2.0423e-01,
         2.0503e-01,  1.9835e-01,  2.2479e-01,  2.8116e-01,  2.8779e-01,
         2.3102e-01,  2.2446e-01,  2.1999e-01,  2.4548e-01,  2.2783e-01,
         2.9439e-01,  2.6557e-01,  2.7824e-01,  2.4694e-01,  2.5957e-01,
         2.4687e-01,  2.2949e-01,  2.2912e-01,  2.3068e-01,  2.4983e-01,
         2.6900e-01,  2.5592e-01,  2.4723e-01,  2.3308e-01,  2.5461e-01,
         3.1140e-01,  2.5118e-01,  2.4296e-01,  2.7283e-01,  2.9276e-01,
         2.5150e-01,  2.3964e-01,  2.3323e-01,  2.3792e-01,  2.7294e-01,
         2.4002e-01,  2.9135e-01,  2.7194e-01,  2.5575e-01,  2.3648e-01,
         2.3703e-01,  2.3836e-01,  2.9598e-01,  2.5072e-01,  2.5571e-01,
         2.5823e-01,  2.6574e-01,  2.5574e-01,  2.5774e-01,  2.5111e-01,
         2.5024e-01,  2.4118e-01,  2.3495e-01,  2.4328e-01,  2.2834e-01,
         2.2825e-01,  2.3783e-01,  2.3783e-01,  2.4476e-01,  2.3251e-01,
         2.4025e-01,  2.3170e-01,  2.5936e-01,  2.4684e-01,  2.5198e-01,
         2.6720e-01,  2.7416e-01,  2.6471e-01,  2.7495e-01,  3.6528e-01,
         2.8992e-01,  3.9822e-01,  3.0785e-01,  2.9439e-01,  2.9808e-01,
         2.9886e-01,  3.1275e-01,  3.6469e-01,  3.1557e-01,  3.1800e-01,
         3.0257e-01,  3.1727e-01,  2.8738e-01,  3.0108e-01,  2.8835e-01,
         3.9375e-01,  3.0442e-01,  2.9451e-01,  2.9630e-01,  3.9131e-01,
         3.1712e-01,  2.9540e-01,  2.9622e-01,  2.8339e-01,  4.1171e-01,
         3.5233e-01,  3.8420e-01,  3.2227e-01,  2.9974e-01,  2.9541e-01,
         2.8541e-01,  2.8185e-01,  2.8221e-01,  2.8391e-01,  3.0248e-01,
         2.8582e-01,  3.0208e-01,  2.8718e-01,  3.1314e-01,  3.1248e-01,
         2.9066e-01,  2.9034e-01,  2.8705e-01,  2.7813e-01,  2.8279e-01,
         2.8141e-01,  3.4486e-01,  3.4494e-01,  3.0217e-01,  2.9461e-01,
         2.8852e-01,  2.9016e-01,  3.5164e-01,  3.1239e-01,  3.7446e-01,
         3.2536e-01,  4.0403e-01,  3.4080e-01,  3.1810e-01,  3.2498e-01,
         4.6663e-01,  3.4895e-01,  3.2923e-01,  3.2558e-01,  3.4396e-01,
         3.3271e-01,  3.4317e-01,  3.2956e-01,  3.1407e-01,  3.0704e-01,
         3.0574e-01,  3.0185e-01,  3.9898e-01,  3.3532e-01,  3.9286e-01,
         3.3196e-01,  3.0934e-01,  3.2224e-01,  3.1592e-01,  3.4806e-01,
         3.2064e-01,  3.1927e-01,  3.0542e-01,  3.1338e-01,  3.1045e-01,
         3.6696e-01,  3.6858e-01,  3.3340e-01,  3.3339e-01,  3.3260e-01,
         3.1703e-01,  4.0444e-01,  3.3943e-01,  3.2730e-01,  3.2878e-01,
         3.2532e-01,  4.2054e-01,  3.5931e-01,  4.2414e-01,  5.5590e-01,
         3.9440e-01,  3.6770e-01,  3.5333e-01,  3.3850e-01,  3.4397e-01,
         3.2787e-01,  4.7368e-01,  3.8013e-01,  3.4541e-01,  3.2326e-01,
         3.3712e-01,  4.8727e-01,  4.9001e-01,  3.9981e-01,  4.0194e-01,
         3.6670e-01,  3.5421e-01,  3.5814e-01,  3.4712e-01,  3.4056e-01,
         3.3846e-01,  3.6278e-01,  3.5451e-01,  3.5302e-01,  3.7368e-01,
         3.4802e-01,  4.8658e-01,  3.9349e-01,  3.6338e-01,  3.5985e-01,
         3.5375e-01,  3.6410e-01,  3.6794e-01,  5.8670e-01,  4.1933e-01,
         4.0130e-01,  4.0418e-01,  3.8709e-01,  4.5970e-01,  5.0714e-01,
         4.3260e-01,  3.9435e-01,  3.9241e-01,  4.9901e-01,  4.8307e-01,
         4.5267e-01,  3.9221e-01,  4.1638e-01,  4.0321e-01], device='cuda:0',
       grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808492
t8: 1641198808492
t9: 1641198808493
t10: 1641198808503
t11: 1641198808504
t12: 1641198808504
t1: 1641198808504
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808515
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0025, 0.9916, 1.0244, 1.0087, 1.0648, 0.9971, 1.0154, 1.0542, 0.9901,
        0.9459, 1.0793, 0.9888, 1.0793, 0.9629, 0.8272, 0.9483, 1.1102, 0.9101,
        0.9272, 0.8897, 0.9348, 1.0317, 1.0048, 0.9342, 1.0599, 0.9949, 0.8925,
        1.0081, 0.9807, 0.7310, 0.9448, 0.9329, 1.1154, 1.0147, 1.0271, 0.9899,
        1.1298, 0.9510, 1.0475, 0.9674, 0.9777, 0.9926, 0.8772, 1.0692, 0.9772,
        1.0330, 0.9094, 1.0648, 1.1284, 0.9856, 0.9385, 1.0581, 0.9524, 0.9835,
        1.0507, 0.9448, 0.8084, 1.0624, 0.8714, 1.0159, 1.1017, 0.9951, 0.9642,
        1.1316, 0.9769, 1.0540, 1.0928, 1.0428, 0.9706, 0.8996, 1.0867, 0.9529,
        1.0556, 1.1108, 0.8988, 0.9792, 0.9576, 0.9769, 0.9576, 1.0398, 0.9957,
        0.9842, 1.0284, 0.9458, 0.8460, 1.0695, 1.0418, 0.8995, 0.9133, 1.0631,
        0.8903, 1.0004, 1.0125, 1.0300, 1.0817, 0.9073, 0.9783, 0.9896, 0.9169,
        0.9688, 1.0152, 1.0553, 0.9989, 1.1634, 1.0640, 1.0911, 1.0400, 1.0280,
        1.0207, 0.9711, 0.9362, 0.9598, 1.0527, 1.0826, 1.0659, 1.2381, 0.9405,
        1.0315, 0.9731, 0.9951, 1.0174, 1.0061, 1.0080, 0.9196, 1.0191, 0.8921,
        1.0403, 1.0643, 1.1094, 0.8463, 0.9931, 1.1887, 0.9404, 0.7617, 0.9862,
        0.9013, 0.9960, 0.9623, 0.9401, 0.9991, 1.0261, 1.1461, 0.9847, 0.9697,
        0.9761, 1.1839, 1.0454, 0.9312, 1.0001, 0.8436, 0.9992, 1.0798, 1.0465,
        1.0124, 0.8311, 0.9872, 0.9197, 1.0502, 1.0148, 1.0654, 1.0921, 1.0019,
        0.9873, 1.1420, 0.9805, 1.0394, 0.9479, 1.0476, 1.0127, 0.9935, 1.0597,
        1.0335, 0.9671, 1.0139, 0.9974, 1.0005, 0.9639, 0.9889, 0.9754, 1.0376,
        0.9416, 1.2238, 1.0815, 1.0261, 0.9918, 1.0059, 0.9416, 0.8746, 0.9274,
        1.0332, 1.1519, 1.2819, 0.9093, 1.1147, 0.8853, 0.9565, 0.9047, 1.1232,
        0.9256, 1.1631, 0.9980, 1.0391, 0.9737, 0.9404, 0.9065, 0.9541, 1.1338,
        1.0444, 0.9379, 0.8721, 1.0600, 0.9832, 0.9115, 0.8962, 1.0438, 1.0116,
        1.0373, 1.0780, 0.9303, 1.0085, 0.9019, 0.9430, 0.9908, 0.9988, 1.0807,
        1.0644, 0.9004, 1.0450, 0.9548, 0.9608, 0.9520, 1.2314, 0.9507, 1.1538,
        0.9774, 1.0442, 1.0284, 0.9819, 1.0089, 1.0060, 0.9943, 0.9809, 1.0989,
        1.0227, 1.0565, 1.0104, 0.9621, 1.0293, 1.0872, 0.9579, 0.9477, 0.9657,
        0.9535, 0.8536, 1.0866, 0.8117, 1.0232, 0.9659, 0.9526, 0.9413, 0.9104,
        0.8662, 0.9768, 0.9396, 1.2167, 0.9130, 1.0641, 0.9367, 1.1717, 0.8681,
        1.0617, 0.9794, 0.9625, 0.8500, 1.0679, 1.0330, 0.9633, 1.0740, 0.8538,
        0.9887, 0.8926, 1.0133, 1.0643, 1.1865, 1.0172, 1.0066, 1.0190, 1.0525,
        0.9631, 1.0193, 0.9665, 1.0492, 0.9323, 0.9484, 1.0478, 1.0702, 1.0744,
        1.0183, 1.0014, 1.0009, 1.0440, 0.9400, 0.9979, 1.0477, 1.0288, 1.0372,
        0.9196, 0.9842, 0.8856, 0.9862, 0.8517, 0.9906, 0.9903, 0.9424, 0.8076,
        1.0175, 1.1663, 1.3198, 0.9255, 1.2945, 0.9384, 1.2014, 1.0085, 1.0312,
        1.0293, 1.0388, 0.9161, 1.0451, 0.8884, 1.0299, 1.0330, 1.1058, 1.0634,
        0.9369, 1.0537, 1.1023, 1.0128, 0.9947, 0.9856, 0.9172, 0.9193, 0.9869,
        0.9763, 1.2784, 1.0307, 0.8919, 1.0077, 1.0539, 0.9727, 0.9968, 0.8862,
        1.2002, 0.8676, 0.8785, 1.0220, 1.0713, 1.0256, 0.9933, 0.9954, 1.0073,
        0.8551, 1.0390, 1.0480, 1.0070, 1.0834, 0.8584, 0.9692, 0.9880, 0.9050,
        0.9914, 1.0664, 1.2106, 0.9943, 1.0268, 1.0304, 0.9475, 0.9769, 0.9723,
        0.9381, 1.0106, 0.8430, 1.0504, 1.0563, 1.1109, 1.0398, 0.9738, 0.9577,
        0.8118, 1.0656, 0.9466, 0.9431, 1.1810, 0.8643, 0.9218, 0.9623, 0.9853,
        1.0968, 0.8585, 0.9504, 0.9413, 1.0180, 0.9396, 0.9742, 0.8116],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808519
t4: 1641198808519
surr1, surr2: tensor([-3.1695e+00, -9.7270e-01, -1.5139e+00, -2.4967e+00, -2.9242e+00,
        -2.9358e+00, -2.8259e+00, -2.9161e+00, -2.7968e+00, -2.5195e+00,
        -2.8271e+00, -2.6273e+00, -2.6867e+00, -2.4896e+00, -2.0287e+00,
        -2.4701e+00, -2.6602e+00, -2.3560e+00, -2.2359e+00, -2.0583e+00,
        -2.0727e+00, -2.2544e+00, -8.4859e-01, -1.6498e-01, -1.7331e+00,
        -1.7950e+00, -1.4830e+00, -1.7035e+00, -1.5691e+00, -1.1320e+00,
        -1.9440e+00, -1.6445e+00, -1.9360e+00, -1.9307e+00, -1.6642e+00,
        -1.5787e+00, -1.6812e+00, -1.7209e+00, -1.6084e+00, -1.5356e+00,
        -1.4873e+00, -1.4303e+00, -1.3191e+00, -1.7305e+00, -1.7050e+00,
        -1.4915e+00, -1.3958e+00, -1.6551e+00, -1.7328e+00, -1.7381e+00,
        -1.3469e+00, -1.6331e+00, -1.4669e+00, -1.8521e+00, -1.3092e+00,
        -1.2542e+00, -9.3424e-01, -1.5855e+00, -1.3669e+00, -1.4236e+00,
        -1.3711e+00, -1.5357e+00, -1.0712e+00, -1.4212e+00, -1.5277e+00,
        -1.2426e+00,  1.2515e+00,  1.3322e+00,  2.0808e+00,  2.1990e+00,
         5.0555e+00,  9.9810e-03,  3.0370e-01,  1.9291e-01, -5.7386e-02,
         1.6762e-01,  2.1208e-01,  2.7343e-01,  3.1003e-01,  3.2135e-01,
         2.7312e-01,  4.1305e-01,  3.5228e-01,  2.8878e-01,  3.9687e-01,
         1.3948e-01, -3.2277e-02,  1.9293e-01,  4.6232e-01,  3.0302e-01,
         1.5551e-01,  1.3586e-01,  2.4162e-01,  2.8080e-01,  3.8788e-01,
         9.2399e-02,  2.1777e-01,  2.9522e-01,  2.6917e-01,  2.0595e-01,
         2.8435e-01,  4.0665e-01,  2.3028e-01,  4.0801e-01, -1.1853e-01,
         8.7959e-02, -1.1216e-01,  4.3591e-02,  1.3264e-01,  1.8374e-01,
         2.3703e-01,  3.0203e-01,  1.9925e-01,  1.8755e+00,  2.8363e+00,
         4.5093e+00,  6.8480e-01,  1.0088e+00,  8.9311e-01,  9.8041e-01,
         9.9577e-01,  9.4714e-01,  8.8643e-01,  8.6539e-01,  8.4057e-01,
         7.6540e-01,  7.7453e-01,  8.1666e-01,  9.1350e-01,  5.3224e-01,
         6.0425e-01,  1.0140e+00,  4.9027e-01,  6.7444e-01,  2.9596e-01,
         7.3651e-01,  6.9750e-01,  8.5477e-01,  8.6907e-01,  8.6078e-01,
         1.0342e+00,  1.0811e+00,  6.0211e-01,  9.3000e-01,  1.0005e+00,
         1.2230e+00,  5.8562e-01,  7.7788e-01,  9.3681e-01,  9.1962e-01,
         5.4275e-01,  1.0435e+00,  7.1275e-01,  9.2084e-01,  8.1216e-01,
         4.8605e-01,  7.2522e-01,  8.9528e-01,  8.4009e-01,  9.5069e-01,
         7.8989e-01,  5.3550e-01,  9.4708e-01,  1.0090e+00,  5.0137e-01,
         9.4539e-01,  6.5930e-01,  6.9489e-01,  8.1079e-01,  9.3074e-01,
         9.9287e-01,  6.7225e-01,  6.8611e-01,  8.7933e-01,  8.5396e-01,
         1.0289e+00,  9.2903e-01,  9.3427e-01,  8.6361e-01,  1.1001e+00,
         8.4241e-01,  9.8674e-01,  4.0444e-01,  5.4348e-01,  8.1996e-01,
         9.5498e-01,  9.4321e-01,  7.2371e-01,  6.3360e-01,  6.1246e-01,
         8.1354e-01,  5.5545e-01,  3.7002e-01,  8.9940e-01,  4.6539e-01,
         6.3494e-01,  7.0239e-01,  7.2859e-01,  3.8593e-01,  7.7935e-01,
         3.2520e-01,  8.4425e-01,  6.6479e-01,  8.4307e-01,  6.9531e-01,
         6.9922e-01,  8.0863e-01,  3.3679e-01,  6.0234e-01,  6.3515e-01,
         6.0937e-01,  6.0842e-01,  6.6921e-01,  5.3888e-01,  5.7407e-01,
         6.1575e-01,  7.0310e-01,  7.5810e-01,  4.4373e-01,  6.0247e-01,
         6.6932e-01,  5.7322e-01,  6.8296e-01,  6.0924e-01,  8.4572e-01,
         5.6974e-01,  4.5112e-01,  6.5017e-01,  5.9991e-01,  6.9886e-01,
         6.2070e-01,  7.4193e-01,  1.9700e-01,  8.4578e-01,  3.0289e-01,
         7.4770e-01,  6.2952e-01,  6.6615e-01,  7.4180e-01,  8.1797e-01,
         7.2106e-01,  6.3499e-01,  8.2600e-01,  2.8334e-01,  6.7623e-01,
         4.0760e-01,  6.4131e-01,  5.8508e-01,  4.9505e-01,  1.6281e-01,
         4.8599e-01,  5.2454e-01,  5.6557e-01,  3.9686e-01,  2.0466e-01,
         9.6927e-02,  2.0269e-01,  1.1032e-03,  4.1996e-01,  3.9276e-01,
         4.0987e-01,  3.1736e-01,  2.6217e-01,  3.0687e-01,  3.4532e-01,
        -8.2023e-02,  3.6155e-01,  3.1761e-01,  5.5820e-01,  5.0802e-02,
         1.2278e-01,  1.6424e-01,  3.9176e-01,  3.4717e-01,  1.8911e-01,
        -2.7747e-02,  2.2490e-01,  4.5966e-01,  2.4545e-01,  1.3583e-01,
         1.7260e-01,  1.6377e-01,  1.9621e-01,  2.3012e-01, -1.2768e-02,
         3.7322e-01,  3.8844e-01,  3.1267e-01,  1.4945e-01,  3.1751e-01,
         2.7274e-01,  2.8515e-01,  1.7797e-01,  2.8530e-01,  2.5135e-01,
         1.7730e-01,  1.2466e-02,  5.8278e-03,  2.4309e-01,  1.7540e-01,
         2.0142e-01, -3.0137e-01,  2.5896e-01,  5.2074e-01,  3.1890e-01,
         4.0731e-01,  2.8619e-01,  2.8136e-01,  3.9440e-01,  2.6931e-01,
         3.0932e-01,  1.4074e-01,  2.8058e-01,  3.2933e-01,  3.1248e-01,
        -4.6175e-03,  9.7766e-02, -1.2062e-01, -1.1251e-01,  4.2650e-01,
        -1.4655e-01,  4.0898e-01, -1.2866e-01,  2.7445e-01,  2.8714e-01,
         2.5255e-01,  2.1784e-01,  9.1822e-02,  7.1620e-02,  2.4630e-01,
         1.8791e-01,  3.5698e-01, -8.1602e-02,  5.5661e-02,  1.8311e-01,
         1.4539e-01, -1.1690e-01,  2.7765e-01,  2.0433e-01,  2.7537e-01,
         4.5152e-02,  3.8933e-02,  1.2844e-01,  6.5221e-02, -3.8115e-01,
         6.1467e-02, -4.5488e-02,  7.9670e-02, -7.1501e-02,  8.3208e-02,
         1.5099e-02, -3.1662e-01, -4.1761e-01, -1.4390e-01, -4.7017e-01,
        -2.6691e-01, -3.5641e-01, -1.8792e-01,  5.6593e-02, -9.5073e-02,
         2.4940e-02, -3.6814e-01, -5.2404e-01, -1.6819e-01,  1.0280e-01,
        -3.0223e-01, -4.3048e-01, -4.2121e-01, -2.6902e-01, -2.0890e-01,
        -1.7008e-01, -4.2314e-01, -6.4255e-01, -1.8564e-01, -2.3914e-01,
        -2.6393e-01, -1.8990e-01, -2.3545e-01, -2.4243e-01, -3.8630e-01,
        -3.1206e-01, -6.0479e-01, -8.9508e-01, -5.9940e-01, -6.9822e-01,
        -5.0256e-01, -4.0937e-01, -3.1243e-01, -1.0555e+00, -8.3606e-01,
        -5.0216e-01, -7.3713e-01, -8.8825e-01, -6.5068e-01, -7.8170e-01,
        -6.9522e-01, -7.3654e-01, -9.0242e-01, -9.1061e-01, -8.7919e-01,
        -9.3490e-01, -7.0837e-01, -8.2317e-01, -6.9619e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1695e+00, -9.7270e-01, -1.5139e+00, -2.4967e+00, -2.9242e+00,
        -2.9358e+00, -2.8259e+00, -2.9161e+00, -2.7968e+00, -2.5195e+00,
        -2.8271e+00, -2.6273e+00, -2.6867e+00, -2.4896e+00, -2.2071e+00,
        -2.4701e+00, -2.6357e+00, -2.3560e+00, -2.2359e+00, -2.0821e+00,
        -2.0727e+00, -2.2544e+00, -8.4859e-01, -1.6498e-01, -1.7331e+00,
        -1.7950e+00, -1.4954e+00, -1.7035e+00, -1.5691e+00, -1.3938e+00,
        -1.9440e+00, -1.6445e+00, -1.9093e+00, -1.9307e+00, -1.6642e+00,
        -1.5787e+00, -1.6369e+00, -1.7209e+00, -1.6084e+00, -1.5356e+00,
        -1.4873e+00, -1.4303e+00, -1.3534e+00, -1.7305e+00, -1.7050e+00,
        -1.4915e+00, -1.3958e+00, -1.6551e+00, -1.6891e+00, -1.7381e+00,
        -1.3469e+00, -1.6331e+00, -1.4669e+00, -1.8521e+00, -1.3092e+00,
        -1.2542e+00, -1.0400e+00, -1.5855e+00, -1.4118e+00, -1.4236e+00,
        -1.3689e+00, -1.5357e+00, -1.0712e+00, -1.3815e+00, -1.5277e+00,
        -1.2426e+00,  1.2515e+00,  1.3322e+00,  2.0808e+00,  2.2000e+00,
         5.0555e+00,  9.9810e-03,  3.0370e-01,  1.9103e-01, -5.7464e-02,
         1.6762e-01,  2.1208e-01,  2.7343e-01,  3.1003e-01,  3.2135e-01,
         2.7312e-01,  4.1305e-01,  3.5228e-01,  2.8878e-01,  4.2218e-01,
         1.3948e-01, -3.2277e-02,  1.9303e-01,  4.6232e-01,  3.0302e-01,
         1.5720e-01,  1.3586e-01,  2.4162e-01,  2.8080e-01,  3.8788e-01,
         9.2399e-02,  2.1777e-01,  2.9522e-01,  2.6917e-01,  2.0595e-01,
         2.8435e-01,  4.0665e-01,  2.3028e-01,  3.8576e-01, -1.1853e-01,
         8.7959e-02, -1.1216e-01,  4.3591e-02,  1.3264e-01,  1.8374e-01,
         2.3703e-01,  3.0203e-01,  1.9925e-01,  1.8755e+00,  2.8363e+00,
         4.0062e+00,  6.8480e-01,  1.0088e+00,  8.9311e-01,  9.8041e-01,
         9.9577e-01,  9.4714e-01,  8.8643e-01,  8.6539e-01,  8.4057e-01,
         7.7220e-01,  7.7453e-01,  8.1666e-01,  9.0579e-01,  5.6602e-01,
         6.0425e-01,  9.3836e-01,  4.9027e-01,  7.9688e-01,  2.9596e-01,
         7.3651e-01,  6.9750e-01,  8.5477e-01,  8.6907e-01,  8.6078e-01,
         1.0342e+00,  1.0376e+00,  6.0211e-01,  9.3000e-01,  1.0005e+00,
         1.1363e+00,  5.8562e-01,  7.7788e-01,  9.3681e-01,  9.8114e-01,
         5.4275e-01,  1.0435e+00,  7.1275e-01,  9.2084e-01,  8.7946e-01,
         4.8605e-01,  7.2522e-01,  8.9528e-01,  8.4009e-01,  9.5069e-01,
         7.8989e-01,  5.3550e-01,  9.4708e-01,  9.7197e-01,  5.0137e-01,
         9.4539e-01,  6.5930e-01,  6.9489e-01,  8.1079e-01,  9.3074e-01,
         9.9287e-01,  6.7225e-01,  6.8611e-01,  8.7933e-01,  8.5396e-01,
         1.0289e+00,  9.2903e-01,  9.3427e-01,  8.6361e-01,  1.1001e+00,
         8.4241e-01,  8.8694e-01,  4.0444e-01,  5.4348e-01,  8.1996e-01,
         9.5498e-01,  9.4321e-01,  7.4469e-01,  6.3360e-01,  6.1246e-01,
         7.7685e-01,  4.7662e-01,  3.7002e-01,  8.8754e-01,  4.7313e-01,
         6.3494e-01,  7.0239e-01,  7.1356e-01,  3.8593e-01,  7.3710e-01,
         3.2520e-01,  8.4425e-01,  6.6479e-01,  8.4307e-01,  6.9531e-01,
         6.9922e-01,  7.8450e-01,  3.3679e-01,  6.0234e-01,  6.5547e-01,
         6.0937e-01,  6.0842e-01,  6.6921e-01,  5.4118e-01,  5.7407e-01,
         6.1575e-01,  7.0310e-01,  7.5810e-01,  4.4373e-01,  6.0247e-01,
         6.6932e-01,  5.7322e-01,  6.8296e-01,  6.0924e-01,  8.4572e-01,
         5.6974e-01,  4.5112e-01,  6.5017e-01,  5.9991e-01,  6.9886e-01,
         6.2070e-01,  6.6277e-01,  1.9700e-01,  8.0634e-01,  3.0289e-01,
         7.4770e-01,  6.2952e-01,  6.6615e-01,  7.4180e-01,  8.1797e-01,
         7.2106e-01,  6.3499e-01,  8.2600e-01,  2.8334e-01,  6.7623e-01,
         4.0760e-01,  6.4131e-01,  5.8508e-01,  4.9505e-01,  1.6281e-01,
         4.8599e-01,  5.2454e-01,  5.6557e-01,  4.1844e-01,  2.0466e-01,
         1.0747e-01,  2.0269e-01,  1.1032e-03,  4.1996e-01,  3.9276e-01,
         4.0987e-01,  3.2973e-01,  2.6217e-01,  3.0687e-01,  3.1219e-01,
        -8.2023e-02,  3.6155e-01,  3.1761e-01,  5.2405e-01,  5.2668e-02,
         1.2278e-01,  1.6424e-01,  3.9176e-01,  3.6760e-01,  1.8911e-01,
        -2.7747e-02,  2.2490e-01,  4.5966e-01,  2.5872e-01,  1.3583e-01,
         1.7403e-01,  1.6377e-01,  1.9621e-01,  2.1334e-01, -1.2768e-02,
         3.7322e-01,  3.8844e-01,  3.1267e-01,  1.4945e-01,  3.1751e-01,
         2.7274e-01,  2.8515e-01,  1.7797e-01,  2.8530e-01,  2.5135e-01,
         1.7730e-01,  1.2466e-02,  5.8278e-03,  2.4309e-01,  1.7540e-01,
         2.0142e-01, -3.0137e-01,  2.5896e-01,  5.2074e-01,  3.1890e-01,
         4.0731e-01,  2.8619e-01,  2.8136e-01,  4.0081e-01,  2.6931e-01,
         3.2685e-01,  1.4074e-01,  2.8058e-01,  3.2933e-01,  3.4823e-01,
        -4.6175e-03,  9.2211e-02, -1.0053e-01, -1.1251e-01,  3.6243e-01,
        -1.4655e-01,  3.7448e-01, -1.2866e-01,  2.7445e-01,  2.8714e-01,
         2.5255e-01,  2.1784e-01,  9.1822e-02,  7.2553e-02,  2.4630e-01,
         1.8791e-01,  3.5511e-01, -8.1602e-02,  5.5661e-02,  1.8311e-01,
         1.4509e-01, -1.1690e-01,  2.7765e-01,  2.0433e-01,  2.7537e-01,
         4.5152e-02,  3.8933e-02,  1.2844e-01,  5.6121e-02, -3.8115e-01,
         6.2027e-02, -4.5488e-02,  7.9670e-02, -7.1501e-02,  8.3208e-02,
         1.5335e-02, -2.9018e-01, -4.3320e-01, -1.4742e-01, -4.7017e-01,
        -2.6691e-01, -3.5641e-01, -1.8792e-01,  5.6593e-02, -9.5073e-02,
         2.6249e-02, -3.6814e-01, -5.2404e-01, -1.6819e-01,  1.0280e-01,
        -3.1689e-01, -4.3048e-01, -4.2121e-01, -2.6902e-01, -2.0890e-01,
        -1.7008e-01, -3.8447e-01, -6.4255e-01, -1.8564e-01, -2.3914e-01,
        -2.6393e-01, -1.8990e-01, -2.3545e-01, -2.4243e-01, -3.8630e-01,
        -3.3317e-01, -6.0479e-01, -8.9508e-01, -5.9353e-01, -6.9822e-01,
        -5.0256e-01, -4.0937e-01, -3.4637e-01, -1.0555e+00, -8.3606e-01,
        -5.0216e-01, -6.8660e-01, -9.2498e-01, -6.5068e-01, -7.8170e-01,
        -6.9522e-01, -7.3654e-01, -9.4607e-01, -9.1061e-01, -8.7919e-01,
        -9.3490e-01, -7.0837e-01, -8.2317e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808529
t6: 1641198808529
state_values: tensor([0.2493, 0.0086, 0.0432, 0.1103, 0.1763, 0.2249, 0.2333, 0.2326, 0.2341,
        0.2370, 0.2729, 0.2295, 0.2136, 0.2063, 0.2195, 0.3429, 0.2663, 0.2286,
        0.2456, 0.2434, 0.2561, 0.2393, 0.1376, 0.1079, 0.1641, 0.1685, 0.1675,
        0.2029, 0.1740, 0.1693, 0.3113, 0.2426, 0.2417, 0.1979, 0.1827, 0.1782,
        0.1765, 0.1793, 0.1874, 0.1773, 0.1844, 0.1789, 0.1777, 0.2335, 0.1908,
        0.1822, 0.1772, 0.2012, 0.1811, 0.1796, 0.1781, 0.1953, 0.1794, 0.2600,
        0.2008, 0.1893, 0.1979, 0.2930, 0.2129, 0.2567, 0.2035, 0.1991, 0.1888,
        0.1999, 0.1957, 0.1937, 0.1061, 0.0922, 0.0768, 0.0785, 0.0560, 0.1166,
        0.1453, 0.1487, 0.1565, 0.1915, 0.1799, 0.1800, 0.1760, 0.1813, 0.1734,
        0.1709, 0.1772, 0.1738, 0.1836, 0.2427, 0.1974, 0.1870, 0.1969, 0.2079,
        0.1920, 0.2182, 0.2002, 0.1918, 0.1863, 0.1898, 0.2196, 0.2052, 0.1992,
        0.2155, 0.2074, 0.1940, 0.1935, 0.1928, 0.1972, 0.1962, 0.1997, 0.2004,
        0.1990, 0.1979, 0.2111, 0.2200, 0.2196, 0.1487, 0.1089, 0.0917, 0.1536,
        0.1882, 0.1821, 0.1901, 0.1879, 0.1858, 0.1889, 0.1880, 0.2177, 0.2005,
        0.2411, 0.2106, 0.2023, 0.2046, 0.2905, 0.2383, 0.2188, 0.2247, 0.3681,
        0.2642, 0.2991, 0.2495, 0.2466, 0.2559, 0.2304, 0.2236, 0.2251, 0.2266,
        0.2303, 0.2284, 0.2261, 0.2228, 0.2527, 0.2262, 0.3422, 0.2574, 0.2417,
        0.2296, 0.2266, 0.3520, 0.2866, 0.2804, 0.2484, 0.2383, 0.2385, 0.2440,
        0.2318, 0.2402, 0.2372, 0.2399, 0.2390, 0.2713, 0.2427, 0.2321, 0.2345,
        0.2379, 0.2373, 0.2492, 0.2363, 0.2288, 0.2289, 0.2416, 0.2392, 0.2324,
        0.2283, 0.2540, 0.2452, 0.2426, 0.2343, 0.2357, 0.2283, 0.2596, 0.3111,
        0.3162, 0.2638, 0.2576, 0.2525, 0.2781, 0.2609, 0.3222, 0.2974, 0.3074,
        0.2791, 0.2918, 0.2795, 0.2625, 0.2622, 0.2641, 0.2835, 0.2994, 0.2885,
        0.2801, 0.2663, 0.2878, 0.3382, 0.2835, 0.2763, 0.3046, 0.3208, 0.2838,
        0.2731, 0.2669, 0.2716, 0.3041, 0.2738, 0.3197, 0.3034, 0.2884, 0.2704,
        0.2708, 0.2722, 0.3238, 0.2832, 0.2886, 0.2911, 0.2982, 0.2881, 0.2906,
        0.2839, 0.2836, 0.2750, 0.2693, 0.2774, 0.2633, 0.2633, 0.2724, 0.2725,
        0.2785, 0.2673, 0.2744, 0.2665, 0.2927, 0.2808, 0.2851, 0.2998, 0.3065,
        0.2976, 0.3071, 0.3860, 0.3194, 0.4133, 0.3351, 0.3237, 0.3276, 0.3285,
        0.3417, 0.3849, 0.3435, 0.3459, 0.3312, 0.3453, 0.3177, 0.3310, 0.3185,
        0.4103, 0.3329, 0.3244, 0.3264, 0.4083, 0.3437, 0.3250, 0.3262, 0.3141,
        0.4264, 0.3762, 0.4013, 0.3482, 0.3287, 0.3245, 0.3157, 0.3128, 0.3133,
        0.3149, 0.3327, 0.3170, 0.3325, 0.3183, 0.3424, 0.3418, 0.3216, 0.3212,
        0.3180, 0.3099, 0.3144, 0.3132, 0.3700, 0.3698, 0.3320, 0.3250, 0.3194,
        0.3210, 0.3745, 0.3404, 0.3941, 0.3520, 0.4200, 0.3660, 0.3452, 0.3521,
        0.4729, 0.3726, 0.3546, 0.3513, 0.3696, 0.3585, 0.3692, 0.3559, 0.3420,
        0.3356, 0.3345, 0.3310, 0.4163, 0.3615, 0.4102, 0.3581, 0.3375, 0.3495,
        0.3438, 0.3744, 0.3486, 0.3469, 0.3344, 0.3421, 0.3396, 0.3888, 0.3911,
        0.3603, 0.3605, 0.3591, 0.3451, 0.4213, 0.3654, 0.3544, 0.3563, 0.3531,
        0.4363, 0.3832, 0.4389, 0.5424, 0.4117, 0.3901, 0.3778, 0.3645, 0.3699,
        0.3551, 0.4792, 0.4003, 0.3710, 0.3510, 0.3637, 0.4889, 0.4902, 0.4180,
        0.4210, 0.3907, 0.3794, 0.3827, 0.3732, 0.3673, 0.3655, 0.3887, 0.3811,
        0.3798, 0.3983, 0.3751, 0.4889, 0.4124, 0.3881, 0.3848, 0.3795, 0.3897,
        0.3933, 0.5725, 0.4359, 0.4206, 0.4237, 0.4081, 0.4697, 0.5036, 0.4490,
        0.4142, 0.4124, 0.4980, 0.4861, 0.4656, 0.4125, 0.4349, 0.4229],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808533
t8: 1641198808533
t9: 1641198808534
t10: 1641198808544
t11: 1641198808545
t12: 1641198808545
t1: 1641198808545
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808556
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0035, 0.9955, 1.0172, 1.0036, 1.0455, 0.9980, 1.0110, 1.0352, 0.9932,
        0.9603, 1.0639, 0.9918, 1.0673, 0.9698, 0.8531, 0.9419, 1.0738, 0.9208,
        0.9374, 0.9080, 0.9451, 1.0261, 1.0047, 0.9414, 1.0514, 0.9961, 0.9081,
        1.0074, 0.9847, 0.7602, 0.9354, 0.9616, 1.0956, 1.0123, 1.0236, 0.9916,
        1.1089, 0.9590, 1.0406, 0.9742, 0.9816, 0.9943, 0.8958, 1.0551, 0.9818,
        1.0296, 0.9231, 1.0568, 1.1054, 0.9878, 0.9480, 1.0495, 0.9610, 0.9862,
        1.0433, 0.9535, 0.8329, 1.0661, 0.9014, 1.0132, 1.0786, 0.9959, 0.9695,
        1.1103, 0.9808, 1.0459, 1.0782, 1.0358, 0.9756, 0.9147, 1.0805, 0.9620,
        1.0496, 1.0934, 0.9125, 0.9826, 0.9663, 0.9804, 0.9638, 1.0349, 0.9968,
        0.9868, 1.0247, 0.9546, 0.8665, 1.0536, 1.0353, 0.9108, 0.9246, 1.0527,
        0.9051, 1.0009, 1.0111, 1.0266, 1.0701, 0.9207, 0.9819, 0.9921, 0.9282,
        0.9734, 1.0135, 1.0481, 0.9994, 1.1386, 1.0541, 1.0741, 1.0320, 1.0213,
        1.0154, 0.9787, 0.9492, 0.9666, 1.0446, 1.0725, 1.0579, 1.2071, 0.9509,
        1.0266, 0.9790, 0.9962, 1.0151, 1.0055, 1.0071, 0.9323, 1.0168, 0.9093,
        1.0349, 1.0549, 1.0964, 0.8646, 0.9942, 1.1428, 0.9471, 0.7881, 0.9835,
        0.9348, 0.9969, 0.9719, 0.9498, 0.9997, 1.0222, 1.1268, 0.9871, 0.9749,
        0.9805, 1.1521, 1.0386, 0.9431, 1.0006, 0.8717, 0.9992, 1.0599, 1.0418,
        1.0112, 0.8562, 0.9867, 0.9420, 1.0437, 1.0127, 1.0575, 1.0789, 1.0020,
        0.9899, 1.1147, 0.9843, 1.0325, 0.9590, 1.0404, 1.0106, 0.9949, 1.0502,
        1.0279, 0.9732, 1.0119, 0.9982, 1.0008, 0.9702, 0.9909, 0.9795, 1.0328,
        0.9503, 1.1925, 1.0697, 1.0225, 0.9936, 1.0051, 0.9530, 0.8932, 0.9400,
        1.0280, 1.1312, 1.2533, 0.9195, 1.0982, 0.9056, 0.9651, 0.9233, 1.1057,
        0.9349, 1.1435, 0.9988, 1.0338, 0.9784, 0.9497, 0.9201, 0.9611, 1.1142,
        1.0394, 0.9462, 0.8896, 1.0501, 0.9866, 0.9221, 0.9101, 1.0378, 1.0105,
        1.0329, 1.0668, 0.9407, 1.0078, 0.9188, 0.9512, 0.9929, 0.9995, 1.0709,
        1.0557, 0.9149, 1.0383, 0.9639, 0.9661, 0.9593, 1.1979, 0.9573, 1.1326,
        0.9814, 1.0376, 1.0234, 0.9854, 1.0077, 1.0053, 0.9956, 0.9843, 1.0830,
        1.0195, 1.0472, 1.0087, 0.9698, 1.0246, 1.0704, 0.9646, 0.9561, 0.9715,
        0.9607, 0.8725, 1.0692, 0.8371, 1.0180, 0.9741, 0.9590, 0.9501, 0.9227,
        0.8853, 0.9811, 0.9513, 1.1863, 0.9231, 1.0566, 0.9475, 1.1491, 0.8868,
        1.0491, 0.9838, 0.9676, 0.8708, 1.0542, 1.0276, 0.9681, 1.0636, 0.8769,
        0.9900, 0.9193, 1.0119, 1.0519, 1.1647, 1.0155, 1.0059, 1.0157, 1.0424,
        0.9706, 1.0165, 0.9734, 1.0418, 0.9439, 0.9561, 1.0406, 1.0607, 1.0644,
        1.0157, 1.0014, 1.0010, 1.0357, 0.9510, 0.9986, 1.0393, 1.0248, 1.0317,
        0.9332, 0.9872, 0.9052, 0.9887, 0.8751, 0.9926, 0.9928, 0.9498, 0.8304,
        1.0116, 1.1232, 1.2891, 0.9347, 1.2469, 0.9499, 1.1699, 1.0073, 1.0261,
        1.0239, 1.0314, 0.9328, 1.0360, 0.9104, 1.0245, 1.0278, 1.0931, 1.0550,
        0.9469, 1.0456, 1.0824, 1.0111, 0.9959, 0.9887, 0.9317, 0.9316, 0.9897,
        0.9799, 1.2425, 1.0276, 0.9070, 1.0066, 1.0442, 0.9766, 0.9977, 0.9027,
        1.1690, 0.8877, 0.8979, 1.0164, 1.0556, 1.0233, 0.9945, 0.9966, 1.0065,
        0.8770, 1.0306, 1.0389, 1.0066, 1.0712, 0.8796, 0.9735, 0.9916, 0.9203,
        0.9931, 1.0554, 1.1850, 0.9954, 1.0228, 1.0248, 0.9576, 0.9809, 0.9776,
        0.9471, 1.0097, 0.8650, 1.0412, 1.0453, 1.0985, 1.0346, 0.9785, 0.9653,
        0.8400, 1.0748, 0.9613, 0.9507, 1.1507, 0.8806, 0.9360, 0.9708, 0.9886,
        1.0829, 0.8731, 0.9607, 0.9544, 1.0158, 0.9490, 0.9781, 0.8379],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808560
t4: 1641198808560
surr1, surr2: tensor([-3.1727e+00, -9.7649e-01, -1.5033e+00, -2.4841e+00, -2.8712e+00,
        -2.9385e+00, -2.8136e+00, -2.8637e+00, -2.8053e+00, -2.5578e+00,
        -2.7865e+00, -2.6353e+00, -2.6567e+00, -2.5075e+00, -2.0920e+00,
        -2.4536e+00, -2.5729e+00, -2.3840e+00, -2.2605e+00, -2.1006e+00,
        -2.0957e+00, -2.2423e+00, -8.4851e-01, -1.6624e-01, -1.7192e+00,
        -1.7972e+00, -1.5089e+00, -1.7024e+00, -1.5754e+00, -1.1772e+00,
        -1.9245e+00, -1.6952e+00, -1.9016e+00, -1.9262e+00, -1.6585e+00,
        -1.5815e+00, -1.6502e+00, -1.7353e+00, -1.5978e+00, -1.5464e+00,
        -1.4932e+00, -1.4327e+00, -1.3471e+00, -1.7076e+00, -1.7130e+00,
        -1.4866e+00, -1.4168e+00, -1.6427e+00, -1.6974e+00, -1.7420e+00,
        -1.3606e+00, -1.6197e+00, -1.4801e+00, -1.8573e+00, -1.3000e+00,
        -1.2658e+00, -9.6255e-01, -1.5910e+00, -1.4139e+00, -1.4200e+00,
        -1.3423e+00, -1.5371e+00, -1.0771e+00, -1.3945e+00, -1.5337e+00,
        -1.2330e+00,  1.2348e+00,  1.3233e+00,  2.0915e+00,  2.2359e+00,
         5.0269e+00,  1.0076e-02,  3.0197e-01,  1.8988e-01, -5.8263e-02,
         1.6820e-01,  2.1399e-01,  2.7442e-01,  3.1207e-01,  3.1984e-01,
         2.7342e-01,  4.1416e-01,  3.5100e-01,  2.9144e-01,  4.0645e-01,
         1.3741e-01, -3.2075e-02,  1.9535e-01,  4.6803e-01,  3.0006e-01,
         1.5809e-01,  1.3592e-01,  2.4129e-01,  2.7985e-01,  3.8373e-01,
         9.3767e-02,  2.1858e-01,  2.9595e-01,  2.7248e-01,  2.0694e-01,
         2.8389e-01,  4.0389e-01,  2.3041e-01,  3.9930e-01, -1.1743e-01,
         8.6588e-02, -1.1129e-01,  4.3304e-02,  1.3196e-01,  1.8517e-01,
         2.4034e-01,  3.0418e-01,  1.9772e-01,  1.8579e+00,  2.8151e+00,
         4.3964e+00,  6.9235e-01,  1.0040e+00,  8.9849e-01,  9.8156e-01,
         9.9348e-01,  9.4661e-01,  8.8564e-01,  8.7740e-01,  8.3869e-01,
         7.8021e-01,  7.7050e-01,  8.0947e-01,  9.0282e-01,  5.4374e-01,
         6.0496e-01,  9.7485e-01,  4.9374e-01,  6.9781e-01,  2.9518e-01,
         7.6387e-01,  6.9813e-01,  8.6332e-01,  8.7805e-01,  8.6128e-01,
         1.0302e+00,  1.0629e+00,  6.0359e-01,  9.3494e-01,  1.0049e+00,
         1.1901e+00,  5.8180e-01,  7.8788e-01,  9.3724e-01,  9.5030e-01,
         5.4276e-01,  1.0242e+00,  7.0953e-01,  9.1979e-01,  8.3669e-01,
         4.8583e-01,  7.4278e-01,  8.8971e-01,  8.3837e-01,  9.4360e-01,
         7.8032e-01,  5.3554e-01,  9.4958e-01,  9.8494e-01,  5.0330e-01,
         9.3914e-01,  6.6700e-01,  6.9008e-01,  8.0913e-01,  9.3204e-01,
         9.8389e-01,  6.6860e-01,  6.9045e-01,  8.7758e-01,  8.5468e-01,
         1.0292e+00,  9.3507e-01,  9.3621e-01,  8.6727e-01,  1.0950e+00,
         8.5016e-01,  9.6156e-01,  4.0003e-01,  5.4155e-01,  8.2146e-01,
         9.5419e-01,  9.5464e-01,  7.3904e-01,  6.4221e-01,  6.0937e-01,
         7.9887e-01,  5.4306e-01,  3.7416e-01,  8.8607e-01,  4.7607e-01,
         6.4064e-01,  7.1686e-01,  7.1724e-01,  3.8980e-01,  7.6621e-01,
         3.2544e-01,  8.3993e-01,  6.6799e-01,  8.5140e-01,  7.0570e-01,
         7.0435e-01,  7.9462e-01,  3.3517e-01,  6.0768e-01,  6.4787e-01,
         6.0368e-01,  6.1049e-01,  6.7694e-01,  5.4724e-01,  5.7074e-01,
         6.1509e-01,  7.0016e-01,  7.5022e-01,  4.4871e-01,  6.0200e-01,
         6.8185e-01,  5.7819e-01,  6.8446e-01,  6.0967e-01,  8.3798e-01,
         5.6508e-01,  4.5837e-01,  6.4601e-01,  6.0564e-01,  7.0276e-01,
         6.2545e-01,  7.2177e-01,  1.9835e-01,  8.3023e-01,  3.0410e-01,
         7.4292e-01,  6.2648e-01,  6.6852e-01,  7.4091e-01,  8.1736e-01,
         7.2201e-01,  6.3722e-01,  8.1409e-01,  2.8247e-01,  6.7027e-01,
         4.0692e-01,  6.4644e-01,  5.8243e-01,  4.8741e-01,  1.6395e-01,
         4.9031e-01,  5.2773e-01,  5.6988e-01,  4.0564e-01,  2.0139e-01,
         9.9961e-02,  2.0165e-01,  1.1125e-03,  4.2281e-01,  3.9643e-01,
         4.1541e-01,  3.2433e-01,  2.6332e-01,  3.1068e-01,  3.3670e-01,
        -8.2930e-02,  3.5902e-01,  3.2128e-01,  5.4744e-01,  5.1893e-02,
         1.2131e-01,  1.6499e-01,  3.9381e-01,  3.5569e-01,  1.8668e-01,
        -2.7602e-02,  2.2602e-01,  4.5520e-01,  2.5207e-01,  1.3602e-01,
         1.7776e-01,  1.6354e-01,  1.9393e-01,  2.2589e-01, -1.2746e-02,
         3.7293e-01,  3.8717e-01,  3.0966e-01,  1.5060e-01,  3.1662e-01,
         2.7471e-01,  2.8315e-01,  1.8019e-01,  2.8763e-01,  2.4962e-01,
         1.7573e-01,  1.2351e-02,  5.8129e-03,  2.4310e-01,  1.7543e-01,
         1.9983e-01, -3.0489e-01,  2.5916e-01,  5.1656e-01,  3.1766e-01,
         4.0514e-01,  2.9041e-01,  2.8222e-01,  4.0311e-01,  2.7000e-01,
         3.1780e-01,  1.4101e-01,  2.8128e-01,  3.3194e-01,  3.2129e-01,
        -4.5910e-03,  9.4152e-02, -1.1782e-01, -1.1363e-01,  4.1081e-01,
        -1.4834e-01,  3.9827e-01, -1.2851e-01,  2.7310e-01,  2.8564e-01,
         2.5074e-01,  2.2182e-01,  9.1022e-02,  7.3395e-02,  2.4502e-01,
         1.8696e-01,  3.5289e-01, -8.0959e-02,  5.6253e-02,  1.8170e-01,
         1.4277e-01, -1.1671e-01,  2.7801e-01,  2.0496e-01,  2.7972e-01,
         4.5756e-02,  3.9044e-02,  1.2892e-01,  6.3393e-02, -3.7999e-01,
         6.2509e-02, -4.5440e-02,  7.8937e-02, -7.1789e-02,  8.3287e-02,
         1.5382e-02, -3.0838e-01, -4.2727e-01, -1.4707e-01, -4.6762e-01,
        -2.6300e-01, -3.5561e-01, -1.8815e-01,  5.6658e-02, -9.5003e-02,
         2.5578e-02, -3.6516e-01, -5.1949e-01, -1.6813e-01,  1.0164e-01,
        -3.0970e-01, -4.3237e-01, -4.2273e-01, -2.7358e-01, -2.0925e-01,
        -1.6833e-01, -4.1419e-01, -6.4329e-01, -1.8492e-01, -2.3785e-01,
        -2.6674e-01, -1.9068e-01, -2.3672e-01, -2.4476e-01, -3.8595e-01,
        -3.2020e-01, -5.9949e-01, -8.8581e-01, -5.9272e-01, -6.9470e-01,
        -5.0496e-01, -4.1262e-01, -3.2328e-01, -1.0647e+00, -8.4910e-01,
        -5.0621e-01, -7.1824e-01, -9.0503e-01, -6.6070e-01, -7.8864e-01,
        -6.9758e-01, -7.2723e-01, -9.1779e-01, -9.2047e-01, -8.9138e-01,
        -9.3282e-01, -7.1545e-01, -8.2644e-01, -7.1872e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1727e+00, -9.7649e-01, -1.5033e+00, -2.4841e+00, -2.8712e+00,
        -2.9385e+00, -2.8136e+00, -2.8637e+00, -2.8053e+00, -2.5578e+00,
        -2.7865e+00, -2.6353e+00, -2.6567e+00, -2.5075e+00, -2.2071e+00,
        -2.4536e+00, -2.5729e+00, -2.3840e+00, -2.2605e+00, -2.1006e+00,
        -2.0957e+00, -2.2423e+00, -8.4851e-01, -1.6624e-01, -1.7192e+00,
        -1.7972e+00, -1.5089e+00, -1.7024e+00, -1.5754e+00, -1.3938e+00,
        -1.9245e+00, -1.6952e+00, -1.9016e+00, -1.9262e+00, -1.6585e+00,
        -1.5815e+00, -1.6369e+00, -1.7353e+00, -1.5978e+00, -1.5464e+00,
        -1.4932e+00, -1.4327e+00, -1.3534e+00, -1.7076e+00, -1.7130e+00,
        -1.4866e+00, -1.4168e+00, -1.6427e+00, -1.6891e+00, -1.7420e+00,
        -1.3606e+00, -1.6197e+00, -1.4801e+00, -1.8573e+00, -1.3000e+00,
        -1.2658e+00, -1.0400e+00, -1.5910e+00, -1.4139e+00, -1.4200e+00,
        -1.3423e+00, -1.5371e+00, -1.0771e+00, -1.3815e+00, -1.5337e+00,
        -1.2330e+00,  1.2348e+00,  1.3233e+00,  2.0915e+00,  2.2359e+00,
         5.0269e+00,  1.0076e-02,  3.0197e-01,  1.8988e-01, -5.8263e-02,
         1.6820e-01,  2.1399e-01,  2.7442e-01,  3.1207e-01,  3.1984e-01,
         2.7342e-01,  4.1416e-01,  3.5100e-01,  2.9144e-01,  4.2218e-01,
         1.3741e-01, -3.2075e-02,  1.9535e-01,  4.6803e-01,  3.0006e-01,
         1.5809e-01,  1.3592e-01,  2.4129e-01,  2.7985e-01,  3.8373e-01,
         9.3767e-02,  2.1858e-01,  2.9595e-01,  2.7248e-01,  2.0694e-01,
         2.8389e-01,  4.0389e-01,  2.3041e-01,  3.8576e-01, -1.1743e-01,
         8.6588e-02, -1.1129e-01,  4.3304e-02,  1.3196e-01,  1.8517e-01,
         2.4034e-01,  3.0418e-01,  1.9772e-01,  1.8579e+00,  2.8151e+00,
         4.0062e+00,  6.9235e-01,  1.0040e+00,  8.9849e-01,  9.8156e-01,
         9.9348e-01,  9.4661e-01,  8.8564e-01,  8.7740e-01,  8.3869e-01,
         7.8021e-01,  7.7050e-01,  8.0947e-01,  9.0282e-01,  5.6602e-01,
         6.0496e-01,  9.3836e-01,  4.9374e-01,  7.9688e-01,  2.9518e-01,
         7.6387e-01,  6.9813e-01,  8.6332e-01,  8.7805e-01,  8.6128e-01,
         1.0302e+00,  1.0376e+00,  6.0359e-01,  9.3494e-01,  1.0049e+00,
         1.1363e+00,  5.8180e-01,  7.8788e-01,  9.3724e-01,  9.8114e-01,
         5.4276e-01,  1.0242e+00,  7.0953e-01,  9.1979e-01,  8.7946e-01,
         4.8583e-01,  7.4278e-01,  8.8971e-01,  8.3837e-01,  9.4360e-01,
         7.8032e-01,  5.3554e-01,  9.4958e-01,  9.7197e-01,  5.0330e-01,
         9.3914e-01,  6.6700e-01,  6.9008e-01,  8.0913e-01,  9.3204e-01,
         9.8389e-01,  6.6860e-01,  6.9045e-01,  8.7758e-01,  8.5468e-01,
         1.0292e+00,  9.3507e-01,  9.3621e-01,  8.6727e-01,  1.0950e+00,
         8.5016e-01,  8.8694e-01,  4.0003e-01,  5.4155e-01,  8.2146e-01,
         9.5419e-01,  9.5464e-01,  7.4469e-01,  6.4221e-01,  6.0937e-01,
         7.7685e-01,  4.7662e-01,  3.7416e-01,  8.8607e-01,  4.7607e-01,
         6.4064e-01,  7.1686e-01,  7.1356e-01,  3.8980e-01,  7.3710e-01,
         3.2544e-01,  8.3993e-01,  6.6799e-01,  8.5140e-01,  7.0570e-01,
         7.0435e-01,  7.8450e-01,  3.3517e-01,  6.0768e-01,  6.5547e-01,
         6.0368e-01,  6.1049e-01,  6.7694e-01,  5.4724e-01,  5.7074e-01,
         6.1509e-01,  7.0016e-01,  7.5022e-01,  4.4871e-01,  6.0200e-01,
         6.8185e-01,  5.7819e-01,  6.8446e-01,  6.0967e-01,  8.3798e-01,
         5.6508e-01,  4.5837e-01,  6.4601e-01,  6.0564e-01,  7.0276e-01,
         6.2545e-01,  6.6277e-01,  1.9835e-01,  8.0634e-01,  3.0410e-01,
         7.4292e-01,  6.2648e-01,  6.6852e-01,  7.4091e-01,  8.1736e-01,
         7.2201e-01,  6.3722e-01,  8.1409e-01,  2.8247e-01,  6.7027e-01,
         4.0692e-01,  6.4644e-01,  5.8243e-01,  4.8741e-01,  1.6395e-01,
         4.9031e-01,  5.2773e-01,  5.6988e-01,  4.1844e-01,  2.0139e-01,
         1.0747e-01,  2.0165e-01,  1.1125e-03,  4.2281e-01,  3.9643e-01,
         4.1541e-01,  3.2973e-01,  2.6332e-01,  3.1068e-01,  3.1219e-01,
        -8.2930e-02,  3.5902e-01,  3.2128e-01,  5.2405e-01,  5.2668e-02,
         1.2131e-01,  1.6499e-01,  3.9381e-01,  3.6760e-01,  1.8668e-01,
        -2.7602e-02,  2.2602e-01,  4.5520e-01,  2.5872e-01,  1.3602e-01,
         1.7776e-01,  1.6354e-01,  1.9393e-01,  2.1334e-01, -1.2746e-02,
         3.7293e-01,  3.8717e-01,  3.0966e-01,  1.5060e-01,  3.1662e-01,
         2.7471e-01,  2.8315e-01,  1.8019e-01,  2.8763e-01,  2.4962e-01,
         1.7573e-01,  1.2351e-02,  5.8129e-03,  2.4310e-01,  1.7543e-01,
         1.9983e-01, -3.0489e-01,  2.5916e-01,  5.1656e-01,  3.1766e-01,
         4.0514e-01,  2.9041e-01,  2.8222e-01,  4.0311e-01,  2.7000e-01,
         3.2685e-01,  1.4101e-01,  2.8128e-01,  3.3194e-01,  3.4823e-01,
        -4.5910e-03,  9.2211e-02, -1.0053e-01, -1.1363e-01,  3.6243e-01,
        -1.4834e-01,  3.7448e-01, -1.2851e-01,  2.7310e-01,  2.8564e-01,
         2.5074e-01,  2.2182e-01,  9.1022e-02,  7.3395e-02,  2.4502e-01,
         1.8696e-01,  3.5289e-01, -8.0959e-02,  5.6253e-02,  1.8170e-01,
         1.4277e-01, -1.1671e-01,  2.7801e-01,  2.0496e-01,  2.7972e-01,
         4.5756e-02,  3.9044e-02,  1.2892e-01,  5.6121e-02, -3.7999e-01,
         6.2509e-02, -4.5440e-02,  7.8937e-02, -7.1789e-02,  8.3287e-02,
         1.5382e-02, -2.9018e-01, -4.3320e-01, -1.4742e-01, -4.6762e-01,
        -2.6300e-01, -3.5561e-01, -1.8815e-01,  5.6658e-02, -9.5003e-02,
         2.6249e-02, -3.6516e-01, -5.1949e-01, -1.6813e-01,  1.0164e-01,
        -3.1689e-01, -4.3237e-01, -4.2273e-01, -2.7358e-01, -2.0925e-01,
        -1.6833e-01, -3.8447e-01, -6.4329e-01, -1.8492e-01, -2.3785e-01,
        -2.6674e-01, -1.9068e-01, -2.3672e-01, -2.4476e-01, -3.8595e-01,
        -3.3317e-01, -5.9949e-01, -8.8581e-01, -5.9272e-01, -6.9470e-01,
        -5.0496e-01, -4.1262e-01, -3.4637e-01, -1.0647e+00, -8.4910e-01,
        -5.0621e-01, -6.8660e-01, -9.2498e-01, -6.6070e-01, -7.8864e-01,
        -6.9758e-01, -7.2723e-01, -9.4607e-01, -9.2047e-01, -8.9138e-01,
        -9.3282e-01, -7.1545e-01, -8.2644e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808569
t6: 1641198808569
state_values: tensor([0.3096, 0.0615, 0.1009, 0.1751, 0.2475, 0.3037, 0.3148, 0.3157, 0.3177,
        0.3217, 0.3548, 0.3145, 0.3000, 0.2922, 0.3061, 0.4208, 0.3498, 0.3133,
        0.3302, 0.3281, 0.3382, 0.3221, 0.2016, 0.1686, 0.2339, 0.2398, 0.2378,
        0.2833, 0.2461, 0.2388, 0.3881, 0.3215, 0.3195, 0.2762, 0.2542, 0.2474,
        0.2449, 0.2485, 0.2616, 0.2455, 0.2567, 0.2479, 0.2459, 0.3119, 0.2639,
        0.2509, 0.2440, 0.2794, 0.2488, 0.2463, 0.2447, 0.2703, 0.2463, 0.3362,
        0.2759, 0.2584, 0.2718, 0.3673, 0.2910, 0.3330, 0.2768, 0.2698, 0.2560,
        0.2721, 0.2650, 0.2628, 0.1690, 0.1474, 0.1230, 0.1254, 0.0996, 0.1809,
        0.2093, 0.2123, 0.2193, 0.2627, 0.2475, 0.2475, 0.2417, 0.2486, 0.2374,
        0.2339, 0.2423, 0.2374, 0.2509, 0.3206, 0.2672, 0.2534, 0.2673, 0.2822,
        0.2601, 0.2940, 0.2704, 0.2592, 0.2519, 0.2563, 0.2947, 0.2768, 0.2688,
        0.2909, 0.2799, 0.2617, 0.2608, 0.2599, 0.2652, 0.2638, 0.2682, 0.2693,
        0.2676, 0.2662, 0.2843, 0.2961, 0.2956, 0.2119, 0.1727, 0.1457, 0.2165,
        0.2562, 0.2474, 0.2579, 0.2546, 0.2514, 0.2554, 0.2540, 0.2923, 0.2696,
        0.3154, 0.2823, 0.2710, 0.2738, 0.3690, 0.3125, 0.2913, 0.2998, 0.4437,
        0.3413, 0.3782, 0.3231, 0.3203, 0.3319, 0.3049, 0.2969, 0.2984, 0.3009,
        0.3051, 0.3035, 0.3000, 0.2959, 0.3284, 0.3008, 0.4198, 0.3331, 0.3161,
        0.3040, 0.3003, 0.4288, 0.3683, 0.3615, 0.3239, 0.3132, 0.3134, 0.3193,
        0.3067, 0.3158, 0.3122, 0.3158, 0.3148, 0.3514, 0.3189, 0.3076, 0.3102,
        0.3139, 0.3133, 0.3264, 0.3125, 0.3037, 0.3039, 0.3185, 0.3159, 0.3088,
        0.3031, 0.3323, 0.3221, 0.3195, 0.3109, 0.3126, 0.3032, 0.3387, 0.3913,
        0.3955, 0.3420, 0.3353, 0.3299, 0.3576, 0.3396, 0.4011, 0.3776, 0.3882,
        0.3578, 0.3716, 0.3580, 0.3418, 0.3416, 0.3439, 0.3629, 0.3800, 0.3681,
        0.3591, 0.3462, 0.3671, 0.4167, 0.3621, 0.3556, 0.3848, 0.4011, 0.3627,
        0.3527, 0.3468, 0.3512, 0.3841, 0.3537, 0.3989, 0.3824, 0.3674, 0.3503,
        0.3506, 0.3519, 0.4029, 0.3621, 0.3677, 0.3702, 0.3771, 0.3667, 0.3698,
        0.3627, 0.3631, 0.3549, 0.3495, 0.3574, 0.3440, 0.3440, 0.3528, 0.3531,
        0.3583, 0.3478, 0.3544, 0.3471, 0.3723, 0.3608, 0.3643, 0.3790, 0.3854,
        0.3769, 0.3861, 0.4653, 0.3968, 0.4914, 0.4137, 0.4013, 0.4061, 0.4074,
        0.4231, 0.4640, 0.4244, 0.4276, 0.4099, 0.4272, 0.3953, 0.4106, 0.3960,
        0.4893, 0.4116, 0.4025, 0.4050, 0.4876, 0.4239, 0.4031, 0.4048, 0.3921,
        0.5044, 0.4574, 0.4807, 0.4291, 0.4071, 0.4028, 0.3941, 0.3911, 0.3917,
        0.3934, 0.4127, 0.3960, 0.4124, 0.3974, 0.4242, 0.4235, 0.4011, 0.4005,
        0.3973, 0.3890, 0.3937, 0.3924, 0.4518, 0.4527, 0.4123, 0.4049, 0.3992,
        0.4009, 0.4552, 0.4208, 0.4738, 0.4327, 0.4990, 0.4471, 0.4251, 0.4329,
        0.5486, 0.4531, 0.4338, 0.4303, 0.4509, 0.4383, 0.4507, 0.4357, 0.4225,
        0.4165, 0.4156, 0.4123, 0.4963, 0.4417, 0.4900, 0.4378, 0.4182, 0.4294,
        0.4242, 0.4564, 0.4292, 0.4271, 0.4157, 0.4232, 0.4210, 0.4694, 0.4731,
        0.4408, 0.4410, 0.4388, 0.4259, 0.5011, 0.4451, 0.4345, 0.4367, 0.4337,
        0.5154, 0.4631, 0.5172, 0.6128, 0.4919, 0.4695, 0.4571, 0.4439, 0.4493,
        0.4355, 0.5555, 0.4809, 0.4501, 0.4318, 0.4436, 0.5647, 0.5656, 0.4989,
        0.5025, 0.4708, 0.4587, 0.4618, 0.4527, 0.4473, 0.4457, 0.4699, 0.4618,
        0.4605, 0.4808, 0.4555, 0.5657, 0.4936, 0.4677, 0.4642, 0.4590, 0.4702,
        0.4742, 0.6403, 0.5158, 0.5025, 0.5059, 0.4891, 0.5471, 0.5782, 0.5289,
        0.4959, 0.4937, 0.5738, 0.5622, 0.5448, 0.4937, 0.5164, 0.5052],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808574
t8: 1641198808574
t9: 1641198808574
t10: 1641198808584
t11: 1641198808586
t12: 1641198808586
t1: 1641198808586
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808596
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0050, 1.0017, 1.0067, 0.9959, 1.0166, 0.9994, 1.0036, 1.0059, 0.9980,
        0.9848, 1.0401, 0.9960, 1.0472, 0.9810, 0.8962, 0.9296, 1.0331, 0.9355,
        0.9516, 0.9314, 0.9592, 1.0189, 1.0044, 0.9509, 1.0396, 0.9976, 0.9325,
        1.0062, 0.9899, 0.8093, 0.9185, 0.9948, 1.0663, 1.0091, 1.0187, 0.9940,
        1.0769, 0.9719, 1.0296, 0.9844, 0.9874, 0.9965, 0.9263, 1.0382, 0.9878,
        1.0237, 0.9436, 1.0447, 1.0728, 0.9912, 0.9635, 1.0355, 0.9741, 0.9903,
        1.0311, 0.9677, 0.8745, 1.0852, 0.9360, 1.0093, 1.0510, 0.9970, 0.9775,
        1.0789, 0.9867, 1.0327, 1.0546, 1.0241, 0.9838, 0.9389, 1.0874, 0.9741,
        1.0404, 1.0674, 0.9359, 0.9878, 0.9780, 0.9855, 0.9734, 1.0273, 0.9981,
        0.9908, 1.0181, 0.9683, 0.9003, 1.0357, 1.0267, 0.9289, 0.9411, 1.0391,
        0.9269, 1.0013, 1.0089, 1.0210, 1.0524, 0.9435, 0.9873, 0.9951, 0.9454,
        0.9802, 1.0108, 1.0370, 1.0000, 1.0986, 1.0375, 1.0457, 1.0185, 1.0100,
        1.0068, 0.9905, 0.9703, 0.9776, 1.0320, 1.0555, 1.0440, 1.1562, 0.9677,
        1.0182, 0.9875, 0.9978, 1.0110, 1.0043, 1.0054, 0.9536, 1.0128, 0.9364,
        1.0267, 1.0414, 1.0755, 0.8967, 0.9949, 1.0899, 0.9574, 0.8310, 0.9786,
        0.9733, 0.9979, 0.9838, 0.9629, 1.0002, 1.0169, 1.0992, 0.9908, 0.9829,
        0.9870, 1.1035, 1.0272, 0.9632, 1.0009, 0.9152, 0.9988, 1.0365, 1.0350,
        1.0092, 0.8954, 0.9792, 0.9679, 1.0340, 1.0099, 1.0464, 1.0581, 1.0018,
        0.9937, 1.0725, 0.9902, 1.0213, 0.9764, 1.0284, 1.0071, 0.9969, 1.0342,
        1.0183, 0.9831, 1.0083, 0.9992, 1.0009, 0.9802, 0.9940, 0.9860, 1.0247,
        0.9642, 1.1419, 1.0502, 1.0160, 0.9963, 1.0034, 0.9714, 0.9241, 0.9546,
        1.0210, 1.1047, 1.2054, 0.9371, 1.0724, 0.9372, 0.9746, 0.9465, 1.0829,
        0.9474, 1.1160, 0.9996, 1.0254, 0.9855, 0.9645, 0.9420, 0.9714, 1.0857,
        1.0313, 0.9599, 0.9176, 1.0363, 0.9909, 0.9387, 0.9296, 1.0294, 1.0086,
        1.0259, 1.0494, 0.9585, 1.0062, 0.9449, 0.9627, 0.9954, 1.0003, 1.0554,
        1.0417, 0.9393, 1.0271, 0.9763, 0.9742, 0.9700, 1.1485, 0.9679, 1.0987,
        0.9875, 1.0265, 1.0153, 0.9908, 1.0054, 1.0038, 0.9974, 0.9898, 1.0572,
        1.0139, 1.0319, 1.0055, 0.9822, 1.0166, 1.0441, 0.9761, 0.9700, 0.9807,
        0.9720, 0.9030, 1.0499, 0.8743, 1.0174, 0.9836, 0.9678, 0.9613, 0.9393,
        0.9112, 0.9867, 0.9652, 1.1461, 0.9389, 1.0457, 0.9633, 1.1144, 0.9175,
        1.0326, 0.9895, 0.9753, 0.9026, 1.0385, 1.0203, 0.9753, 1.0475, 0.9134,
        0.9892, 0.9506, 1.0095, 1.0367, 1.1341, 1.0124, 1.0044, 1.0101, 1.0262,
        0.9827, 1.0115, 0.9840, 1.0296, 0.9626, 0.9681, 1.0297, 1.0454, 1.0474,
        1.0110, 1.0012, 1.0009, 1.0221, 0.9691, 0.9995, 1.0260, 1.0180, 1.0225,
        0.9559, 0.9917, 0.9353, 0.9922, 0.9053, 0.9951, 0.9954, 0.9597, 0.8615,
        1.0099, 1.0740, 1.2454, 0.9495, 1.1763, 0.9675, 1.1222, 1.0052, 1.0178,
        1.0151, 1.0192, 0.9607, 1.0216, 0.9433, 1.0176, 1.0207, 1.0736, 1.0414,
        0.9634, 1.0326, 1.0524, 1.0082, 0.9977, 0.9933, 0.9554, 0.9508, 0.9932,
        0.9851, 1.1866, 1.0221, 0.9318, 1.0049, 1.0304, 0.9825, 0.9989, 0.9293,
        1.1251, 0.9172, 0.9259, 1.0149, 1.0377, 1.0197, 0.9962, 0.9980, 1.0051,
        0.9123, 1.0246, 1.0269, 1.0058, 1.0525, 0.9150, 0.9763, 0.9946, 0.9397,
        0.9952, 1.0413, 1.1474, 0.9970, 1.0162, 1.0160, 0.9736, 0.9872, 0.9854,
        0.9617, 1.0079, 0.9004, 1.0387, 1.0317, 1.0786, 1.0261, 0.9857, 0.9771,
        0.8861, 1.0933, 0.9778, 0.9612, 1.1125, 0.9063, 0.9559, 0.9782, 0.9923,
        1.0642, 0.8958, 0.9735, 0.9704, 1.0125, 0.9609, 0.9832, 0.8758],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808600
t4: 1641198808600
surr1, surr2: tensor([-3.1774e+00, -9.8257e-01, -1.4877e+00, -2.4651e+00, -2.7919e+00,
        -2.9425e+00, -2.7929e+00, -2.7825e+00, -2.8189e+00, -2.6230e+00,
        -2.7243e+00, -2.6463e+00, -2.6067e+00, -2.5364e+00, -2.1977e+00,
        -2.4214e+00, -2.4754e+00, -2.4218e+00, -2.2947e+00, -2.1548e+00,
        -2.1270e+00, -2.2266e+00, -8.4823e-01, -1.6793e-01, -1.6999e+00,
        -1.8000e+00, -1.5495e+00, -1.7003e+00, -1.5837e+00, -1.2532e+00,
        -1.8898e+00, -1.7537e+00, -1.8508e+00, -1.9200e+00, -1.6506e+00,
        -1.5853e+00, -1.6026e+00, -1.7586e+00, -1.5809e+00, -1.5626e+00,
        -1.5020e+00, -1.4360e+00, -1.3929e+00, -1.6804e+00, -1.7235e+00,
        -1.4781e+00, -1.4482e+00, -1.6239e+00, -1.6473e+00, -1.7481e+00,
        -1.3827e+00, -1.5983e+00, -1.5003e+00, -1.8651e+00, -1.2847e+00,
        -1.2846e+00, -1.0105e+00, -1.6195e+00, -1.4682e+00, -1.4144e+00,
        -1.3080e+00, -1.5388e+00, -1.0860e+00, -1.3550e+00, -1.5430e+00,
        -1.2174e+00,  1.2078e+00,  1.3084e+00,  2.1090e+00,  2.2950e+00,
         5.0588e+00,  1.0203e-02,  2.9935e-01,  1.8538e-01, -5.9754e-02,
         1.6909e-01,  2.1658e-01,  2.7585e-01,  3.1515e-01,  3.1747e-01,
         2.7378e-01,  4.1583e-01,  3.4876e-01,  2.9565e-01,  4.2234e-01,
         1.3508e-01, -3.1809e-02,  1.9922e-01,  4.7638e-01,  2.9618e-01,
         1.6189e-01,  1.3598e-01,  2.4076e-01,  2.7835e-01,  3.7739e-01,
         9.6088e-02,  2.1977e-01,  2.9686e-01,  2.7754e-01,  2.0838e-01,
         2.8312e-01,  3.9960e-01,  2.3055e-01,  3.8526e-01, -1.1559e-01,
         8.4302e-02, -1.0984e-01,  4.2827e-02,  1.3084e-01,  1.8741e-01,
         2.4568e-01,  3.0763e-01,  1.9533e-01,  1.8285e+00,  2.7782e+00,
         4.2110e+00,  7.0458e-01,  9.9585e-01,  9.0634e-01,  9.8311e-01,
         9.8947e-01,  9.4549e-01,  8.8409e-01,  8.9746e-01,  8.3539e-01,
         8.0342e-01,  7.6442e-01,  7.9908e-01,  8.8565e-01,  5.6397e-01,
         6.0535e-01,  9.2973e-01,  4.9914e-01,  7.3582e-01,  2.9370e-01,
         7.9534e-01,  6.9885e-01,  8.7385e-01,  8.9019e-01,  8.6173e-01,
         1.0249e+00,  1.0369e+00,  6.0584e-01,  9.4260e-01,  1.0116e+00,
         1.1399e+00,  5.7542e-01,  8.0465e-01,  9.3759e-01,  9.9767e-01,
         5.4255e-01,  1.0016e+00,  7.0489e-01,  9.1795e-01,  8.7493e-01,
         4.8212e-01,  7.6321e-01,  8.8151e-01,  8.3602e-01,  9.3368e-01,
         7.6526e-01,  5.3545e-01,  9.5325e-01,  9.4766e-01,  5.0631e-01,
         9.2888e-01,  6.7914e-01,  6.8212e-01,  8.0630e-01,  9.3390e-01,
         9.6893e-01,  6.6240e-01,  6.9747e-01,  8.7446e-01,  8.5554e-01,
         1.0294e+00,  9.4477e-01,  9.3911e-01,  8.7298e-01,  1.0864e+00,
         8.6259e-01,  9.2070e-01,  3.9273e-01,  5.3811e-01,  8.2367e-01,
         9.5260e-01,  9.7305e-01,  7.6464e-01,  6.5218e-01,  6.0523e-01,
         7.8016e-01,  5.2228e-01,  3.8134e-01,  8.6528e-01,  4.9268e-01,
         6.4692e-01,  7.3483e-01,  7.0246e-01,  3.9504e-01,  7.4780e-01,
         3.2570e-01,  8.3306e-01,  6.7286e-01,  8.6468e-01,  7.2251e-01,
         7.1185e-01,  7.7433e-01,  3.3256e-01,  6.1644e-01,  6.6825e-01,
         5.9577e-01,  6.1315e-01,  6.8915e-01,  5.5897e-01,  5.6614e-01,
         6.1393e-01,  6.9540e-01,  7.3802e-01,  4.5720e-01,  6.0106e-01,
         7.0124e-01,  5.8521e-01,  6.8612e-01,  6.1014e-01,  8.2591e-01,
         5.5757e-01,  4.7058e-01,  6.3905e-01,  6.1343e-01,  7.0864e-01,
         6.3242e-01,  6.9197e-01,  2.0056e-01,  8.0542e-01,  3.0601e-01,
         7.3500e-01,  6.2148e-01,  6.7218e-01,  7.3923e-01,  8.1612e-01,
         7.2334e-01,  6.4077e-01,  7.9468e-01,  2.8090e-01,  6.6044e-01,
         4.0566e-01,  6.5470e-01,  5.7790e-01,  4.7541e-01,  1.6589e-01,
         4.9743e-01,  5.3268e-01,  5.7655e-01,  4.1983e-01,  1.9774e-01,
         1.0440e-01,  2.0154e-01,  1.1233e-03,  4.2668e-01,  4.0113e-01,
         4.2291e-01,  3.3382e-01,  2.6482e-01,  3.1525e-01,  3.2527e-01,
        -8.4346e-02,  3.5530e-01,  3.2663e-01,  5.3091e-01,  5.3690e-02,
         1.1941e-01,  1.6595e-01,  3.9697e-01,  3.6868e-01,  1.8390e-01,
        -2.7406e-02,  2.2771e-01,  4.4833e-01,  2.6257e-01,  1.3591e-01,
         1.8381e-01,  1.6316e-01,  1.9113e-01,  2.1996e-01, -1.2708e-02,
         3.7239e-01,  3.8506e-01,  3.0486e-01,  1.5248e-01,  3.1508e-01,
         2.7768e-01,  2.7983e-01,  1.8377e-01,  2.9123e-01,  2.4700e-01,
         1.7320e-01,  1.2153e-02,  5.7864e-03,  2.4305e-01,  1.7541e-01,
         1.9720e-01, -3.1071e-01,  2.5939e-01,  5.0997e-01,  3.1555e-01,
         4.0152e-01,  2.9748e-01,  2.8351e-01,  4.1651e-01,  2.7094e-01,
         3.2879e-01,  1.4137e-01,  2.8202e-01,  3.3539e-01,  3.3333e-01,
        -4.5832e-03,  9.0032e-02, -1.1383e-01, -1.1542e-01,  3.8758e-01,
        -1.5109e-01,  3.8203e-01, -1.2823e-01,  2.7088e-01,  2.8318e-01,
         2.4777e-01,  2.2846e-01,  8.9756e-02,  7.6040e-02,  2.4335e-01,
         1.8568e-01,  3.4659e-01, -7.9914e-02,  5.7237e-02,  1.7944e-01,
         1.3882e-01, -1.1637e-01,  2.7850e-01,  2.0592e-01,  2.8682e-01,
         4.6697e-02,  3.9182e-02,  1.2961e-01,  6.0540e-02, -3.7794e-01,
         6.4220e-02, -4.5362e-02,  7.7889e-02, -7.2220e-02,  8.3383e-02,
         1.5834e-02, -2.9679e-01, -4.4148e-01, -1.5167e-01, -4.6690e-01,
        -2.5853e-01, -3.5437e-01, -1.8846e-01,  5.6740e-02, -9.4864e-02,
         2.6608e-02, -3.6303e-01, -5.1348e-01, -1.6799e-01,  9.9869e-02,
        -3.2218e-01, -4.3362e-01, -4.2402e-01, -2.7934e-01, -2.0969e-01,
        -1.6608e-01, -4.0105e-01, -6.4429e-01, -1.8373e-01, -2.3580e-01,
        -2.7120e-01, -1.9190e-01, -2.3861e-01, -2.4853e-01, -3.8528e-01,
        -3.3330e-01, -5.9809e-01, -8.7428e-01, -5.8197e-01, -6.8898e-01,
        -5.0867e-01, -4.1766e-01, -3.4104e-01, -1.0830e+00, -8.6360e-01,
        -5.1178e-01, -6.9439e-01, -9.3143e-01, -6.7471e-01, -7.9468e-01,
        -7.0017e-01, -7.1464e-01, -9.4163e-01, -9.3272e-01, -9.0639e-01,
        -9.2985e-01, -7.2444e-01, -8.3076e-01, -7.5125e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1774e+00, -9.8257e-01, -1.4877e+00, -2.4651e+00, -2.7919e+00,
        -2.9425e+00, -2.7929e+00, -2.7825e+00, -2.8189e+00, -2.6230e+00,
        -2.7243e+00, -2.6463e+00, -2.6067e+00, -2.5364e+00, -2.2071e+00,
        -2.4214e+00, -2.4754e+00, -2.4218e+00, -2.2947e+00, -2.1548e+00,
        -2.1270e+00, -2.2266e+00, -8.4823e-01, -1.6793e-01, -1.6999e+00,
        -1.8000e+00, -1.5495e+00, -1.7003e+00, -1.5837e+00, -1.3938e+00,
        -1.8898e+00, -1.7537e+00, -1.8508e+00, -1.9200e+00, -1.6506e+00,
        -1.5853e+00, -1.6026e+00, -1.7586e+00, -1.5809e+00, -1.5626e+00,
        -1.5020e+00, -1.4360e+00, -1.3929e+00, -1.6804e+00, -1.7235e+00,
        -1.4781e+00, -1.4482e+00, -1.6239e+00, -1.6473e+00, -1.7481e+00,
        -1.3827e+00, -1.5983e+00, -1.5003e+00, -1.8651e+00, -1.2847e+00,
        -1.2846e+00, -1.0400e+00, -1.6195e+00, -1.4682e+00, -1.4144e+00,
        -1.3080e+00, -1.5388e+00, -1.0860e+00, -1.3550e+00, -1.5430e+00,
        -1.2174e+00,  1.2078e+00,  1.3084e+00,  2.1090e+00,  2.2950e+00,
         5.0588e+00,  1.0203e-02,  2.9935e-01,  1.8538e-01, -5.9754e-02,
         1.6909e-01,  2.1658e-01,  2.7585e-01,  3.1515e-01,  3.1747e-01,
         2.7378e-01,  4.1583e-01,  3.4876e-01,  2.9565e-01,  4.2234e-01,
         1.3508e-01, -3.1809e-02,  1.9922e-01,  4.7638e-01,  2.9618e-01,
         1.6189e-01,  1.3598e-01,  2.4076e-01,  2.7835e-01,  3.7739e-01,
         9.6088e-02,  2.1977e-01,  2.9686e-01,  2.7754e-01,  2.0838e-01,
         2.8312e-01,  3.9960e-01,  2.3055e-01,  3.8526e-01, -1.1559e-01,
         8.4302e-02, -1.0984e-01,  4.2827e-02,  1.3084e-01,  1.8741e-01,
         2.4568e-01,  3.0763e-01,  1.9533e-01,  1.8285e+00,  2.7782e+00,
         4.0062e+00,  7.0458e-01,  9.9585e-01,  9.0634e-01,  9.8311e-01,
         9.8947e-01,  9.4549e-01,  8.8409e-01,  8.9746e-01,  8.3539e-01,
         8.0342e-01,  7.6442e-01,  7.9908e-01,  8.8565e-01,  5.6602e-01,
         6.0535e-01,  9.2973e-01,  4.9914e-01,  7.9688e-01,  2.9370e-01,
         7.9534e-01,  6.9885e-01,  8.7385e-01,  8.9019e-01,  8.6173e-01,
         1.0249e+00,  1.0369e+00,  6.0584e-01,  9.4260e-01,  1.0116e+00,
         1.1363e+00,  5.7542e-01,  8.0465e-01,  9.3759e-01,  9.9767e-01,
         5.4255e-01,  1.0016e+00,  7.0489e-01,  9.1795e-01,  8.7946e-01,
         4.8212e-01,  7.6321e-01,  8.8151e-01,  8.3602e-01,  9.3368e-01,
         7.6526e-01,  5.3545e-01,  9.5325e-01,  9.4766e-01,  5.0631e-01,
         9.2888e-01,  6.7914e-01,  6.8212e-01,  8.0630e-01,  9.3390e-01,
         9.6893e-01,  6.6240e-01,  6.9747e-01,  8.7446e-01,  8.5554e-01,
         1.0294e+00,  9.4477e-01,  9.3911e-01,  8.7298e-01,  1.0864e+00,
         8.6259e-01,  8.8694e-01,  3.9273e-01,  5.3811e-01,  8.2367e-01,
         9.5260e-01,  9.7305e-01,  7.6464e-01,  6.5218e-01,  6.0523e-01,
         7.7685e-01,  4.7662e-01,  3.8134e-01,  8.6528e-01,  4.9268e-01,
         6.4692e-01,  7.3483e-01,  7.0246e-01,  3.9504e-01,  7.3710e-01,
         3.2570e-01,  8.3306e-01,  6.7286e-01,  8.6468e-01,  7.2251e-01,
         7.1185e-01,  7.7433e-01,  3.3256e-01,  6.1644e-01,  6.6825e-01,
         5.9577e-01,  6.1315e-01,  6.8915e-01,  5.5897e-01,  5.6614e-01,
         6.1393e-01,  6.9540e-01,  7.3802e-01,  4.5720e-01,  6.0106e-01,
         7.0124e-01,  5.8521e-01,  6.8612e-01,  6.1014e-01,  8.2591e-01,
         5.5757e-01,  4.7058e-01,  6.3905e-01,  6.1343e-01,  7.0864e-01,
         6.3242e-01,  6.6277e-01,  2.0056e-01,  8.0542e-01,  3.0601e-01,
         7.3500e-01,  6.2148e-01,  6.7218e-01,  7.3923e-01,  8.1612e-01,
         7.2334e-01,  6.4077e-01,  7.9468e-01,  2.8090e-01,  6.6044e-01,
         4.0566e-01,  6.5470e-01,  5.7790e-01,  4.7541e-01,  1.6589e-01,
         4.9743e-01,  5.3268e-01,  5.7655e-01,  4.1983e-01,  1.9774e-01,
         1.0747e-01,  2.0154e-01,  1.1233e-03,  4.2668e-01,  4.0113e-01,
         4.2291e-01,  3.3382e-01,  2.6482e-01,  3.1525e-01,  3.1219e-01,
        -8.4346e-02,  3.5530e-01,  3.2663e-01,  5.2405e-01,  5.3690e-02,
         1.1941e-01,  1.6595e-01,  3.9697e-01,  3.6868e-01,  1.8390e-01,
        -2.7406e-02,  2.2771e-01,  4.4833e-01,  2.6257e-01,  1.3591e-01,
         1.8381e-01,  1.6316e-01,  1.9113e-01,  2.1334e-01, -1.2708e-02,
         3.7239e-01,  3.8506e-01,  3.0486e-01,  1.5248e-01,  3.1508e-01,
         2.7768e-01,  2.7983e-01,  1.8377e-01,  2.9123e-01,  2.4700e-01,
         1.7320e-01,  1.2153e-02,  5.7864e-03,  2.4305e-01,  1.7541e-01,
         1.9720e-01, -3.1071e-01,  2.5939e-01,  5.0997e-01,  3.1555e-01,
         4.0152e-01,  2.9748e-01,  2.8351e-01,  4.1651e-01,  2.7094e-01,
         3.2879e-01,  1.4137e-01,  2.8202e-01,  3.3539e-01,  3.4823e-01,
        -4.5832e-03,  9.0032e-02, -1.0053e-01, -1.1542e-01,  3.6243e-01,
        -1.5109e-01,  3.7448e-01, -1.2823e-01,  2.7088e-01,  2.8318e-01,
         2.4777e-01,  2.2846e-01,  8.9756e-02,  7.6040e-02,  2.4335e-01,
         1.8568e-01,  3.4659e-01, -7.9914e-02,  5.7237e-02,  1.7944e-01,
         1.3882e-01, -1.1637e-01,  2.7850e-01,  2.0592e-01,  2.8682e-01,
         4.6697e-02,  3.9182e-02,  1.2961e-01,  5.6121e-02, -3.7794e-01,
         6.4220e-02, -4.5362e-02,  7.7889e-02, -7.2220e-02,  8.3383e-02,
         1.5834e-02, -2.9018e-01, -4.4148e-01, -1.5167e-01, -4.6690e-01,
        -2.5853e-01, -3.5437e-01, -1.8846e-01,  5.6740e-02, -9.4864e-02,
         2.6608e-02, -3.6303e-01, -5.1348e-01, -1.6799e-01,  9.9869e-02,
        -3.2218e-01, -4.3362e-01, -4.2402e-01, -2.7934e-01, -2.0969e-01,
        -1.6608e-01, -3.8447e-01, -6.4429e-01, -1.8373e-01, -2.3580e-01,
        -2.7120e-01, -1.9190e-01, -2.3861e-01, -2.4853e-01, -3.8528e-01,
        -3.3330e-01, -5.9809e-01, -8.7428e-01, -5.8197e-01, -6.8898e-01,
        -5.0867e-01, -4.1766e-01, -3.4637e-01, -1.0830e+00, -8.6360e-01,
        -5.1178e-01, -6.8660e-01, -9.3143e-01, -6.7471e-01, -7.9468e-01,
        -7.0017e-01, -7.1464e-01, -9.4607e-01, -9.3272e-01, -9.0639e-01,
        -9.2985e-01, -7.2444e-01, -8.3076e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808610
t6: 1641198808610
state_values: tensor([0.3885, 0.1288, 0.1749, 0.2579, 0.3445, 0.4019, 0.4159, 0.4191, 0.4223,
        0.4280, 0.4600, 0.4224, 0.4096, 0.4024, 0.4169, 0.5295, 0.4596, 0.4238,
        0.4412, 0.4396, 0.4480, 0.4338, 0.3025, 0.2570, 0.3442, 0.3499, 0.3482,
        0.3920, 0.3561, 0.3494, 0.5017, 0.4324, 0.4287, 0.3846, 0.3641, 0.3578,
        0.3555, 0.3587, 0.3722, 0.3567, 0.3679, 0.3595, 0.3575, 0.4203, 0.3744,
        0.3625, 0.3549, 0.3907, 0.3603, 0.3571, 0.3556, 0.3823, 0.3574, 0.4486,
        0.3871, 0.3701, 0.3838, 0.4855, 0.4019, 0.4467, 0.3879, 0.3810, 0.3667,
        0.3842, 0.3765, 0.3746, 0.2545, 0.2317, 0.2088, 0.2116, 0.1786, 0.2754,
        0.3137, 0.3170, 0.3257, 0.3751, 0.3587, 0.3587, 0.3519, 0.3595, 0.3466,
        0.3426, 0.3519, 0.3463, 0.3615, 0.4384, 0.3793, 0.3630, 0.3805, 0.3990,
        0.3712, 0.4125, 0.3841, 0.3701, 0.3611, 0.3665, 0.4127, 0.3919, 0.3820,
        0.4099, 0.3962, 0.3734, 0.3723, 0.3714, 0.3774, 0.3761, 0.3812, 0.3829,
        0.3808, 0.3792, 0.4020, 0.4168, 0.4159, 0.3113, 0.2590, 0.2323, 0.3190,
        0.3687, 0.3575, 0.3703, 0.3658, 0.3619, 0.3666, 0.3648, 0.4126, 0.3840,
        0.4383, 0.3988, 0.3849, 0.3883, 0.4910, 0.4359, 0.4092, 0.4205, 0.5693,
        0.4650, 0.4995, 0.4473, 0.4449, 0.4566, 0.4261, 0.4163, 0.4179, 0.4217,
        0.4271, 0.4252, 0.4202, 0.4156, 0.4539, 0.4220, 0.5444, 0.4576, 0.4390,
        0.4252, 0.4209, 0.5547, 0.4916, 0.4857, 0.4473, 0.4355, 0.4357, 0.4420,
        0.4284, 0.4386, 0.4339, 0.4385, 0.4372, 0.4763, 0.4417, 0.4293, 0.4321,
        0.4359, 0.4354, 0.4501, 0.4346, 0.4249, 0.4251, 0.4414, 0.4384, 0.4307,
        0.4242, 0.4564, 0.4442, 0.4414, 0.4321, 0.4341, 0.4237, 0.4631, 0.5159,
        0.5199, 0.4649, 0.4572, 0.4509, 0.4824, 0.4620, 0.5265, 0.5033, 0.5149,
        0.4819, 0.4975, 0.4820, 0.4640, 0.4638, 0.4667, 0.4889, 0.5072, 0.4947,
        0.4836, 0.4686, 0.4938, 0.5453, 0.4869, 0.4795, 0.5127, 0.5300, 0.4881,
        0.4762, 0.4692, 0.4743, 0.5125, 0.4776, 0.5264, 0.5105, 0.4936, 0.4733,
        0.4736, 0.4752, 0.5309, 0.4868, 0.4941, 0.4972, 0.5055, 0.4925, 0.4968,
        0.4877, 0.4887, 0.4787, 0.4723, 0.4821, 0.4660, 0.4660, 0.4764, 0.4769,
        0.4827, 0.4705, 0.4782, 0.4697, 0.4999, 0.4859, 0.4897, 0.5077, 0.5155,
        0.5053, 0.5161, 0.5977, 0.5269, 0.6221, 0.5431, 0.5323, 0.5374, 0.5389,
        0.5554, 0.5963, 0.5560, 0.5598, 0.5403, 0.5595, 0.5263, 0.5424, 0.5267,
        0.6205, 0.5419, 0.5337, 0.5367, 0.6192, 0.5546, 0.5339, 0.5363, 0.5223,
        0.6352, 0.5909, 0.6126, 0.5607, 0.5377, 0.5326, 0.5239, 0.5210, 0.5218,
        0.5236, 0.5449, 0.5267, 0.5447, 0.5281, 0.5573, 0.5565, 0.5321, 0.5313,
        0.5276, 0.5190, 0.5241, 0.5227, 0.5847, 0.5870, 0.5439, 0.5356, 0.5295,
        0.5313, 0.5879, 0.5530, 0.6068, 0.5653, 0.6310, 0.5802, 0.5573, 0.5660,
        0.6805, 0.5858, 0.5653, 0.5615, 0.5846, 0.5703, 0.5847, 0.5678, 0.5536,
        0.5469, 0.5460, 0.5425, 0.6289, 0.5746, 0.6227, 0.5704, 0.5483, 0.5612,
        0.5552, 0.5910, 0.5614, 0.5582, 0.5458, 0.5543, 0.5521, 0.6040, 0.6088,
        0.5747, 0.5750, 0.5714, 0.5569, 0.6339, 0.5786, 0.5666, 0.5697, 0.5660,
        0.6479, 0.5964, 0.6490, 0.7464, 0.6251, 0.6034, 0.5908, 0.5766, 0.5831,
        0.5671, 0.6885, 0.6147, 0.5835, 0.5627, 0.5762, 0.6991, 0.7002, 0.6326,
        0.6370, 0.6061, 0.5930, 0.5956, 0.5865, 0.5804, 0.5787, 0.6058, 0.5974,
        0.5960, 0.6171, 0.5905, 0.7013, 0.6275, 0.6025, 0.5986, 0.5935, 0.6059,
        0.6104, 0.7755, 0.6489, 0.6373, 0.6408, 0.6235, 0.6796, 0.7146, 0.6623,
        0.6307, 0.6282, 0.7102, 0.6983, 0.6793, 0.6288, 0.6511, 0.6403],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808614
t8: 1641198808614
t9: 1641198808614
t10: 1641198808624
t11: 1641198808626
t12: 1641198808626
t1: 1641198808626
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808636
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0065, 1.0085, 0.9952, 0.9881, 0.9867, 1.0005, 0.9955, 0.9750, 1.0027,
        1.0111, 1.0139, 0.9997, 1.0260, 0.9922, 0.9452, 0.9096, 0.9980, 0.9485,
        0.9671, 0.9559, 0.9737, 1.0116, 1.0039, 0.9610, 1.0271, 0.9988, 0.9569,
        1.0044, 0.9945, 0.8631, 0.8928, 1.0255, 1.0347, 1.0060, 1.0136, 0.9965,
        1.0448, 0.9845, 1.0174, 0.9944, 0.9931, 0.9984, 0.9579, 1.0249, 0.9932,
        1.0173, 0.9646, 1.0298, 1.0415, 0.9943, 0.9797, 1.0203, 0.9870, 0.9941,
        1.0180, 0.9819, 0.9215, 1.1183, 0.9670, 1.0055, 1.0255, 0.9981, 0.9853,
        1.0466, 0.9924, 1.0185, 1.0304, 1.0119, 0.9920, 0.9654, 1.1168, 0.9849,
        1.0291, 1.0419, 0.9591, 0.9934, 0.9892, 0.9905, 0.9826, 1.0190, 0.9991,
        0.9944, 1.0109, 0.9826, 0.9379, 1.0307, 1.0178, 0.9472, 0.9593, 1.0251,
        0.9479, 1.0014, 1.0063, 1.0152, 1.0343, 0.9666, 0.9927, 0.9976, 0.9627,
        0.9870, 1.0076, 1.0257, 1.0003, 1.0590, 1.0208, 1.0169, 1.0046, 0.9981,
        0.9975, 1.0027, 0.9933, 0.9893, 1.0185, 1.0384, 1.0298, 1.1055, 0.9844,
        1.0086, 0.9961, 0.9991, 1.0064, 1.0027, 1.0032, 0.9758, 1.0079, 0.9638,
        1.0174, 1.0277, 1.0550, 0.9295, 0.9924, 1.0410, 0.9671, 0.8798, 0.9706,
        1.0088, 0.9988, 0.9946, 0.9769, 1.0004, 1.0117, 1.0688, 0.9941, 0.9908,
        0.9934, 1.0551, 1.0157, 0.9838, 1.0008, 0.9608, 0.9976, 1.0149, 1.0282,
        1.0068, 0.9361, 0.9663, 0.9924, 1.0235, 1.0070, 1.0354, 1.0376, 1.0013,
        0.9973, 1.0289, 0.9958, 1.0090, 0.9943, 1.0149, 1.0031, 0.9985, 1.0175,
        1.0083, 0.9931, 1.0040, 0.9998, 1.0007, 0.9903, 0.9969, 0.9925, 1.0156,
        0.9784, 1.0892, 1.0311, 1.0092, 0.9987, 1.0012, 0.9903, 0.9589, 0.9671,
        1.0138, 1.0803, 1.1603, 0.9546, 1.0442, 0.9695, 0.9826, 0.9690, 1.0597,
        0.9599, 1.0860, 1.0001, 1.0166, 0.9925, 0.9800, 0.9655, 0.9819, 1.0570,
        1.0231, 0.9735, 0.9482, 1.0246, 0.9945, 0.9561, 0.9520, 1.0205, 1.0064,
        1.0187, 1.0318, 0.9763, 1.0040, 0.9712, 0.9756, 0.9974, 1.0007, 1.0399,
        1.0274, 0.9645, 1.0142, 0.9881, 0.9823, 0.9805, 1.0993, 0.9780, 1.0632,
        0.9933, 1.0147, 1.0066, 0.9960, 1.0025, 1.0018, 0.9989, 0.9951, 1.0299,
        1.0077, 1.0154, 1.0020, 0.9948, 1.0076, 1.0170, 0.9874, 0.9849, 0.9901,
        0.9834, 0.9352, 1.0334, 0.9116, 1.0385, 0.9921, 0.9768, 0.9729, 0.9565,
        0.9385, 0.9911, 0.9781, 1.1050, 0.9536, 1.0332, 0.9787, 1.0775, 0.9479,
        1.0230, 0.9946, 0.9830, 0.9360, 1.0247, 1.0133, 0.9824, 1.0308, 0.9507,
        0.9820, 0.9819, 1.0067, 1.0229, 1.1047, 1.0092, 1.0025, 1.0041, 1.0093,
        0.9948, 1.0057, 0.9945, 1.0164, 0.9815, 0.9807, 1.0181, 1.0302, 1.0304,
        1.0061, 1.0006, 1.0004, 1.0079, 0.9880, 1.0000, 1.0120, 1.0108, 1.0126,
        0.9795, 0.9961, 0.9660, 0.9952, 0.9345, 0.9971, 0.9974, 0.9692, 0.8963,
        1.0168, 1.0314, 1.1981, 0.9639, 1.1032, 0.9849, 1.0718, 1.0027, 1.0089,
        1.0057, 1.0063, 0.9899, 1.0084, 0.9763, 1.0118, 1.0138, 1.0539, 1.0279,
        0.9801, 1.0183, 1.0227, 1.0049, 0.9992, 0.9977, 0.9807, 0.9717, 0.9963,
        0.9901, 1.1314, 1.0163, 0.9585, 1.0029, 1.0168, 0.9881, 0.9997, 0.9562,
        1.0782, 0.9459, 0.9557, 1.0225, 1.0222, 1.0160, 0.9977, 0.9991, 1.0032,
        0.9495, 1.0324, 1.0152, 1.0046, 1.0337, 0.9518, 0.9625, 0.9967, 0.9573,
        0.9971, 1.0272, 1.1072, 0.9982, 1.0092, 1.0064, 0.9901, 0.9935, 0.9929,
        0.9764, 1.0056, 0.9364, 1.0624, 1.0188, 1.0587, 1.0174, 0.9927, 0.9892,
        0.9376, 1.1218, 0.9921, 0.9723, 1.0748, 0.9311, 0.9728, 0.9845, 0.9954,
        1.0452, 0.9213, 0.9850, 0.9860, 1.0089, 0.9720, 0.9883, 0.9147],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808641
t4: 1641198808641
surr1, surr2: tensor([-3.1820e+00, -9.8924e-01, -1.4708e+00, -2.4457e+00, -2.7097e+00,
        -2.9457e+00, -2.7706e+00, -2.6970e+00, -2.8323e+00, -2.6933e+00,
        -2.6556e+00, -2.6562e+00, -2.5539e+00, -2.5655e+00, -2.3180e+00,
        -2.3692e+00, -2.3913e+00, -2.4556e+00, -2.3320e+00, -2.2116e+00,
        -2.1592e+00, -2.2106e+00, -8.4779e-01, -1.6972e-01, -1.6796e+00,
        -1.8021e+00, -1.5900e+00, -1.6973e+00, -1.5911e+00, -1.3367e+00,
        -1.8369e+00, -1.8078e+00, -1.7960e+00, -1.9141e+00, -1.6422e+00,
        -1.5892e+00, -1.5548e+00, -1.7815e+00, -1.5622e+00, -1.5784e+00,
        -1.5107e+00, -1.4387e+00, -1.4405e+00, -1.6588e+00, -1.7328e+00,
        -1.4689e+00, -1.4805e+00, -1.6008e+00, -1.5993e+00, -1.7535e+00,
        -1.4060e+00, -1.5748e+00, -1.5202e+00, -1.8722e+00, -1.2684e+00,
        -1.3036e+00, -1.0648e+00, -1.6689e+00, -1.5169e+00, -1.4091e+00,
        -1.2763e+00, -1.5404e+00, -1.0945e+00, -1.3144e+00, -1.5519e+00,
        -1.2007e+00,  1.1801e+00,  1.2927e+00,  2.1267e+00,  2.3600e+00,
         5.1954e+00,  1.0316e-02,  2.9610e-01,  1.8094e-01, -6.1236e-02,
         1.7005e-01,  2.1906e-01,  2.7723e-01,  3.1815e-01,  3.1490e-01,
         2.7405e-01,  4.1735e-01,  3.4627e-01,  3.0001e-01,  4.3998e-01,
         1.3443e-01, -3.1533e-02,  2.0314e-01,  4.8560e-01,  2.9220e-01,
         1.6557e-01,  1.3599e-01,  2.4013e-01,  2.7676e-01,  3.7088e-01,
         9.8438e-02,  2.2097e-01,  2.9761e-01,  2.8261e-01,  2.0984e-01,
         2.8222e-01,  3.9526e-01,  2.3062e-01,  3.7139e-01, -1.1372e-01,
         8.1978e-02, -1.0834e-01,  4.2321e-02,  1.2963e-01,  1.8972e-01,
         2.5151e-01,  3.1132e-01,  1.9278e-01,  1.7989e+00,  2.7403e+00,
         4.0261e+00,  7.1677e-01,  9.8643e-01,  9.1415e-01,  9.8433e-01,
         9.8498e-01,  9.4400e-01,  8.8218e-01,  9.1833e-01,  8.3130e-01,
         8.2696e-01,  7.5747e-01,  7.8857e-01,  8.6874e-01,  5.8460e-01,
         6.0381e-01,  8.8803e-01,  5.0421e-01,  7.7898e-01,  2.9131e-01,
         8.2434e-01,  6.9946e-01,  8.8349e-01,  9.0314e-01,  8.6190e-01,
         1.0197e+00,  1.0082e+00,  6.0786e-01,  9.5023e-01,  1.0182e+00,
         1.0899e+00,  5.6902e-01,  8.2187e-01,  9.3751e-01,  1.0475e+00,
         5.4188e-01,  9.8079e-01,  7.0024e-01,  9.1581e-01,  9.1471e-01,
         4.7576e-01,  7.8253e-01,  8.7253e-01,  8.3363e-01,  9.2393e-01,
         7.5043e-01,  5.3519e-01,  9.5662e-01,  9.0914e-01,  5.0919e-01,
         9.1774e-01,  6.9160e-01,  6.7320e-01,  8.0314e-01,  9.3544e-01,
         9.5333e-01,  6.5588e-01,  7.0455e-01,  8.7073e-01,  8.5607e-01,
         1.0291e+00,  9.5449e-01,  9.4184e-01,  8.7874e-01,  1.0768e+00,
         8.7531e-01,  8.7827e-01,  3.8559e-01,  5.3451e-01,  8.2569e-01,
         9.5049e-01,  9.9198e-01,  7.9342e-01,  6.6073e-01,  6.0093e-01,
         7.6291e-01,  5.0274e-01,  3.8846e-01,  8.4251e-01,  5.0965e-01,
         6.5222e-01,  7.5236e-01,  6.8742e-01,  4.0023e-01,  7.2772e-01,
         3.2586e-01,  8.2597e-01,  6.7765e-01,  8.7859e-01,  7.4053e-01,
         7.1960e-01,  7.5383e-01,  3.2993e-01,  6.2521e-01,  6.9058e-01,
         5.8900e-01,  6.1542e-01,  7.0192e-01,  5.7248e-01,  5.6125e-01,
         6.1258e-01,  6.9048e-01,  7.2557e-01,  4.6570e-01,  5.9975e-01,
         7.2076e-01,  5.9302e-01,  6.8751e-01,  6.1038e-01,  8.1376e-01,
         5.4993e-01,  4.8324e-01,  6.3098e-01,  6.2086e-01,  7.1455e-01,
         6.3930e-01,  6.6233e-01,  2.0266e-01,  7.7936e-01,  3.0781e-01,
         7.2653e-01,  6.1618e-01,  6.7575e-01,  7.3716e-01,  8.1453e-01,
         7.2444e-01,  6.4421e-01,  7.7414e-01,  2.7918e-01,  6.4994e-01,
         4.0423e-01,  6.6310e-01,  5.7273e-01,  4.6308e-01,  1.6782e-01,
         5.0507e-01,  5.3781e-01,  5.8331e-01,  4.3481e-01,  1.9464e-01,
         1.0886e-01,  2.0572e-01,  1.1331e-03,  4.3065e-01,  4.0596e-01,
         4.3065e-01,  3.4382e-01,  2.6601e-01,  3.1945e-01,  3.1361e-01,
        -8.5673e-02,  3.5108e-01,  3.3186e-01,  5.1334e-01,  5.5472e-02,
         1.1830e-01,  1.6680e-01,  4.0008e-01,  3.8230e-01,  1.8146e-01,
        -2.7218e-02,  2.2935e-01,  4.4117e-01,  2.7331e-01,  1.3491e-01,
         1.8987e-01,  1.6271e-01,  1.8858e-01,  2.1426e-01, -1.2667e-02,
         3.7171e-01,  3.8276e-01,  2.9983e-01,  1.5437e-01,  3.1328e-01,
         2.8063e-01,  2.7624e-01,  1.8738e-01,  2.9502e-01,  2.4422e-01,
         1.7067e-01,  1.1956e-02,  5.7580e-03,  2.4290e-01,  1.7532e-01,
         1.9445e-01, -3.1675e-01,  2.5952e-01,  5.0298e-01,  3.1331e-01,
         3.9764e-01,  3.0480e-01,  2.8478e-01,  4.3020e-01,  2.7178e-01,
         3.3937e-01,  1.4165e-01,  2.8260e-01,  3.3869e-01,  3.4681e-01,
        -4.6144e-03,  8.6462e-02, -1.0950e-01, -1.1718e-01,  3.6348e-01,
        -1.5381e-01,  3.6489e-01, -1.2791e-01,  2.6852e-01,  2.8056e-01,
         2.4465e-01,  2.3538e-01,  8.8593e-02,  7.8704e-02,  2.4197e-01,
         1.8441e-01,  3.4024e-01, -7.8880e-02,  5.8227e-02,  1.7695e-01,
         1.3489e-01, -1.1599e-01,  2.7891e-01,  2.0683e-01,  2.9441e-01,
         4.7727e-02,  3.9303e-02,  1.3026e-01,  5.7722e-02, -3.7580e-01,
         6.6057e-02, -4.5271e-02,  7.6860e-02, -7.2630e-02,  8.3449e-02,
         1.6293e-02, -2.8443e-01, -4.5529e-01, -1.5654e-01, -4.7038e-01,
        -2.5467e-01, -3.5309e-01, -1.8875e-01,  5.6801e-02, -9.4687e-02,
         2.7692e-02, -3.6577e-01, -5.0765e-01, -1.6779e-01,  9.8087e-02,
        -3.3513e-01, -4.2749e-01, -4.2489e-01, -2.8457e-01, -2.1010e-01,
        -1.6383e-01, -3.8700e-01, -6.4508e-01, -1.8246e-01, -2.3357e-01,
        -2.7580e-01, -1.9314e-01, -2.4044e-01, -2.5235e-01, -3.8440e-01,
        -3.4662e-01, -6.1172e-01, -8.6336e-01, -5.7126e-01, -6.8318e-01,
        -5.1231e-01, -4.2281e-01, -3.6082e-01, -1.1112e+00, -8.7624e-01,
        -5.1766e-01, -6.7084e-01, -9.5698e-01, -6.8668e-01, -7.9978e-01,
        -7.0234e-01, -7.0191e-01, -9.6844e-01, -9.4376e-01, -9.2096e-01,
        -9.2649e-01, -7.3282e-01, -8.3511e-01, -7.8459e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1820e+00, -9.8924e-01, -1.4708e+00, -2.4457e+00, -2.7097e+00,
        -2.9457e+00, -2.7706e+00, -2.6970e+00, -2.8323e+00, -2.6933e+00,
        -2.6556e+00, -2.6562e+00, -2.5539e+00, -2.5655e+00, -2.3180e+00,
        -2.3692e+00, -2.3913e+00, -2.4556e+00, -2.3320e+00, -2.2116e+00,
        -2.1592e+00, -2.2106e+00, -8.4779e-01, -1.6972e-01, -1.6796e+00,
        -1.8021e+00, -1.5900e+00, -1.6973e+00, -1.5911e+00, -1.3938e+00,
        -1.8517e+00, -1.8078e+00, -1.7960e+00, -1.9141e+00, -1.6422e+00,
        -1.5892e+00, -1.5548e+00, -1.7815e+00, -1.5622e+00, -1.5784e+00,
        -1.5107e+00, -1.4387e+00, -1.4405e+00, -1.6588e+00, -1.7328e+00,
        -1.4689e+00, -1.4805e+00, -1.6008e+00, -1.5993e+00, -1.7535e+00,
        -1.4060e+00, -1.5748e+00, -1.5202e+00, -1.8722e+00, -1.2684e+00,
        -1.3036e+00, -1.0648e+00, -1.6416e+00, -1.5169e+00, -1.4091e+00,
        -1.2763e+00, -1.5404e+00, -1.0945e+00, -1.3144e+00, -1.5519e+00,
        -1.2007e+00,  1.1801e+00,  1.2927e+00,  2.1267e+00,  2.3600e+00,
         5.1175e+00,  1.0316e-02,  2.9610e-01,  1.8094e-01, -6.1236e-02,
         1.7005e-01,  2.1906e-01,  2.7723e-01,  3.1815e-01,  3.1490e-01,
         2.7405e-01,  4.1735e-01,  3.4627e-01,  3.0001e-01,  4.3998e-01,
         1.3443e-01, -3.1533e-02,  2.0314e-01,  4.8560e-01,  2.9220e-01,
         1.6557e-01,  1.3599e-01,  2.4013e-01,  2.7676e-01,  3.7088e-01,
         9.8438e-02,  2.2097e-01,  2.9761e-01,  2.8261e-01,  2.0984e-01,
         2.8222e-01,  3.9526e-01,  2.3062e-01,  3.7139e-01, -1.1372e-01,
         8.1978e-02, -1.0834e-01,  4.2321e-02,  1.2963e-01,  1.8972e-01,
         2.5151e-01,  3.1132e-01,  1.9278e-01,  1.7989e+00,  2.7403e+00,
         4.0062e+00,  7.1677e-01,  9.8643e-01,  9.1415e-01,  9.8433e-01,
         9.8498e-01,  9.4400e-01,  8.8218e-01,  9.1833e-01,  8.3130e-01,
         8.2696e-01,  7.5747e-01,  7.8857e-01,  8.6874e-01,  5.8460e-01,
         6.0381e-01,  8.8803e-01,  5.0421e-01,  7.9688e-01,  2.9131e-01,
         8.2434e-01,  6.9946e-01,  8.8349e-01,  9.0314e-01,  8.6190e-01,
         1.0197e+00,  1.0082e+00,  6.0786e-01,  9.5023e-01,  1.0182e+00,
         1.0899e+00,  5.6902e-01,  8.2187e-01,  9.3751e-01,  1.0475e+00,
         5.4188e-01,  9.8079e-01,  7.0024e-01,  9.1581e-01,  9.1471e-01,
         4.7576e-01,  7.8253e-01,  8.7253e-01,  8.3363e-01,  9.2393e-01,
         7.5043e-01,  5.3519e-01,  9.5662e-01,  9.0914e-01,  5.0919e-01,
         9.1774e-01,  6.9160e-01,  6.7320e-01,  8.0314e-01,  9.3544e-01,
         9.5333e-01,  6.5588e-01,  7.0455e-01,  8.7073e-01,  8.5607e-01,
         1.0291e+00,  9.5449e-01,  9.4184e-01,  8.7874e-01,  1.0768e+00,
         8.7531e-01,  8.7827e-01,  3.8559e-01,  5.3451e-01,  8.2569e-01,
         9.5049e-01,  9.9198e-01,  7.9342e-01,  6.6073e-01,  6.0093e-01,
         7.6291e-01,  4.7662e-01,  3.8846e-01,  8.4251e-01,  5.0965e-01,
         6.5222e-01,  7.5236e-01,  6.8742e-01,  4.0023e-01,  7.2772e-01,
         3.2586e-01,  8.2597e-01,  6.7765e-01,  8.7859e-01,  7.4053e-01,
         7.1960e-01,  7.5383e-01,  3.2993e-01,  6.2521e-01,  6.9058e-01,
         5.8900e-01,  6.1542e-01,  7.0192e-01,  5.7248e-01,  5.6125e-01,
         6.1258e-01,  6.9048e-01,  7.2557e-01,  4.6570e-01,  5.9975e-01,
         7.2076e-01,  5.9302e-01,  6.8751e-01,  6.1038e-01,  8.1376e-01,
         5.4993e-01,  4.8324e-01,  6.3098e-01,  6.2086e-01,  7.1455e-01,
         6.3930e-01,  6.6233e-01,  2.0266e-01,  7.7936e-01,  3.0781e-01,
         7.2653e-01,  6.1618e-01,  6.7575e-01,  7.3716e-01,  8.1453e-01,
         7.2444e-01,  6.4421e-01,  7.7414e-01,  2.7918e-01,  6.4994e-01,
         4.0423e-01,  6.6310e-01,  5.7273e-01,  4.6308e-01,  1.6782e-01,
         5.0507e-01,  5.3781e-01,  5.8331e-01,  4.3481e-01,  1.9464e-01,
         1.0886e-01,  2.0572e-01,  1.1331e-03,  4.3065e-01,  4.0596e-01,
         4.3065e-01,  3.4382e-01,  2.6601e-01,  3.1945e-01,  3.1219e-01,
        -8.5673e-02,  3.5108e-01,  3.3186e-01,  5.1334e-01,  5.5472e-02,
         1.1830e-01,  1.6680e-01,  4.0008e-01,  3.8230e-01,  1.8146e-01,
        -2.7218e-02,  2.2935e-01,  4.4117e-01,  2.7331e-01,  1.3491e-01,
         1.8987e-01,  1.6271e-01,  1.8858e-01,  2.1334e-01, -1.2667e-02,
         3.7171e-01,  3.8276e-01,  2.9983e-01,  1.5437e-01,  3.1328e-01,
         2.8063e-01,  2.7624e-01,  1.8738e-01,  2.9502e-01,  2.4422e-01,
         1.7067e-01,  1.1956e-02,  5.7580e-03,  2.4290e-01,  1.7532e-01,
         1.9445e-01, -3.1675e-01,  2.5952e-01,  5.0298e-01,  3.1331e-01,
         3.9764e-01,  3.0480e-01,  2.8478e-01,  4.3020e-01,  2.7178e-01,
         3.3937e-01,  1.4165e-01,  2.8260e-01,  3.3869e-01,  3.4823e-01,
        -4.6144e-03,  8.6462e-02, -1.0053e-01, -1.1718e-01,  3.6243e-01,
        -1.5381e-01,  3.6489e-01, -1.2791e-01,  2.6852e-01,  2.8056e-01,
         2.4465e-01,  2.3538e-01,  8.8593e-02,  7.8704e-02,  2.4197e-01,
         1.8441e-01,  3.4024e-01, -7.8880e-02,  5.8227e-02,  1.7695e-01,
         1.3489e-01, -1.1599e-01,  2.7891e-01,  2.0683e-01,  2.9441e-01,
         4.7727e-02,  3.9303e-02,  1.3026e-01,  5.6121e-02, -3.7580e-01,
         6.6057e-02, -4.5271e-02,  7.6860e-02, -7.2630e-02,  8.3449e-02,
         1.6293e-02, -2.8443e-01, -4.5529e-01, -1.5654e-01, -4.7038e-01,
        -2.5467e-01, -3.5309e-01, -1.8875e-01,  5.6801e-02, -9.4687e-02,
         2.7692e-02, -3.6577e-01, -5.0765e-01, -1.6779e-01,  9.8087e-02,
        -3.3513e-01, -4.2749e-01, -4.2489e-01, -2.8457e-01, -2.1010e-01,
        -1.6383e-01, -3.8447e-01, -6.4508e-01, -1.8246e-01, -2.3357e-01,
        -2.7580e-01, -1.9314e-01, -2.4044e-01, -2.5235e-01, -3.8440e-01,
        -3.4662e-01, -6.1172e-01, -8.6336e-01, -5.7126e-01, -6.8318e-01,
        -5.1231e-01, -4.2281e-01, -3.6082e-01, -1.0896e+00, -8.7624e-01,
        -5.1766e-01, -6.7084e-01, -9.5698e-01, -6.8668e-01, -7.9978e-01,
        -7.0234e-01, -7.0191e-01, -9.6844e-01, -9.4376e-01, -9.2096e-01,
        -9.2649e-01, -7.3282e-01, -8.3511e-01, -7.8459e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808650
t6: 1641198808650
state_values: tensor([0.4853, 0.2124, 0.2676, 0.3679, 0.4579, 0.5220, 0.5410, 0.5473, 0.5527,
        0.5618, 0.5983, 0.5592, 0.5472, 0.5404, 0.5578, 0.6804, 0.6060, 0.5675,
        0.5881, 0.5875, 0.5961, 0.5824, 0.4389, 0.3887, 0.4855, 0.4922, 0.4902,
        0.5406, 0.4996, 0.4919, 0.6545, 0.5876, 0.5829, 0.5334, 0.5099, 0.5026,
        0.5003, 0.5036, 0.5208, 0.5023, 0.5162, 0.5065, 0.5041, 0.5780, 0.5237,
        0.5105, 0.5024, 0.5452, 0.5092, 0.5054, 0.5048, 0.5358, 0.5075, 0.6075,
        0.5416, 0.5220, 0.5386, 0.6451, 0.5582, 0.6061, 0.5434, 0.5349, 0.5203,
        0.5401, 0.5312, 0.5301, 0.3891, 0.3581, 0.3281, 0.3339, 0.2897, 0.4165,
        0.4656, 0.4693, 0.4793, 0.5329, 0.5158, 0.5162, 0.5088, 0.5174, 0.5033,
        0.4991, 0.5095, 0.5035, 0.5207, 0.5983, 0.5378, 0.5221, 0.5404, 0.5588,
        0.5311, 0.5723, 0.5440, 0.5303, 0.5215, 0.5266, 0.5727, 0.5523, 0.5425,
        0.5703, 0.5567, 0.5342, 0.5330, 0.5323, 0.5375, 0.5365, 0.5412, 0.5433,
        0.5414, 0.5400, 0.5630, 0.5778, 0.5769, 0.4639, 0.4007, 0.3617, 0.4732,
        0.5318, 0.5201, 0.5332, 0.5286, 0.5244, 0.5293, 0.5274, 0.5748, 0.5464,
        0.6010, 0.5604, 0.5468, 0.5499, 0.6622, 0.5998, 0.5705, 0.5836, 0.7449,
        0.6339, 0.6731, 0.6138, 0.6115, 0.6255, 0.5906, 0.5802, 0.5815, 0.5862,
        0.5923, 0.5904, 0.5845, 0.5801, 0.6236, 0.5875, 0.7218, 0.6274, 0.6057,
        0.5911, 0.5867, 0.7318, 0.6675, 0.6612, 0.6164, 0.6029, 0.6031, 0.6101,
        0.5957, 0.6069, 0.6012, 0.6069, 0.6054, 0.6513, 0.6108, 0.5974, 0.6005,
        0.6044, 0.6040, 0.6211, 0.6036, 0.5934, 0.5935, 0.6113, 0.6080, 0.6000,
        0.5930, 0.6291, 0.6140, 0.6114, 0.6018, 0.6040, 0.5931, 0.6373, 0.6949,
        0.6986, 0.6386, 0.6294, 0.6222, 0.6599, 0.6358, 0.7056, 0.6822, 0.6950,
        0.6588, 0.6762, 0.6586, 0.6392, 0.6389, 0.6426, 0.6674, 0.6873, 0.6738,
        0.6608, 0.6450, 0.6727, 0.7252, 0.6644, 0.6570, 0.6929, 0.7117, 0.6664,
        0.6537, 0.6462, 0.6513, 0.6930, 0.6555, 0.7070, 0.6907, 0.6725, 0.6509,
        0.6508, 0.6525, 0.7112, 0.6648, 0.6732, 0.6766, 0.6856, 0.6703, 0.6761,
        0.6653, 0.6673, 0.6565, 0.6498, 0.6605, 0.6433, 0.6434, 0.6545, 0.6553,
        0.6604, 0.6479, 0.6558, 0.6471, 0.6799, 0.6646, 0.6677, 0.6880, 0.6965,
        0.6856, 0.6973, 0.7783, 0.7068, 0.8053, 0.7235, 0.7134, 0.7193, 0.7211,
        0.7378, 0.7766, 0.7378, 0.7420, 0.7214, 0.7416, 0.7075, 0.7252, 0.7074,
        0.8037, 0.7231, 0.7154, 0.7190, 0.8025, 0.7355, 0.7152, 0.7185, 0.7032,
        0.8208, 0.7724, 0.7954, 0.7424, 0.7194, 0.7132, 0.7049, 0.7021, 0.7030,
        0.7047, 0.7281, 0.7085, 0.7280, 0.7100, 0.7407, 0.7401, 0.7145, 0.7132,
        0.7091, 0.7003, 0.7060, 0.7046, 0.7663, 0.7699, 0.7271, 0.7180, 0.7116,
        0.7135, 0.7701, 0.7364, 0.7893, 0.7483, 0.8173, 0.7627, 0.7405, 0.7495,
        0.8722, 0.7676, 0.7470, 0.7431, 0.7673, 0.7521, 0.7675, 0.7498, 0.7367,
        0.7301, 0.7293, 0.7255, 0.8147, 0.7573, 0.8076, 0.7532, 0.7317, 0.7441,
        0.7385, 0.7744, 0.7450, 0.7411, 0.7292, 0.7382, 0.7363, 0.7882, 0.7941,
        0.7588, 0.7590, 0.7543, 0.7407, 0.8210, 0.7619, 0.7502, 0.7537, 0.7502,
        0.8374, 0.7784, 0.8382, 0.9407, 0.8113, 0.7863, 0.7736, 0.7600, 0.7665,
        0.7510, 0.8810, 0.7987, 0.7666, 0.7467, 0.7596, 0.8914, 0.8924, 0.8207,
        0.8264, 0.7904, 0.7763, 0.7780, 0.7700, 0.7642, 0.7626, 0.7905, 0.7817,
        0.7805, 0.8041, 0.7750, 0.8946, 0.8141, 0.7862, 0.7818, 0.7772, 0.7905,
        0.7959, 0.9716, 0.8394, 0.8268, 0.8313, 0.8097, 0.8732, 0.9077, 0.8559,
        0.8190, 0.8155, 0.9029, 0.8931, 0.8749, 0.8170, 0.8434, 0.8308],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808655
t8: 1641198808655
t9: 1641198808655
t10: 1641198808666
t11: 1641198808668
t12: 1641198808668
t1: 1641198808668
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808678
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0074, 1.0138, 0.9858, 0.9819, 0.9647, 1.0011, 0.9891, 0.9528, 1.0058,
        1.0305, 0.9944, 1.0021, 1.0106, 0.9998, 0.9831, 0.8927, 0.9743, 0.9577,
        0.9798, 0.9769, 0.9858, 1.0054, 1.0034, 0.9692, 1.0174, 0.9994, 0.9753,
        1.0027, 0.9977, 0.9032, 0.8717, 1.0477, 1.0066, 1.0036, 1.0096, 0.9980,
        1.0217, 0.9928, 1.0082, 1.0012, 0.9970, 0.9995, 0.9824, 1.0202, 0.9968,
        1.0125, 0.9793, 1.0177, 1.0195, 0.9962, 0.9913, 1.0085, 0.9958, 0.9967,
        1.0084, 0.9916, 0.9586, 1.1441, 0.9910, 1.0022, 1.0060, 0.9987, 0.9908,
        1.0239, 0.9960, 1.0081, 1.0137, 1.0035, 0.9976, 0.9855, 1.1434, 0.9926,
        1.0207, 1.0235, 0.9757, 0.9975, 0.9973, 0.9938, 0.9893, 1.0125, 0.9996,
        0.9970, 1.0054, 0.9923, 0.9674, 1.0338, 1.0111, 0.9600, 0.9748, 1.0135,
        0.9628, 1.0012, 1.0040, 1.0111, 1.0218, 0.9819, 0.9967, 0.9991, 0.9754,
        0.9920, 1.0050, 1.0177, 1.0003, 1.0307, 1.0098, 0.9977, 0.9951, 0.9897,
        0.9907, 1.0113, 1.0102, 0.9984, 1.0083, 1.0268, 1.0204, 1.0708, 0.9956,
        1.0009, 1.0019, 0.9997, 1.0027, 1.0014, 1.0015, 0.9914, 1.0035, 0.9838,
        1.0098, 1.0177, 1.0412, 0.9524, 0.9893, 1.0041, 0.9737, 0.9166, 0.9638,
        1.0371, 0.9995, 1.0028, 0.9876, 1.0003, 1.0073, 1.0481, 0.9961, 0.9964,
        0.9978, 1.0210, 1.0080, 0.9980, 1.0004, 0.9955, 0.9964, 0.9982, 1.0230,
        1.0050, 0.9659, 0.9565, 1.0125, 1.0141, 1.0047, 1.0269, 1.0237, 1.0008,
        0.9995, 0.9986, 0.9994, 1.0001, 1.0070, 1.0044, 1.0001, 0.9995, 1.0054,
        1.0013, 0.9998, 1.0006, 1.0000, 1.0002, 0.9974, 0.9987, 0.9969, 1.0088,
        0.9882, 1.0509, 1.0185, 1.0045, 1.0002, 0.9993, 1.0037, 0.9861, 0.9735,
        1.0082, 1.0626, 1.1313, 0.9660, 1.0219, 0.9926, 0.9868, 0.9874, 1.0392,
        0.9689, 1.0630, 1.0002, 1.0102, 0.9972, 0.9908, 0.9838, 0.9901, 1.0366,
        1.0175, 0.9832, 0.9719, 1.0165, 0.9969, 0.9685, 0.9693, 1.0128, 1.0047,
        1.0135, 1.0194, 0.9882, 1.0019, 0.9902, 0.9865, 0.9986, 1.0007, 1.0288,
        1.0178, 0.9812, 1.0056, 0.9965, 0.9880, 0.9879, 1.0643, 0.9848, 1.0373,
        0.9972, 1.0060, 1.0002, 0.9996, 1.0003, 1.0001, 0.9998, 0.9988, 1.0107,
        1.0033, 1.0040, 0.9994, 1.0035, 1.0006, 0.9979, 0.9951, 0.9956, 0.9970,
        0.9915, 0.9596, 1.0228, 0.9391, 1.0687, 0.9985, 0.9835, 0.9817, 0.9700,
        0.9615, 0.9941, 0.9876, 1.0732, 0.9633, 1.0224, 0.9896, 1.0497, 0.9690,
        1.0192, 0.9981, 0.9884, 0.9606, 1.0201, 1.0077, 0.9871, 1.0188, 0.9777,
        0.9740, 1.0071, 1.0042, 1.0124, 1.0836, 1.0069, 1.0010, 0.9996, 0.9973,
        1.0031, 1.0012, 1.0018, 1.0067, 0.9950, 0.9907, 1.0091, 1.0191, 1.0187,
        1.0026, 1.0000, 0.9998, 0.9974, 1.0009, 1.0001, 1.0016, 1.0057, 1.0057,
        0.9959, 0.9994, 0.9887, 0.9973, 0.9570, 0.9984, 0.9987, 0.9768, 0.9238,
        1.0292, 0.9993, 1.1660, 0.9733, 1.0495, 0.9967, 1.0340, 1.0008, 1.0027,
        0.9989, 0.9971, 1.0105, 1.0009, 1.0010, 1.0079, 1.0081, 1.0402, 1.0188,
        0.9912, 1.0070, 1.0020, 1.0026, 1.0000, 1.0007, 0.9988, 0.9888, 0.9983,
        0.9933, 1.0927, 1.0123, 0.9770, 1.0015, 1.0060, 0.9917, 1.0000, 0.9767,
        1.0419, 0.9657, 0.9734, 1.0287, 1.0105, 1.0132, 0.9986, 0.9996, 1.0015,
        0.9765, 1.0456, 1.0060, 1.0037, 1.0205, 0.9779, 0.9475, 0.9983, 0.9704,
        0.9985, 1.0165, 1.0806, 0.9988, 1.0038, 0.9995, 1.0016, 0.9980, 0.9980,
        0.9870, 1.0036, 0.9627, 1.0927, 1.0087, 1.0452, 1.0114, 0.9975, 0.9976,
        0.9773, 1.1448, 1.0031, 0.9810, 1.0462, 0.9480, 0.9851, 0.9904, 0.9975,
        1.0307, 0.9385, 0.9917, 0.9995, 1.0057, 0.9803, 0.9922, 0.9446],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808682
t4: 1641198808683
surr1, surr2: tensor([-3.1850e+00, -9.9442e-01, -1.4568e+00, -2.4304e+00, -2.6492e+00,
        -2.9475e+00, -2.7527e+00, -2.6356e+00, -2.8411e+00, -2.7449e+00,
        -2.6047e+00, -2.6627e+00, -2.5156e+00, -2.5850e+00, -2.4109e+00,
        -2.3252e+00, -2.3345e+00, -2.4795e+00, -2.3626e+00, -2.2601e+00,
        -2.1858e+00, -2.1971e+00, -8.4734e-01, -1.7115e-01, -1.6637e+00,
        -1.8032e+00, -1.6206e+00, -1.6944e+00, -1.5962e+00, -1.3987e+00,
        -1.7935e+00, -1.8468e+00, -1.7471e+00, -1.9097e+00, -1.6357e+00,
        -1.5916e+00, -1.5203e+00, -1.7966e+00, -1.5480e+00, -1.5893e+00,
        -1.5166e+00, -1.4403e+00, -1.4773e+00, -1.6511e+00, -1.7392e+00,
        -1.4619e+00, -1.5031e+00, -1.5819e+00, -1.5655e+00, -1.7568e+00,
        -1.4226e+00, -1.5565e+00, -1.5337e+00, -1.8770e+00, -1.2564e+00,
        -1.3164e+00, -1.1077e+00, -1.7074e+00, -1.5545e+00, -1.4045e+00,
        -1.2520e+00, -1.5413e+00, -1.1007e+00, -1.2859e+00, -1.5576e+00,
        -1.1885e+00,  1.1609e+00,  1.2821e+00,  2.1385e+00,  2.4090e+00,
         5.3193e+00,  1.0397e-02,  2.9367e-01,  1.7774e-01, -6.2300e-02,
         1.7075e-01,  2.2087e-01,  2.7817e-01,  3.2029e-01,  3.1291e-01,
         2.7419e-01,  4.1842e-01,  3.4440e-01,  3.0298e-01,  4.5380e-01,
         1.3483e-01, -3.1328e-02,  2.0590e-01,  4.9343e-01,  2.8888e-01,
         1.6817e-01,  1.3596e-01,  2.3959e-01,  2.7564e-01,  3.6640e-01,
         9.9999e-02,  2.2187e-01,  2.9806e-01,  2.8633e-01,  2.1088e-01,
         2.8150e-01,  3.9217e-01,  2.3063e-01,  3.6144e-01, -1.1250e-01,
         8.0428e-02, -1.0732e-01,  4.1965e-02,  1.2875e-01,  1.9135e-01,
         2.5577e-01,  3.1418e-01,  1.9084e-01,  1.7787e+00,  2.7153e+00,
         3.8999e+00,  7.2493e-01,  9.7889e-01,  9.1954e-01,  9.8492e-01,
         9.8140e-01,  9.4274e-01,  8.8065e-01,  9.3298e-01,  8.2766e-01,
         8.4409e-01,  7.5180e-01,  7.8089e-01,  8.5741e-01,  5.9895e-01,
         6.0195e-01,  8.5653e-01,  5.0761e-01,  8.1160e-01,  2.8924e-01,
         8.4743e-01,  6.9992e-01,  8.9073e-01,  9.1300e-01,  8.6180e-01,
         1.0152e+00,  9.8866e-01,  6.0909e-01,  9.5558e-01,  1.0226e+00,
         1.0547e+00,  5.6470e-01,  8.3375e-01,  9.3706e-01,  1.0853e+00,
         5.4126e-01,  9.6462e-01,  6.9672e-01,  9.1412e-01,  9.4386e-01,
         4.7097e-01,  7.9837e-01,  8.6454e-01,  8.3169e-01,  9.1632e-01,
         7.4039e-01,  5.3491e-01,  9.5876e-01,  8.8233e-01,  5.1103e-01,
         9.0963e-01,  7.0042e-01,  6.6625e-01,  8.0070e-01,  9.3632e-01,
         9.4196e-01,  6.5135e-01,  7.0932e-01,  8.6779e-01,  8.5623e-01,
         1.0287e+00,  9.6135e-01,  9.4351e-01,  8.8263e-01,  1.0696e+00,
         8.8412e-01,  8.4737e-01,  3.8089e-01,  5.3201e-01,  8.2693e-01,
         9.4871e-01,  1.0054e+00,  8.1592e-01,  6.6511e-01,  5.9762e-01,
         7.5041e-01,  4.9019e-01,  3.9307e-01,  8.2452e-01,  5.2181e-01,
         6.5505e-01,  7.6659e-01,  6.7414e-01,  4.0397e-01,  7.1228e-01,
         3.2590e-01,  8.2070e-01,  6.8086e-01,  8.8832e-01,  7.5462e-01,
         7.2562e-01,  7.3925e-01,  3.2811e-01,  6.3141e-01,  7.0781e-01,
         5.8435e-01,  6.1690e-01,  7.1104e-01,  5.8285e-01,  5.5700e-01,
         6.1150e-01,  6.8700e-01,  7.1686e-01,  4.7137e-01,  5.9851e-01,
         7.3484e-01,  5.9969e-01,  6.8838e-01,  6.1042e-01,  8.0507e-01,
         5.4477e-01,  4.9161e-01,  6.2565e-01,  6.2611e-01,  7.1871e-01,
         6.4412e-01,  6.4124e-01,  2.0407e-01,  7.6039e-01,  3.0900e-01,
         7.2031e-01,  6.1227e-01,  6.7815e-01,  7.3547e-01,  8.1319e-01,
         7.2504e-01,  6.4661e-01,  7.5972e-01,  2.7797e-01,  6.4260e-01,
         4.0317e-01,  6.6893e-01,  5.6877e-01,  4.5437e-01,  1.6914e-01,
         5.1057e-01,  5.4157e-01,  5.8814e-01,  4.4613e-01,  1.9264e-01,
         1.1214e-01,  2.1169e-01,  1.1404e-03,  4.3358e-01,  4.0964e-01,
         4.3670e-01,  3.5225e-01,  2.6681e-01,  3.2254e-01,  3.0459e-01,
        -8.6539e-02,  3.4740e-01,  3.3554e-01,  5.0010e-01,  5.6704e-02,
         1.1786e-01,  1.6738e-01,  4.0227e-01,  3.9236e-01,  1.8065e-01,
        -2.7066e-02,  2.3047e-01,  4.3603e-01,  2.8107e-01,  1.3382e-01,
         1.9475e-01,  1.6229e-01,  1.8665e-01,  2.1016e-01, -1.2639e-02,
         3.7114e-01,  3.8105e-01,  2.9627e-01,  1.5565e-01,  3.1187e-01,
         2.8269e-01,  2.7360e-01,  1.8995e-01,  2.9803e-01,  2.4207e-01,
         1.6884e-01,  1.1820e-02,  5.7380e-03,  2.4275e-01,  1.7521e-01,
         1.9244e-01, -3.2089e-01,  2.5953e-01,  4.9784e-01,  3.1173e-01,
         3.9492e-01,  3.0993e-01,  2.8570e-01,  4.4032e-01,  2.7235e-01,
         3.4754e-01,  1.4184e-01,  2.8297e-01,  3.4136e-01,  3.5743e-01,
        -4.6707e-03,  8.3772e-02, -1.0656e-01, -1.1832e-01,  3.4579e-01,
        -1.5565e-01,  3.5201e-01, -1.2767e-01,  2.6686e-01,  2.7866e-01,
         2.4241e-01,  2.4028e-01,  8.7934e-02,  8.0697e-02,  2.4104e-01,
         1.8339e-01,  3.3580e-01, -7.8180e-02,  5.8887e-02,  1.7499e-01,
         1.3216e-01, -1.1572e-01,  2.7913e-01,  2.0745e-01,  2.9986e-01,
         4.8563e-02,  3.9383e-02,  1.3069e-01,  5.5746e-02, -3.7435e-01,
         6.7336e-02, -4.5210e-02,  7.6042e-02, -7.2901e-02,  8.3474e-02,
         1.6643e-02, -2.7485e-01, -4.6483e-01, -1.5944e-01, -4.7325e-01,
        -2.5177e-01, -3.5213e-01, -1.8892e-01,  5.6832e-02, -9.4530e-02,
         2.8479e-02, -3.7047e-01, -5.0305e-01, -1.6763e-01,  9.6829e-02,
        -3.4432e-01, -4.2084e-01, -4.2557e-01, -2.8847e-01, -2.1038e-01,
        -1.6211e-01, -3.7768e-01, -6.4550e-01, -1.8148e-01, -2.3197e-01,
        -2.7901e-01, -1.9401e-01, -2.4167e-01, -2.5507e-01, -3.8362e-01,
        -3.5637e-01, -6.2917e-01, -8.5475e-01, -5.6394e-01, -6.7912e-01,
        -5.1475e-01, -4.2642e-01, -3.7610e-01, -1.1340e+00, -8.8595e-01,
        -5.2232e-01, -6.5300e-01, -9.7430e-01, -6.9536e-01, -8.0455e-01,
        -7.0383e-01, -6.9218e-01, -9.8652e-01, -9.5019e-01, -9.3352e-01,
        -9.2357e-01, -7.3906e-01, -8.3838e-01, -8.1026e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1850e+00, -9.9442e-01, -1.4568e+00, -2.4304e+00, -2.6492e+00,
        -2.9475e+00, -2.7527e+00, -2.6356e+00, -2.8411e+00, -2.7449e+00,
        -2.6047e+00, -2.6627e+00, -2.5156e+00, -2.5850e+00, -2.4109e+00,
        -2.3443e+00, -2.3345e+00, -2.4795e+00, -2.3626e+00, -2.2601e+00,
        -2.1858e+00, -2.1971e+00, -8.4734e-01, -1.7115e-01, -1.6637e+00,
        -1.8032e+00, -1.6206e+00, -1.6944e+00, -1.5962e+00, -1.3987e+00,
        -1.8517e+00, -1.8468e+00, -1.7471e+00, -1.9097e+00, -1.6357e+00,
        -1.5916e+00, -1.5203e+00, -1.7966e+00, -1.5480e+00, -1.5893e+00,
        -1.5166e+00, -1.4403e+00, -1.4773e+00, -1.6511e+00, -1.7392e+00,
        -1.4619e+00, -1.5031e+00, -1.5819e+00, -1.5655e+00, -1.7568e+00,
        -1.4226e+00, -1.5565e+00, -1.5337e+00, -1.8770e+00, -1.2564e+00,
        -1.3164e+00, -1.1077e+00, -1.6416e+00, -1.5545e+00, -1.4045e+00,
        -1.2520e+00, -1.5413e+00, -1.1007e+00, -1.2859e+00, -1.5576e+00,
        -1.1885e+00,  1.1609e+00,  1.2821e+00,  2.1385e+00,  2.4090e+00,
         5.1175e+00,  1.0397e-02,  2.9367e-01,  1.7774e-01, -6.2300e-02,
         1.7075e-01,  2.2087e-01,  2.7817e-01,  3.2029e-01,  3.1291e-01,
         2.7419e-01,  4.1842e-01,  3.4440e-01,  3.0298e-01,  4.5380e-01,
         1.3483e-01, -3.1328e-02,  2.0590e-01,  4.9343e-01,  2.8888e-01,
         1.6817e-01,  1.3596e-01,  2.3959e-01,  2.7564e-01,  3.6640e-01,
         9.9999e-02,  2.2187e-01,  2.9806e-01,  2.8633e-01,  2.1088e-01,
         2.8150e-01,  3.9217e-01,  2.3063e-01,  3.6144e-01, -1.1250e-01,
         8.0428e-02, -1.0732e-01,  4.1965e-02,  1.2875e-01,  1.9135e-01,
         2.5577e-01,  3.1418e-01,  1.9084e-01,  1.7787e+00,  2.7153e+00,
         3.8999e+00,  7.2493e-01,  9.7889e-01,  9.1954e-01,  9.8492e-01,
         9.8140e-01,  9.4274e-01,  8.8065e-01,  9.3298e-01,  8.2766e-01,
         8.4409e-01,  7.5180e-01,  7.8089e-01,  8.5741e-01,  5.9895e-01,
         6.0195e-01,  8.5653e-01,  5.0761e-01,  8.1160e-01,  2.8924e-01,
         8.4743e-01,  6.9992e-01,  8.9073e-01,  9.1300e-01,  8.6180e-01,
         1.0152e+00,  9.8866e-01,  6.0909e-01,  9.5558e-01,  1.0226e+00,
         1.0547e+00,  5.6470e-01,  8.3375e-01,  9.3706e-01,  1.0853e+00,
         5.4126e-01,  9.6462e-01,  6.9672e-01,  9.1412e-01,  9.4386e-01,
         4.7097e-01,  7.9837e-01,  8.6454e-01,  8.3169e-01,  9.1632e-01,
         7.4039e-01,  5.3491e-01,  9.5876e-01,  8.8233e-01,  5.1103e-01,
         9.0963e-01,  7.0042e-01,  6.6625e-01,  8.0070e-01,  9.3632e-01,
         9.4196e-01,  6.5135e-01,  7.0932e-01,  8.6779e-01,  8.5623e-01,
         1.0287e+00,  9.6135e-01,  9.4351e-01,  8.8263e-01,  1.0696e+00,
         8.8412e-01,  8.4737e-01,  3.8089e-01,  5.3201e-01,  8.2693e-01,
         9.4871e-01,  1.0054e+00,  8.1592e-01,  6.6511e-01,  5.9762e-01,
         7.5041e-01,  4.7662e-01,  3.9307e-01,  8.2452e-01,  5.2181e-01,
         6.5505e-01,  7.6659e-01,  6.7414e-01,  4.0397e-01,  7.1228e-01,
         3.2590e-01,  8.2070e-01,  6.8086e-01,  8.8832e-01,  7.5462e-01,
         7.2562e-01,  7.3925e-01,  3.2811e-01,  6.3141e-01,  7.0781e-01,
         5.8435e-01,  6.1690e-01,  7.1104e-01,  5.8285e-01,  5.5700e-01,
         6.1150e-01,  6.8700e-01,  7.1686e-01,  4.7137e-01,  5.9851e-01,
         7.3484e-01,  5.9969e-01,  6.8838e-01,  6.1042e-01,  8.0507e-01,
         5.4477e-01,  4.9161e-01,  6.2565e-01,  6.2611e-01,  7.1871e-01,
         6.4412e-01,  6.4124e-01,  2.0407e-01,  7.6039e-01,  3.0900e-01,
         7.2031e-01,  6.1227e-01,  6.7815e-01,  7.3547e-01,  8.1319e-01,
         7.2504e-01,  6.4661e-01,  7.5972e-01,  2.7797e-01,  6.4260e-01,
         4.0317e-01,  6.6893e-01,  5.6877e-01,  4.5437e-01,  1.6914e-01,
         5.1057e-01,  5.4157e-01,  5.8814e-01,  4.4613e-01,  1.9264e-01,
         1.1214e-01,  2.1169e-01,  1.1404e-03,  4.3358e-01,  4.0964e-01,
         4.3670e-01,  3.5225e-01,  2.6681e-01,  3.2254e-01,  3.0459e-01,
        -8.6539e-02,  3.4740e-01,  3.3554e-01,  5.0010e-01,  5.6704e-02,
         1.1786e-01,  1.6738e-01,  4.0227e-01,  3.9236e-01,  1.8065e-01,
        -2.7066e-02,  2.3047e-01,  4.3603e-01,  2.8107e-01,  1.3382e-01,
         1.9475e-01,  1.6229e-01,  1.8665e-01,  2.1016e-01, -1.2639e-02,
         3.7114e-01,  3.8105e-01,  2.9627e-01,  1.5565e-01,  3.1187e-01,
         2.8269e-01,  2.7360e-01,  1.8995e-01,  2.9803e-01,  2.4207e-01,
         1.6884e-01,  1.1820e-02,  5.7380e-03,  2.4275e-01,  1.7521e-01,
         1.9244e-01, -3.2089e-01,  2.5953e-01,  4.9784e-01,  3.1173e-01,
         3.9492e-01,  3.0993e-01,  2.8570e-01,  4.4032e-01,  2.7235e-01,
         3.4754e-01,  1.4184e-01,  2.8297e-01,  3.4136e-01,  3.5743e-01,
        -4.6707e-03,  8.3772e-02, -1.0053e-01, -1.1832e-01,  3.4579e-01,
        -1.5565e-01,  3.5201e-01, -1.2767e-01,  2.6686e-01,  2.7866e-01,
         2.4241e-01,  2.4028e-01,  8.7934e-02,  8.0697e-02,  2.4104e-01,
         1.8339e-01,  3.3580e-01, -7.8180e-02,  5.8887e-02,  1.7499e-01,
         1.3216e-01, -1.1572e-01,  2.7913e-01,  2.0745e-01,  2.9986e-01,
         4.8563e-02,  3.9383e-02,  1.3069e-01,  5.5746e-02, -3.7435e-01,
         6.7336e-02, -4.5210e-02,  7.6042e-02, -7.2901e-02,  8.3474e-02,
         1.6643e-02, -2.7485e-01, -4.6483e-01, -1.5944e-01, -4.7325e-01,
        -2.5177e-01, -3.5213e-01, -1.8892e-01,  5.6832e-02, -9.4530e-02,
         2.8479e-02, -3.7047e-01, -5.0305e-01, -1.6763e-01,  9.6829e-02,
        -3.4432e-01, -4.2084e-01, -4.2557e-01, -2.8847e-01, -2.1038e-01,
        -1.6211e-01, -3.7768e-01, -6.4550e-01, -1.8148e-01, -2.3197e-01,
        -2.7901e-01, -1.9401e-01, -2.4167e-01, -2.5507e-01, -3.8362e-01,
        -3.5637e-01, -6.2917e-01, -8.5475e-01, -5.6394e-01, -6.7912e-01,
        -5.1475e-01, -4.2642e-01, -3.7610e-01, -1.0896e+00, -8.8595e-01,
        -5.2232e-01, -6.5300e-01, -9.7430e-01, -6.9536e-01, -8.0455e-01,
        -7.0383e-01, -6.9218e-01, -9.8652e-01, -9.5019e-01, -9.3352e-01,
        -9.2357e-01, -7.3906e-01, -8.3838e-01, -8.1026e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808692
t6: 1641198808692
state_values: tensor([0.5981, 0.3268, 0.3868, 0.4979, 0.5998, 0.6749, 0.6994, 0.7084, 0.7159,
        0.7287, 0.7742, 0.7283, 0.7161, 0.7100, 0.7304, 0.8738, 0.7890, 0.7432,
        0.7690, 0.7693, 0.7802, 0.7648, 0.6185, 0.5605, 0.6675, 0.6743, 0.6731,
        0.7241, 0.6836, 0.6765, 0.8543, 0.7750, 0.7708, 0.7183, 0.6958, 0.6890,
        0.6873, 0.6900, 0.7086, 0.6903, 0.7049, 0.6956, 0.6934, 0.7703, 0.7129,
        0.7005, 0.6919, 0.7387, 0.6995, 0.6946, 0.6948, 0.7304, 0.6984, 0.8057,
        0.7372, 0.7159, 0.7355, 0.8523, 0.7560, 0.8076, 0.7416, 0.7323, 0.7161,
        0.7397, 0.7287, 0.7285, 0.5750, 0.5394, 0.5026, 0.5106, 0.4560, 0.6032,
        0.6555, 0.6588, 0.6694, 0.7324, 0.7132, 0.7141, 0.7056, 0.7160, 0.6994,
        0.6946, 0.7071, 0.7001, 0.7210, 0.8079, 0.7401, 0.7225, 0.7451, 0.7671,
        0.7341, 0.7833, 0.7500, 0.7339, 0.7237, 0.7296, 0.7849, 0.7609, 0.7493,
        0.7826, 0.7667, 0.7401, 0.7387, 0.7381, 0.7435, 0.7430, 0.7482, 0.7514,
        0.7494, 0.7480, 0.7756, 0.7935, 0.7924, 0.6607, 0.5903, 0.5472, 0.6698,
        0.7400, 0.7259, 0.7416, 0.7361, 0.7311, 0.7370, 0.7349, 0.7913, 0.7576,
        0.8183, 0.7740, 0.7581, 0.7617, 0.8791, 0.8176, 0.7851, 0.8005, 0.9742,
        0.8512, 0.8913, 0.8313, 0.8295, 0.8439, 0.8083, 0.7969, 0.7974, 0.8034,
        0.8105, 0.8086, 0.8008, 0.7967, 0.8428, 0.8055, 0.9487, 0.8459, 0.8241,
        0.8088, 0.8043, 0.9601, 0.8866, 0.8806, 0.8354, 0.8220, 0.8219, 0.8285,
        0.8143, 0.8264, 0.8192, 0.8265, 0.8248, 0.8712, 0.8305, 0.8164, 0.8200,
        0.8238, 0.8235, 0.8415, 0.8234, 0.8126, 0.8127, 0.8323, 0.8286, 0.8201,
        0.8122, 0.8504, 0.8339, 0.8313, 0.8214, 0.8242, 0.8124, 0.8589, 0.9185,
        0.9240, 0.8595, 0.8495, 0.8417, 0.8810, 0.8563, 0.9306, 0.9047, 0.9203,
        0.8794, 0.8978, 0.8787, 0.8603, 0.8600, 0.8642, 0.8892, 0.9116, 0.8957,
        0.8816, 0.8663, 0.8946, 0.9547, 0.8855, 0.8786, 0.9180, 0.9406, 0.8883,
        0.8757, 0.8681, 0.8729, 0.9181, 0.8779, 0.9352, 0.9162, 0.8951, 0.8735,
        0.8731, 0.8747, 0.9386, 0.8869, 0.8959, 0.8995, 0.9100, 0.8919, 0.8989,
        0.8871, 0.8900, 0.8791, 0.8725, 0.8836, 0.8665, 0.8667, 0.8777, 0.8788,
        0.8829, 0.8710, 0.8787, 0.8703, 0.9035, 0.8879, 0.8902, 0.9126, 0.9232,
        0.9104, 0.9242, 1.0172, 0.9335, 1.0437, 0.9526, 0.9420, 0.9494, 0.9519,
        0.9719, 1.0162, 0.9715, 0.9768, 0.9509, 0.9760, 0.9353, 0.9570, 0.9345,
        1.0427, 0.9529, 0.9446, 0.9493, 1.0419, 0.9673, 0.9441, 0.9485, 0.9301,
        1.0600, 1.0123, 1.0354, 0.9767, 0.9492, 0.9408, 0.9318, 0.9289, 0.9300,
        0.9319, 0.9603, 0.9370, 0.9605, 0.9387, 0.9759, 0.9752, 0.9444, 0.9425,
        0.9374, 0.9274, 0.9342, 0.9327, 1.0046, 1.0102, 0.9596, 0.9485, 0.9409,
        0.9431, 1.0107, 0.9710, 1.0308, 0.9848, 1.0580, 1.0015, 0.9753, 0.9863,
        1.1177, 1.0066, 0.9815, 0.9761, 1.0066, 0.9871, 1.0070, 0.9848, 0.9701,
        0.9624, 0.9615, 0.9571, 1.0555, 0.9945, 1.0489, 0.9899, 0.9646, 0.9788,
        0.9722, 1.0157, 0.9806, 0.9751, 0.9616, 0.9726, 0.9707, 1.0321, 1.0380,
        0.9978, 0.9979, 0.9905, 0.9754, 1.0629, 1.0007, 0.9868, 0.9914, 0.9871,
        1.0809, 1.0189, 1.0815, 1.1917, 1.0531, 1.0280, 1.0141, 0.9982, 1.0061,
        0.9879, 1.1286, 1.0404, 1.0059, 0.9828, 0.9976, 1.1400, 1.1411, 1.0633,
        1.0704, 1.0338, 1.0177, 1.0186, 1.0101, 1.0035, 1.0016, 1.0347, 1.0250,
        1.0237, 1.0487, 1.0173, 1.1449, 1.0562, 1.0289, 1.0239, 1.0188, 1.0344,
        1.0405, 1.2249, 1.0837, 1.0713, 1.0765, 1.0524, 1.1209, 1.1585, 1.1029,
        1.0627, 1.0584, 1.1534, 1.1442, 1.1245, 1.0609, 1.0898, 1.0758],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808696
t8: 1641198808696
t9: 1641198808696
t10: 1641198808707
t11: 1641198808708
t12: 1641198808708
t1: 1641198808708
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808719
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0076, 1.0162, 0.9801, 0.9790, 0.9557, 1.0012, 0.9864, 0.9446, 1.0066,
        1.0386, 0.9844, 1.0030, 1.0041, 1.0017, 1.0008, 0.8849, 0.9603, 0.9615,
        0.9874, 0.9903, 0.9936, 1.0011, 1.0031, 0.9736, 1.0119, 0.9996, 0.9824,
        1.0016, 0.9990, 0.9200, 0.8623, 1.0606, 0.9895, 1.0026, 1.0077, 0.9984,
        1.0126, 0.9948, 1.0041, 1.0034, 0.9986, 0.9999, 0.9924, 1.0174, 0.9981,
        1.0105, 0.9840, 1.0110, 1.0118, 0.9965, 0.9958, 1.0028, 0.9988, 0.9977,
        1.0043, 0.9942, 0.9758, 1.1532, 1.0045, 1.0000, 0.9941, 0.9988, 0.9928,
        1.0142, 0.9970, 1.0037, 1.0095, 1.0021, 0.9987, 0.9939, 1.1561, 0.9958,
        1.0167, 1.0171, 0.9799, 0.9993, 1.0013, 0.9952, 0.9921, 1.0097, 0.9997,
        0.9979, 1.0031, 0.9954, 0.9810, 1.0350, 1.0087, 0.9636, 0.9851, 1.0056,
        0.9676, 1.0008, 1.0029, 1.0098, 1.0181, 0.9859, 0.9989, 0.9998, 0.9803,
        0.9943, 1.0038, 1.0152, 1.0003, 1.0198, 1.0082, 0.9938, 0.9934, 0.9875,
        0.9886, 1.0141, 1.0175, 1.0031, 1.0034, 1.0235, 1.0192, 1.0612, 0.9978,
        0.9972, 1.0041, 0.9999, 1.0012, 1.0009, 1.0009, 0.9963, 1.0009, 0.9916,
        1.0045, 1.0138, 1.0380, 0.9574, 0.9879, 0.9814, 0.9750, 0.9333, 0.9608,
        1.0551, 0.9999, 1.0078, 0.9940, 1.0001, 1.0046, 1.0416, 0.9964, 0.9986,
        0.9996, 1.0063, 1.0069, 1.0012, 1.0000, 1.0121, 0.9960, 0.9876, 1.0207,
        1.0044, 0.9762, 0.9530, 1.0257, 1.0071, 1.0034, 1.0230, 1.0202, 1.0007,
        1.0003, 0.9870, 1.0001, 0.9967, 1.0108, 0.9996, 0.9991, 0.9997, 1.0011,
        0.9997, 1.0015, 0.9992, 1.0000, 1.0000, 0.9999, 0.9994, 0.9986, 1.0059,
        0.9914, 1.0345, 1.0165, 1.0036, 1.0006, 0.9985, 1.0091, 1.0003, 0.9764,
        1.0042, 1.0538, 1.1276, 0.9673, 1.0100, 1.0011, 0.9894, 0.9997, 1.0256,
        0.9712, 1.0501, 1.0002, 1.0076, 0.9987, 0.9955, 0.9924, 0.9946, 1.0273,
        1.0164, 0.9858, 0.9827, 1.0112, 0.9978, 0.9736, 0.9779, 1.0076, 1.0040,
        1.0120, 1.0157, 0.9907, 1.0007, 0.9988, 0.9919, 0.9992, 1.0007, 1.0244,
        1.0155, 0.9847, 1.0023, 0.9999, 0.9905, 0.9912, 1.0490, 0.9861, 1.0265,
        0.9979, 1.0027, 0.9982, 1.0005, 0.9993, 0.9995, 1.0000, 1.0000, 1.0037,
        1.0025, 1.0010, 0.9987, 1.0062, 0.9976, 0.9916, 0.9967, 0.9999, 1.0001,
        0.9953, 0.9710, 1.0169, 0.9485, 1.0783, 1.0022, 0.9875, 0.9867, 0.9776,
        0.9764, 0.9963, 0.9935, 1.0539, 0.9644, 1.0170, 0.9934, 1.0368, 0.9744,
        1.0176, 0.9995, 0.9906, 0.9716, 1.0176, 1.0048, 0.9885, 1.0136, 0.9861,
        0.9708, 1.0233, 1.0026, 1.0060, 1.0752, 1.0066, 1.0005, 0.9980, 0.9934,
        1.0052, 0.9993, 1.0042, 1.0026, 0.9989, 0.9960, 1.0047, 1.0157, 1.0158,
        1.0019, 0.9998, 0.9995, 0.9935, 1.0041, 1.0000, 0.9976, 1.0045, 1.0039,
        1.0002, 1.0010, 0.9994, 0.9984, 0.9704, 0.9993, 0.9994, 0.9813, 0.9395,
        1.0329, 0.9818, 1.1620, 0.9744, 1.0242, 0.9999, 1.0154, 1.0003, 1.0008,
        0.9968, 0.9944, 1.0164, 0.9975, 1.0115, 1.0055, 1.0050, 1.0357, 1.0167,
        0.9938, 1.0012, 0.9950, 1.0021, 1.0001, 1.0018, 1.0060, 0.9982, 0.9992,
        0.9946, 1.0776, 1.0118, 0.9822, 1.0010, 1.0007, 0.9928, 1.0000, 0.9850,
        1.0233, 0.9713, 0.9864, 1.0304, 1.0040, 1.0120, 0.9988, 0.9998, 1.0008,
        0.9872, 1.0509, 1.0013, 1.0033, 1.0154, 0.9855, 0.9403, 0.9994, 0.9785,
        0.9993, 1.0098, 1.0743, 0.9989, 1.0016, 0.9973, 1.0050, 1.0001, 1.0003,
        0.9914, 1.0026, 0.9725, 1.1044, 1.0033, 1.0411, 1.0099, 0.9989, 1.0009,
        0.9958, 1.1548, 1.0092, 0.9864, 1.0287, 0.9512, 0.9917, 0.9951, 0.9987,
        1.0220, 0.9421, 0.9953, 1.0088, 1.0036, 0.9848, 0.9944, 0.9618],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808723
t4: 1641198808723
surr1, surr2: tensor([-3.1856e+00, -9.9685e-01, -1.4485e+00, -2.4232e+00, -2.6246e+00,
        -2.9479e+00, -2.7453e+00, -2.6131e+00, -2.8432e+00, -2.7665e+00,
        -2.5785e+00, -2.6651e+00, -2.4993e+00, -2.5899e+00, -2.4543e+00,
        -2.3050e+00, -2.3009e+00, -2.4893e+00, -2.3810e+00, -2.2910e+00,
        -2.2032e+00, -2.1877e+00, -8.4710e-01, -1.7194e-01, -1.6546e+00,
        -1.8035e+00, -1.6324e+00, -1.6927e+00, -1.5983e+00, -1.4248e+00,
        -1.7741e+00, -1.8696e+00, -1.7174e+00, -1.9076e+00, -1.6326e+00,
        -1.5923e+00, -1.5068e+00, -1.8002e+00, -1.5417e+00, -1.5927e+00,
        -1.5190e+00, -1.4409e+00, -1.4923e+00, -1.6466e+00, -1.7414e+00,
        -1.4590e+00, -1.5102e+00, -1.5715e+00, -1.5537e+00, -1.7573e+00,
        -1.4291e+00, -1.5477e+00, -1.5383e+00, -1.8789e+00, -1.2514e+00,
        -1.3198e+00, -1.1276e+00, -1.7210e+00, -1.5757e+00, -1.4014e+00,
        -1.2372e+00, -1.5415e+00, -1.1030e+00, -1.2738e+00, -1.5592e+00,
        -1.1833e+00,  1.1561e+00,  1.2802e+00,  2.1411e+00,  2.4296e+00,
         5.3783e+00,  1.0430e-02,  2.9253e-01,  1.7663e-01, -6.2563e-02,
         1.7106e-01,  2.2175e-01,  2.7857e-01,  3.2121e-01,  3.1203e-01,
         2.7423e-01,  4.1880e-01,  3.4362e-01,  3.0391e-01,  4.6019e-01,
         1.3498e-01, -3.1252e-02,  2.0668e-01,  4.9867e-01,  2.8664e-01,
         1.6901e-01,  1.3592e-01,  2.3933e-01,  2.7528e-01,  3.6509e-01,
         1.0041e-01,  2.2234e-01,  2.9825e-01,  2.8777e-01,  2.1138e-01,
         2.8116e-01,  3.9120e-01,  2.3062e-01,  3.5764e-01, -1.1232e-01,
         8.0114e-02, -1.0713e-01,  4.1871e-02,  1.2847e-01,  1.9188e-01,
         2.5762e-01,  3.1564e-01,  1.8992e-01,  1.7730e+00,  2.7122e+00,
         3.8648e+00,  7.2651e-01,  9.7523e-01,  9.2152e-01,  9.8511e-01,
         9.7986e-01,  9.4230e-01,  8.8014e-01,  9.3758e-01,  8.2555e-01,
         8.5079e-01,  7.4791e-01,  7.7793e-01,  8.5471e-01,  6.0213e-01,
         6.0108e-01,  8.3717e-01,  5.0828e-01,  8.2632e-01,  2.8836e-01,
         8.6216e-01,  7.0023e-01,  8.9517e-01,  9.1891e-01,  8.6164e-01,
         1.0125e+00,  9.8257e-01,  6.0928e-01,  9.5765e-01,  1.0245e+00,
         1.0395e+00,  5.6405e-01,  8.3636e-01,  9.3668e-01,  1.1033e+00,
         5.4102e-01,  9.5435e-01,  6.9513e-01,  9.1363e-01,  9.5388e-01,
         4.6922e-01,  8.0877e-01,  8.5858e-01,  8.3061e-01,  9.1287e-01,
         7.3790e-01,  5.3482e-01,  9.5950e-01,  8.7214e-01,  5.1138e-01,
         9.0658e-01,  7.0305e-01,  6.6302e-01,  7.9989e-01,  9.3654e-01,
         9.3796e-01,  6.5028e-01,  7.1051e-01,  8.6656e-01,  8.5624e-01,
         1.0284e+00,  9.6376e-01,  9.4417e-01,  8.8416e-01,  1.0665e+00,
         8.8697e-01,  8.3416e-01,  3.8014e-01,  5.3156e-01,  8.2726e-01,
         9.4795e-01,  1.0108e+00,  8.2770e-01,  6.6705e-01,  5.9525e-01,
         7.4425e-01,  4.8859e-01,  3.9362e-01,  8.1492e-01,  5.2627e-01,
         6.5674e-01,  7.7615e-01,  6.6532e-01,  4.0493e-01,  7.0369e-01,
         3.2591e-01,  8.1861e-01,  6.8185e-01,  8.9248e-01,  7.6121e-01,
         7.2888e-01,  7.3265e-01,  3.2777e-01,  6.3308e-01,  7.1571e-01,
         5.8129e-01,  6.1745e-01,  7.1476e-01,  5.8800e-01,  5.5413e-01,
         6.1111e-01,  6.8599e-01,  7.1431e-01,  4.7257e-01,  5.9781e-01,
         7.4126e-01,  6.0296e-01,  6.8875e-01,  6.1041e-01,  8.0166e-01,
         5.4358e-01,  4.9336e-01,  6.2358e-01,  6.2822e-01,  7.2051e-01,
         6.4625e-01,  6.3207e-01,  2.0434e-01,  7.5245e-01,  3.0924e-01,
         7.1795e-01,  6.1105e-01,  6.7881e-01,  7.3478e-01,  8.1267e-01,
         7.2522e-01,  6.4740e-01,  7.5444e-01,  2.7773e-01,  6.4070e-01,
         4.0291e-01,  6.7072e-01,  5.6707e-01,  4.5152e-01,  1.6940e-01,
         5.1274e-01,  5.4321e-01,  5.9036e-01,  4.5143e-01,  1.9154e-01,
         1.1327e-01,  2.1360e-01,  1.1445e-03,  4.3536e-01,  4.1171e-01,
         4.4014e-01,  3.5771e-01,  2.6741e-01,  3.2448e-01,  2.9910e-01,
        -8.6645e-02,  3.4557e-01,  3.3684e-01,  4.9394e-01,  5.7022e-02,
         1.1768e-01,  1.6761e-01,  4.0316e-01,  3.9685e-01,  1.8020e-01,
        -2.6990e-02,  2.3078e-01,  4.3381e-01,  2.8347e-01,  1.3338e-01,
         1.9786e-01,  1.6204e-01,  1.8547e-01,  2.0853e-01, -1.2635e-02,
         3.7094e-01,  3.8045e-01,  2.9512e-01,  1.5598e-01,  3.1127e-01,
         2.8338e-01,  2.7250e-01,  1.9070e-01,  2.9962e-01,  2.4102e-01,
         1.6827e-01,  1.1786e-02,  5.7341e-03,  2.4269e-01,  1.7516e-01,
         1.9168e-01, -3.2192e-01,  2.5950e-01,  4.9583e-01,  3.1136e-01,
         3.9424e-01,  3.1125e-01,  2.8617e-01,  4.4507e-01,  2.7264e-01,
         3.5242e-01,  1.4196e-01,  2.8317e-01,  3.4292e-01,  3.6350e-01,
        -4.6876e-03,  8.2306e-02, -1.0620e-01, -1.1846e-01,  3.3744e-01,
        -1.5616e-01,  3.4566e-01, -1.2761e-01,  2.6635e-01,  2.7808e-01,
         2.4174e-01,  2.4169e-01,  8.7634e-02,  8.1541e-02,  2.4046e-01,
         1.8282e-01,  3.3436e-01, -7.8019e-02,  5.9043e-02,  1.7399e-01,
         1.3124e-01, -1.1567e-01,  2.7918e-01,  2.0767e-01,  3.0203e-01,
         4.9028e-02,  3.9421e-02,  1.3086e-01,  5.4979e-02, -3.7415e-01,
         6.7693e-02, -4.5184e-02,  7.5643e-02, -7.2979e-02,  8.3479e-02,
         1.6783e-02, -2.6995e-01, -4.6754e-01, -1.6157e-01, -4.7404e-01,
        -2.5014e-01, -3.5171e-01, -1.8896e-01,  5.6841e-02, -9.4460e-02,
         2.8793e-02, -3.7234e-01, -5.0069e-01, -1.6758e-01,  9.6349e-02,
        -3.4698e-01, -4.1766e-01, -4.2608e-01, -2.9087e-01, -2.1055e-01,
        -1.6106e-01, -3.7549e-01, -6.4554e-01, -1.8109e-01, -2.3145e-01,
        -2.7995e-01, -1.9441e-01, -2.4221e-01, -2.5622e-01, -3.8323e-01,
        -3.6000e-01, -6.3588e-01, -8.5022e-01, -5.6178e-01, -6.7812e-01,
        -5.1551e-01, -4.2785e-01, -3.8323e-01, -1.1439e+00, -8.9140e-01,
        -5.2522e-01, -6.4212e-01, -9.7759e-01, -7.0001e-01, -8.0839e-01,
        -7.0469e-01, -6.8629e-01, -9.9033e-01, -9.5366e-01, -9.4224e-01,
        -9.2161e-01, -7.4243e-01, -8.4027e-01, -8.2500e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1856e+00, -9.9685e-01, -1.4485e+00, -2.4232e+00, -2.6246e+00,
        -2.9479e+00, -2.7453e+00, -2.6131e+00, -2.8432e+00, -2.7665e+00,
        -2.5785e+00, -2.6651e+00, -2.4993e+00, -2.5899e+00, -2.4543e+00,
        -2.3443e+00, -2.3009e+00, -2.4893e+00, -2.3810e+00, -2.2910e+00,
        -2.2032e+00, -2.1877e+00, -8.4710e-01, -1.7194e-01, -1.6546e+00,
        -1.8035e+00, -1.6324e+00, -1.6927e+00, -1.5983e+00, -1.4248e+00,
        -1.8517e+00, -1.8696e+00, -1.7174e+00, -1.9076e+00, -1.6326e+00,
        -1.5923e+00, -1.5068e+00, -1.8002e+00, -1.5417e+00, -1.5927e+00,
        -1.5190e+00, -1.4409e+00, -1.4923e+00, -1.6466e+00, -1.7414e+00,
        -1.4590e+00, -1.5102e+00, -1.5715e+00, -1.5537e+00, -1.7573e+00,
        -1.4291e+00, -1.5477e+00, -1.5383e+00, -1.8789e+00, -1.2514e+00,
        -1.3198e+00, -1.1276e+00, -1.6416e+00, -1.5757e+00, -1.4014e+00,
        -1.2372e+00, -1.5415e+00, -1.1030e+00, -1.2738e+00, -1.5592e+00,
        -1.1833e+00,  1.1561e+00,  1.2802e+00,  2.1411e+00,  2.4296e+00,
         5.1175e+00,  1.0430e-02,  2.9253e-01,  1.7663e-01, -6.2563e-02,
         1.7106e-01,  2.2175e-01,  2.7857e-01,  3.2121e-01,  3.1203e-01,
         2.7423e-01,  4.1880e-01,  3.4362e-01,  3.0391e-01,  4.6019e-01,
         1.3498e-01, -3.1252e-02,  2.0668e-01,  4.9867e-01,  2.8664e-01,
         1.6901e-01,  1.3592e-01,  2.3933e-01,  2.7528e-01,  3.6509e-01,
         1.0041e-01,  2.2234e-01,  2.9825e-01,  2.8777e-01,  2.1138e-01,
         2.8116e-01,  3.9120e-01,  2.3062e-01,  3.5764e-01, -1.1232e-01,
         8.0114e-02, -1.0713e-01,  4.1871e-02,  1.2847e-01,  1.9188e-01,
         2.5762e-01,  3.1564e-01,  1.8992e-01,  1.7730e+00,  2.7122e+00,
         3.8648e+00,  7.2651e-01,  9.7523e-01,  9.2152e-01,  9.8511e-01,
         9.7986e-01,  9.4230e-01,  8.8014e-01,  9.3758e-01,  8.2555e-01,
         8.5079e-01,  7.4791e-01,  7.7793e-01,  8.5471e-01,  6.0213e-01,
         6.0108e-01,  8.3717e-01,  5.0828e-01,  8.2632e-01,  2.8836e-01,
         8.6216e-01,  7.0023e-01,  8.9517e-01,  9.1891e-01,  8.6164e-01,
         1.0125e+00,  9.8257e-01,  6.0928e-01,  9.5765e-01,  1.0245e+00,
         1.0395e+00,  5.6405e-01,  8.3636e-01,  9.3668e-01,  1.1033e+00,
         5.4102e-01,  9.5435e-01,  6.9513e-01,  9.1363e-01,  9.5388e-01,
         4.6922e-01,  8.0877e-01,  8.5858e-01,  8.3061e-01,  9.1287e-01,
         7.3790e-01,  5.3482e-01,  9.5950e-01,  8.7214e-01,  5.1138e-01,
         9.0658e-01,  7.0305e-01,  6.6302e-01,  7.9989e-01,  9.3654e-01,
         9.3796e-01,  6.5028e-01,  7.1051e-01,  8.6656e-01,  8.5624e-01,
         1.0284e+00,  9.6376e-01,  9.4417e-01,  8.8416e-01,  1.0665e+00,
         8.8697e-01,  8.3416e-01,  3.8014e-01,  5.3156e-01,  8.2726e-01,
         9.4795e-01,  1.0108e+00,  8.2770e-01,  6.6705e-01,  5.9525e-01,
         7.4425e-01,  4.7662e-01,  3.9362e-01,  8.1492e-01,  5.2627e-01,
         6.5674e-01,  7.7615e-01,  6.6532e-01,  4.0493e-01,  7.0369e-01,
         3.2591e-01,  8.1861e-01,  6.8185e-01,  8.9248e-01,  7.6121e-01,
         7.2888e-01,  7.3265e-01,  3.2777e-01,  6.3308e-01,  7.1571e-01,
         5.8129e-01,  6.1745e-01,  7.1476e-01,  5.8800e-01,  5.5413e-01,
         6.1111e-01,  6.8599e-01,  7.1431e-01,  4.7257e-01,  5.9781e-01,
         7.4126e-01,  6.0296e-01,  6.8875e-01,  6.1041e-01,  8.0166e-01,
         5.4358e-01,  4.9336e-01,  6.2358e-01,  6.2822e-01,  7.2051e-01,
         6.4625e-01,  6.3207e-01,  2.0434e-01,  7.5245e-01,  3.0924e-01,
         7.1795e-01,  6.1105e-01,  6.7881e-01,  7.3478e-01,  8.1267e-01,
         7.2522e-01,  6.4740e-01,  7.5444e-01,  2.7773e-01,  6.4070e-01,
         4.0291e-01,  6.7072e-01,  5.6707e-01,  4.5152e-01,  1.6940e-01,
         5.1274e-01,  5.4321e-01,  5.9036e-01,  4.5143e-01,  1.9154e-01,
         1.1327e-01,  2.1360e-01,  1.1445e-03,  4.3536e-01,  4.1171e-01,
         4.4014e-01,  3.5771e-01,  2.6741e-01,  3.2448e-01,  2.9910e-01,
        -8.6645e-02,  3.4557e-01,  3.3684e-01,  4.9394e-01,  5.7022e-02,
         1.1768e-01,  1.6761e-01,  4.0316e-01,  3.9685e-01,  1.8020e-01,
        -2.6990e-02,  2.3078e-01,  4.3381e-01,  2.8347e-01,  1.3338e-01,
         1.9786e-01,  1.6204e-01,  1.8547e-01,  2.0853e-01, -1.2635e-02,
         3.7094e-01,  3.8045e-01,  2.9512e-01,  1.5598e-01,  3.1127e-01,
         2.8338e-01,  2.7250e-01,  1.9070e-01,  2.9962e-01,  2.4102e-01,
         1.6827e-01,  1.1786e-02,  5.7341e-03,  2.4269e-01,  1.7516e-01,
         1.9168e-01, -3.2192e-01,  2.5950e-01,  4.9583e-01,  3.1136e-01,
         3.9424e-01,  3.1125e-01,  2.8617e-01,  4.4507e-01,  2.7264e-01,
         3.5242e-01,  1.4196e-01,  2.8317e-01,  3.4292e-01,  3.6350e-01,
        -4.6876e-03,  8.2306e-02, -1.0053e-01, -1.1846e-01,  3.3744e-01,
        -1.5616e-01,  3.4566e-01, -1.2761e-01,  2.6635e-01,  2.7808e-01,
         2.4174e-01,  2.4169e-01,  8.7634e-02,  8.1541e-02,  2.4046e-01,
         1.8282e-01,  3.3436e-01, -7.8019e-02,  5.9043e-02,  1.7399e-01,
         1.3124e-01, -1.1567e-01,  2.7918e-01,  2.0767e-01,  3.0203e-01,
         4.9028e-02,  3.9421e-02,  1.3086e-01,  5.4979e-02, -3.7415e-01,
         6.7693e-02, -4.5184e-02,  7.5643e-02, -7.2979e-02,  8.3479e-02,
         1.6783e-02, -2.6995e-01, -4.6754e-01, -1.6157e-01, -4.7404e-01,
        -2.5014e-01, -3.5171e-01, -1.8896e-01,  5.6841e-02, -9.4460e-02,
         2.8793e-02, -3.7234e-01, -5.0069e-01, -1.6758e-01,  9.6349e-02,
        -3.4698e-01, -4.1766e-01, -4.2608e-01, -2.9087e-01, -2.1055e-01,
        -1.6106e-01, -3.7549e-01, -6.4554e-01, -1.8109e-01, -2.3145e-01,
        -2.7995e-01, -1.9441e-01, -2.4221e-01, -2.5622e-01, -3.8323e-01,
        -3.6000e-01, -6.3336e-01, -8.5022e-01, -5.6178e-01, -6.7812e-01,
        -5.1551e-01, -4.2785e-01, -3.8323e-01, -1.0896e+00, -8.9140e-01,
        -5.2522e-01, -6.4212e-01, -9.7759e-01, -7.0001e-01, -8.0839e-01,
        -7.0469e-01, -6.8629e-01, -9.9033e-01, -9.5366e-01, -9.4224e-01,
        -9.2161e-01, -7.4243e-01, -8.4027e-01, -8.2500e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808732
t6: 1641198808732
state_values: tensor([0.7276, 0.4638, 0.5313, 0.6542, 0.7757, 0.8604, 0.8932, 0.9078, 0.9189,
        0.9377, 0.9911, 0.9434, 0.9325, 0.9261, 0.9531, 1.1039, 1.0208, 0.9711,
        1.0019, 1.0039, 1.0176, 1.0018, 0.8287, 0.7663, 0.8843, 0.8929, 0.8921,
        0.9566, 0.9063, 0.8983, 1.1005, 1.0210, 1.0171, 0.9516, 0.9245, 0.9168,
        0.9153, 0.9183, 0.9418, 0.9202, 0.9385, 0.9278, 0.9254, 1.0167, 0.9483,
        0.9351, 0.9252, 0.9812, 0.9353, 0.9291, 0.9302, 0.9716, 0.9348, 1.0585,
        0.9791, 0.9543, 0.9780, 1.1117, 1.0011, 1.0625, 0.9847, 0.9728, 0.9555,
        0.9832, 0.9689, 0.9697, 0.8060, 0.7630, 0.7203, 0.7302, 0.6670, 0.8375,
        0.8974, 0.9009, 0.9114, 0.9773, 0.9568, 0.9580, 0.9493, 0.9601, 0.9431,
        0.9385, 0.9513, 0.9442, 0.9659, 1.0658, 0.9846, 0.9667, 0.9923, 1.0190,
        0.9792, 1.0383, 0.9982, 0.9796, 0.9693, 0.9749, 1.0403, 1.0117, 0.9976,
        1.0379, 1.0188, 0.9870, 0.9854, 0.9851, 0.9895, 0.9894, 0.9947, 0.9990,
        0.9970, 0.9955, 1.0293, 1.0511, 1.0499, 0.9097, 0.8329, 0.7820, 0.9174,
        0.9896, 0.9752, 0.9914, 0.9858, 0.9806, 0.9867, 0.9846, 1.0495, 1.0089,
        1.0811, 1.0282, 1.0090, 1.0125, 1.1509, 1.0803, 1.0401, 1.0598, 1.2480,
        1.1203, 1.1648, 1.0964, 1.0948, 1.1122, 1.0694, 1.0556, 1.0550, 1.0631,
        1.0721, 1.0701, 1.0591, 1.0548, 1.1110, 1.0665, 1.2237, 1.1146, 1.0880,
        1.0698, 1.0646, 1.2352, 1.1629, 1.1569, 1.1023, 1.0861, 1.0857, 1.0928,
        1.0766, 1.0915, 1.0816, 1.0914, 1.0893, 1.1455, 1.0965, 1.0797, 1.0840,
        1.0882, 1.0880, 1.1103, 1.0884, 1.0755, 1.0757, 1.0996, 1.0951, 1.0852,
        1.0754, 1.1216, 1.0999, 1.0973, 1.0860, 1.0897, 1.0757, 1.1319, 1.1972,
        1.2041, 1.1325, 1.1193, 1.1087, 1.1577, 1.1277, 1.2092, 1.1847, 1.2009,
        1.1556, 1.1780, 1.1542, 1.1329, 1.1325, 1.1381, 1.1684, 1.1929, 1.1766,
        1.1583, 1.1404, 1.1750, 1.2346, 1.1634, 1.1555, 1.1995, 1.2225, 1.1675,
        1.1523, 1.1431, 1.1484, 1.2000, 1.1552, 1.2177, 1.1986, 1.1762, 1.1502,
        1.1492, 1.1510, 1.2214, 1.1662, 1.1775, 1.1818, 1.1928, 1.1709, 1.1806,
        1.1653, 1.1699, 1.1566, 1.1488, 1.1625, 1.1419, 1.1422, 1.1555, 1.1570,
        1.1608, 1.1471, 1.1560, 1.1464, 1.1866, 1.1678, 1.1694, 1.1959, 1.2070,
        1.1941, 1.2081, 1.3078, 1.2154, 1.3378, 1.2338, 1.2250, 1.2329, 1.2357,
        1.2571, 1.3065, 1.2559, 1.2623, 1.2328, 1.2610, 1.2186, 1.2412, 1.2171,
        1.3368, 1.2353, 1.2281, 1.2333, 1.3365, 1.2496, 1.2273, 1.2325, 1.2135,
        1.3575, 1.3032, 1.3311, 1.2623, 1.2329, 1.2234, 1.2154, 1.2129, 1.2141,
        1.2158, 1.2452, 1.2215, 1.2456, 1.2232, 1.2631, 1.2625, 1.2293, 1.2270,
        1.2217, 1.2121, 1.2193, 1.2179, 1.2936, 1.3022, 1.2449, 1.2334, 1.2259,
        1.2281, 1.3036, 1.2583, 1.3259, 1.2733, 1.3570, 1.2917, 1.2624, 1.2754,
        1.4212, 1.2968, 1.2672, 1.2604, 1.2975, 1.2731, 1.2981, 1.2712, 1.2560,
        1.2476, 1.2469, 1.2423, 1.3540, 1.2834, 1.3468, 1.2785, 1.2505, 1.2656,
        1.2587, 1.3093, 1.2688, 1.2615, 1.2474, 1.2601, 1.2585, 1.3294, 1.3366,
        1.2892, 1.2893, 1.2787, 1.2631, 1.3632, 1.2916, 1.2760, 1.2819, 1.2771,
        1.3824, 1.3108, 1.3825, 1.4981, 1.3517, 1.3222, 1.3065, 1.2887, 1.2977,
        1.2776, 1.4333, 1.3362, 1.2969, 1.2718, 1.2879, 1.4455, 1.4466, 1.3644,
        1.3733, 1.3305, 1.3110, 1.3104, 1.3020, 1.2950, 1.2931, 1.3324, 1.3211,
        1.3197, 1.3493, 1.3121, 1.4518, 1.3547, 1.3240, 1.3178, 1.3125, 1.3317,
        1.3393, 1.5335, 1.3855, 1.3741, 1.3800, 1.3509, 1.4257, 1.4654, 1.4076,
        1.3644, 1.3581, 1.4601, 1.4527, 1.4318, 1.3623, 1.3943, 1.3793],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808736
t8: 1641198808736
t9: 1641198808737
t10: 1641198808747
t11: 1641198808748
t12: 1641198808748
t1: 1641198808748
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808759
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0072, 1.0162, 0.9779, 0.9788, 0.9572, 1.0010, 0.9867, 0.9475, 1.0055,
        1.0373, 0.9794, 1.0029, 1.0049, 0.9996, 1.0012, 0.8861, 0.9501, 0.9603,
        0.9912, 0.9968, 0.9981, 0.9987, 1.0030, 0.9750, 1.0096, 0.9994, 0.9798,
        1.0010, 0.9990, 0.9164, 0.8632, 1.0693, 0.9780, 1.0028, 1.0077, 0.9980,
        1.0149, 0.9918, 1.0041, 1.0024, 0.9985, 0.9999, 0.9902, 1.0117, 0.9976,
        1.0109, 0.9804, 1.0084, 1.0152, 0.9956, 0.9944, 1.0019, 0.9973, 0.9976,
        1.0050, 0.9916, 0.9774, 1.1473, 1.0080, 0.9977, 0.9885, 0.9986, 0.9921,
        1.0149, 0.9959, 1.0044, 1.0151, 1.0061, 0.9966, 0.9927, 1.1514, 0.9954,
        1.0166, 1.0205, 0.9737, 1.0001, 1.0020, 0.9952, 0.9918, 1.0098, 0.9996,
        0.9976, 1.0036, 0.9934, 0.9829, 1.0295, 1.0083, 0.9600, 0.9911, 1.0013,
        0.9641, 1.0006, 1.0024, 1.0108, 1.0216, 0.9803, 0.9996, 0.9998, 0.9791,
        0.9946, 1.0039, 1.0166, 1.0003, 1.0240, 1.0142, 1.0021, 0.9976, 0.9902,
        0.9902, 1.0124, 1.0171, 1.0050, 1.0027, 1.0275, 1.0245, 1.0712, 0.9926,
        0.9964, 1.0034, 0.9999, 1.0013, 1.0012, 1.0011, 0.9940, 1.0000, 0.9904,
        1.0017, 1.0151, 1.0433, 0.9475, 0.9885, 0.9702, 0.9719, 0.9324, 0.9612,
        1.0635, 1.0002, 1.0100, 0.9967, 0.9999, 1.0037, 1.0473, 0.9954, 0.9980,
        0.9996, 1.0072, 1.0108, 0.9954, 0.9998, 1.0127, 0.9963, 0.9824, 1.0208,
        1.0050, 0.9708, 0.9540, 1.0326, 1.0024, 1.0031, 1.0227, 1.0253, 1.0009,
        1.0000, 0.9894, 0.9986, 0.9980, 1.0077, 0.9987, 0.9996, 0.9996, 1.0022,
        1.0023, 0.9992, 0.9995, 1.0000, 1.0001, 0.9990, 0.9993, 0.9983, 1.0063,
        0.9893, 1.0325, 1.0220, 1.0061, 1.0002, 0.9987, 1.0074, 1.0040, 0.9805,
        1.0021, 1.0529, 1.1449, 0.9604, 1.0066, 0.9981, 0.9926, 1.0066, 1.0176,
        0.9705, 1.0453, 1.0002, 1.0086, 0.9977, 0.9951, 0.9937, 0.9962, 1.0271,
        1.0191, 0.9826, 0.9839, 1.0073, 0.9975, 0.9723, 0.9807, 1.0045, 1.0044,
        1.0133, 1.0191, 0.9865, 1.0005, 0.9982, 0.9956, 0.9995, 1.0007, 1.0262,
        1.0192, 0.9784, 1.0009, 0.9995, 0.9905, 0.9910, 1.0498, 0.9828, 1.0274,
        0.9966, 1.0032, 0.9999, 0.9996, 0.9996, 0.9998, 0.9999, 0.9996, 1.0046,
        1.0044, 1.0047, 0.9997, 1.0044, 0.9978, 0.9955, 0.9932, 0.9993, 1.0007,
        0.9954, 0.9714, 1.0097, 0.9457, 1.0751, 1.0033, 0.9892, 0.9885, 0.9807,
        0.9836, 0.9979, 0.9964, 1.0446, 0.9586, 1.0144, 0.9914, 1.0350, 0.9672,
        1.0118, 0.9994, 0.9901, 0.9707, 1.0114, 1.0045, 0.9871, 1.0141, 0.9793,
        0.9723, 1.0318, 1.0013, 1.0034, 1.0770, 1.0079, 1.0008, 0.9985, 0.9961,
        1.0025, 0.9994, 1.0028, 1.0027, 0.9958, 0.9974, 1.0041, 1.0184, 1.0202,
        1.0034, 1.0000, 0.9996, 0.9949, 0.9997, 0.9999, 0.9976, 1.0058, 1.0059,
        0.9946, 1.0015, 1.0008, 0.9992, 0.9759, 1.0001, 0.9997, 0.9829, 0.9454,
        1.0287, 0.9769, 1.1797, 0.9687, 1.0200, 0.9966, 1.0117, 1.0009, 1.0021,
        0.9986, 0.9968, 1.0104, 0.9960, 1.0103, 1.0029, 1.0040, 1.0392, 1.0205,
        0.9901, 1.0002, 0.9975, 1.0032, 1.0000, 1.0014, 1.0044, 1.0018, 0.9995,
        0.9944, 1.0816, 1.0141, 0.9769, 1.0007, 1.0000, 0.9918, 1.0000, 0.9830,
        1.0123, 0.9653, 0.9970, 1.0282, 1.0022, 1.0123, 0.9986, 0.9997, 1.0009,
        0.9844, 1.0475, 1.0004, 1.0036, 1.0173, 0.9779, 0.9430, 1.0003, 0.9819,
        0.9997, 1.0071, 1.0847, 0.9986, 1.0023, 0.9987, 1.0020, 1.0005, 1.0003,
        0.9910, 1.0025, 0.9688, 1.0960, 1.0020, 1.0455, 1.0122, 0.9979, 1.0008,
        0.9977, 1.1533, 1.0111, 0.9894, 1.0204, 0.9430, 0.9976, 0.9987, 0.9993,
        1.0187, 0.9342, 1.0000, 1.0142, 1.0024, 0.9862, 0.9955, 0.9671],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808763
t4: 1641198808763
surr1, surr2: tensor([-3.1842e+00, -9.9680e-01, -1.4452e+00, -2.4229e+00, -2.6288e+00,
        -2.9475e+00, -2.7460e+00, -2.6209e+00, -2.8403e+00, -2.7630e+00,
        -2.5654e+00, -2.6646e+00, -2.5013e+00, -2.5846e+00, -2.4552e+00,
        -2.3082e+00, -2.2764e+00, -2.4862e+00, -2.3901e+00, -2.3062e+00,
        -2.2131e+00, -2.1823e+00, -8.4707e-01, -1.7218e-01, -1.6509e+00,
        -1.8032e+00, -1.6280e+00, -1.6915e+00, -1.5984e+00, -1.4191e+00,
        -1.7760e+00, -1.8850e+00, -1.6976e+00, -1.9080e+00, -1.6326e+00,
        -1.5917e+00, -1.5103e+00, -1.7947e+00, -1.5417e+00, -1.5911e+00,
        -1.5189e+00, -1.4408e+00, -1.4890e+00, -1.6373e+00, -1.7406e+00,
        -1.4596e+00, -1.5048e+00, -1.5675e+00, -1.5589e+00, -1.7558e+00,
        -1.4272e+00, -1.5464e+00, -1.5360e+00, -1.8788e+00, -1.2522e+00,
        -1.3163e+00, -1.1295e+00, -1.7122e+00, -1.5812e+00, -1.3982e+00,
        -1.2302e+00, -1.5411e+00, -1.1022e+00, -1.2747e+00, -1.5573e+00,
        -1.1841e+00,  1.1625e+00,  1.2853e+00,  2.1366e+00,  2.4267e+00,
         5.3564e+00,  1.0426e-02,  2.9249e-01,  1.7723e-01, -6.2171e-02,
         1.7119e-01,  2.2191e-01,  2.7857e-01,  3.2111e-01,  3.1208e-01,
         2.7420e-01,  4.1868e-01,  3.4377e-01,  3.0330e-01,  4.6108e-01,
         1.3428e-01, -3.1240e-02,  2.0589e-01,  5.0169e-01,  2.8541e-01,
         1.6839e-01,  1.3588e-01,  2.3922e-01,  2.7554e-01,  3.6634e-01,
         9.9831e-02,  2.2251e-01,  2.9827e-01,  2.8744e-01,  2.1145e-01,
         2.8119e-01,  3.9173e-01,  2.3063e-01,  3.5910e-01, -1.1299e-01,
         8.0780e-02, -1.0758e-01,  4.1986e-02,  1.2868e-01,  1.9155e-01,
         2.5752e-01,  3.1627e-01,  1.8977e-01,  1.7799e+00,  2.7263e+00,
         3.9012e+00,  7.2277e-01,  9.7452e-01,  9.2093e-01,  9.8511e-01,
         9.8004e-01,  9.4254e-01,  8.8035e-01,  9.3545e-01,  8.2478e-01,
         8.4977e-01,  7.4580e-01,  7.7896e-01,  8.5910e-01,  5.9591e-01,
         6.0149e-01,  8.2763e-01,  5.0671e-01,  8.2554e-01,  2.8846e-01,
         8.6905e-01,  7.0045e-01,  8.9716e-01,  9.2142e-01,  8.6152e-01,
         1.0115e+00,  9.8793e-01,  6.0865e-01,  9.5705e-01,  1.0245e+00,
         1.0404e+00,  5.6624e-01,  8.3151e-01,  9.3652e-01,  1.1040e+00,
         5.4117e-01,  9.4933e-01,  6.9525e-01,  9.1419e-01,  9.4869e-01,
         4.6971e-01,  8.1423e-01,  8.5459e-01,  8.3038e-01,  9.1257e-01,
         7.4155e-01,  5.3495e-01,  9.5928e-01,  8.7425e-01,  5.1060e-01,
         9.0770e-01,  7.0087e-01,  6.6242e-01,  8.0035e-01,  9.3639e-01,
         9.3895e-01,  6.5198e-01,  7.0888e-01,  8.6678e-01,  8.5624e-01,
         1.0285e+00,  9.6290e-01,  9.4415e-01,  8.8391e-01,  1.0669e+00,
         8.8504e-01,  8.3254e-01,  3.8218e-01,  5.3285e-01,  8.2692e-01,
         9.4817e-01,  1.0092e+00,  8.3076e-01,  6.6989e-01,  5.9399e-01,
         7.4357e-01,  4.9608e-01,  3.9079e-01,  8.1219e-01,  5.2473e-01,
         6.5889e-01,  7.8154e-01,  6.6011e-01,  4.0463e-01,  7.0043e-01,
         3.2590e-01,  8.1943e-01,  6.8119e-01,  8.9215e-01,  7.6215e-01,
         7.3003e-01,  7.3249e-01,  3.2863e-01,  6.3107e-01,  7.1655e-01,
         5.7906e-01,  6.1727e-01,  7.1382e-01,  5.8972e-01,  5.5244e-01,
         6.1136e-01,  6.8684e-01,  7.1670e-01,  4.7055e-01,  5.9766e-01,
         7.4083e-01,  6.0520e-01,  6.8894e-01,  6.1042e-01,  8.0305e-01,
         5.4557e-01,  4.9018e-01,  6.2274e-01,  6.2802e-01,  7.2050e-01,
         6.4613e-01,  6.3254e-01,  2.0365e-01,  7.5315e-01,  3.0882e-01,
         7.1831e-01,  6.1204e-01,  6.7816e-01,  7.3499e-01,  8.1289e-01,
         7.2513e-01,  6.4709e-01,  7.5517e-01,  2.7827e-01,  6.4309e-01,
         4.0330e-01,  6.6949e-01,  5.6718e-01,  4.5330e-01,  1.6881e-01,
         5.1247e-01,  5.4356e-01,  5.9047e-01,  4.5164e-01,  1.9018e-01,
         1.1293e-01,  2.1296e-01,  1.1459e-03,  4.3609e-01,  4.1245e-01,
         4.4152e-01,  3.6037e-01,  2.6784e-01,  3.2541e-01,  2.9646e-01,
        -8.6124e-02,  3.4467e-01,  3.3618e-01,  4.9307e-01,  5.6599e-02,
         1.1701e-01,  1.6760e-01,  4.0299e-01,  3.9650e-01,  1.7910e-01,
        -2.6982e-02,  2.3047e-01,  4.3403e-01,  2.8152e-01,  1.3358e-01,
         1.9951e-01,  1.6183e-01,  1.8499e-01,  2.0889e-01, -1.2651e-02,
         3.7107e-01,  3.8065e-01,  2.9590e-01,  1.5555e-01,  3.1132e-01,
         2.8300e-01,  2.7252e-01,  1.9011e-01,  3.0005e-01,  2.4087e-01,
         1.6872e-01,  1.1837e-02,  5.7425e-03,  2.4274e-01,  1.7518e-01,
         1.9196e-01, -3.2052e-01,  2.5949e-01,  4.9585e-01,  3.1177e-01,
         3.9501e-01,  3.0953e-01,  2.8632e-01,  4.4570e-01,  2.7285e-01,
         3.5443e-01,  1.4207e-01,  2.8326e-01,  3.4349e-01,  3.6580e-01,
        -4.6684e-03,  8.1891e-02, -1.0782e-01, -1.1776e-01,  3.3605e-01,
        -1.5564e-01,  3.4443e-01, -1.2768e-01,  2.6670e-01,  2.7857e-01,
         2.4234e-01,  2.4028e-01,  8.7506e-02,  8.1448e-02,  2.3984e-01,
         1.8264e-01,  3.3548e-01, -7.8308e-02,  5.8822e-02,  1.7381e-01,
         1.3157e-01, -1.1580e-01,  2.7914e-01,  2.0760e-01,  3.0154e-01,
         4.9206e-02,  3.9432e-02,  1.3083e-01,  5.5182e-02, -3.7499e-01,
         6.7325e-02, -4.5172e-02,  7.5595e-02, -7.2905e-02,  8.3478e-02,
         1.6748e-02, -2.6704e-01, -4.6465e-01, -1.6330e-01, -4.7304e-01,
        -2.4970e-01, -3.5182e-01, -1.8892e-01,  5.6839e-02, -9.4473e-02,
         2.8711e-02, -3.7115e-01, -5.0022e-01, -1.6763e-01,  9.6524e-02,
        -3.4431e-01, -4.1886e-01, -4.2644e-01, -2.9189e-01, -2.1064e-01,
        -1.6062e-01, -3.7912e-01, -6.4531e-01, -1.8122e-01, -2.3179e-01,
        -2.7912e-01, -1.9448e-01, -2.4222e-01, -2.5610e-01, -3.8320e-01,
        -3.5863e-01, -6.3103e-01, -8.4913e-01, -5.6411e-01, -6.7968e-01,
        -5.1495e-01, -4.2781e-01, -3.8397e-01, -1.1424e+00, -8.9309e-01,
        -5.2679e-01, -6.3690e-01, -9.6915e-01, -7.0420e-01, -8.1132e-01,
        -7.0510e-01, -6.8408e-01, -9.8203e-01, -9.5811e-01, -9.4722e-01,
        -9.2052e-01, -7.4352e-01, -8.4112e-01, -8.2957e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1842e+00, -9.9680e-01, -1.4452e+00, -2.4229e+00, -2.6288e+00,
        -2.9475e+00, -2.7460e+00, -2.6209e+00, -2.8403e+00, -2.7630e+00,
        -2.5654e+00, -2.6646e+00, -2.5013e+00, -2.5846e+00, -2.4552e+00,
        -2.3443e+00, -2.2764e+00, -2.4862e+00, -2.3901e+00, -2.3062e+00,
        -2.2131e+00, -2.1823e+00, -8.4707e-01, -1.7218e-01, -1.6509e+00,
        -1.8032e+00, -1.6280e+00, -1.6915e+00, -1.5984e+00, -1.4191e+00,
        -1.8517e+00, -1.8850e+00, -1.6976e+00, -1.9080e+00, -1.6326e+00,
        -1.5917e+00, -1.5103e+00, -1.7947e+00, -1.5417e+00, -1.5911e+00,
        -1.5189e+00, -1.4408e+00, -1.4890e+00, -1.6373e+00, -1.7406e+00,
        -1.4596e+00, -1.5048e+00, -1.5675e+00, -1.5589e+00, -1.7558e+00,
        -1.4272e+00, -1.5464e+00, -1.5360e+00, -1.8788e+00, -1.2522e+00,
        -1.3163e+00, -1.1295e+00, -1.6416e+00, -1.5812e+00, -1.3982e+00,
        -1.2302e+00, -1.5411e+00, -1.1022e+00, -1.2747e+00, -1.5573e+00,
        -1.1841e+00,  1.1625e+00,  1.2853e+00,  2.1366e+00,  2.4267e+00,
         5.1175e+00,  1.0426e-02,  2.9249e-01,  1.7723e-01, -6.2171e-02,
         1.7119e-01,  2.2191e-01,  2.7857e-01,  3.2111e-01,  3.1208e-01,
         2.7420e-01,  4.1868e-01,  3.4377e-01,  3.0330e-01,  4.6108e-01,
         1.3428e-01, -3.1240e-02,  2.0589e-01,  5.0169e-01,  2.8541e-01,
         1.6839e-01,  1.3588e-01,  2.3922e-01,  2.7554e-01,  3.6634e-01,
         9.9831e-02,  2.2251e-01,  2.9827e-01,  2.8744e-01,  2.1145e-01,
         2.8119e-01,  3.9173e-01,  2.3063e-01,  3.5910e-01, -1.1299e-01,
         8.0780e-02, -1.0758e-01,  4.1986e-02,  1.2868e-01,  1.9155e-01,
         2.5752e-01,  3.1627e-01,  1.8977e-01,  1.7799e+00,  2.7263e+00,
         3.9012e+00,  7.2277e-01,  9.7452e-01,  9.2093e-01,  9.8511e-01,
         9.8004e-01,  9.4254e-01,  8.8035e-01,  9.3545e-01,  8.2478e-01,
         8.4977e-01,  7.4580e-01,  7.7896e-01,  8.5910e-01,  5.9591e-01,
         6.0149e-01,  8.2763e-01,  5.0671e-01,  8.2554e-01,  2.8846e-01,
         8.6905e-01,  7.0045e-01,  8.9716e-01,  9.2142e-01,  8.6152e-01,
         1.0115e+00,  9.8793e-01,  6.0865e-01,  9.5705e-01,  1.0245e+00,
         1.0404e+00,  5.6624e-01,  8.3151e-01,  9.3652e-01,  1.1040e+00,
         5.4117e-01,  9.4933e-01,  6.9525e-01,  9.1419e-01,  9.4869e-01,
         4.6971e-01,  8.1423e-01,  8.5459e-01,  8.3038e-01,  9.1257e-01,
         7.4155e-01,  5.3495e-01,  9.5928e-01,  8.7425e-01,  5.1060e-01,
         9.0770e-01,  7.0087e-01,  6.6242e-01,  8.0035e-01,  9.3639e-01,
         9.3895e-01,  6.5198e-01,  7.0888e-01,  8.6678e-01,  8.5624e-01,
         1.0285e+00,  9.6290e-01,  9.4415e-01,  8.8391e-01,  1.0669e+00,
         8.8504e-01,  8.3254e-01,  3.8218e-01,  5.3285e-01,  8.2692e-01,
         9.4817e-01,  1.0092e+00,  8.3076e-01,  6.6989e-01,  5.9399e-01,
         7.4357e-01,  4.7662e-01,  3.9079e-01,  8.1219e-01,  5.2473e-01,
         6.5889e-01,  7.8154e-01,  6.6011e-01,  4.0463e-01,  7.0043e-01,
         3.2590e-01,  8.1943e-01,  6.8119e-01,  8.9215e-01,  7.6215e-01,
         7.3003e-01,  7.3249e-01,  3.2863e-01,  6.3107e-01,  7.1655e-01,
         5.7906e-01,  6.1727e-01,  7.1382e-01,  5.8972e-01,  5.5244e-01,
         6.1136e-01,  6.8684e-01,  7.1670e-01,  4.7055e-01,  5.9766e-01,
         7.4083e-01,  6.0520e-01,  6.8894e-01,  6.1042e-01,  8.0305e-01,
         5.4557e-01,  4.9018e-01,  6.2274e-01,  6.2802e-01,  7.2050e-01,
         6.4613e-01,  6.3254e-01,  2.0365e-01,  7.5315e-01,  3.0882e-01,
         7.1831e-01,  6.1204e-01,  6.7816e-01,  7.3499e-01,  8.1289e-01,
         7.2513e-01,  6.4709e-01,  7.5517e-01,  2.7827e-01,  6.4309e-01,
         4.0330e-01,  6.6949e-01,  5.6718e-01,  4.5330e-01,  1.6881e-01,
         5.1247e-01,  5.4356e-01,  5.9047e-01,  4.5164e-01,  1.9018e-01,
         1.1293e-01,  2.1296e-01,  1.1459e-03,  4.3609e-01,  4.1245e-01,
         4.4152e-01,  3.6037e-01,  2.6784e-01,  3.2541e-01,  2.9646e-01,
        -8.6124e-02,  3.4467e-01,  3.3618e-01,  4.9307e-01,  5.6599e-02,
         1.1701e-01,  1.6760e-01,  4.0299e-01,  3.9650e-01,  1.7910e-01,
        -2.6982e-02,  2.3047e-01,  4.3403e-01,  2.8152e-01,  1.3358e-01,
         1.9951e-01,  1.6183e-01,  1.8499e-01,  2.0889e-01, -1.2651e-02,
         3.7107e-01,  3.8065e-01,  2.9590e-01,  1.5555e-01,  3.1132e-01,
         2.8300e-01,  2.7252e-01,  1.9011e-01,  3.0005e-01,  2.4087e-01,
         1.6872e-01,  1.1837e-02,  5.7425e-03,  2.4274e-01,  1.7518e-01,
         1.9196e-01, -3.2052e-01,  2.5949e-01,  4.9585e-01,  3.1177e-01,
         3.9501e-01,  3.0953e-01,  2.8632e-01,  4.4570e-01,  2.7285e-01,
         3.5443e-01,  1.4207e-01,  2.8326e-01,  3.4349e-01,  3.6580e-01,
        -4.6684e-03,  8.1891e-02, -1.0053e-01, -1.1776e-01,  3.3605e-01,
        -1.5564e-01,  3.4443e-01, -1.2768e-01,  2.6670e-01,  2.7857e-01,
         2.4234e-01,  2.4028e-01,  8.7506e-02,  8.1448e-02,  2.3984e-01,
         1.8264e-01,  3.3548e-01, -7.8308e-02,  5.8822e-02,  1.7381e-01,
         1.3157e-01, -1.1580e-01,  2.7914e-01,  2.0760e-01,  3.0154e-01,
         4.9206e-02,  3.9432e-02,  1.3083e-01,  5.5182e-02, -3.7499e-01,
         6.7325e-02, -4.5172e-02,  7.5595e-02, -7.2905e-02,  8.3478e-02,
         1.6748e-02, -2.6704e-01, -4.6465e-01, -1.6330e-01, -4.7304e-01,
        -2.4970e-01, -3.5182e-01, -1.8892e-01,  5.6839e-02, -9.4473e-02,
         2.8711e-02, -3.7115e-01, -5.0022e-01, -1.6763e-01,  9.6524e-02,
        -3.4431e-01, -4.1886e-01, -4.2644e-01, -2.9189e-01, -2.1064e-01,
        -1.6062e-01, -3.7912e-01, -6.4531e-01, -1.8122e-01, -2.3179e-01,
        -2.7912e-01, -1.9448e-01, -2.4222e-01, -2.5610e-01, -3.8320e-01,
        -3.5863e-01, -6.3103e-01, -8.4913e-01, -5.6411e-01, -6.7968e-01,
        -5.1495e-01, -4.2781e-01, -3.8397e-01, -1.0896e+00, -8.9309e-01,
        -5.2679e-01, -6.3690e-01, -9.6915e-01, -7.0420e-01, -8.1132e-01,
        -7.0510e-01, -6.8408e-01, -9.8203e-01, -9.5811e-01, -9.4722e-01,
        -9.2052e-01, -7.4352e-01, -8.4112e-01, -8.2957e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808772
t6: 1641198808772
state_values: tensor([0.8758, 0.6234, 0.6982, 0.8365, 0.9737, 1.0706, 1.1121, 1.1326, 1.1478,
        1.1733, 1.2350, 1.1863, 1.1782, 1.1739, 1.2058, 1.3654, 1.2829, 1.2298,
        1.2661, 1.2704, 1.2869, 1.2714, 1.0859, 1.0121, 1.1521, 1.1620, 1.1623,
        1.2322, 1.1803, 1.1724, 1.3833, 1.3019, 1.2990, 1.2307, 1.2037, 1.1959,
        1.1952, 1.1976, 1.2256, 1.2021, 1.2233, 1.2123, 1.2099, 1.3037, 1.2343,
        1.2215, 1.2110, 1.2712, 1.2230, 1.2152, 1.2179, 1.2631, 1.2238, 1.3499,
        1.2707, 1.2455, 1.2713, 1.4097, 1.2932, 1.3574, 1.2784, 1.2657, 1.2485,
        1.2783, 1.2627, 1.2650, 1.0721, 1.0274, 0.9854, 0.9968, 0.9337, 1.1078,
        1.1814, 1.1857, 1.1974, 1.2766, 1.2536, 1.2553, 1.2448, 1.2580, 1.2371,
        1.2316, 1.2471, 1.2385, 1.2652, 1.3677, 1.2848, 1.2652, 1.2951, 1.3225,
        1.2810, 1.3423, 1.3014, 1.2818, 1.2693, 1.2755, 1.3452, 1.3161, 1.3018,
        1.3433, 1.3240, 1.2912, 1.2889, 1.2888, 1.2925, 1.2930, 1.2980, 1.3033,
        1.3016, 1.3004, 1.3357, 1.3584, 1.3571, 1.1992, 1.1072, 1.0556, 1.2058,
        1.2949, 1.2776, 1.2976, 1.2907, 1.2844, 1.2918, 1.2892, 1.3592, 1.3175,
        1.3911, 1.3372, 1.3175, 1.3203, 1.4654, 1.3901, 1.3480, 1.3696, 1.5789,
        1.4325, 1.4824, 1.4074, 1.4064, 1.4254, 1.3801, 1.3660, 1.3641, 1.3735,
        1.3833, 1.3815, 1.3682, 1.3649, 1.4247, 1.3781, 1.5533, 1.4279, 1.3991,
        1.3811, 1.3761, 1.5674, 1.4832, 1.4771, 1.4154, 1.3985, 1.3978, 1.4044,
        1.3890, 1.4044, 1.3929, 1.4043, 1.4020, 1.4645, 1.4099, 1.3930, 1.3977,
        1.4014, 1.4015, 1.4255, 1.4026, 1.3897, 1.3900, 1.4148, 1.4102, 1.4005,
        1.3901, 1.4389, 1.4133, 1.4112, 1.4006, 1.4049, 1.3909, 1.4513, 1.5271,
        1.5365, 1.4513, 1.4350, 1.4225, 1.4813, 1.4445, 1.5408, 1.5135, 1.5333,
        1.4778, 1.5057, 1.4758, 1.4519, 1.4513, 1.4587, 1.4951, 1.5246, 1.5052,
        1.4816, 1.4610, 1.5032, 1.5733, 1.4883, 1.4796, 1.5325, 1.5607, 1.4943,
        1.4762, 1.4653, 1.4710, 1.5335, 1.4803, 1.5555, 1.5326, 1.5056, 1.4747,
        1.4729, 1.4751, 1.5599, 1.4937, 1.5077, 1.5130, 1.5262, 1.4975, 1.5111,
        1.4913, 1.4983, 1.4824, 1.4733, 1.4902, 1.4658, 1.4663, 1.4821, 1.4844,
        1.4872, 1.4719, 1.4821, 1.4713, 1.5197, 1.4970, 1.4976, 1.5305, 1.5442,
        1.5290, 1.5457, 1.6553, 1.5520, 1.6860, 1.5734, 1.5648, 1.5750, 1.5788,
        1.6040, 1.6547, 1.6022, 1.6097, 1.5727, 1.6078, 1.5572, 1.5857, 1.5543,
        1.6858, 1.5763, 1.5688, 1.5757, 1.6860, 1.5935, 1.5674, 1.5746, 1.5508,
        1.7077, 1.6528, 1.6824, 1.6097, 1.5748, 1.5617, 1.5532, 1.5506, 1.5521,
        1.5537, 1.5906, 1.5615, 1.5914, 1.5636, 1.6112, 1.6105, 1.5714, 1.5680,
        1.5613, 1.5504, 1.5593, 1.5579, 1.6413, 1.6527, 1.5906, 1.5763, 1.5673,
        1.5699, 1.6548, 1.6060, 1.6779, 1.6226, 1.7093, 1.6423, 1.6106, 1.6253,
        1.7754, 1.6467, 1.6135, 1.6053, 1.6481, 1.6194, 1.6489, 1.6175, 1.6024,
        1.5934, 1.5925, 1.5870, 1.7057, 1.6324, 1.6988, 1.6274, 1.5969, 1.6124,
        1.6052, 1.6611, 1.6170, 1.6076, 1.5932, 1.6075, 1.6061, 1.6828, 1.6906,
        1.6403, 1.6404, 1.6263, 1.6107, 1.7163, 1.6423, 1.6251, 1.6321, 1.6268,
        1.7365, 1.6604, 1.7361, 1.8539, 1.7047, 1.6740, 1.6581, 1.6394, 1.6492,
        1.6272, 1.7888, 1.6878, 1.6478, 1.6210, 1.6378, 1.8010, 1.8020, 1.7190,
        1.7293, 1.6844, 1.6633, 1.6606, 1.6537, 1.6461, 1.6440, 1.6866, 1.6747,
        1.6734, 1.7047, 1.6655, 1.8086, 1.7071, 1.6766, 1.6694, 1.6647, 1.6855,
        1.6940, 1.8961, 1.7401, 1.7301, 1.7367, 1.7039, 1.7817, 1.8217, 1.7647,
        1.7200, 1.7120, 1.8165, 1.8108, 1.7899, 1.7179, 1.7519, 1.7361],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808776
t8: 1641198808776
t9: 1641198808777
t10: 1641198808787
t11: 1641198808788
t12: 1641198808788
t1: 1641198808789
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808799
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0061, 1.0139, 0.9788, 0.9812, 0.9682, 1.0005, 0.9894, 0.9597, 1.0029,
        1.0282, 0.9785, 1.0016, 1.0122, 0.9935, 0.9863, 0.8941, 0.9439, 0.9543,
        0.9920, 0.9982, 1.0013, 0.9978, 1.0033, 0.9743, 1.0105, 0.9989, 0.9683,
        1.0005, 0.9987, 0.9052, 0.8725, 1.0747, 0.9694, 1.0042, 1.0095, 0.9969,
        1.0276, 0.9839, 1.0077, 0.9980, 0.9969, 0.9995, 0.9771, 1.0032, 0.9965,
        1.0129, 0.9694, 1.0072, 1.0288, 0.9934, 0.9879, 1.0050, 0.9913, 0.9965,
        1.0098, 0.9837, 0.9659, 1.1304, 1.0024, 0.9954, 0.9886, 0.9981, 0.9888,
        1.0247, 0.9926, 1.0096, 1.0288, 1.0148, 0.9915, 0.9835, 1.1329, 0.9916,
        1.0184, 1.0328, 0.9583, 0.9999, 0.9999, 0.9940, 0.9887, 1.0127, 0.9993,
        0.9962, 1.0065, 0.9865, 0.9743, 1.0184, 1.0106, 0.9491, 0.9930, 1.0000,
        0.9560, 1.0005, 1.0028, 1.0138, 1.0317, 0.9657, 0.9993, 0.9994, 0.9723,
        0.9945, 1.0050, 1.0220, 1.0003, 1.0416, 1.0260, 1.0210, 1.0070, 0.9970,
        0.9950, 1.0068, 1.0103, 1.0043, 1.0057, 1.0381, 1.0349, 1.0960, 0.9812,
        0.9982, 1.0006, 0.9997, 1.0031, 1.0021, 1.0022, 0.9846, 1.0005, 0.9809,
        0.9997, 1.0189, 1.0565, 0.9242, 0.9909, 0.9689, 0.9650, 0.9158, 0.9642,
        1.0639, 1.0005, 1.0098, 0.9962, 0.9999, 1.0043, 1.0570, 0.9930, 0.9948,
        0.9978, 1.0221, 1.0187, 0.9826, 0.9999, 0.9989, 0.9971, 0.9820, 1.0235,
        1.0066, 0.9515, 0.9588, 1.0345, 0.9997, 1.0038, 1.0254, 1.0380, 1.0014,
        0.9987, 1.0032, 0.9950, 1.0025, 0.9984, 1.0015, 1.0015, 0.9990, 1.0088,
        1.0079, 0.9935, 1.0009, 1.0000, 1.0004, 0.9952, 0.9986, 0.9961, 1.0095,
        0.9821, 1.0448, 1.0348, 1.0110, 0.9991, 0.9998, 0.9998, 0.9987, 0.9879,
        1.0006, 1.0570, 1.1826, 0.9457, 1.0109, 0.9841, 0.9968, 1.0091, 1.0143,
        0.9663, 1.0470, 1.0000, 1.0128, 0.9944, 0.9902, 0.9888, 0.9965, 1.0294,
        1.0252, 0.9742, 0.9767, 1.0042, 0.9962, 0.9653, 0.9810, 1.0036, 1.0053,
        1.0171, 1.0290, 0.9754, 1.0010, 0.9890, 0.9976, 0.9995, 1.0007, 1.0337,
        1.0285, 0.9623, 1.0007, 0.9957, 0.9882, 0.9877, 1.0650, 0.9752, 1.0390,
        0.9933, 1.0075, 1.0042, 0.9970, 1.0007, 1.0007, 0.9995, 0.9977, 1.0137,
        1.0083, 1.0135, 1.0020, 0.9986, 1.0003, 1.0076, 0.9854, 0.9951, 0.9990,
        0.9926, 0.9618, 0.9999, 0.9361, 1.0576, 1.0023, 0.9887, 0.9880, 0.9797,
        0.9848, 0.9994, 0.9963, 1.0440, 0.9463, 1.0146, 0.9841, 1.0431, 0.9477,
        1.0026, 0.9987, 0.9875, 0.9592, 1.0015, 1.0064, 0.9831, 1.0199, 0.9592,
        0.9774, 1.0334, 1.0002, 1.0046, 1.0887, 1.0107, 1.0020, 1.0010, 1.0038,
        0.9954, 1.0012, 0.9988, 1.0068, 0.9864, 0.9955, 1.0069, 1.0268, 1.0313,
        1.0066, 1.0005, 1.0000, 1.0009, 0.9887, 1.0000, 1.0020, 1.0098, 1.0112,
        0.9816, 1.0014, 0.9934, 0.9997, 0.9743, 1.0008, 0.9997, 0.9820, 0.9450,
        1.0178, 0.9836, 1.2080, 0.9564, 1.0327, 0.9876, 1.0201, 1.0024, 1.0061,
        1.0033, 1.0039, 0.9940, 0.9956, 0.9989, 0.9996, 1.0050, 1.0500, 1.0297,
        0.9801, 1.0030, 1.0094, 1.0055, 0.9995, 0.9999, 0.9966, 1.0006, 0.9996,
        0.9934, 1.1008, 1.0188, 0.9614, 1.0003, 1.0036, 0.9888, 0.9999, 0.9718,
        1.0042, 0.9492, 1.0071, 1.0229, 1.0049, 1.0141, 0.9979, 0.9995, 1.0019,
        0.9689, 1.0345, 1.0029, 1.0045, 1.0256, 0.9566, 0.9538, 1.0009, 0.9809,
        0.9999, 1.0079, 1.0991, 0.9976, 1.0056, 1.0030, 0.9937, 0.9994, 0.9983,
        0.9861, 1.0033, 0.9521, 1.0724, 1.0046, 1.0578, 1.0180, 0.9943, 0.9973,
        0.9843, 1.1416, 1.0093, 0.9903, 1.0200, 0.9240, 1.0039, 1.0014, 0.9994,
        1.0203, 0.9220, 1.0055, 1.0166, 1.0021, 0.9846, 0.9955, 0.9617],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808803
t4: 1641198808803
surr1, surr2: tensor([-3.1808e+00, -9.9453e-01, -1.4466e+00, -2.4288e+00, -2.6589e+00,
        -2.9460e+00, -2.7535e+00, -2.6547e+00, -2.8327e+00, -2.7386e+00,
        -2.5630e+00, -2.6613e+00, -2.5196e+00, -2.5687e+00, -2.4188e+00,
        -2.3289e+00, -2.2616e+00, -2.4706e+00, -2.3922e+00, -2.3092e+00,
        -2.2203e+00, -2.1804e+00, -8.4727e-01, -1.7206e-01, -1.6523e+00,
        -1.8024e+00, -1.6090e+00, -1.6908e+00, -1.5979e+00, -1.4019e+00,
        -1.7951e+00, -1.8945e+00, -1.6825e+00, -1.9106e+00, -1.6356e+00,
        -1.5899e+00, -1.5291e+00, -1.7803e+00, -1.5473e+00, -1.5843e+00,
        -1.5164e+00, -1.4402e+00, -1.4693e+00, -1.6237e+00, -1.7387e+00,
        -1.4625e+00, -1.4878e+00, -1.5656e+00, -1.5797e+00, -1.7519e+00,
        -1.4178e+00, -1.5511e+00, -1.5269e+00, -1.8767e+00, -1.2582e+00,
        -1.3059e+00, -1.1162e+00, -1.6870e+00, -1.5723e+00, -1.3949e+00,
        -1.2303e+00, -1.5405e+00, -1.0985e+00, -1.2869e+00, -1.5522e+00,
        -1.1902e+00,  1.1782e+00,  1.2965e+00,  2.1257e+00,  2.4041e+00,
         5.2707e+00,  1.0386e-02,  2.9301e-01,  1.7937e-01, -6.1184e-02,
         1.7116e-01,  2.2145e-01,  2.7822e-01,  3.2010e-01,  3.1298e-01,
         2.7411e-01,  4.1809e-01,  3.4477e-01,  3.0119e-01,  4.5704e-01,
         1.3282e-01, -3.1312e-02,  2.0357e-01,  5.0264e-01,  2.8504e-01,
         1.6699e-01,  1.3587e-01,  2.3930e-01,  2.7637e-01,  3.6995e-01,
         9.8342e-02,  2.2245e-01,  2.9814e-01,  2.8544e-01,  2.1143e-01,
         2.8151e-01,  3.9382e-01,  2.3062e-01,  3.6529e-01, -1.1430e-01,
         8.2307e-02, -1.0860e-01,  4.2276e-02,  1.2931e-01,  1.9048e-01,
         2.5579e-01,  3.1602e-01,  1.9035e-01,  1.7984e+00,  2.7539e+00,
         3.9918e+00,  7.1445e-01,  9.7625e-01,  9.1835e-01,  9.8493e-01,
         9.8181e-01,  9.4336e-01,  8.8133e-01,  9.2661e-01,  8.2519e-01,
         8.4158e-01,  7.4427e-01,  7.8185e-01,  8.6994e-01,  5.8122e-01,
         6.0291e-01,  8.2648e-01,  5.0308e-01,  8.1086e-01,  2.8936e-01,
         8.6932e-01,  7.0061e-01,  8.9695e-01,  9.2095e-01,  8.6150e-01,
         1.0122e+00,  9.9707e-01,  6.0718e-01,  9.5405e-01,  1.0226e+00,
         1.0558e+00,  5.7070e-01,  8.2088e-01,  9.3662e-01,  1.0889e+00,
         5.4162e-01,  9.4893e-01,  6.9705e-01,  9.1561e-01,  9.2979e-01,
         4.7207e-01,  8.1571e-01,  8.5222e-01,  8.3098e-01,  9.1500e-01,
         7.5078e-01,  5.3521e-01,  9.5805e-01,  8.8641e-01,  5.0875e-01,
         9.1180e-01,  6.9442e-01,  6.6433e-01,  8.0187e-01,  9.3585e-01,
         9.4510e-01,  6.5566e-01,  7.0483e-01,  8.6803e-01,  8.5619e-01,
         1.0288e+00,  9.5919e-01,  9.4350e-01,  8.8197e-01,  1.0703e+00,
         8.7860e-01,  8.4241e-01,  3.8696e-01,  5.3547e-01,  8.2596e-01,
         9.4916e-01,  1.0016e+00,  8.2636e-01,  6.7494e-01,  5.9309e-01,
         7.4651e-01,  5.1239e-01,  3.8482e-01,  8.1569e-01,  5.1734e-01,
         6.6170e-01,  7.8347e-01,  6.5795e-01,  4.0291e-01,  7.0155e-01,
         3.2585e-01,  8.2284e-01,  6.7891e-01,  8.8774e-01,  7.5845e-01,
         7.3026e-01,  7.3413e-01,  3.3061e-01,  6.2564e-01,  7.1132e-01,
         5.7731e-01,  6.1646e-01,  7.0865e-01,  5.8991e-01,  5.5194e-01,
         6.1190e-01,  6.8943e-01,  7.2366e-01,  4.6525e-01,  5.9800e-01,
         7.3400e-01,  6.0640e-01,  6.8896e-01,  6.1042e-01,  8.0888e-01,
         5.5051e-01,  4.8211e-01,  6.2263e-01,  6.2562e-01,  7.1883e-01,
         6.4398e-01,  6.4169e-01,  2.0207e-01,  7.6160e-01,  3.0779e-01,
         7.2142e-01,  6.1467e-01,  6.7641e-01,  7.3580e-01,  8.1362e-01,
         7.2482e-01,  6.4587e-01,  7.6200e-01,  2.7934e-01,  6.4869e-01,
         4.0423e-01,  6.6566e-01,  5.6862e-01,  4.5880e-01,  1.6748e-01,
         5.1032e-01,  5.4263e-01,  5.8881e-01,  4.4720e-01,  1.8832e-01,
         1.1178e-01,  2.0949e-01,  1.1447e-03,  4.3591e-01,  4.1225e-01,
         4.4109e-01,  3.6078e-01,  2.6823e-01,  3.2540e-01,  2.9631e-01,
        -8.5019e-02,  3.4473e-01,  3.3369e-01,  4.9694e-01,  5.5461e-02,
         1.1595e-01,  1.6748e-01,  4.0191e-01,  3.9179e-01,  1.7736e-01,
        -2.7033e-02,  2.2952e-01,  4.3650e-01,  2.7573e-01,  1.3428e-01,
         1.9982e-01,  1.6165e-01,  1.8521e-01,  2.1116e-01, -1.2686e-02,
         3.7149e-01,  3.8159e-01,  2.9820e-01,  1.5446e-01,  3.1187e-01,
         2.8185e-01,  2.7364e-01,  1.8831e-01,  2.9948e-01,  2.4154e-01,
         1.7010e-01,  1.1967e-02,  5.7609e-03,  2.4286e-01,  1.7525e-01,
         1.9311e-01, -3.1697e-01,  2.5951e-01,  4.9802e-01,  3.1301e-01,
         3.9709e-01,  3.0548e-01,  2.8628e-01,  4.4240e-01,  2.7301e-01,
         3.5382e-01,  1.4218e-01,  2.8326e-01,  3.4317e-01,  3.6563e-01,
        -4.6189e-03,  8.2450e-02, -1.1040e-01, -1.1627e-01,  3.4026e-01,
        -1.5423e-01,  3.4729e-01, -1.2788e-01,  2.6777e-01,  2.7990e-01,
         2.4405e-01,  2.3636e-01,  8.7471e-02,  8.0523e-02,  2.3906e-01,
         1.8283e-01,  3.3896e-01, -7.9012e-02,  5.8231e-02,  1.7429e-01,
         1.3314e-01, -1.1606e-01,  2.7900e-01,  2.0729e-01,  2.9920e-01,
         4.9142e-02,  3.9433e-02,  1.3070e-01,  5.6163e-02, -3.7676e-01,
         6.6260e-02, -4.5154e-02,  7.5866e-02, -7.2683e-02,  8.3470e-02,
         1.6558e-02, -2.6490e-01, -4.5687e-01, -1.6496e-01, -4.7058e-01,
        -2.5037e-01, -3.5242e-01, -1.8879e-01,  5.6826e-02, -9.4561e-02,
         2.8258e-02, -3.6653e-01, -5.0151e-01, -1.6777e-01,  9.7314e-02,
        -3.3681e-01, -4.2363e-01, -4.2668e-01, -2.9160e-01, -2.1067e-01,
        -1.6074e-01, -3.8414e-01, -6.4472e-01, -1.8181e-01, -2.3279e-01,
        -2.7679e-01, -1.9427e-01, -2.4174e-01, -2.5484e-01, -3.8350e-01,
        -3.5247e-01, -6.1748e-01, -8.5131e-01, -5.7075e-01, -6.8357e-01,
        -5.1312e-01, -4.2631e-01, -3.7882e-01, -1.1308e+00, -8.9143e-01,
        -5.2727e-01, -6.3664e-01, -9.4967e-01, -7.0861e-01, -8.1346e-01,
        -7.0517e-01, -6.8520e-01, -9.6924e-01, -9.6343e-01, -9.4955e-01,
        -9.2028e-01, -7.4227e-01, -8.4117e-01, -8.2490e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1808e+00, -9.9453e-01, -1.4466e+00, -2.4288e+00, -2.6589e+00,
        -2.9460e+00, -2.7535e+00, -2.6547e+00, -2.8327e+00, -2.7386e+00,
        -2.5630e+00, -2.6613e+00, -2.5196e+00, -2.5687e+00, -2.4188e+00,
        -2.3443e+00, -2.2616e+00, -2.4706e+00, -2.3922e+00, -2.3092e+00,
        -2.2203e+00, -2.1804e+00, -8.4727e-01, -1.7206e-01, -1.6523e+00,
        -1.8024e+00, -1.6090e+00, -1.6908e+00, -1.5979e+00, -1.4019e+00,
        -1.8517e+00, -1.8945e+00, -1.6825e+00, -1.9106e+00, -1.6356e+00,
        -1.5899e+00, -1.5291e+00, -1.7803e+00, -1.5473e+00, -1.5843e+00,
        -1.5164e+00, -1.4402e+00, -1.4693e+00, -1.6237e+00, -1.7387e+00,
        -1.4625e+00, -1.4878e+00, -1.5656e+00, -1.5797e+00, -1.7519e+00,
        -1.4178e+00, -1.5511e+00, -1.5269e+00, -1.8767e+00, -1.2582e+00,
        -1.3059e+00, -1.1162e+00, -1.6416e+00, -1.5723e+00, -1.3949e+00,
        -1.2303e+00, -1.5405e+00, -1.0985e+00, -1.2869e+00, -1.5522e+00,
        -1.1902e+00,  1.1782e+00,  1.2965e+00,  2.1257e+00,  2.4041e+00,
         5.1175e+00,  1.0386e-02,  2.9301e-01,  1.7937e-01, -6.1184e-02,
         1.7116e-01,  2.2145e-01,  2.7822e-01,  3.2010e-01,  3.1298e-01,
         2.7411e-01,  4.1809e-01,  3.4477e-01,  3.0119e-01,  4.5704e-01,
         1.3282e-01, -3.1312e-02,  2.0357e-01,  5.0264e-01,  2.8504e-01,
         1.6699e-01,  1.3587e-01,  2.3930e-01,  2.7637e-01,  3.6995e-01,
         9.8342e-02,  2.2245e-01,  2.9814e-01,  2.8544e-01,  2.1143e-01,
         2.8151e-01,  3.9382e-01,  2.3062e-01,  3.6529e-01, -1.1430e-01,
         8.2307e-02, -1.0860e-01,  4.2276e-02,  1.2931e-01,  1.9048e-01,
         2.5579e-01,  3.1602e-01,  1.9035e-01,  1.7984e+00,  2.7539e+00,
         3.9918e+00,  7.1445e-01,  9.7625e-01,  9.1835e-01,  9.8493e-01,
         9.8181e-01,  9.4336e-01,  8.8133e-01,  9.2661e-01,  8.2519e-01,
         8.4158e-01,  7.4427e-01,  7.8185e-01,  8.6994e-01,  5.8122e-01,
         6.0291e-01,  8.2648e-01,  5.0308e-01,  8.1086e-01,  2.8936e-01,
         8.6932e-01,  7.0061e-01,  8.9695e-01,  9.2095e-01,  8.6150e-01,
         1.0122e+00,  9.9707e-01,  6.0718e-01,  9.5405e-01,  1.0226e+00,
         1.0558e+00,  5.7070e-01,  8.2088e-01,  9.3662e-01,  1.0889e+00,
         5.4162e-01,  9.4893e-01,  6.9705e-01,  9.1561e-01,  9.2979e-01,
         4.7207e-01,  8.1571e-01,  8.5222e-01,  8.3098e-01,  9.1500e-01,
         7.5078e-01,  5.3521e-01,  9.5805e-01,  8.8641e-01,  5.0875e-01,
         9.1180e-01,  6.9442e-01,  6.6433e-01,  8.0187e-01,  9.3585e-01,
         9.4510e-01,  6.5566e-01,  7.0483e-01,  8.6803e-01,  8.5619e-01,
         1.0288e+00,  9.5919e-01,  9.4350e-01,  8.8197e-01,  1.0703e+00,
         8.7860e-01,  8.4241e-01,  3.8696e-01,  5.3547e-01,  8.2596e-01,
         9.4916e-01,  1.0016e+00,  8.2636e-01,  6.7494e-01,  5.9309e-01,
         7.4651e-01,  4.7662e-01,  3.8482e-01,  8.1569e-01,  5.1734e-01,
         6.6170e-01,  7.8347e-01,  6.5795e-01,  4.0291e-01,  7.0155e-01,
         3.2585e-01,  8.2284e-01,  6.7891e-01,  8.8774e-01,  7.5845e-01,
         7.3026e-01,  7.3413e-01,  3.3061e-01,  6.2564e-01,  7.1132e-01,
         5.7731e-01,  6.1646e-01,  7.0865e-01,  5.8991e-01,  5.5194e-01,
         6.1190e-01,  6.8943e-01,  7.2366e-01,  4.6525e-01,  5.9800e-01,
         7.3400e-01,  6.0640e-01,  6.8896e-01,  6.1042e-01,  8.0888e-01,
         5.5051e-01,  4.8211e-01,  6.2263e-01,  6.2562e-01,  7.1883e-01,
         6.4398e-01,  6.4169e-01,  2.0207e-01,  7.6160e-01,  3.0779e-01,
         7.2142e-01,  6.1467e-01,  6.7641e-01,  7.3580e-01,  8.1362e-01,
         7.2482e-01,  6.4587e-01,  7.6200e-01,  2.7934e-01,  6.4869e-01,
         4.0423e-01,  6.6566e-01,  5.6862e-01,  4.5880e-01,  1.6748e-01,
         5.1032e-01,  5.4263e-01,  5.8881e-01,  4.4720e-01,  1.8832e-01,
         1.1178e-01,  2.0949e-01,  1.1447e-03,  4.3591e-01,  4.1225e-01,
         4.4109e-01,  3.6078e-01,  2.6823e-01,  3.2540e-01,  2.9631e-01,
        -8.5019e-02,  3.4473e-01,  3.3369e-01,  4.9694e-01,  5.5461e-02,
         1.1595e-01,  1.6748e-01,  4.0191e-01,  3.9179e-01,  1.7736e-01,
        -2.7033e-02,  2.2952e-01,  4.3650e-01,  2.7573e-01,  1.3428e-01,
         1.9982e-01,  1.6165e-01,  1.8521e-01,  2.1116e-01, -1.2686e-02,
         3.7149e-01,  3.8159e-01,  2.9820e-01,  1.5446e-01,  3.1187e-01,
         2.8185e-01,  2.7364e-01,  1.8831e-01,  2.9948e-01,  2.4154e-01,
         1.7010e-01,  1.1967e-02,  5.7609e-03,  2.4286e-01,  1.7525e-01,
         1.9311e-01, -3.1697e-01,  2.5951e-01,  4.9802e-01,  3.1301e-01,
         3.9709e-01,  3.0548e-01,  2.8628e-01,  4.4240e-01,  2.7301e-01,
         3.5382e-01,  1.4218e-01,  2.8326e-01,  3.4317e-01,  3.6563e-01,
        -4.6189e-03,  8.2450e-02, -1.0053e-01, -1.1627e-01,  3.4026e-01,
        -1.5423e-01,  3.4729e-01, -1.2788e-01,  2.6777e-01,  2.7990e-01,
         2.4405e-01,  2.3636e-01,  8.7471e-02,  8.0523e-02,  2.3906e-01,
         1.8283e-01,  3.3896e-01, -7.9012e-02,  5.8231e-02,  1.7429e-01,
         1.3314e-01, -1.1606e-01,  2.7900e-01,  2.0729e-01,  2.9920e-01,
         4.9142e-02,  3.9433e-02,  1.3070e-01,  5.6121e-02, -3.7676e-01,
         6.6260e-02, -4.5154e-02,  7.5866e-02, -7.2683e-02,  8.3470e-02,
         1.6558e-02, -2.6490e-01, -4.5687e-01, -1.6496e-01, -4.7058e-01,
        -2.5037e-01, -3.5242e-01, -1.8879e-01,  5.6826e-02, -9.4561e-02,
         2.8258e-02, -3.6653e-01, -5.0151e-01, -1.6777e-01,  9.7314e-02,
        -3.3681e-01, -4.2363e-01, -4.2668e-01, -2.9160e-01, -2.1067e-01,
        -1.6074e-01, -3.8414e-01, -6.4472e-01, -1.8181e-01, -2.3279e-01,
        -2.7679e-01, -1.9427e-01, -2.4174e-01, -2.5484e-01, -3.8350e-01,
        -3.5247e-01, -6.1748e-01, -8.5131e-01, -5.7075e-01, -6.8357e-01,
        -5.1312e-01, -4.2631e-01, -3.7882e-01, -1.0896e+00, -8.9143e-01,
        -5.2727e-01, -6.3664e-01, -9.4967e-01, -7.0861e-01, -8.1346e-01,
        -7.0517e-01, -6.8520e-01, -9.6924e-01, -9.6343e-01, -9.4955e-01,
        -9.2028e-01, -7.4227e-01, -8.4117e-01, -8.2490e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808812
t6: 1641198808812
state_values: tensor([1.0337, 0.7980, 0.8790, 1.0368, 1.1906, 1.2962, 1.3469, 1.3746, 1.3951,
        1.4281, 1.4967, 1.4507, 1.4473, 1.4452, 1.4829, 1.6485, 1.5677, 1.5145,
        1.5552, 1.5626, 1.5817, 1.5683, 1.3736, 1.2982, 1.4465, 1.4576, 1.4594,
        1.5361, 1.4813, 1.4740, 1.6937, 1.6137, 1.6121, 1.5388, 1.5115, 1.5040,
        1.5042, 1.5059, 1.5383, 1.5133, 1.5374, 1.5263, 1.5241, 1.6248, 1.5497,
        1.5379, 1.5270, 1.5933, 1.5409, 1.5316, 1.5362, 1.5860, 1.5434, 1.6767,
        1.5944, 1.5676, 1.5968, 1.7466, 1.6186, 1.6885, 1.6047, 1.5901, 1.5732,
        1.6059, 1.5878, 1.5919, 1.3906, 1.3374, 1.2870, 1.3022, 1.2262, 1.4286,
        1.5052, 1.5093, 1.5209, 1.6078, 1.5834, 1.5858, 1.5746, 1.5891, 1.5664,
        1.5607, 1.5777, 1.5684, 1.5981, 1.7133, 1.6173, 1.5973, 1.6311, 1.6613,
        1.6154, 1.6853, 1.6380, 1.6169, 1.6035, 1.6097, 1.6898, 1.6549, 1.6394,
        1.6886, 1.6650, 1.6284, 1.6256, 1.6260, 1.6278, 1.6293, 1.6341, 1.6406,
        1.6392, 1.6383, 1.6808, 1.7094, 1.7079, 1.5326, 1.4383, 1.3750, 1.5369,
        1.6345, 1.6155, 1.6382, 1.6304, 1.6235, 1.6317, 1.6290, 1.7132, 1.6627,
        1.7524, 1.6863, 1.6625, 1.6649, 1.8349, 1.7514, 1.6977, 1.7264, 1.9519,
        1.8008, 1.8549, 1.7723, 1.7718, 1.7944, 1.7404, 1.7230, 1.7191, 1.7319,
        1.7446, 1.7426, 1.7238, 1.7211, 1.7943, 1.7386, 1.9292, 1.7978, 1.7639,
        1.7422, 1.7363, 1.9439, 1.8592, 1.8538, 1.7845, 1.7646, 1.7633, 1.7703,
        1.7527, 1.7720, 1.7556, 1.7715, 1.7688, 1.8408, 1.7790, 1.7585, 1.7646,
        1.7686, 1.7689, 1.7984, 1.7710, 1.7554, 1.7560, 1.7868, 1.7812, 1.7697,
        1.7566, 1.8157, 1.7830, 1.7809, 1.7691, 1.7749, 1.7580, 1.8296, 1.9079,
        1.9188, 1.8295, 1.8103, 1.7943, 1.8611, 1.8211, 1.9223, 1.8955, 1.9169,
        1.8569, 1.8877, 1.8545, 1.8306, 1.8300, 1.8384, 1.8771, 1.9085, 1.8881,
        1.8616, 1.8406, 1.8859, 1.9574, 1.8694, 1.8610, 1.9172, 1.9465, 1.8768,
        1.8579, 1.8462, 1.8517, 1.9185, 1.8625, 1.9415, 1.9183, 1.8896, 1.8572,
        1.8546, 1.8567, 1.9462, 1.8769, 1.8924, 1.8980, 1.9121, 1.8791, 1.8957,
        1.8732, 1.8821, 1.8652, 1.8558, 1.8741, 1.8483, 1.8490, 1.8656, 1.8683,
        1.8697, 1.8544, 1.8649, 1.8541, 1.9057, 1.8814, 1.8807, 1.9169, 1.9319,
        1.9161, 1.9338, 2.0440, 1.9378, 2.0766, 1.9584, 1.9523, 1.9635, 1.9678,
        1.9940, 2.0444, 1.9916, 1.9997, 1.9590, 1.9973, 1.9451, 1.9753, 1.9411,
        2.0767, 1.9629, 1.9569, 1.9647, 2.0780, 1.9808, 1.9553, 1.9636, 1.9386,
        2.1035, 2.0437, 2.0757, 1.9999, 1.9635, 1.9482, 1.9412, 1.9390, 1.9406,
        1.9418, 1.9807, 1.9506, 1.9818, 1.9527, 2.0026, 2.0020, 1.9613, 1.9573,
        1.9502, 1.9395, 1.9492, 1.9479, 2.0303, 2.0448, 1.9813, 1.9663, 1.9574,
        1.9599, 2.0474, 1.9977, 2.0729, 2.0149, 2.1084, 2.0347, 2.0025, 2.0182,
        2.1856, 2.0381, 2.0026, 1.9935, 2.0404, 2.0083, 2.0412, 2.0065, 1.9929,
        1.9839, 1.9832, 1.9774, 2.1043, 2.0237, 2.0976, 2.0195, 1.9884, 2.0031,
        1.9961, 2.0548, 2.0089, 1.9980, 1.9844, 1.9995, 1.9985, 2.0800, 2.0896,
        2.0339, 2.0341, 2.0168, 2.0026, 2.1180, 2.0354, 2.0176, 2.0256, 2.0200,
        2.1416, 2.0513, 2.1409, 2.2775, 2.1037, 2.0677, 2.0514, 2.0326, 2.0428,
        2.0204, 2.2032, 2.0832, 2.0406, 2.0139, 2.0305, 2.2173, 2.2185, 2.1218,
        2.1352, 2.0818, 2.0572, 2.0522, 2.0471, 2.0395, 2.0374, 2.0855, 2.0713,
        2.0700, 2.1072, 2.0606, 2.2281, 2.1060, 2.0720, 2.0628, 2.0585, 2.0839,
        2.0945, 2.3227, 2.1456, 2.1363, 2.1446, 2.1027, 2.1963, 2.2429, 2.1769,
        2.1243, 2.1132, 2.2366, 2.2312, 2.2071, 2.1217, 2.1626, 2.1437],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808817
t8: 1641198808817
t9: 1641198808817
t10: 1641198808827
t11: 1641198808829
t12: 1641198808829
t1: 1641198808829
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808839
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0043, 1.0088, 0.9833, 0.9858, 0.9875, 0.9996, 0.9944, 0.9784, 0.9988,
        1.0128, 0.9813, 0.9995, 1.0248, 0.9841, 0.9583, 0.9082, 0.9431, 0.9423,
        0.9862, 0.9916, 1.0017, 0.9996, 1.0038, 0.9691, 1.0156, 0.9980, 0.9498,
        1.0006, 0.9972, 0.8802, 0.8884, 1.0734, 0.9678, 1.0071, 1.0142, 0.9953,
        1.0489, 0.9721, 1.0147, 0.9910, 0.9938, 0.9986, 0.9554, 0.9946, 0.9939,
        1.0161, 0.9524, 1.0108, 1.0509, 0.9901, 0.9768, 1.0118, 0.9819, 0.9944,
        1.0179, 0.9717, 0.9414, 1.1053, 0.9858, 0.9941, 0.9982, 0.9972, 0.9830,
        1.0426, 0.9875, 1.0185, 1.0490, 1.0270, 0.9840, 0.9672, 1.1040, 0.9860,
        1.0229, 1.0524, 0.9358, 0.9997, 0.9955, 0.9920, 0.9833, 1.0175, 0.9986,
        0.9937, 1.0112, 0.9756, 0.9556, 1.0047, 1.0157, 0.9322, 0.9885, 1.0029,
        0.9430, 1.0006, 1.0041, 1.0176, 1.0468, 0.9445, 0.9984, 0.9985, 0.9608,
        0.9928, 1.0064, 1.0307, 1.0001, 1.0708, 1.0422, 1.0482, 1.0203, 1.0070,
        1.0023, 0.9978, 0.9980, 1.0009, 1.0120, 1.0540, 1.0500, 1.1366, 0.9652,
        1.0025, 0.9959, 0.9992, 1.0062, 1.0033, 1.0039, 0.9691, 1.0025, 0.9645,
        0.9993, 1.0254, 1.0760, 0.8913, 0.9945, 0.9798, 0.9549, 0.8858, 0.9693,
        1.0540, 1.0005, 1.0057, 0.9900, 1.0001, 1.0075, 1.0766, 0.9893, 0.9894,
        0.9945, 1.0486, 1.0300, 0.9639, 1.0003, 0.9742, 0.9981, 0.9883, 1.0295,
        1.0088, 0.9205, 0.9675, 1.0296, 1.0013, 1.0059, 1.0336, 1.0569, 1.0018,
        0.9964, 1.0277, 0.9898, 1.0101, 0.9851, 1.0079, 1.0045, 0.9979, 1.0198,
        1.0162, 0.9852, 1.0035, 0.9997, 1.0007, 0.9886, 0.9973, 0.9923, 1.0150,
        0.9709, 1.0708, 1.0531, 1.0177, 0.9971, 1.0013, 0.9883, 0.9847, 0.9971,
        1.0002, 1.0702, 1.2377, 0.9248, 1.0227, 0.9618, 1.0001, 1.0056, 1.0185,
        0.9574, 1.0585, 0.9996, 1.0196, 0.9890, 0.9815, 0.9782, 0.9949, 1.0383,
        1.0336, 0.9613, 0.9613, 1.0031, 0.9945, 0.9561, 0.9746, 1.0057, 1.0067,
        1.0232, 1.0440, 0.9591, 1.0023, 0.9733, 0.9973, 0.9992, 1.0007, 1.0431,
        1.0416, 0.9390, 1.0003, 0.9889, 0.9839, 0.9817, 1.0932, 0.9643, 1.0604,
        0.9881, 1.0151, 1.0107, 0.9930, 1.0026, 1.0021, 0.9986, 0.9944, 1.0295,
        1.0137, 1.0266, 1.0050, 0.9896, 1.0050, 1.0258, 0.9745, 0.9870, 0.9952,
        0.9870, 0.9436, 0.9932, 0.9144, 1.0283, 0.9982, 0.9846, 0.9832, 0.9730,
        0.9768, 0.9998, 0.9921, 1.0593, 0.9321, 1.0195, 0.9725, 1.0549, 0.9198,
        0.9941, 0.9966, 0.9840, 0.9402, 0.9913, 1.0107, 0.9769, 1.0302, 0.9290,
        0.9852, 1.0264, 0.9999, 1.0115, 1.1155, 1.0143, 1.0035, 1.0052, 1.0157,
        0.9856, 1.0045, 0.9921, 1.0145, 0.9715, 0.9905, 1.0126, 1.0395, 1.0476,
        1.0112, 1.0010, 1.0005, 1.0100, 0.9730, 1.0001, 1.0098, 1.0158, 1.0193,
        0.9625, 1.0004, 0.9791, 1.0000, 0.9637, 1.0012, 0.9993, 0.9768, 0.9302,
        1.0033, 1.0050, 1.2600, 0.9386, 1.0641, 0.9737, 1.0410, 1.0045, 1.0127,
        1.0104, 1.0137, 0.9700, 0.9962, 0.9797, 0.9971, 1.0085, 1.0639, 1.0429,
        0.9654, 1.0094, 1.0291, 1.0086, 0.9985, 0.9974, 0.9828, 0.9941, 0.9992,
        0.9917, 1.1255, 1.0252, 0.9379, 0.9999, 1.0101, 0.9841, 0.9996, 0.9534,
        1.0035, 0.9255, 1.0116, 1.0156, 1.0130, 1.0178, 0.9969, 0.9990, 1.0033,
        0.9434, 1.0152, 1.0093, 1.0056, 1.0393, 0.9254, 0.9714, 1.0011, 0.9738,
        0.9996, 1.0142, 1.1275, 0.9960, 1.0109, 1.0099, 0.9813, 0.9968, 0.9946,
        0.9777, 1.0048, 0.9258, 1.0377, 1.0120, 1.0738, 1.0264, 0.9886, 0.9909,
        0.9583, 1.1205, 1.0026, 0.9869, 1.0333, 0.8978, 1.0087, 1.0023, 0.9987,
        1.0302, 0.9012, 1.0093, 1.0160, 1.0034, 0.9777, 0.9937, 0.9437],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808844
t4: 1641198808844
surr1, surr2: tensor([-3.1752e+00, -9.8958e-01, -1.4532e+00, -2.4400e+00, -2.7118e+00,
        -2.9433e+00, -2.7674e+00, -2.7065e+00, -2.8214e+00, -2.6978e+00,
        -2.5704e+00, -2.6557e+00, -2.5508e+00, -2.5444e+00, -2.3500e+00,
        -2.3657e+00, -2.2598e+00, -2.4396e+00, -2.3780e+00, -2.2940e+00,
        -2.2211e+00, -2.1844e+00, -8.4775e-01, -1.7114e-01, -1.6608e+00,
        -1.8007e+00, -1.5782e+00, -1.6909e+00, -1.5955e+00, -1.3631e+00,
        -1.8278e+00, -1.8922e+00, -1.6799e+00, -1.9162e+00, -1.6432e+00,
        -1.5874e+00, -1.5609e+00, -1.7590e+00, -1.5579e+00, -1.5730e+00,
        -1.5117e+00, -1.4390e+00, -1.4367e+00, -1.6097e+00, -1.7342e+00,
        -1.4671e+00, -1.4618e+00, -1.5711e+00, -1.6136e+00, -1.7460e+00,
        -1.4019e+00, -1.5617e+00, -1.5124e+00, -1.8727e+00, -1.2683e+00,
        -1.2900e+00, -1.0879e+00, -1.6495e+00, -1.5463e+00, -1.3931e+00,
        -1.2423e+00, -1.5390e+00, -1.0920e+00, -1.3095e+00, -1.5443e+00,
        -1.2007e+00,  1.2013e+00,  1.3121e+00,  2.1094e+00,  2.3644e+00,
         5.1360e+00,  1.0328e-02,  2.9430e-01,  1.8276e-01, -5.9750e-02,
         1.7112e-01,  2.2046e-01,  2.7768e-01,  3.1837e-01,  3.1446e-01,
         2.7392e-01,  4.1705e-01,  3.4639e-01,  2.9787e-01,  4.4828e-01,
         1.3104e-01, -3.1470e-02,  1.9993e-01,  5.0038e-01,  2.8586e-01,
         1.6471e-01,  1.3589e-01,  2.3962e-01,  2.7741e-01,  3.7536e-01,
         9.6186e-02,  2.2224e-01,  2.9787e-01,  2.8207e-01,  2.1106e-01,
         2.8190e-01,  3.9719e-01,  2.3057e-01,  3.7552e-01, -1.1611e-01,
         8.4498e-02, -1.1003e-01,  4.2698e-02,  1.3025e-01,  1.8880e-01,
         2.5268e-01,  3.1496e-01,  1.9154e-01,  1.8258e+00,  2.7940e+00,
         4.1395e+00,  7.0280e-01,  9.8049e-01,  9.1398e-01,  9.8451e-01,
         9.8477e-01,  9.4455e-01,  8.8282e-01,  9.1202e-01,  8.2683e-01,
         8.2754e-01,  7.4401e-01,  7.8679e-01,  8.8600e-01,  5.6055e-01,
         6.0511e-01,  8.3580e-01,  4.9785e-01,  7.8431e-01,  2.9089e-01,
         8.6125e-01,  7.0067e-01,  8.9337e-01,  9.1519e-01,  8.6165e-01,
         1.0154e+00,  1.0156e+00,  6.0493e-01,  9.4886e-01,  1.0193e+00,
         1.0832e+00,  5.7700e-01,  8.0522e-01,  9.3695e-01,  1.0621e+00,
         5.4219e-01,  9.5504e-01,  7.0112e-01,  9.1765e-01,  8.9950e-01,
         4.7635e-01,  8.1185e-01,  8.5362e-01,  8.3273e-01,  9.2227e-01,
         7.6442e-01,  5.3546e-01,  9.5584e-01,  9.0805e-01,  5.0612e-01,
         9.1874e-01,  6.8517e-01,  6.6854e-01,  8.0420e-01,  9.3482e-01,
         9.5544e-01,  6.6101e-01,  6.9896e-01,  8.7029e-01,  8.5599e-01,
         1.0292e+00,  9.5283e-01,  9.4222e-01,  8.7860e-01,  1.0761e+00,
         8.6857e-01,  8.6343e-01,  3.9383e-01,  5.3901e-01,  8.2436e-01,
         9.5063e-01,  9.8997e-01,  8.1477e-01,  6.8121e-01,  5.9285e-01,
         7.5580e-01,  5.3630e-01,  3.7634e-01,  8.2517e-01,  5.0560e-01,
         6.6389e-01,  7.8072e-01,  6.6070e-01,  3.9917e-01,  7.0929e-01,
         3.2569e-01,  8.2834e-01,  6.7522e-01,  8.7993e-01,  7.5027e-01,
         7.2910e-01,  7.4047e-01,  3.3331e-01,  6.1735e-01,  7.0011e-01,
         5.7667e-01,  6.1541e-01,  7.0195e-01,  5.8605e-01,  5.5311e-01,
         6.1274e-01,  6.9353e-01,  7.3419e-01,  4.5746e-01,  5.9876e-01,
         7.2229e-01,  6.0626e-01,  6.8876e-01,  6.1041e-01,  8.1629e-01,
         5.5755e-01,  4.7046e-01,  6.2238e-01,  6.2136e-01,  7.1566e-01,
         6.4005e-01,  6.5870e-01,  1.9980e-01,  7.7729e-01,  3.0618e-01,
         7.2682e-01,  6.1868e-01,  6.7371e-01,  7.3716e-01,  8.1478e-01,
         7.2419e-01,  6.4371e-01,  7.7384e-01,  2.8083e-01,  6.5708e-01,
         4.0546e-01,  6.5965e-01,  5.7127e-01,  4.6711e-01,  1.6563e-01,
         5.0614e-01,  5.4057e-01,  5.8547e-01,  4.3873e-01,  1.8706e-01,
         1.0919e-01,  2.0368e-01,  1.1401e-03,  4.3408e-01,  4.1027e-01,
         4.3805e-01,  3.5788e-01,  2.6834e-01,  3.2401e-01,  3.0065e-01,
        -8.3739e-02,  3.4640e-01,  3.2975e-01,  5.0255e-01,  5.3826e-02,
         1.1496e-01,  1.6712e-01,  4.0048e-01,  3.8403e-01,  1.7555e-01,
        -2.7147e-02,  2.2808e-01,  4.4093e-01,  2.6707e-01,  1.3536e-01,
         1.9847e-01,  1.6161e-01,  1.8649e-01,  2.1635e-01, -1.2731e-02,
         3.7207e-01,  3.8318e-01,  3.0174e-01,  1.5294e-01,  3.1290e-01,
         2.7998e-01,  2.7571e-01,  1.8546e-01,  2.9796e-01,  2.4289e-01,
         1.7221e-01,  1.2156e-02,  5.7872e-03,  2.4299e-01,  1.7534e-01,
         1.9486e-01, -3.1197e-01,  2.5953e-01,  5.0188e-01,  3.1487e-01,
         4.0027e-01,  2.9951e-01,  2.8598e-01,  4.3605e-01,  2.7307e-01,
         3.5000e-01,  1.4223e-01,  2.8314e-01,  3.4136e-01,  3.5991e-01,
        -4.5532e-03,  8.4251e-02, -1.1516e-01, -1.1410e-01,  3.5061e-01,
        -1.5206e-01,  3.5439e-01, -1.2815e-01,  2.6954e-01,  2.8188e-01,
         2.4645e-01,  2.3067e-01,  8.7523e-02,  7.8974e-02,  2.3847e-01,
         1.8345e-01,  3.4345e-01, -8.0025e-02,  5.7352e-02,  1.7540e-01,
         1.3574e-01, -1.1641e-01,  2.7874e-01,  2.0677e-01,  2.9507e-01,
         4.8826e-02,  3.9420e-02,  1.3047e-01,  5.7419e-02, -3.7910e-01,
         6.4636e-02, -4.5136e-02,  7.6352e-02, -7.2337e-02,  8.3446e-02,
         1.6245e-02, -2.6472e-01, -4.4548e-01, -1.6570e-01, -4.6723e-01,
        -2.5240e-01, -3.5373e-01, -1.8860e-01,  5.6797e-02, -9.4696e-02,
         2.7514e-02, -3.5971e-01, -5.0469e-01, -1.6795e-01,  9.8617e-02,
        -3.2582e-01, -4.3145e-01, -4.2678e-01, -2.8949e-01, -2.1061e-01,
        -1.6176e-01, -3.9410e-01, -6.4366e-01, -1.8278e-01, -2.3439e-01,
        -2.7336e-01, -1.9378e-01, -2.4084e-01, -2.5268e-01, -3.8407e-01,
        -3.4273e-01, -5.9747e-01, -8.5758e-01, -5.7937e-01, -6.8919e-01,
        -5.1016e-01, -4.2354e-01, -3.6882e-01, -1.1099e+00, -8.8558e-01,
        -5.2549e-01, -6.4493e-01, -9.2273e-01, -7.1204e-01, -8.1424e-01,
        -7.0470e-01, -6.9179e-01, -9.4733e-01, -9.6707e-01, -9.4893e-01,
        -9.2144e-01, -7.3710e-01, -8.3967e-01, -8.0948e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1752e+00, -9.8958e-01, -1.4532e+00, -2.4400e+00, -2.7118e+00,
        -2.9433e+00, -2.7674e+00, -2.7065e+00, -2.8214e+00, -2.6978e+00,
        -2.5704e+00, -2.6557e+00, -2.5508e+00, -2.5444e+00, -2.3500e+00,
        -2.3657e+00, -2.2598e+00, -2.4396e+00, -2.3780e+00, -2.2940e+00,
        -2.2211e+00, -2.1844e+00, -8.4775e-01, -1.7114e-01, -1.6608e+00,
        -1.8007e+00, -1.5782e+00, -1.6909e+00, -1.5955e+00, -1.3938e+00,
        -1.8517e+00, -1.8922e+00, -1.6799e+00, -1.9162e+00, -1.6432e+00,
        -1.5874e+00, -1.5609e+00, -1.7590e+00, -1.5579e+00, -1.5730e+00,
        -1.5117e+00, -1.4390e+00, -1.4367e+00, -1.6097e+00, -1.7342e+00,
        -1.4671e+00, -1.4618e+00, -1.5711e+00, -1.6136e+00, -1.7460e+00,
        -1.4019e+00, -1.5617e+00, -1.5124e+00, -1.8727e+00, -1.2683e+00,
        -1.2900e+00, -1.0879e+00, -1.6416e+00, -1.5463e+00, -1.3931e+00,
        -1.2423e+00, -1.5390e+00, -1.0920e+00, -1.3095e+00, -1.5443e+00,
        -1.2007e+00,  1.2013e+00,  1.3121e+00,  2.1094e+00,  2.3644e+00,
         5.1175e+00,  1.0328e-02,  2.9430e-01,  1.8276e-01, -5.9750e-02,
         1.7112e-01,  2.2046e-01,  2.7768e-01,  3.1837e-01,  3.1446e-01,
         2.7392e-01,  4.1705e-01,  3.4639e-01,  2.9787e-01,  4.4828e-01,
         1.3104e-01, -3.1470e-02,  1.9993e-01,  5.0038e-01,  2.8586e-01,
         1.6471e-01,  1.3589e-01,  2.3962e-01,  2.7741e-01,  3.7536e-01,
         9.6186e-02,  2.2224e-01,  2.9787e-01,  2.8207e-01,  2.1106e-01,
         2.8190e-01,  3.9719e-01,  2.3057e-01,  3.7552e-01, -1.1611e-01,
         8.4498e-02, -1.1003e-01,  4.2698e-02,  1.3025e-01,  1.8880e-01,
         2.5268e-01,  3.1496e-01,  1.9154e-01,  1.8258e+00,  2.7940e+00,
         4.0062e+00,  7.0280e-01,  9.8049e-01,  9.1398e-01,  9.8451e-01,
         9.8477e-01,  9.4455e-01,  8.8282e-01,  9.1202e-01,  8.2683e-01,
         8.2754e-01,  7.4401e-01,  7.8679e-01,  8.8600e-01,  5.6602e-01,
         6.0511e-01,  8.3580e-01,  4.9785e-01,  7.9688e-01,  2.9089e-01,
         8.6125e-01,  7.0067e-01,  8.9337e-01,  9.1519e-01,  8.6165e-01,
         1.0154e+00,  1.0156e+00,  6.0493e-01,  9.4886e-01,  1.0193e+00,
         1.0832e+00,  5.7700e-01,  8.0522e-01,  9.3695e-01,  1.0621e+00,
         5.4219e-01,  9.5504e-01,  7.0112e-01,  9.1765e-01,  8.9950e-01,
         4.7635e-01,  8.1185e-01,  8.5362e-01,  8.3273e-01,  9.2227e-01,
         7.6442e-01,  5.3546e-01,  9.5584e-01,  9.0805e-01,  5.0612e-01,
         9.1874e-01,  6.8517e-01,  6.6854e-01,  8.0420e-01,  9.3482e-01,
         9.5544e-01,  6.6101e-01,  6.9896e-01,  8.7029e-01,  8.5599e-01,
         1.0292e+00,  9.5283e-01,  9.4222e-01,  8.7860e-01,  1.0761e+00,
         8.6857e-01,  8.6343e-01,  3.9383e-01,  5.3901e-01,  8.2436e-01,
         9.5063e-01,  9.8997e-01,  8.1477e-01,  6.8121e-01,  5.9285e-01,
         7.5580e-01,  4.7662e-01,  3.7634e-01,  8.2517e-01,  5.0560e-01,
         6.6389e-01,  7.8072e-01,  6.6070e-01,  3.9917e-01,  7.0929e-01,
         3.2569e-01,  8.2834e-01,  6.7522e-01,  8.7993e-01,  7.5027e-01,
         7.2910e-01,  7.4047e-01,  3.3331e-01,  6.1735e-01,  7.0011e-01,
         5.7667e-01,  6.1541e-01,  7.0195e-01,  5.8605e-01,  5.5311e-01,
         6.1274e-01,  6.9353e-01,  7.3419e-01,  4.5746e-01,  5.9876e-01,
         7.2229e-01,  6.0626e-01,  6.8876e-01,  6.1041e-01,  8.1629e-01,
         5.5755e-01,  4.7046e-01,  6.2238e-01,  6.2136e-01,  7.1566e-01,
         6.4005e-01,  6.5870e-01,  1.9980e-01,  7.7729e-01,  3.0618e-01,
         7.2682e-01,  6.1868e-01,  6.7371e-01,  7.3716e-01,  8.1478e-01,
         7.2419e-01,  6.4371e-01,  7.7384e-01,  2.8083e-01,  6.5708e-01,
         4.0546e-01,  6.5965e-01,  5.7127e-01,  4.6711e-01,  1.6563e-01,
         5.0614e-01,  5.4057e-01,  5.8547e-01,  4.3873e-01,  1.8706e-01,
         1.0919e-01,  2.0368e-01,  1.1401e-03,  4.3408e-01,  4.1027e-01,
         4.3805e-01,  3.5788e-01,  2.6834e-01,  3.2401e-01,  3.0065e-01,
        -8.3739e-02,  3.4640e-01,  3.2975e-01,  5.0255e-01,  5.3826e-02,
         1.1496e-01,  1.6712e-01,  4.0048e-01,  3.8403e-01,  1.7555e-01,
        -2.7147e-02,  2.2808e-01,  4.4093e-01,  2.6707e-01,  1.3536e-01,
         1.9847e-01,  1.6161e-01,  1.8649e-01,  2.1334e-01, -1.2731e-02,
         3.7207e-01,  3.8318e-01,  3.0174e-01,  1.5294e-01,  3.1290e-01,
         2.7998e-01,  2.7571e-01,  1.8546e-01,  2.9796e-01,  2.4289e-01,
         1.7221e-01,  1.2156e-02,  5.7872e-03,  2.4299e-01,  1.7534e-01,
         1.9486e-01, -3.1197e-01,  2.5953e-01,  5.0188e-01,  3.1487e-01,
         4.0027e-01,  2.9951e-01,  2.8598e-01,  4.3605e-01,  2.7307e-01,
         3.5000e-01,  1.4223e-01,  2.8314e-01,  3.4136e-01,  3.5991e-01,
        -4.5532e-03,  8.4251e-02, -1.0053e-01, -1.1410e-01,  3.5061e-01,
        -1.5206e-01,  3.5439e-01, -1.2815e-01,  2.6954e-01,  2.8188e-01,
         2.4645e-01,  2.3067e-01,  8.7523e-02,  7.8974e-02,  2.3847e-01,
         1.8345e-01,  3.4345e-01, -8.0025e-02,  5.7352e-02,  1.7540e-01,
         1.3574e-01, -1.1641e-01,  2.7874e-01,  2.0677e-01,  2.9507e-01,
         4.8826e-02,  3.9420e-02,  1.3047e-01,  5.6121e-02, -3.7910e-01,
         6.4636e-02, -4.5136e-02,  7.6352e-02, -7.2337e-02,  8.3446e-02,
         1.6245e-02, -2.6472e-01, -4.4548e-01, -1.6570e-01, -4.6723e-01,
        -2.5240e-01, -3.5373e-01, -1.8860e-01,  5.6797e-02, -9.4696e-02,
         2.7514e-02, -3.5971e-01, -5.0469e-01, -1.6795e-01,  9.8617e-02,
        -3.2582e-01, -4.3145e-01, -4.2678e-01, -2.8949e-01, -2.1061e-01,
        -1.6176e-01, -3.8447e-01, -6.4366e-01, -1.8278e-01, -2.3439e-01,
        -2.7336e-01, -1.9378e-01, -2.4084e-01, -2.5268e-01, -3.8407e-01,
        -3.4273e-01, -5.9747e-01, -8.5758e-01, -5.7937e-01, -6.8919e-01,
        -5.1016e-01, -4.2354e-01, -3.6882e-01, -1.0896e+00, -8.8558e-01,
        -5.2549e-01, -6.4493e-01, -9.2498e-01, -7.1204e-01, -8.1424e-01,
        -7.0470e-01, -6.9179e-01, -9.4733e-01, -9.6707e-01, -9.4893e-01,
        -9.2144e-01, -7.3710e-01, -8.3967e-01, -8.0948e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808852
t6: 1641198808853
state_values: tensor([1.1985, 0.9816, 1.0767, 1.2468, 1.4149, 1.5304, 1.5913, 1.6251, 1.6497,
        1.6910, 1.7687, 1.7239, 1.7251, 1.7255, 1.7701, 1.9487, 1.8664, 1.8110,
        1.8574, 1.8687, 1.8917, 1.8802, 1.6803, 1.5997, 1.7578, 1.7701, 1.7738,
        1.8566, 1.8004, 1.7942, 2.0355, 1.9482, 1.9486, 1.8666, 1.8398, 1.8329,
        1.8344, 1.8356, 1.8723, 1.8465, 1.8734, 1.8626, 1.8610, 1.9756, 1.8885,
        1.8777, 1.8668, 1.9437, 1.8833, 1.8723, 1.8793, 1.9389, 1.8895, 2.0425,
        1.9501, 1.9196, 1.9553, 2.1200, 1.9796, 2.0606, 1.9661, 1.9486, 1.9307,
        1.9697, 1.9475, 1.9543, 1.7309, 1.6733, 1.6209, 1.6381, 1.5615, 1.7709,
        1.8589, 1.8641, 1.8769, 1.9797, 1.9524, 1.9561, 1.9434, 1.9610, 1.9344,
        1.9285, 1.9486, 1.9379, 1.9736, 2.0993, 1.9943, 1.9724, 2.0135, 2.0488,
        1.9952, 2.0735, 2.0224, 1.9979, 1.9826, 1.9894, 2.0791, 2.0436, 2.0257,
        2.0794, 2.0553, 2.0138, 2.0104, 2.0115, 2.0114, 2.0143, 2.0193, 2.0279,
        2.0269, 2.0263, 2.0729, 2.1031, 2.1016, 1.9063, 1.7932, 1.7239, 1.9085,
        2.0252, 2.0032, 2.0300, 2.0214, 2.0135, 2.0234, 2.0205, 2.1094, 2.0569,
        2.1500, 2.0822, 2.0572, 2.0588, 2.2329, 2.1493, 2.0921, 2.1240, 2.3622,
        2.2000, 2.2567, 2.1716, 2.1719, 2.1954, 2.1397, 2.1217, 2.1160, 2.1306,
        2.1443, 2.1426, 2.1205, 2.1192, 2.1956, 2.1386, 2.3394, 2.1992, 2.1639,
        2.1423, 2.1367, 2.3572, 2.2648, 2.2601, 2.1868, 2.1665, 2.1648, 2.1710,
        2.1543, 2.1745, 2.1552, 2.1737, 2.1708, 2.2453, 2.1816, 2.1611, 2.1677,
        2.1712, 2.1718, 2.2031, 2.1748, 2.1591, 2.1598, 2.1921, 2.1862, 2.1749,
        2.1611, 2.2225, 2.1860, 2.1844, 2.1735, 2.1802, 2.1630, 2.2374, 2.3222,
        2.3358, 2.2370, 2.2159, 2.1980, 2.2714, 2.2271, 2.3391, 2.3097, 2.3344,
        2.2658, 2.3013, 2.2629, 2.2387, 2.2379, 2.2474, 2.2904, 2.3258, 2.3028,
        2.2717, 2.2495, 2.3004, 2.3832, 2.2816, 2.2730, 2.3367, 2.3711, 2.2905,
        2.2699, 2.2569, 2.2623, 2.3386, 2.2754, 2.3661, 2.3392, 2.3057, 2.2701,
        2.2664, 2.2686, 2.3723, 2.2915, 2.3094, 2.3159, 2.3327, 2.2919, 2.3129,
        2.2861, 2.2977, 2.2788, 2.2686, 2.2894, 2.2609, 2.2619, 2.2802, 2.2836,
        2.2832, 2.2676, 2.2786, 2.2674, 2.3267, 2.2979, 2.2956, 2.3399, 2.3582,
        2.3396, 2.3607, 2.4889, 2.3624, 2.5245, 2.3852, 2.3812, 2.3952, 2.4006,
        2.4319, 2.4913, 2.4288, 2.4387, 2.3871, 2.4354, 2.3729, 2.4099, 2.3669,
        2.5253, 2.3920, 2.3870, 2.3969, 2.5276, 2.4133, 2.3847, 2.3957, 2.3653,
        2.5534, 2.4904, 2.5270, 2.4388, 2.3950, 2.3747, 2.3685, 2.3665, 2.3684,
        2.3695, 2.4170, 2.3809, 2.4186, 2.3834, 2.4439, 2.4432, 2.3941, 2.3890,
        2.3803, 2.3685, 2.3804, 2.3791, 2.4735, 2.4934, 2.4184, 2.4003, 2.3900,
        2.3929, 2.4973, 2.4386, 2.5262, 2.4591, 2.5613, 2.4820, 2.4441, 2.4635,
        2.6401, 2.4849, 2.4411, 2.4296, 2.4886, 2.4475, 2.4898, 2.4455, 2.4316,
        2.4212, 2.4206, 2.4137, 2.5563, 2.4682, 2.5515, 2.4641, 2.4274, 2.4437,
        2.4359, 2.5071, 2.4519, 2.4375, 2.4228, 2.4412, 2.4404, 2.5340, 2.5446,
        2.4828, 2.4831, 2.4594, 2.4446, 2.5721, 2.4841, 2.4627, 2.4730, 2.4663,
        2.5961, 2.4998, 2.5963, 2.7380, 2.5574, 2.5205, 2.5026, 2.4807, 2.4930,
        2.4664, 2.6591, 2.5350, 2.4894, 2.4586, 2.4773, 2.6732, 2.6747, 2.5769,
        2.5920, 2.5368, 2.5096, 2.5013, 2.4976, 2.4888, 2.4865, 2.5407, 2.5261,
        2.5249, 2.5636, 2.5152, 2.6860, 2.5581, 2.5253, 2.5150, 2.5114, 2.5387,
        2.5502, 2.7896, 2.5997, 2.5929, 2.6020, 2.5555, 2.6543, 2.7014, 2.6349,
        2.5807, 2.5675, 2.6941, 2.6900, 2.6661, 2.5779, 2.6207, 2.6009],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808857
t8: 1641198808857
t9: 1641198808857
t10: 1641198808867
t11: 1641198808869
t12: 1641198808869
t1: 1641198808869
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808879
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0022, 1.0026, 0.9892, 0.9910, 1.0098, 0.9984, 1.0001, 1.0000, 0.9941,
        0.9960, 0.9861, 0.9969, 1.0394, 0.9734, 0.9263, 0.9245, 0.9453, 0.9266,
        0.9768, 0.9803, 0.9995, 1.0030, 1.0044, 0.9616, 1.0227, 0.9968, 0.9288,
        1.0010, 0.9950, 0.8478, 0.9064, 1.0673, 0.9710, 1.0107, 1.0200, 0.9931,
        1.0740, 0.9586, 1.0227, 0.9827, 0.9900, 0.9974, 0.9303, 0.9888, 0.9903,
        1.0202, 0.9331, 1.0165, 1.0770, 0.9859, 0.9639, 1.0202, 0.9710, 0.9918,
        1.0273, 0.9580, 0.9122, 1.0766, 0.9624, 0.9936, 1.0132, 0.9957, 0.9767,
        1.0640, 0.9815, 1.0288, 1.0724, 1.0404, 0.9752, 0.9485, 1.0715, 0.9793,
        1.0290, 1.0750, 0.9107, 0.9986, 0.9906, 0.9898, 0.9781, 1.0221, 0.9975,
        0.9906, 1.0166, 0.9630, 0.9330, 0.9904, 1.0223, 0.9157, 0.9811, 1.0078,
        0.9266, 1.0009, 1.0059, 1.0219, 1.0636, 0.9211, 0.9969, 0.9975, 0.9484,
        0.9902, 1.0082, 1.0397, 0.9995, 1.1053, 1.0605, 1.0776, 1.0351, 1.0179,
        1.0102, 0.9877, 0.9841, 0.9964, 1.0196, 1.0720, 1.0669, 1.1842, 0.9474,
        1.0078, 0.9902, 0.9986, 1.0095, 1.0046, 1.0057, 0.9513, 1.0049, 0.9453,
        1.0008, 1.0339, 1.0978, 0.8556, 0.9984, 0.9977, 0.9432, 0.8564, 0.9748,
        1.0372, 1.0005, 0.9997, 0.9808, 1.0003, 1.0118, 1.1019, 0.9846, 0.9829,
        0.9904, 1.0807, 1.0425, 0.9428, 1.0006, 0.9455, 0.9991, 0.9987, 1.0369,
        1.0108, 0.8855, 0.9776, 1.0207, 1.0052, 1.0086, 1.0445, 1.0783, 1.0020,
        0.9935, 1.0570, 0.9838, 1.0189, 0.9699, 1.0156, 1.0076, 0.9964, 1.0326,
        1.0253, 0.9756, 1.0064, 0.9993, 1.0009, 0.9807, 0.9955, 0.9877, 1.0213,
        0.9579, 1.1034, 1.0741, 1.0249, 0.9947, 1.0029, 0.9750, 0.9673, 1.0016,
        1.0006, 1.0883, 1.2943, 0.9013, 1.0375, 0.9363, 1.0024, 0.9988, 1.0271,
        0.9454, 1.0761, 0.9987, 1.0271, 0.9826, 0.9712, 0.9651, 0.9920, 1.0511,
        1.0428, 0.9466, 0.9427, 1.0040, 0.9922, 0.9450, 0.9650, 1.0093, 1.0083,
        1.0289, 1.0609, 0.9408, 1.0038, 0.9548, 0.9947, 0.9987, 1.0006, 1.0541,
        1.0563, 0.9131, 1.0009, 0.9807, 0.9798, 0.9758, 1.1190, 0.9516, 1.0857,
        0.9818, 1.0238, 1.0180, 0.9882, 1.0045, 1.0036, 0.9974, 0.9903, 1.0481,
        1.0195, 1.0414, 1.0081, 0.9796, 1.0104, 1.0471, 0.9623, 0.9773, 0.9904,
        0.9802, 0.9223, 0.9907, 0.8866, 0.9957, 0.9922, 0.9785, 0.9762, 0.9636,
        0.9646, 0.9996, 0.9853, 1.0830, 0.9152, 1.0265, 0.9590, 1.0714, 0.8887,
        0.9893, 0.9935, 0.9792, 0.9214, 0.9848, 1.0165, 0.9704, 1.0418, 0.8956,
        0.9939, 1.0146, 1.0003, 1.0216, 1.1508, 1.0179, 1.0051, 1.0098, 1.0290,
        0.9744, 1.0083, 0.9842, 1.0233, 0.9544, 0.9840, 1.0186, 1.0538, 1.0660,
        1.0160, 1.0014, 1.0009, 1.0205, 0.9559, 1.0001, 1.0190, 1.0224, 1.0283,
        0.9408, 0.9988, 0.9618, 0.9999, 0.9486, 1.0013, 0.9985, 0.9694, 0.9087,
        0.9911, 1.0360, 1.3281, 0.9186, 1.1042, 0.9576, 1.0673, 1.0066, 1.0202,
        1.0183, 1.0247, 0.9447, 0.9983, 0.9570, 0.9961, 1.0130, 1.0798, 1.0576,
        0.9486, 1.0172, 1.0521, 1.0117, 0.9972, 0.9944, 0.9667, 0.9856, 0.9986,
        0.9892, 1.1591, 1.0320, 0.9115, 0.9999, 1.0165, 0.9785, 0.9991, 0.9323,
        1.0086, 0.8983, 1.0127, 1.0074, 1.0245, 1.0224, 0.9954, 0.9982, 1.0047,
        0.9144, 0.9984, 1.0176, 1.0064, 1.0552, 0.8909, 0.9916, 1.0011, 0.9630,
        0.9990, 1.0236, 1.1654, 0.9937, 1.0169, 1.0176, 0.9672, 0.9936, 0.9900,
        0.9677, 1.0062, 0.8957, 1.0011, 1.0222, 1.0923, 1.0356, 0.9818, 0.9831,
        0.9282, 1.0964, 0.9929, 0.9814, 1.0540, 0.8690, 1.0083, 1.0024, 0.9975,
        1.0443, 0.8750, 1.0097, 1.0119, 1.0054, 0.9681, 0.9909, 0.9184],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808884
t4: 1641198808884
surr1, surr2: tensor([-3.1685e+00, -9.8352e-01, -1.4618e+00, -2.4529e+00, -2.7733e+00,
        -2.9396e+00, -2.7833e+00, -2.7664e+00, -2.8081e+00, -2.6530e+00,
        -2.5828e+00, -2.6487e+00, -2.5871e+00, -2.5167e+00, -2.2716e+00,
        -2.4081e+00, -2.2651e+00, -2.3990e+00, -2.3554e+00, -2.2678e+00,
        -2.2162e+00, -2.1917e+00, -8.4823e-01, -1.6982e-01, -1.6723e+00,
        -1.7985e+00, -1.5432e+00, -1.6916e+00, -1.5920e+00, -1.3130e+00,
        -1.8648e+00, -1.8815e+00, -1.6853e+00, -1.9232e+00, -1.6526e+00,
        -1.5839e+00, -1.5982e+00, -1.7345e+00, -1.5702e+00, -1.5599e+00,
        -1.5060e+00, -1.4372e+00, -1.3990e+00, -1.6004e+00, -1.7279e+00,
        -1.4731e+00, -1.4321e+00, -1.5801e+00, -1.6538e+00, -1.7387e+00,
        -1.3833e+00, -1.5746e+00, -1.4955e+00, -1.8678e+00, -1.2800e+00,
        -1.2717e+00, -1.0542e+00, -1.6066e+00, -1.5096e+00, -1.3925e+00,
        -1.2609e+00, -1.5367e+00, -1.0850e+00, -1.3362e+00, -1.5349e+00,
        -1.2128e+00,  1.2281e+00,  1.3292e+00,  2.0906e+00,  2.3184e+00,
         4.9847e+00,  1.0258e-02,  2.9604e-01,  1.8669e-01, -5.8146e-02,
         1.7094e-01,  2.1937e-01,  2.7703e-01,  3.1669e-01,  3.1587e-01,
         2.7362e-01,  4.1574e-01,  3.4823e-01,  2.9403e-01,  4.3768e-01,
         1.2917e-01, -3.1672e-02,  1.9639e-01,  4.9662e-01,  2.8727e-01,
         1.6185e-01,  1.3593e-01,  2.4005e-01,  2.7857e-01,  3.8139e-01,
         9.3805e-02,  2.2190e-01,  2.9758e-01,  2.7841e-01,  2.1050e-01,
         2.8241e-01,  4.0066e-01,  2.3043e-01,  3.8762e-01, -1.1815e-01,
         8.6872e-02, -1.1163e-01,  4.3163e-02,  1.3128e-01,  1.8687e-01,
         2.4917e-01,  3.1356e-01,  1.9297e-01,  1.8570e+00,  2.8391e+00,
         4.3127e+00,  6.8984e-01,  9.8561e-01,  9.0878e-01,  9.8385e-01,
         9.8804e-01,  9.4571e-01,  8.8434e-01,  8.9526e-01,  8.2889e-01,
         8.1103e-01,  7.4509e-01,  7.9334e-01,  9.0400e-01,  5.3808e-01,
         6.0751e-01,  8.5110e-01,  4.9174e-01,  7.5830e-01,  2.9257e-01,
         8.4755e-01,  7.0067e-01,  8.8797e-01,  9.0671e-01,  8.6182e-01,
         1.0198e+00,  1.0395e+00,  6.0208e-01,  9.4263e-01,  1.0151e+00,
         1.1163e+00,  5.8403e-01,  7.8761e-01,  9.3729e-01,  1.0307e+00,
         5.4270e-01,  9.6505e-01,  7.0622e-01,  9.1944e-01,  8.6532e-01,
         4.8133e-01,  8.0487e-01,  8.5697e-01,  8.3498e-01,  9.3199e-01,
         7.7988e-01,  5.3554e-01,  9.5305e-01,  9.3395e-01,  5.0304e-01,
         9.2675e-01,  6.7462e-01,  6.7363e-01,  8.0672e-01,  9.3340e-01,
         9.6741e-01,  6.6696e-01,  6.9215e-01,  8.7279e-01,  8.5557e-01,
         1.0294e+00,  9.4528e-01,  9.4057e-01,  8.7448e-01,  1.0827e+00,
         8.5696e-01,  8.8971e-01,  4.0168e-01,  5.4284e-01,  8.2240e-01,
         9.5209e-01,  9.7665e-01,  8.0037e-01,  6.8427e-01,  5.9310e-01,
         7.6857e-01,  5.6079e-01,  3.6677e-01,  8.3708e-01,  4.9223e-01,
         6.6540e-01,  7.7550e-01,  6.6628e-01,  3.9419e-01,  7.2106e-01,
         3.2540e-01,  8.3448e-01,  6.7089e-01,  8.7072e-01,  7.4026e-01,
         7.2699e-01,  7.4964e-01,  3.3626e-01,  6.0791e-01,  6.8656e-01,
         5.7714e-01,  6.1394e-01,  6.9377e-01,  5.8029e-01,  5.5507e-01,
         6.1373e-01,  6.9743e-01,  7.4606e-01,  4.4878e-01,  5.9963e-01,
         7.0857e-01,  6.0469e-01,  6.8840e-01,  6.1032e-01,  8.2490e-01,
         5.6541e-01,  4.5751e-01,  6.2274e-01,  6.1620e-01,  7.1268e-01,
         6.3620e-01,  6.7424e-01,  1.9718e-01,  7.9582e-01,  3.0425e-01,
         7.3306e-01,  6.2316e-01,  6.7047e-01,  7.3862e-01,  8.1596e-01,
         7.2329e-01,  6.4106e-01,  7.8786e-01,  2.8246e-01,  6.6653e-01,
         4.0668e-01,  6.5295e-01,  5.7436e-01,  4.7679e-01,  1.6356e-01,
         5.0117e-01,  5.3798e-01,  5.8141e-01,  4.2883e-01,  1.8661e-01,
         1.0587e-01,  1.9723e-01,  1.1332e-03,  4.3139e-01,  4.0735e-01,
         4.3381e-01,  3.5341e-01,  2.6829e-01,  3.2179e-01,  3.0737e-01,
        -8.2217e-02,  3.4879e-01,  3.2519e-01,  5.1044e-01,  5.2007e-02,
         1.1441e-01,  1.6661e-01,  3.9854e-01,  3.7635e-01,  1.7440e-01,
        -2.7302e-02,  2.2656e-01,  4.4588e-01,  2.5746e-01,  1.3655e-01,
         1.9619e-01,  1.6168e-01,  1.8835e-01,  2.2319e-01, -1.2776e-02,
         3.7264e-01,  3.8492e-01,  3.0568e-01,  1.5121e-01,  3.1408e-01,
         2.7775e-01,  2.7811e-01,  1.8220e-01,  2.9603e-01,  2.4435e-01,
         1.7458e-01,  1.2369e-02,  5.8149e-03,  2.4308e-01,  1.7540e-01,
         1.9689e-01, -3.0648e-01,  2.5953e-01,  5.0644e-01,  3.1691e-01,
         4.0381e-01,  2.9277e-01,  2.8554e-01,  4.2833e-01,  2.7305e-01,
         3.4450e-01,  1.4225e-01,  2.8291e-01,  3.3877e-01,  3.5159e-01,
        -4.4979e-03,  8.6849e-02, -1.2138e-01, -1.1167e-01,  3.6380e-01,
        -1.4955e-01,  3.6333e-01, -1.2842e-01,  2.7152e-01,  2.8409e-01,
         2.4913e-01,  2.2464e-01,  8.7707e-02,  7.7146e-02,  2.3821e-01,
         1.8427e-01,  3.4860e-01, -8.1154e-02,  5.6358e-02,  1.7677e-01,
         1.3877e-01, -1.1677e-01,  2.7837e-01,  2.0613e-01,  2.9023e-01,
         4.8406e-02,  3.9394e-02,  1.3014e-01,  5.9134e-02, -3.8161e-01,
         6.2822e-02, -4.5138e-02,  7.6837e-02, -7.1924e-02,  8.3400e-02,
         1.5885e-02, -2.6606e-01, -4.3236e-01, -1.6588e-01, -4.6348e-01,
        -2.5524e-01, -3.5531e-01, -1.8832e-01,  5.6749e-02, -9.4833e-02,
         2.6671e-02, -3.5373e-01, -5.0885e-01, -1.6809e-01,  1.0013e-01,
        -3.1367e-01, -4.4044e-01, -4.2678e-01, -2.8627e-01, -2.1049e-01,
        -1.6326e-01, -4.0732e-01, -6.4217e-01, -1.8386e-01, -2.3617e-01,
        -2.6941e-01, -1.9315e-01, -2.3974e-01, -2.5008e-01, -3.8462e-01,
        -3.3157e-01, -5.7643e-01, -8.6622e-01, -5.8937e-01, -6.9534e-01,
        -5.0667e-01, -4.2023e-01, -3.5723e-01, -1.0860e+00, -8.7698e-01,
        -5.2254e-01, -6.5786e-01, -8.9307e-01, -7.1176e-01, -8.1430e-01,
        -7.0387e-01, -7.0129e-01, -9.1975e-01, -9.6737e-01, -9.4510e-01,
        -9.2333e-01, -7.2985e-01, -8.3726e-01, -7.8778e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1685e+00, -9.8352e-01, -1.4618e+00, -2.4529e+00, -2.7733e+00,
        -2.9396e+00, -2.7833e+00, -2.7664e+00, -2.8081e+00, -2.6530e+00,
        -2.5828e+00, -2.6487e+00, -2.5871e+00, -2.5167e+00, -2.2716e+00,
        -2.4081e+00, -2.2651e+00, -2.3990e+00, -2.3554e+00, -2.2678e+00,
        -2.2162e+00, -2.1917e+00, -8.4823e-01, -1.6982e-01, -1.6723e+00,
        -1.7985e+00, -1.5432e+00, -1.6916e+00, -1.5920e+00, -1.3938e+00,
        -1.8648e+00, -1.8815e+00, -1.6853e+00, -1.9232e+00, -1.6526e+00,
        -1.5839e+00, -1.5982e+00, -1.7345e+00, -1.5702e+00, -1.5599e+00,
        -1.5060e+00, -1.4372e+00, -1.3990e+00, -1.6004e+00, -1.7279e+00,
        -1.4731e+00, -1.4321e+00, -1.5801e+00, -1.6538e+00, -1.7387e+00,
        -1.3833e+00, -1.5746e+00, -1.4955e+00, -1.8678e+00, -1.2800e+00,
        -1.2717e+00, -1.0542e+00, -1.6066e+00, -1.5096e+00, -1.3925e+00,
        -1.2609e+00, -1.5367e+00, -1.0850e+00, -1.3362e+00, -1.5349e+00,
        -1.2128e+00,  1.2281e+00,  1.3292e+00,  2.0906e+00,  2.3184e+00,
         4.9847e+00,  1.0258e-02,  2.9604e-01,  1.8669e-01, -5.8146e-02,
         1.7094e-01,  2.1937e-01,  2.7703e-01,  3.1669e-01,  3.1587e-01,
         2.7362e-01,  4.1574e-01,  3.4823e-01,  2.9403e-01,  4.3768e-01,
         1.2917e-01, -3.1672e-02,  1.9639e-01,  4.9662e-01,  2.8727e-01,
         1.6185e-01,  1.3593e-01,  2.4005e-01,  2.7857e-01,  3.8139e-01,
         9.3805e-02,  2.2190e-01,  2.9758e-01,  2.7841e-01,  2.1050e-01,
         2.8241e-01,  4.0066e-01,  2.3043e-01,  3.8576e-01, -1.1815e-01,
         8.6872e-02, -1.1163e-01,  4.3163e-02,  1.3128e-01,  1.8687e-01,
         2.4917e-01,  3.1356e-01,  1.9297e-01,  1.8570e+00,  2.8391e+00,
         4.0062e+00,  6.8984e-01,  9.8561e-01,  9.0878e-01,  9.8385e-01,
         9.8804e-01,  9.4571e-01,  8.8434e-01,  8.9526e-01,  8.2889e-01,
         8.1103e-01,  7.4509e-01,  7.9334e-01,  9.0400e-01,  5.6602e-01,
         6.0751e-01,  8.5110e-01,  4.9174e-01,  7.9688e-01,  2.9257e-01,
         8.4755e-01,  7.0067e-01,  8.8797e-01,  9.0671e-01,  8.6182e-01,
         1.0198e+00,  1.0376e+00,  6.0208e-01,  9.4263e-01,  1.0151e+00,
         1.1163e+00,  5.8403e-01,  7.8761e-01,  9.3729e-01,  1.0307e+00,
         5.4270e-01,  9.6505e-01,  7.0622e-01,  9.1944e-01,  8.7946e-01,
         4.8133e-01,  8.0487e-01,  8.5697e-01,  8.3498e-01,  9.3199e-01,
         7.7988e-01,  5.3554e-01,  9.5305e-01,  9.3395e-01,  5.0304e-01,
         9.2675e-01,  6.7462e-01,  6.7363e-01,  8.0672e-01,  9.3340e-01,
         9.6741e-01,  6.6696e-01,  6.9215e-01,  8.7279e-01,  8.5557e-01,
         1.0294e+00,  9.4528e-01,  9.4057e-01,  8.7448e-01,  1.0827e+00,
         8.5696e-01,  8.8694e-01,  4.0168e-01,  5.4284e-01,  8.2240e-01,
         9.5209e-01,  9.7665e-01,  8.0037e-01,  6.8427e-01,  5.9310e-01,
         7.6857e-01,  4.7662e-01,  3.6677e-01,  8.3708e-01,  4.9223e-01,
         6.6540e-01,  7.7550e-01,  6.6628e-01,  3.9419e-01,  7.2106e-01,
         3.2540e-01,  8.3448e-01,  6.7089e-01,  8.7072e-01,  7.4026e-01,
         7.2699e-01,  7.4964e-01,  3.3626e-01,  6.0791e-01,  6.8656e-01,
         5.7714e-01,  6.1394e-01,  6.9377e-01,  5.8029e-01,  5.5507e-01,
         6.1373e-01,  6.9743e-01,  7.4606e-01,  4.4878e-01,  5.9963e-01,
         7.0857e-01,  6.0469e-01,  6.8840e-01,  6.1032e-01,  8.2490e-01,
         5.6541e-01,  4.5751e-01,  6.2274e-01,  6.1620e-01,  7.1268e-01,
         6.3620e-01,  6.6277e-01,  1.9718e-01,  7.9582e-01,  3.0425e-01,
         7.3306e-01,  6.2316e-01,  6.7047e-01,  7.3862e-01,  8.1596e-01,
         7.2329e-01,  6.4106e-01,  7.8786e-01,  2.8246e-01,  6.6653e-01,
         4.0668e-01,  6.5295e-01,  5.7436e-01,  4.7679e-01,  1.6356e-01,
         5.0117e-01,  5.3798e-01,  5.8141e-01,  4.2883e-01,  1.8661e-01,
         1.0747e-01,  1.9723e-01,  1.1332e-03,  4.3139e-01,  4.0735e-01,
         4.3381e-01,  3.5341e-01,  2.6829e-01,  3.2179e-01,  3.0737e-01,
        -8.2217e-02,  3.4879e-01,  3.2519e-01,  5.1044e-01,  5.2668e-02,
         1.1441e-01,  1.6661e-01,  3.9854e-01,  3.7635e-01,  1.7440e-01,
        -2.7302e-02,  2.2656e-01,  4.4588e-01,  2.5872e-01,  1.3655e-01,
         1.9619e-01,  1.6168e-01,  1.8835e-01,  2.1334e-01, -1.2776e-02,
         3.7264e-01,  3.8492e-01,  3.0568e-01,  1.5121e-01,  3.1408e-01,
         2.7775e-01,  2.7811e-01,  1.8220e-01,  2.9603e-01,  2.4435e-01,
         1.7458e-01,  1.2369e-02,  5.8149e-03,  2.4308e-01,  1.7540e-01,
         1.9689e-01, -3.0648e-01,  2.5953e-01,  5.0644e-01,  3.1691e-01,
         4.0381e-01,  2.9277e-01,  2.8554e-01,  4.2833e-01,  2.7305e-01,
         3.4450e-01,  1.4225e-01,  2.8291e-01,  3.3877e-01,  3.5159e-01,
        -4.4979e-03,  8.6849e-02, -1.0053e-01, -1.1167e-01,  3.6243e-01,
        -1.4955e-01,  3.6333e-01, -1.2842e-01,  2.7152e-01,  2.8409e-01,
         2.4913e-01,  2.2464e-01,  8.7707e-02,  7.7146e-02,  2.3821e-01,
         1.8427e-01,  3.4860e-01, -8.1154e-02,  5.6358e-02,  1.7677e-01,
         1.3877e-01, -1.1677e-01,  2.7837e-01,  2.0613e-01,  2.9023e-01,
         4.8406e-02,  3.9394e-02,  1.3014e-01,  5.6121e-02, -3.8161e-01,
         6.2822e-02, -4.5138e-02,  7.6837e-02, -7.1924e-02,  8.3400e-02,
         1.5885e-02, -2.6606e-01, -4.3320e-01, -1.6588e-01, -4.6348e-01,
        -2.5524e-01, -3.5531e-01, -1.8832e-01,  5.6749e-02, -9.4833e-02,
         2.6671e-02, -3.5373e-01, -5.0885e-01, -1.6809e-01,  1.0013e-01,
        -3.1689e-01, -4.4044e-01, -4.2678e-01, -2.8627e-01, -2.1049e-01,
        -1.6326e-01, -3.8447e-01, -6.4217e-01, -1.8386e-01, -2.3617e-01,
        -2.6941e-01, -1.9315e-01, -2.3974e-01, -2.5008e-01, -3.8462e-01,
        -3.3317e-01, -5.7643e-01, -8.6622e-01, -5.8937e-01, -6.9534e-01,
        -5.0667e-01, -4.2023e-01, -3.5723e-01, -1.0860e+00, -8.7698e-01,
        -5.2254e-01, -6.5786e-01, -9.2498e-01, -7.1176e-01, -8.1430e-01,
        -7.0387e-01, -7.0129e-01, -9.4607e-01, -9.6737e-01, -9.4510e-01,
        -9.2333e-01, -7.2985e-01, -8.3726e-01, -7.8778e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808893
t6: 1641198808893
state_values: tensor([1.3732, 1.1600, 1.2697, 1.4529, 1.6332, 1.7574, 1.8294, 1.8714, 1.9013,
        1.9508, 2.0423, 1.9927, 1.9997, 2.0035, 2.0573, 2.2669, 2.1739, 2.1116,
        2.1681, 2.1842, 2.2138, 2.2036, 1.9872, 1.9033, 2.0741, 2.0897, 2.0963,
        2.1922, 2.1307, 2.1258, 2.3878, 2.2970, 2.2996, 2.2117, 2.1844, 2.1783,
        2.1818, 2.1823, 2.2272, 2.1990, 2.2314, 2.2207, 2.2199, 2.3395, 2.2507,
        2.2415, 2.2304, 2.3118, 2.2505, 2.2380, 2.2479, 2.3104, 2.2599, 2.4183,
        2.3234, 2.2928, 2.3311, 2.5065, 2.3547, 2.4428, 2.3438, 2.3250, 2.3087,
        2.3495, 2.3255, 2.3345, 2.0927, 2.0260, 1.9691, 1.9893, 1.9058, 2.1409,
        2.2410, 2.2464, 2.2587, 2.3657, 2.3390, 2.3438, 2.3312, 2.3500, 2.3227,
        2.3173, 2.3382, 2.3274, 2.3651, 2.4986, 2.3845, 2.3636, 2.4076, 2.4465,
        2.3885, 2.4735, 2.4179, 2.3925, 2.3769, 2.3833, 2.4811, 2.4428, 2.4234,
        2.4827, 2.4567, 2.4112, 2.4075, 2.4093, 2.4068, 2.4109, 2.4152, 2.4260,
        2.4257, 2.4256, 2.4780, 2.5116, 2.5103, 2.3055, 2.1873, 2.1061, 2.3043,
        2.4268, 2.4039, 2.4332, 2.4240, 2.4156, 2.4267, 2.4237, 2.5217, 2.4642,
        2.5678, 2.4923, 2.4649, 2.4657, 2.6646, 2.5679, 2.5017, 2.5393, 2.8069,
        2.6280, 2.6925, 2.5949, 2.5961, 2.6244, 2.5582, 2.5381, 2.5297, 2.5476,
        2.5640, 2.5623, 2.5346, 2.5348, 2.6251, 2.5579, 2.7846, 2.6295, 2.5870,
        2.5622, 2.5561, 2.8046, 2.7061, 2.7014, 2.6162, 2.5924, 2.5900, 2.5964,
        2.5782, 2.6025, 2.5770, 2.6013, 2.5979, 2.6856, 2.6110, 2.5874, 2.5956,
        2.5993, 2.6003, 2.6385, 2.6048, 2.5867, 2.5877, 2.6267, 2.6195, 2.6066,
        2.5899, 2.6636, 2.6171, 2.6161, 2.6045, 2.6132, 2.5931, 2.6818, 2.7744,
        2.7889, 2.6809, 2.6547, 2.6320, 2.7195, 2.6680, 2.7916, 2.7629, 2.7890,
        2.7126, 2.7539, 2.7086, 2.6834, 2.6825, 2.6945, 2.7426, 2.7810, 2.7566,
        2.7201, 2.6965, 2.7542, 2.8399, 2.7324, 2.7237, 2.7927, 2.8281, 2.7430,
        2.7205, 2.7062, 2.7111, 2.7948, 2.7270, 2.8239, 2.7969, 2.7614, 2.7220,
        2.7173, 2.7196, 2.8308, 2.7457, 2.7665, 2.7736, 2.7911, 2.7442, 2.7702,
        2.7382, 2.7533, 2.7322, 2.7211, 2.7449, 2.7133, 2.7146, 2.7351, 2.7393,
        2.7370, 2.7208, 2.7327, 2.7210, 2.7857, 2.7552, 2.7510, 2.7992, 2.8187,
        2.7998, 2.8218, 2.9586, 2.8208, 2.9991, 2.8428, 2.8417, 2.8569, 2.8629,
        2.8961, 2.9622, 2.8927, 2.9042, 2.8459, 2.8993, 2.8334, 2.8727, 2.8258,
        2.9990, 2.8511, 2.8479, 2.8588, 3.0024, 2.8732, 2.8453, 2.8575, 2.8255,
        3.0339, 2.9598, 3.0031, 2.9030, 2.8566, 2.8332, 2.8289, 2.8274, 2.8293,
        2.8298, 2.8800, 2.8425, 2.8820, 2.8451, 2.9088, 2.9084, 2.8568, 2.8509,
        2.8415, 2.8302, 2.8430, 2.8418, 2.9374, 2.9622, 2.8821, 2.8631, 2.8528,
        2.8557, 2.9674, 2.9035, 3.0024, 2.9258, 3.0445, 2.9510, 2.9096, 2.9315,
        3.1422, 2.9526, 2.9032, 2.8905, 2.9576, 2.9091, 2.9587, 2.9073, 2.8951,
        2.8846, 2.8840, 2.8768, 3.0364, 2.9333, 3.0328, 2.9298, 2.8920, 2.9074,
        2.8998, 2.9780, 2.9169, 2.9006, 2.8869, 2.9063, 2.9059, 3.0107, 3.0240,
        2.9512, 2.9518, 2.9228, 2.9095, 3.0571, 2.9522, 2.9287, 2.9403, 2.9331,
        3.0866, 2.9661, 3.0870, 3.2578, 3.0379, 2.9935, 2.9728, 2.9487, 2.9621,
        2.9331, 3.1637, 3.0089, 2.9567, 2.9248, 2.9428, 3.1803, 3.1828, 3.0623,
        3.0825, 3.0141, 2.9807, 2.9679, 2.9658, 2.9562, 2.9535, 3.0187, 3.0011,
        2.9999, 3.0469, 2.9883, 3.1970, 3.0354, 2.9985, 2.9853, 2.9820, 3.0155,
        3.0299, 3.3137, 3.0875, 3.0820, 3.0939, 3.0328, 3.1581, 3.2155, 3.1343,
        3.0665, 3.0484, 3.2055, 3.2016, 3.1728, 3.0622, 3.1166, 3.0915],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808897
t8: 1641198808897
t9: 1641198808898
t10: 1641198808908
t11: 1641198808909
t12: 1641198808909
t1: 1641198808909
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808920
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([1.0001, 0.9976, 0.9927, 0.9956, 1.0298, 0.9969, 1.0047, 1.0190, 0.9896,
        0.9825, 0.9889, 0.9943, 1.0517, 0.9633, 0.9019, 0.9298, 0.9490, 0.9089,
        0.9690, 0.9703, 0.9970, 1.0060, 1.0048, 0.9546, 1.0287, 0.9954, 0.9105,
        1.0013, 0.9928, 0.8191, 0.9118, 1.0593, 0.9749, 1.0144, 1.0255, 0.9908,
        1.0961, 0.9451, 1.0290, 0.9749, 0.9867, 0.9962, 0.9084, 0.9863, 0.9863,
        1.0238, 0.9159, 1.0202, 1.0992, 0.9814, 0.9526, 1.0265, 0.9606, 0.9894,
        1.0353, 0.9450, 0.8900, 1.0667, 0.9357, 0.9933, 1.0289, 0.9937, 0.9712,
        1.0821, 0.9754, 1.0374, 1.0938, 1.0529, 0.9671, 0.9346, 1.0651, 0.9720,
        1.0340, 1.0958, 0.8872, 0.9979, 0.9865, 0.9876, 0.9733, 1.0260, 0.9962,
        0.9876, 1.0211, 0.9518, 0.9157, 0.9861, 1.0292, 0.9000, 0.9765, 1.0117,
        0.9099, 1.0011, 1.0074, 1.0257, 1.0788, 0.8993, 0.9961, 0.9965, 0.9384,
        0.9881, 1.0098, 1.0472, 0.9986, 1.1366, 1.0782, 1.1056, 1.0488, 1.0275,
        1.0169, 0.9788, 0.9734, 0.9936, 1.0259, 1.0893, 1.0828, 1.2277, 0.9310,
        1.0114, 0.9853, 0.9979, 1.0123, 1.0054, 1.0070, 0.9355, 1.0061, 0.9276,
        1.0017, 1.0422, 1.1187, 0.8225, 0.9996, 1.0148, 0.9318, 0.8324, 0.9767,
        1.0182, 1.0006, 0.9932, 0.9721, 1.0004, 1.0160, 1.1269, 0.9797, 0.9771,
        0.9869, 1.1089, 1.0545, 0.9235, 1.0008, 0.9209, 0.9993, 1.0099, 1.0447,
        1.0124, 0.8554, 0.9807, 1.0123, 1.0082, 1.0113, 1.0552, 1.0984, 1.0018,
        0.9908, 1.0826, 0.9777, 1.0263, 0.9561, 1.0213, 1.0103, 0.9948, 1.0437,
        1.0337, 0.9666, 1.0086, 0.9986, 1.0009, 0.9737, 0.9939, 0.9834, 1.0265,
        0.9457, 1.1299, 1.0945, 1.0314, 0.9924, 1.0040, 0.9635, 0.9549, 1.0031,
        1.0012, 1.1084, 1.3527, 0.8790, 1.0474, 0.9133, 1.0027, 0.9932, 1.0338,
        0.9324, 1.0917, 0.9975, 1.0336, 0.9766, 0.9625, 0.9549, 0.9901, 1.0632,
        1.0514, 0.9329, 0.9284, 1.0049, 0.9895, 0.9349, 0.9576, 1.0118, 1.0098,
        1.0339, 1.0765, 0.9237, 1.0047, 0.9384, 0.9930, 0.9981, 1.0003, 1.0644,
        1.0701, 0.8893, 1.0020, 0.9731, 0.9761, 0.9707, 1.1424, 0.9390, 1.1065,
        0.9756, 1.0310, 1.0244, 0.9837, 1.0060, 1.0046, 0.9961, 0.9865, 1.0643,
        1.0248, 1.0543, 1.0106, 0.9706, 1.0146, 1.0665, 0.9505, 0.9695, 0.9867,
        0.9743, 0.9045, 0.9888, 0.8579, 0.9891, 0.9858, 0.9728, 0.9696, 0.9556,
        0.9552, 0.9993, 0.9779, 1.1055, 0.8971, 1.0318, 0.9466, 1.0846, 0.8593,
        0.9879, 0.9903, 0.9748, 0.9049, 0.9819, 1.0226, 0.9640, 1.0505, 0.8654,
        0.9964, 1.0036, 1.0007, 1.0324, 1.1882, 1.0209, 1.0062, 1.0136, 1.0407,
        0.9639, 1.0112, 0.9770, 1.0306, 0.9385, 0.9796, 1.0229, 1.0672, 1.0833,
        1.0202, 1.0014, 1.0010, 1.0296, 0.9400, 0.9999, 1.0269, 1.0283, 1.0364,
        0.9212, 0.9978, 0.9469, 0.9998, 0.9339, 1.0013, 0.9975, 0.9619, 0.8891,
        0.9886, 1.0717, 1.4053, 0.8995, 1.1341, 0.9425, 1.0864, 1.0083, 1.0265,
        1.0252, 1.0345, 0.9220, 0.9998, 0.9362, 0.9952, 1.0174, 1.0947, 1.0714,
        0.9331, 1.0229, 1.0734, 1.0143, 0.9958, 0.9916, 0.9532, 0.9816, 0.9980,
        0.9866, 1.1909, 1.0381, 0.8884, 1.0000, 1.0227, 0.9729, 0.9984, 0.9138,
        1.0134, 0.8718, 1.0119, 1.0043, 1.0376, 1.0268, 0.9938, 0.9973, 1.0058,
        0.8892, 0.9940, 1.0261, 1.0069, 1.0693, 0.8597, 0.9979, 1.0011, 0.9512,
        0.9985, 1.0331, 1.2057, 0.9910, 1.0220, 1.0242, 0.9543, 0.9911, 0.9860,
        0.9589, 1.0071, 0.8688, 0.9932, 1.0331, 1.1110, 1.0439, 0.9755, 0.9766,
        0.9045, 1.0884, 0.9817, 0.9766, 1.0739, 0.8431, 1.0066, 1.0024, 0.9962,
        1.0585, 0.8481, 1.0085, 1.0083, 1.0073, 0.9581, 0.9883, 0.8940],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808924
t4: 1641198808924
surr1, surr2: tensor([-3.1617e+00, -9.7855e-01, -1.4671e+00, -2.4643e+00, -2.8282e+00,
        -2.9354e+00, -2.7961e+00, -2.8187e+00, -2.7954e+00, -2.6171e+00,
        -2.5903e+00, -2.6418e+00, -2.6178e+00, -2.4907e+00, -2.2118e+00,
        -2.4219e+00, -2.2739e+00, -2.3530e+00, -2.3365e+00, -2.2448e+00,
        -2.2107e+00, -2.1984e+00, -8.4851e-01, -1.6857e-01, -1.6821e+00,
        -1.7961e+00, -1.5128e+00, -1.6921e+00, -1.5883e+00, -1.2685e+00,
        -1.8759e+00, -1.8673e+00, -1.6922e+00, -1.9301e+00, -1.6615e+00,
        -1.5803e+00, -1.6311e+00, -1.7102e+00, -1.5800e+00, -1.5475e+00,
        -1.5009e+00, -1.4355e+00, -1.3660e+00, -1.5962e+00, -1.7209e+00,
        -1.4782e+00, -1.4058e+00, -1.5857e+00, -1.6878e+00, -1.7308e+00,
        -1.3672e+00, -1.5843e+00, -1.4796e+00, -1.8633e+00, -1.2900e+00,
        -1.2545e+00, -1.0284e+00, -1.5919e+00, -1.4678e+00, -1.3921e+00,
        -1.2804e+00, -1.5337e+00, -1.0789e+00, -1.3590e+00, -1.5253e+00,
        -1.2230e+00,  1.2527e+00,  1.3451e+00,  2.0733e+00,  2.2845e+00,
         4.9549e+00,  1.0181e-02,  2.9750e-01,  1.9030e-01, -5.6644e-02,
         1.7082e-01,  2.1848e-01,  2.7644e-01,  3.1511e-01,  3.1709e-01,
         2.7327e-01,  4.1451e-01,  3.4977e-01,  2.9059e-01,  4.2957e-01,
         1.2861e-01, -3.1886e-02,  1.9303e-01,  4.9431e-01,  2.8837e-01,
         1.5894e-01,  1.3595e-01,  2.4042e-01,  2.7963e-01,  3.8683e-01,
         9.1581e-02,  2.2172e-01,  2.9729e-01,  2.7548e-01,  2.1006e-01,
         2.8285e-01,  4.0353e-01,  2.3022e-01,  3.9859e-01, -1.2013e-01,
         8.9128e-02, -1.1310e-01,  4.3566e-02,  1.3215e-01,  1.8519e-01,
         2.4647e-01,  3.1267e-01,  1.9418e-01,  1.8869e+00,  2.8814e+00,
         4.4711e+00,  6.7788e-01,  9.8915e-01,  9.0424e-01,  9.8318e-01,
         9.9075e-01,  9.4655e-01,  8.8551e-01,  8.8042e-01,  8.2988e-01,
         7.9592e-01,  7.4582e-01,  7.9976e-01,  9.2118e-01,  5.1731e-01,
         6.0821e-01,  8.6569e-01,  4.8580e-01,  7.3699e-01,  2.9313e-01,
         8.3204e-01,  7.0068e-01,  8.8226e-01,  8.9866e-01,  8.6189e-01,
         1.0240e+00,  1.0630e+00,  5.9906e-01,  9.3703e-01,  1.0115e+00,
         1.1455e+00,  5.9075e-01,  7.7146e-01,  9.3744e-01,  1.0040e+00,
         5.4283e-01,  9.7592e-01,  7.1149e-01,  9.2091e-01,  8.3583e-01,
         4.8285e-01,  7.9821e-01,  8.5950e-01,  8.3720e-01,  9.4153e-01,
         7.9447e-01,  5.3542e-01,  9.5041e-01,  9.5657e-01,  4.9990e-01,
         9.3342e-01,  6.6502e-01,  6.7743e-01,  8.0885e-01,  9.3192e-01,
         9.7782e-01,  6.7239e-01,  6.8575e-01,  8.7472e-01,  8.5504e-01,
         1.0293e+00,  9.3845e-01,  9.3906e-01,  8.7071e-01,  1.0883e+00,
         8.4610e-01,  9.1106e-01,  4.0930e-01,  5.4629e-01,  8.2045e-01,
         9.5317e-01,  9.6514e-01,  7.9015e-01,  6.8532e-01,  5.9346e-01,
         7.8281e-01,  5.8612e-01,  3.5769e-01,  8.4507e-01,  4.8013e-01,
         6.6558e-01,  7.7113e-01,  6.7063e-01,  3.8877e-01,  7.3154e-01,
         3.2501e-01,  8.3973e-01,  6.6680e-01,  8.6294e-01,  7.3244e-01,
         7.2561e-01,  7.5828e-01,  3.3906e-01,  5.9914e-01,  6.7617e-01,
         5.7767e-01,  6.1230e-01,  6.8633e-01,  5.7580e-01,  5.5646e-01,
         6.1460e-01,  7.0082e-01,  7.5701e-01,  4.4062e-01,  6.0020e-01,
         6.9640e-01,  6.0363e-01,  6.8804e-01,  6.1013e-01,  8.3293e-01,
         5.7280e-01,  4.4554e-01,  6.2341e-01,  6.1140e-01,  7.1004e-01,
         6.3289e-01,  6.8834e-01,  1.9456e-01,  8.1110e-01,  3.0233e-01,
         7.3822e-01,  6.2708e-01,  6.6738e-01,  7.3973e-01,  8.1685e-01,
         7.2236e-01,  6.3863e-01,  7.9998e-01,  2.8393e-01,  6.7480e-01,
         4.0769e-01,  6.4700e-01,  5.7676e-01,  4.8564e-01,  1.6155e-01,
         4.9715e-01,  5.3595e-01,  5.7793e-01,  4.2056e-01,  1.8624e-01,
         1.0245e-01,  1.9592e-01,  1.1259e-03,  4.2886e-01,  4.0458e-01,
         4.3021e-01,  3.4995e-01,  2.6820e-01,  3.1937e-01,  3.1376e-01,
        -8.0591e-02,  3.5058e-01,  3.2097e-01,  5.1671e-01,  5.0285e-02,
         1.1424e-01,  1.6607e-01,  3.9674e-01,  3.6962e-01,  1.7389e-01,
        -2.7468e-02,  2.2506e-01,  4.4959e-01,  2.4877e-01,  1.3689e-01,
         1.9407e-01,  1.6173e-01,  1.9034e-01,  2.3045e-01, -1.2814e-02,
         3.7304e-01,  3.8637e-01,  3.0915e-01,  1.4957e-01,  3.1498e-01,
         2.7570e-01,  2.8009e-01,  1.7917e-01,  2.9470e-01,  2.4537e-01,
         1.7680e-01,  1.2570e-02,  5.8389e-03,  2.4310e-01,  1.7543e-01,
         1.9864e-01, -3.0136e-01,  2.5950e-01,  5.1039e-01,  3.1876e-01,
         4.0698e-01,  2.8666e-01,  2.8526e-01,  4.2168e-01,  2.7302e-01,
         3.3914e-01,  1.4225e-01,  2.8264e-01,  3.3615e-01,  3.4400e-01,
        -4.4863e-03,  8.9842e-02, -1.2844e-01, -1.0935e-01,  3.7365e-01,
        -1.4718e-01,  3.6985e-01, -1.2863e-01,  2.7321e-01,  2.8601e-01,
         2.5151e-01,  2.1924e-01,  8.7839e-02,  7.5475e-02,  2.3799e-01,
         1.8508e-01,  3.5339e-01, -8.2214e-02,  5.5435e-02,  1.7775e-01,
         1.4158e-01, -1.1708e-01,  2.7798e-01,  2.0555e-01,  2.8615e-01,
         4.8211e-02,  3.9373e-02,  1.2980e-01,  6.0760e-02, -3.8389e-01,
         6.1226e-02, -4.5141e-02,  7.7304e-02, -7.1518e-02,  8.3345e-02,
         1.5570e-02, -2.6735e-01, -4.1962e-01, -1.6574e-01, -4.6204e-01,
        -2.5851e-01, -3.5683e-01, -1.8801e-01,  5.6698e-02, -9.4935e-02,
         2.5935e-02, -3.5218e-01, -5.1309e-01, -1.6818e-01,  1.0146e-01,
        -3.0270e-01, -4.4323e-01, -4.2676e-01, -2.8277e-01, -2.1038e-01,
        -1.6477e-01, -4.2142e-01, -6.4040e-01, -1.8477e-01, -2.3769e-01,
        -2.6582e-01, -1.9267e-01, -2.3875e-01, -2.4781e-01, -3.8497e-01,
        -3.2161e-01, -5.7186e-01, -8.7545e-01, -5.9949e-01, -7.0097e-01,
        -5.0340e-01, -4.1745e-01, -3.4809e-01, -1.0781e+00, -8.6708e-01,
        -5.1996e-01, -6.7030e-01, -8.6650e-01, -7.1051e-01, -8.1428e-01,
        -7.0292e-01, -7.1084e-01, -8.9149e-01, -9.6625e-01, -9.4173e-01,
        -9.2505e-01, -7.2230e-01, -8.3503e-01, -7.6684e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1617e+00, -9.7855e-01, -1.4671e+00, -2.4643e+00, -2.8282e+00,
        -2.9354e+00, -2.7961e+00, -2.8187e+00, -2.7954e+00, -2.6171e+00,
        -2.5903e+00, -2.6418e+00, -2.6178e+00, -2.4907e+00, -2.2118e+00,
        -2.4219e+00, -2.2739e+00, -2.3530e+00, -2.3365e+00, -2.2448e+00,
        -2.2107e+00, -2.1984e+00, -8.4851e-01, -1.6857e-01, -1.6821e+00,
        -1.7961e+00, -1.5128e+00, -1.6921e+00, -1.5883e+00, -1.3938e+00,
        -1.8759e+00, -1.8673e+00, -1.6922e+00, -1.9301e+00, -1.6615e+00,
        -1.5803e+00, -1.6311e+00, -1.7102e+00, -1.5800e+00, -1.5475e+00,
        -1.5009e+00, -1.4355e+00, -1.3660e+00, -1.5962e+00, -1.7209e+00,
        -1.4782e+00, -1.4058e+00, -1.5857e+00, -1.6878e+00, -1.7308e+00,
        -1.3672e+00, -1.5843e+00, -1.4796e+00, -1.8633e+00, -1.2900e+00,
        -1.2545e+00, -1.0400e+00, -1.5919e+00, -1.4678e+00, -1.3921e+00,
        -1.2804e+00, -1.5337e+00, -1.0789e+00, -1.3590e+00, -1.5253e+00,
        -1.2230e+00,  1.2527e+00,  1.3451e+00,  2.0733e+00,  2.2845e+00,
         4.9549e+00,  1.0181e-02,  2.9750e-01,  1.9030e-01, -5.7464e-02,
         1.7082e-01,  2.1848e-01,  2.7644e-01,  3.1511e-01,  3.1709e-01,
         2.7327e-01,  4.1451e-01,  3.4977e-01,  2.9059e-01,  4.2957e-01,
         1.2861e-01, -3.1886e-02,  1.9303e-01,  4.9431e-01,  2.8837e-01,
         1.5894e-01,  1.3595e-01,  2.4042e-01,  2.7963e-01,  3.8683e-01,
         9.1655e-02,  2.2172e-01,  2.9729e-01,  2.7548e-01,  2.1006e-01,
         2.8285e-01,  4.0353e-01,  2.3022e-01,  3.8576e-01, -1.2013e-01,
         8.8676e-02, -1.1310e-01,  4.3566e-02,  1.3215e-01,  1.8519e-01,
         2.4647e-01,  3.1267e-01,  1.9418e-01,  1.8869e+00,  2.8814e+00,
         4.0062e+00,  6.7788e-01,  9.8915e-01,  9.0424e-01,  9.8318e-01,
         9.9075e-01,  9.4655e-01,  8.8551e-01,  8.8042e-01,  8.2988e-01,
         7.9592e-01,  7.4582e-01,  7.9976e-01,  9.0579e-01,  5.6602e-01,
         6.0821e-01,  8.6569e-01,  4.8580e-01,  7.9688e-01,  2.9313e-01,
         8.3204e-01,  7.0068e-01,  8.8226e-01,  8.9866e-01,  8.6189e-01,
         1.0240e+00,  1.0376e+00,  5.9906e-01,  9.3703e-01,  1.0115e+00,
         1.1363e+00,  5.9075e-01,  7.7146e-01,  9.3744e-01,  1.0040e+00,
         5.4283e-01,  9.7592e-01,  7.1149e-01,  9.2091e-01,  8.7946e-01,
         4.8285e-01,  7.9821e-01,  8.5950e-01,  8.3720e-01,  9.4153e-01,
         7.9447e-01,  5.3542e-01,  9.5041e-01,  9.5657e-01,  4.9990e-01,
         9.3342e-01,  6.6502e-01,  6.7743e-01,  8.0885e-01,  9.3192e-01,
         9.7782e-01,  6.7239e-01,  6.8575e-01,  8.7472e-01,  8.5504e-01,
         1.0293e+00,  9.3845e-01,  9.3906e-01,  8.7071e-01,  1.0883e+00,
         8.4610e-01,  8.8694e-01,  4.0930e-01,  5.4629e-01,  8.2045e-01,
         9.5317e-01,  9.6514e-01,  7.9015e-01,  6.8532e-01,  5.9346e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.4507e-01,  4.8013e-01,
         6.6558e-01,  7.7113e-01,  6.7063e-01,  3.8877e-01,  7.3154e-01,
         3.2501e-01,  8.3973e-01,  6.6680e-01,  8.6294e-01,  7.3244e-01,
         7.2561e-01,  7.5828e-01,  3.3906e-01,  5.9914e-01,  6.7617e-01,
         5.7767e-01,  6.1230e-01,  6.8633e-01,  5.7580e-01,  5.5646e-01,
         6.1460e-01,  7.0082e-01,  7.5701e-01,  4.4062e-01,  6.0020e-01,
         6.9640e-01,  6.0363e-01,  6.8804e-01,  6.1013e-01,  8.3293e-01,
         5.7280e-01,  4.5092e-01,  6.2341e-01,  6.1140e-01,  7.1004e-01,
         6.3289e-01,  6.6277e-01,  1.9456e-01,  8.0634e-01,  3.0233e-01,
         7.3822e-01,  6.2708e-01,  6.6738e-01,  7.3973e-01,  8.1685e-01,
         7.2236e-01,  6.3863e-01,  7.9998e-01,  2.8393e-01,  6.7480e-01,
         4.0769e-01,  6.4700e-01,  5.7676e-01,  4.8564e-01,  1.6155e-01,
         4.9715e-01,  5.3595e-01,  5.7793e-01,  4.2056e-01,  1.8624e-01,
         1.0747e-01,  1.9592e-01,  1.1259e-03,  4.2886e-01,  4.0458e-01,
         4.3021e-01,  3.4995e-01,  2.6820e-01,  3.1937e-01,  3.1219e-01,
        -8.0855e-02,  3.5058e-01,  3.2097e-01,  5.1671e-01,  5.2668e-02,
         1.1424e-01,  1.6607e-01,  3.9674e-01,  3.6962e-01,  1.7389e-01,
        -2.7468e-02,  2.2506e-01,  4.4959e-01,  2.5872e-01,  1.3689e-01,
         1.9407e-01,  1.6173e-01,  1.9034e-01,  2.1334e-01, -1.2814e-02,
         3.7304e-01,  3.8637e-01,  3.0915e-01,  1.4957e-01,  3.1498e-01,
         2.7570e-01,  2.8009e-01,  1.7917e-01,  2.9470e-01,  2.4537e-01,
         1.7680e-01,  1.2570e-02,  5.8389e-03,  2.4310e-01,  1.7543e-01,
         1.9864e-01, -3.0136e-01,  2.5950e-01,  5.1039e-01,  3.1876e-01,
         4.0698e-01,  2.8666e-01,  2.8526e-01,  4.2168e-01,  2.7302e-01,
         3.3914e-01,  1.4225e-01,  2.8264e-01,  3.3615e-01,  3.4823e-01,
        -4.4863e-03,  8.9842e-02, -1.0053e-01, -1.0941e-01,  3.6243e-01,
        -1.4718e-01,  3.6985e-01, -1.2863e-01,  2.7321e-01,  2.8601e-01,
         2.5151e-01,  2.1924e-01,  8.7839e-02,  7.5475e-02,  2.3799e-01,
         1.8508e-01,  3.5339e-01, -8.2214e-02,  5.5435e-02,  1.7775e-01,
         1.4158e-01, -1.1708e-01,  2.7798e-01,  2.0555e-01,  2.8615e-01,
         4.8211e-02,  3.9373e-02,  1.2980e-01,  5.6121e-02, -3.8389e-01,
         6.2027e-02, -4.5141e-02,  7.7304e-02, -7.1518e-02,  8.3345e-02,
         1.5570e-02, -2.6735e-01, -4.3320e-01, -1.6574e-01, -4.6204e-01,
        -2.5851e-01, -3.5683e-01, -1.8801e-01,  5.6698e-02, -9.4935e-02,
         2.6249e-02, -3.5218e-01, -5.1309e-01, -1.6818e-01,  1.0146e-01,
        -3.1689e-01, -4.4323e-01, -4.2676e-01, -2.8277e-01, -2.1038e-01,
        -1.6477e-01, -3.8447e-01, -6.4040e-01, -1.8477e-01, -2.3769e-01,
        -2.6582e-01, -1.9267e-01, -2.3875e-01, -2.4781e-01, -3.8497e-01,
        -3.3317e-01, -5.7186e-01, -8.7545e-01, -5.9353e-01, -7.0097e-01,
        -5.0340e-01, -4.1745e-01, -3.4809e-01, -1.0781e+00, -8.6708e-01,
        -5.1996e-01, -6.7030e-01, -9.2498e-01, -7.1051e-01, -8.1428e-01,
        -7.0292e-01, -7.1084e-01, -9.4607e-01, -9.6625e-01, -9.4173e-01,
        -9.2505e-01, -7.2230e-01, -8.3503e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808933
t6: 1641198808933
state_values: tensor([1.5397, 1.3215, 1.4451, 1.6386, 1.8303, 1.9660, 2.0512, 2.1025, 2.1381,
        2.2015, 2.3114, 2.2561, 2.2665, 2.2711, 2.3365, 2.5703, 2.4732, 2.4023,
        2.4685, 2.4890, 2.5251, 2.5146, 2.2828, 2.1907, 2.3826, 2.4001, 2.4095,
        2.5140, 2.4492, 2.4464, 2.7335, 2.6348, 2.6395, 2.5432, 2.5162, 2.5109,
        2.5163, 2.5156, 2.5680, 2.5381, 2.5754, 2.5649, 2.5649, 2.6953, 2.5987,
        2.5912, 2.5799, 2.6697, 2.6035, 2.5888, 2.6022, 2.6714, 2.6169, 2.7890,
        2.6883, 2.6554, 2.6990, 2.8955, 2.7235, 2.8224, 2.7151, 2.6934, 2.6777,
        2.7233, 2.6953, 2.7075, 2.4534, 2.3853, 2.3253, 2.3506, 2.2572, 2.5025,
        2.6093, 2.6157, 2.6282, 2.7477, 2.7196, 2.7261, 2.7130, 2.7343, 2.7045,
        2.6994, 2.7227, 2.7110, 2.7536, 2.9022, 2.7746, 2.7520, 2.8053, 2.8481,
        2.7826, 2.8780, 2.8175, 2.7886, 2.7705, 2.7772, 2.8883, 2.8470, 2.8260,
        2.8915, 2.8637, 2.8136, 2.8092, 2.8121, 2.8068, 2.8127, 2.8166, 2.8303,
        2.8308, 2.8315, 2.8906, 2.9280, 2.9272, 2.6991, 2.5711, 2.4871, 2.6940,
        2.8363, 2.8104, 2.8451, 2.8353, 2.8260, 2.8391, 2.8360, 2.9441, 2.8813,
        2.9942, 2.9128, 2.8827, 2.8827, 3.1038, 2.9958, 2.9218, 2.9661, 3.2667,
        3.0634, 3.1386, 3.0274, 3.0295, 3.0606, 2.9882, 2.9661, 2.9545, 2.9763,
        2.9945, 2.9931, 2.9604, 2.9623, 3.0606, 2.9889, 3.2450, 3.0660, 3.0182,
        2.9937, 2.9879, 3.2684, 3.1544, 3.1499, 3.0524, 3.0264, 3.0234, 3.0288,
        3.0117, 3.0375, 3.0083, 3.0357, 3.0321, 3.1305, 3.0465, 3.0223, 3.0312,
        3.0343, 3.0358, 3.0786, 3.0414, 3.0231, 3.0244, 3.0664, 3.0584, 3.0449,
        3.0275, 3.1074, 3.0529, 3.0527, 3.0420, 3.0518, 3.0315, 3.1285, 3.2362,
        3.2550, 3.1272, 3.0960, 3.0692, 3.1713, 3.1107, 3.2569, 3.2228, 3.2551,
        3.1617, 3.2110, 3.1558, 3.1294, 3.1283, 3.1426, 3.1986, 3.2457, 3.2159,
        3.1697, 3.1437, 3.2121, 3.3183, 3.1856, 3.1764, 3.2596, 3.3034, 3.1982,
        3.1724, 3.1556, 3.1598, 3.2615, 3.1796, 3.2985, 3.2662, 3.2216, 3.1746,
        3.1680, 3.1703, 3.3072, 3.2013, 3.2283, 3.2372, 3.2592, 3.1972, 3.2324,
        3.1897, 3.2109, 3.1850, 3.1723, 3.2009, 3.1642, 3.1659, 3.1892, 3.1943,
        3.1892, 3.1719, 3.1849, 3.1724, 3.2521, 3.2136, 3.2060, 3.2685, 3.2934,
        3.2702, 3.2976, 3.4566, 3.2931, 3.5003, 3.3191, 3.3208, 3.3405, 3.3483,
        3.3871, 3.4623, 3.3836, 3.3961, 3.3229, 3.3898, 3.3093, 3.3597, 3.2977,
        3.5003, 3.3290, 3.3271, 3.3413, 3.5047, 3.3561, 3.3231, 3.3395, 3.2987,
        3.5366, 3.4595, 3.5069, 3.3950, 3.3373, 3.3057, 3.3023, 3.3009, 3.3033,
        3.3033, 3.3671, 3.3199, 3.3698, 3.3231, 3.4025, 3.4027, 3.3381, 3.3303,
        3.3176, 3.3046, 3.3209, 3.3196, 3.4328, 3.4631, 3.3691, 3.3454, 3.3327,
        3.3361, 3.4701, 3.3963, 3.5082, 3.4231, 3.5507, 3.4519, 3.4039, 3.4302,
        3.6509, 3.4528, 3.3923, 3.3755, 3.4584, 3.3985, 3.4598, 3.3962, 3.3832,
        3.3704, 3.3697, 3.3605, 3.5414, 3.4299, 3.5399, 3.4270, 3.3802, 3.3980,
        3.3887, 3.4821, 3.4106, 3.3880, 3.3729, 3.3976, 3.3976, 3.5169, 3.5311,
        3.4524, 3.4533, 3.4154, 3.4009, 3.5649, 3.4534, 3.4251, 3.4399, 3.4311,
        3.5956, 3.4664, 3.5954, 3.7789, 3.5452, 3.4976, 3.4762, 3.4491, 3.4644,
        3.4302, 3.6746, 3.5125, 3.4573, 3.4194, 3.4403, 3.6918, 3.6965, 3.5711,
        3.5925, 3.5211, 3.4843, 3.4675, 3.4677, 3.4568, 3.4537, 3.5261, 3.5071,
        3.5059, 3.5560, 3.4933, 3.7120, 3.5398, 3.5024, 3.4874, 3.4848, 3.5219,
        3.5381, 3.8413, 3.5946, 3.5921, 3.6050, 3.5374, 3.6704, 3.7332, 3.6468,
        3.5763, 3.5550, 3.7218, 3.7184, 3.6867, 3.5710, 3.6282, 3.6018],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808938
t8: 1641198808938
t9: 1641198808938
t10: 1641198808949
t11: 1641198808950
t12: 1641198808950
t1: 1641198808950
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198808961
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9983, 0.9955, 0.9911, 0.9979, 1.0401, 0.9957, 1.0067, 1.0289, 0.9863,
        0.9771, 0.9839, 0.9924, 1.0573, 0.9562, 0.8939, 0.9267, 0.9490, 0.8938,
        0.9681, 0.9708, 0.9984, 1.0067, 1.0048, 0.9516, 1.0302, 0.9944, 0.9013,
        1.0010, 0.9916, 0.8074, 0.9079, 1.0548, 0.9704, 1.0170, 1.0285, 0.9895,
        1.1050, 0.9346, 1.0315, 0.9696, 0.9851, 0.9955, 0.8956, 0.9833, 0.9832,
        1.0251, 0.9058, 1.0182, 1.1129, 0.9775, 0.9464, 1.0280, 0.9534, 0.9883,
        1.0394, 0.9356, 0.8832, 1.0708, 0.9128, 0.9917, 1.0370, 0.9920, 0.9686,
        1.0891, 0.9705, 1.0417, 1.1086, 1.0620, 0.9616, 0.9303, 1.0756, 0.9659,
        1.0352, 1.1094, 0.8696, 0.9990, 0.9853, 0.9869, 0.9713, 1.0274, 0.9953,
        0.9859, 1.0234, 0.9443, 0.9105, 0.9868, 1.0345, 0.8903, 0.9797, 1.0108,
        0.8984, 1.0010, 1.0081, 1.0278, 1.0886, 0.8831, 0.9970, 0.9962, 0.9337,
        0.9881, 1.0104, 1.0511, 0.9978, 1.1555, 1.0924, 1.1261, 1.0586, 1.0336,
        1.0210, 0.9733, 0.9697, 0.9945, 1.0287, 1.1015, 1.0949, 1.2562, 0.9180,
        1.0118, 0.9825, 0.9976, 1.0137, 1.0059, 1.0077, 0.9263, 1.0052, 0.9171,
        0.9998, 1.0470, 1.1315, 0.7980, 0.9988, 1.0208, 0.9226, 0.8256, 0.9759,
        1.0051, 1.0007, 0.9899, 0.9688, 1.0004, 1.0182, 1.1415, 0.9755, 0.9738,
        0.9852, 1.1237, 1.0640, 0.9100, 1.0007, 0.9092, 0.9992, 1.0164, 1.0503,
        1.0133, 0.8379, 0.9785, 1.0101, 1.0067, 1.0130, 1.0612, 1.1119, 1.0014,
        0.9892, 1.0958, 0.9726, 1.0299, 0.9471, 1.0224, 1.0119, 0.9937, 1.0498,
        1.0395, 0.9602, 1.0095, 0.9982, 1.0008, 0.9695, 0.9932, 0.9811, 1.0291,
        0.9378, 1.1392, 1.1111, 1.0361, 0.9908, 1.0045, 0.9574, 0.9537, 1.0048,
        1.0005, 1.1223, 1.3987, 0.8612, 1.0465, 0.8983, 1.0042, 0.9945, 1.0318,
        0.9226, 1.0952, 0.9963, 1.0373, 0.9725, 0.9584, 0.9520, 0.9915, 1.0677,
        1.0581, 0.9233, 0.9244, 1.0029, 0.9875, 0.9306, 0.9576, 1.0109, 1.0105,
        1.0365, 1.0868, 0.9110, 1.0049, 0.9294, 0.9953, 0.9980, 1.0001, 1.0694,
        1.0804, 0.8720, 1.0004, 0.9691, 0.9752, 0.9688, 1.1506, 0.9283, 1.1153,
        0.9708, 1.0345, 1.0285, 0.9807, 1.0067, 1.0052, 0.9952, 0.9843, 1.0726,
        1.0287, 1.0623, 1.0121, 0.9655, 1.0161, 1.0785, 0.9413, 0.9663, 0.9858,
        0.9717, 0.8995, 0.9834, 0.8365, 1.0003, 0.9812, 0.9707, 0.9673, 0.9538,
        0.9558, 0.9998, 0.9739, 1.1131, 0.8826, 1.0318, 0.9397, 1.0851, 0.8367,
        0.9852, 0.9881, 0.9730, 0.8992, 0.9786, 1.0272, 0.9599, 1.0535, 0.8444,
        0.9940, 1.0013, 1.0001, 1.0390, 1.2153, 1.0230, 1.0067, 1.0155, 1.0476,
        0.9564, 1.0123, 0.9726, 1.0340, 0.9274, 0.9808, 1.0238, 1.0767, 1.0963,
        1.0231, 1.0014, 1.0010, 1.0345, 0.9285, 0.9999, 1.0309, 1.0325, 1.0414,
        0.9079, 0.9983, 0.9397, 1.0003, 0.9270, 1.0017, 0.9970, 0.9580, 0.8824,
        0.9925, 1.1009, 1.4710, 0.8843, 1.1385, 0.9313, 1.0882, 1.0094, 1.0301,
        1.0295, 1.0408, 0.9069, 0.9982, 0.9228, 0.9927, 1.0198, 1.1028, 1.0817,
        0.9219, 1.0238, 1.0879, 1.0161, 0.9949, 0.9901, 0.9466, 0.9853, 0.9981,
        0.9853, 1.2051, 1.0430, 0.8740, 0.9997, 1.0256, 0.9689, 0.9981, 0.9039,
        1.0072, 0.8530, 1.0205, 1.0051, 1.0484, 1.0297, 0.9928, 0.9967, 1.0064,
        0.8746, 0.9972, 1.0324, 1.0072, 1.0772, 0.8379, 0.9931, 1.0013, 0.9442,
        0.9986, 1.0380, 1.2350, 0.9884, 1.0246, 1.0279, 0.9458, 0.9907, 0.9840,
        0.9543, 1.0073, 0.8556, 0.9999, 1.0416, 1.1232, 1.0499, 0.9712, 0.9737,
        0.8960, 1.0925, 0.9723, 0.9761, 1.0808, 0.8225, 1.0108, 1.0039, 0.9956,
        1.0657, 0.8288, 1.0104, 1.0105, 1.0078, 0.9522, 0.9876, 0.8830],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198808966
t4: 1641198808966
surr1, surr2: tensor([-3.1561e+00, -9.7646e-01, -1.4647e+00, -2.4701e+00, -2.8564e+00,
        -2.9318e+00, -2.8016e+00, -2.8462e+00, -2.7858e+00, -2.6026e+00,
        -2.5770e+00, -2.6369e+00, -2.6318e+00, -2.4722e+00, -2.1922e+00,
        -2.4140e+00, -2.2738e+00, -2.3140e+00, -2.3345e+00, -2.2460e+00,
        -2.2138e+00, -2.1998e+00, -8.4859e-01, -1.6805e-01, -1.6845e+00,
        -1.7942e+00, -1.4976e+00, -1.6915e+00, -1.5865e+00, -1.2504e+00,
        -1.8679e+00, -1.8593e+00, -1.6843e+00, -1.9351e+00, -1.6663e+00,
        -1.5781e+00, -1.6443e+00, -1.6912e+00, -1.5839e+00, -1.5391e+00,
        -1.4984e+00, -1.4344e+00, -1.3468e+00, -1.5914e+00, -1.7155e+00,
        -1.4802e+00, -1.3902e+00, -1.5826e+00, -1.7089e+00, -1.7239e+00,
        -1.3582e+00, -1.5866e+00, -1.4684e+00, -1.8612e+00, -1.2951e+00,
        -1.2420e+00, -1.0206e+00, -1.5980e+00, -1.4319e+00, -1.3898e+00,
        -1.2905e+00, -1.5310e+00, -1.0760e+00, -1.3678e+00, -1.5176e+00,
        -1.2280e+00,  1.2696e+00,  1.3568e+00,  2.0615e+00,  2.2741e+00,
         5.0042e+00,  1.0117e-02,  2.9783e-01,  1.9267e-01, -5.5525e-02,
         1.7100e-01,  2.1821e-01,  2.7624e-01,  3.1447e-01,  3.1750e-01,
         2.7301e-01,  4.1378e-01,  3.5056e-01,  2.8832e-01,  4.2712e-01,
         1.2870e-01, -3.2052e-02,  1.9095e-01,  4.9591e-01,  2.8811e-01,
         1.5692e-01,  1.3594e-01,  2.4058e-01,  2.8018e-01,  3.9036e-01,
         8.9934e-02,  2.2194e-01,  2.9717e-01,  2.7409e-01,  2.1006e-01,
         2.8302e-01,  4.0503e-01,  2.3003e-01,  4.0523e-01, -1.2170e-01,
         9.0783e-02, -1.1416e-01,  4.3829e-02,  1.3268e-01,  1.8416e-01,
         2.4551e-01,  3.1295e-01,  1.9470e-01,  1.9081e+00,  2.9136e+00,
         4.5751e+00,  6.6841e-01,  9.8952e-01,  9.0175e-01,  9.8285e-01,
         9.9215e-01,  9.4698e-01,  8.8612e-01,  8.7169e-01,  8.2913e-01,
         7.8691e-01,  7.4436e-01,  8.0341e-01,  9.3177e-01,  5.0186e-01,
         6.0771e-01,  8.7075e-01,  4.8098e-01,  7.3096e-01,  2.9289e-01,
         8.2133e-01,  7.0076e-01,  8.7928e-01,  8.9561e-01,  8.6190e-01,
         1.0262e+00,  1.0768e+00,  5.9647e-01,  9.3388e-01,  1.0098e+00,
         1.1607e+00,  5.9608e-01,  7.6017e-01,  9.3740e-01,  9.9117e-01,
         5.4275e-01,  9.8222e-01,  7.1532e-01,  9.2173e-01,  8.1882e-01,
         4.8178e-01,  7.9647e-01,  8.5817e-01,  8.3861e-01,  9.4691e-01,
         8.0420e-01,  5.3520e-01,  9.4885e-01,  9.6825e-01,  4.9731e-01,
         9.3671e-01,  6.5873e-01,  6.7820e-01,  8.1018e-01,  9.3092e-01,
         9.8356e-01,  6.7621e-01,  6.8127e-01,  8.7551e-01,  8.5466e-01,
         1.0292e+00,  9.3447e-01,  9.3837e-01,  8.6863e-01,  1.0911e+00,
         8.3897e-01,  9.1851e-01,  4.1550e-01,  5.4874e-01,  8.1918e-01,
         9.5363e-01,  9.5899e-01,  7.8913e-01,  6.8645e-01,  5.9305e-01,
         7.9263e-01,  6.0604e-01,  3.5044e-01,  8.4435e-01,  4.7225e-01,
         6.6661e-01,  7.7216e-01,  6.6932e-01,  3.8469e-01,  7.3387e-01,
         3.2462e-01,  8.4274e-01,  6.6400e-01,  8.5922e-01,  7.3018e-01,
         7.2664e-01,  7.6149e-01,  3.4122e-01,  5.9294e-01,  6.7324e-01,
         5.7653e-01,  6.1107e-01,  6.8322e-01,  5.7584e-01,  5.5595e-01,
         6.1508e-01,  7.0255e-01,  7.6427e-01,  4.3454e-01,  6.0029e-01,
         6.8975e-01,  6.0499e-01,  6.8795e-01,  6.1001e-01,  8.3685e-01,
         5.7829e-01,  4.3687e-01,  6.2243e-01,  6.0887e-01,  7.0934e-01,
         6.3168e-01,  6.9327e-01,  1.9236e-01,  8.1757e-01,  3.0084e-01,
         7.4072e-01,  6.2958e-01,  6.6533e-01,  7.4023e-01,  8.1728e-01,
         7.2175e-01,  6.3721e-01,  8.0627e-01,  2.8499e-01,  6.7995e-01,
         4.0833e-01,  6.4359e-01,  5.7761e-01,  4.9107e-01,  1.5999e-01,
         4.9552e-01,  5.3546e-01,  5.7639e-01,  4.1822e-01,  1.8522e-01,
         9.9891e-02,  1.9815e-01,  1.1206e-03,  4.2797e-01,  4.0361e-01,
         4.2943e-01,  3.5017e-01,  2.6834e-01,  3.1808e-01,  3.1591e-01,
        -7.9295e-02,  3.5061e-01,  3.1864e-01,  5.1697e-01,  4.8963e-02,
         1.1393e-01,  1.6570e-01,  3.9602e-01,  3.6727e-01,  1.7330e-01,
        -2.7590e-02,  2.2412e-01,  4.5090e-01,  2.4275e-01,  1.3657e-01,
         1.9362e-01,  1.6164e-01,  1.9157e-01,  2.3571e-01, -1.2841e-02,
         3.7325e-01,  3.8713e-01,  3.1121e-01,  1.4841e-01,  3.1534e-01,
         2.7446e-01,  2.8102e-01,  1.7705e-01,  2.9506e-01,  2.4560e-01,
         1.7838e-01,  1.2720e-02,  5.8556e-03,  2.4309e-01,  1.7543e-01,
         1.9959e-01, -2.9769e-01,  2.5949e-01,  5.1237e-01,  3.2005e-01,
         4.0897e-01,  2.8252e-01,  2.8539e-01,  4.1847e-01,  2.7315e-01,
         3.3667e-01,  1.4230e-01,  2.8247e-01,  3.3480e-01,  3.4144e-01,
        -4.5040e-03,  9.2284e-02, -1.3444e-01, -1.0750e-01,  3.7510e-01,
        -1.4544e-01,  3.7046e-01, -1.2877e-01,  2.7416e-01,  2.8720e-01,
         2.5303e-01,  2.1565e-01,  8.7700e-02,  7.4394e-02,  2.3740e-01,
         1.8552e-01,  3.5603e-01, -8.3003e-02,  5.4770e-02,  1.7791e-01,
         1.4349e-01, -1.1729e-01,  2.7773e-01,  2.0525e-01,  2.8420e-01,
         4.8394e-02,  3.9377e-02,  1.2963e-01,  6.1485e-02, -3.8570e-01,
         6.0236e-02, -4.5126e-02,  7.7526e-02, -7.1221e-02,  8.3314e-02,
         1.5402e-02, -2.6569e-01, -4.1058e-01, -1.6715e-01, -4.6242e-01,
        -2.6120e-01, -3.5784e-01, -1.8783e-01,  5.6667e-02, -9.4985e-02,
         2.5509e-02, -3.5330e-01, -5.1623e-01, -1.6822e-01,  1.0221e-01,
        -2.9501e-01, -4.4108e-01, -4.2686e-01, -2.8068e-01, -2.1040e-01,
        -1.6554e-01, -4.3166e-01, -6.3874e-01, -1.8525e-01, -2.3856e-01,
        -2.6346e-01, -1.9258e-01, -2.3828e-01, -2.4664e-01, -3.8502e-01,
        -3.1674e-01, -5.7571e-01, -8.8269e-01, -6.0603e-01, -7.0497e-01,
        -5.0117e-01, -4.1620e-01, -3.4483e-01, -1.0821e+00, -8.5874e-01,
        -5.1974e-01, -6.7464e-01, -8.4537e-01, -7.1347e-01, -8.1550e-01,
        -7.0247e-01, -7.1566e-01, -8.7123e-01, -9.6812e-01, -9.4382e-01,
        -9.2554e-01, -7.1788e-01, -8.3444e-01, -7.5745e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1561e+00, -9.7646e-01, -1.4647e+00, -2.4701e+00, -2.8564e+00,
        -2.9318e+00, -2.8016e+00, -2.8462e+00, -2.7858e+00, -2.6026e+00,
        -2.5770e+00, -2.6369e+00, -2.6318e+00, -2.4722e+00, -2.2071e+00,
        -2.4140e+00, -2.2738e+00, -2.3300e+00, -2.3345e+00, -2.2460e+00,
        -2.2138e+00, -2.1998e+00, -8.4859e-01, -1.6805e-01, -1.6845e+00,
        -1.7942e+00, -1.4976e+00, -1.6915e+00, -1.5865e+00, -1.3938e+00,
        -1.8679e+00, -1.8593e+00, -1.6843e+00, -1.9351e+00, -1.6663e+00,
        -1.5781e+00, -1.6369e+00, -1.6912e+00, -1.5839e+00, -1.5391e+00,
        -1.4984e+00, -1.4344e+00, -1.3534e+00, -1.5914e+00, -1.7155e+00,
        -1.4802e+00, -1.3902e+00, -1.5826e+00, -1.6891e+00, -1.7239e+00,
        -1.3582e+00, -1.5866e+00, -1.4684e+00, -1.8612e+00, -1.2951e+00,
        -1.2420e+00, -1.0400e+00, -1.5980e+00, -1.4319e+00, -1.3898e+00,
        -1.2905e+00, -1.5310e+00, -1.0760e+00, -1.3678e+00, -1.5176e+00,
        -1.2280e+00,  1.2598e+00,  1.3568e+00,  2.0615e+00,  2.2741e+00,
         5.0042e+00,  1.0117e-02,  2.9783e-01,  1.9103e-01, -5.7464e-02,
         1.7100e-01,  2.1821e-01,  2.7624e-01,  3.1447e-01,  3.1750e-01,
         2.7301e-01,  4.1378e-01,  3.5056e-01,  2.8832e-01,  4.2712e-01,
         1.2870e-01, -3.2052e-02,  1.9303e-01,  4.9591e-01,  2.8811e-01,
         1.5720e-01,  1.3594e-01,  2.4058e-01,  2.8018e-01,  3.9036e-01,
         9.1655e-02,  2.2194e-01,  2.9717e-01,  2.7409e-01,  2.1006e-01,
         2.8302e-01,  4.0503e-01,  2.3003e-01,  3.8576e-01, -1.2170e-01,
         8.8676e-02, -1.1416e-01,  4.3829e-02,  1.3268e-01,  1.8416e-01,
         2.4551e-01,  3.1295e-01,  1.9470e-01,  1.9055e+00,  2.9136e+00,
         4.0062e+00,  6.6841e-01,  9.8952e-01,  9.0175e-01,  9.8285e-01,
         9.9215e-01,  9.4698e-01,  8.8612e-01,  8.7169e-01,  8.2913e-01,
         7.8691e-01,  7.4436e-01,  8.0341e-01,  9.0579e-01,  5.6602e-01,
         6.0771e-01,  8.7075e-01,  4.8098e-01,  7.9688e-01,  2.9289e-01,
         8.2133e-01,  7.0076e-01,  8.7928e-01,  8.9561e-01,  8.6190e-01,
         1.0262e+00,  1.0376e+00,  5.9647e-01,  9.3388e-01,  1.0098e+00,
         1.1363e+00,  5.9608e-01,  7.6017e-01,  9.3740e-01,  9.9117e-01,
         5.4275e-01,  9.8222e-01,  7.1532e-01,  9.2173e-01,  8.7946e-01,
         4.8178e-01,  7.9647e-01,  8.5817e-01,  8.3861e-01,  9.4691e-01,
         7.9559e-01,  5.3520e-01,  9.4885e-01,  9.6825e-01,  4.9731e-01,
         9.3671e-01,  6.5873e-01,  6.7820e-01,  8.1018e-01,  9.3092e-01,
         9.8356e-01,  6.7621e-01,  6.8127e-01,  8.7551e-01,  8.5466e-01,
         1.0292e+00,  9.3447e-01,  9.3837e-01,  8.6863e-01,  1.0911e+00,
         8.3897e-01,  8.8694e-01,  4.1136e-01,  5.4874e-01,  8.1918e-01,
         9.5363e-01,  9.5899e-01,  7.8913e-01,  6.8645e-01,  5.9305e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.4435e-01,  4.7313e-01,
         6.6661e-01,  7.7216e-01,  6.6932e-01,  3.8469e-01,  7.3387e-01,
         3.2462e-01,  8.4274e-01,  6.6400e-01,  8.5922e-01,  7.3018e-01,
         7.2664e-01,  7.6149e-01,  3.4122e-01,  5.9294e-01,  6.7324e-01,
         5.7653e-01,  6.1107e-01,  6.8322e-01,  5.7584e-01,  5.5595e-01,
         6.1508e-01,  7.0255e-01,  7.6427e-01,  4.3454e-01,  6.0029e-01,
         6.8975e-01,  6.0499e-01,  6.8795e-01,  6.1001e-01,  8.3685e-01,
         5.7829e-01,  4.5092e-01,  6.2243e-01,  6.0887e-01,  7.0934e-01,
         6.3168e-01,  6.6277e-01,  1.9236e-01,  8.0634e-01,  3.0084e-01,
         7.4072e-01,  6.2958e-01,  6.6533e-01,  7.4023e-01,  8.1728e-01,
         7.2175e-01,  6.3721e-01,  8.0627e-01,  2.8499e-01,  6.7995e-01,
         4.0833e-01,  6.4359e-01,  5.7761e-01,  4.9107e-01,  1.5999e-01,
         4.9552e-01,  5.3546e-01,  5.7639e-01,  4.1844e-01,  1.8522e-01,
         1.0747e-01,  1.9815e-01,  1.1206e-03,  4.2797e-01,  4.0361e-01,
         4.2943e-01,  3.5017e-01,  2.6834e-01,  3.1808e-01,  3.1219e-01,
        -8.0855e-02,  3.5061e-01,  3.1864e-01,  5.1697e-01,  5.2668e-02,
         1.1393e-01,  1.6570e-01,  3.9602e-01,  3.6760e-01,  1.7330e-01,
        -2.7590e-02,  2.2412e-01,  4.5090e-01,  2.5872e-01,  1.3657e-01,
         1.9362e-01,  1.6164e-01,  1.9157e-01,  2.1334e-01, -1.2841e-02,
         3.7325e-01,  3.8713e-01,  3.1121e-01,  1.4841e-01,  3.1534e-01,
         2.7446e-01,  2.8102e-01,  1.7705e-01,  2.9506e-01,  2.4560e-01,
         1.7838e-01,  1.2720e-02,  5.8556e-03,  2.4309e-01,  1.7543e-01,
         1.9959e-01, -2.9769e-01,  2.5949e-01,  5.1237e-01,  3.2005e-01,
         4.0897e-01,  2.8252e-01,  2.8539e-01,  4.1847e-01,  2.7315e-01,
         3.3667e-01,  1.4230e-01,  2.8247e-01,  3.3480e-01,  3.4823e-01,
        -4.5040e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.6243e-01,
        -1.4544e-01,  3.7046e-01, -1.2877e-01,  2.7416e-01,  2.8720e-01,
         2.5303e-01,  2.1565e-01,  8.7700e-02,  7.4394e-02,  2.3740e-01,
         1.8552e-01,  3.5511e-01, -8.3003e-02,  5.4770e-02,  1.7791e-01,
         1.4349e-01, -1.1729e-01,  2.7773e-01,  2.0525e-01,  2.8420e-01,
         4.8394e-02,  3.9377e-02,  1.2963e-01,  5.6121e-02, -3.8570e-01,
         6.2027e-02, -4.5126e-02,  7.7526e-02, -7.1221e-02,  8.3314e-02,
         1.5402e-02, -2.6569e-01, -4.3320e-01, -1.6715e-01, -4.6242e-01,
        -2.6120e-01, -3.5784e-01, -1.8783e-01,  5.6667e-02, -9.4985e-02,
         2.6249e-02, -3.5330e-01, -5.1623e-01, -1.6822e-01,  1.0221e-01,
        -3.1689e-01, -4.4108e-01, -4.2686e-01, -2.8068e-01, -2.1040e-01,
        -1.6554e-01, -3.8447e-01, -6.3874e-01, -1.8525e-01, -2.3856e-01,
        -2.6346e-01, -1.9258e-01, -2.3828e-01, -2.4664e-01, -3.8502e-01,
        -3.3317e-01, -5.7571e-01, -8.8269e-01, -5.9353e-01, -7.0497e-01,
        -5.0117e-01, -4.1620e-01, -3.4637e-01, -1.0821e+00, -8.5874e-01,
        -5.1974e-01, -6.7464e-01, -9.2498e-01, -7.1347e-01, -8.1550e-01,
        -7.0247e-01, -7.1566e-01, -9.4607e-01, -9.6812e-01, -9.4382e-01,
        -9.2554e-01, -7.1788e-01, -8.3444e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198808975
t6: 1641198808975
state_values: tensor([1.6750, 1.4576, 1.5878, 1.7892, 1.9936, 2.1427, 2.2408, 2.2993, 2.3392,
        2.4124, 2.5306, 2.4778, 2.4935, 2.5008, 2.5745, 2.8315, 2.7279, 2.6515,
        2.7269, 2.7518, 2.7942, 2.7843, 2.5386, 2.4468, 2.6482, 2.6691, 2.6819,
        2.7938, 2.7290, 2.7278, 3.0444, 2.9373, 2.9448, 2.8335, 2.8093, 2.8058,
        2.8131, 2.8120, 2.8685, 2.8395, 2.8799, 2.8705, 2.8718, 3.0200, 2.9087,
        2.9032, 2.8921, 2.9933, 2.9199, 2.9030, 2.9206, 2.9979, 2.9386, 3.1362,
        3.0200, 2.9838, 3.0340, 3.2558, 3.0609, 3.1799, 3.0544, 3.0289, 3.0141,
        3.0653, 3.0329, 3.0488, 2.7760, 2.7032, 2.6436, 2.6710, 2.5773, 2.8288,
        2.9475, 2.9555, 2.9685, 3.1022, 3.0722, 3.0806, 3.0671, 3.0913, 3.0589,
        3.0542, 3.0803, 3.0678, 3.1157, 3.2838, 3.1355, 3.1142, 3.1743, 3.2237,
        3.1482, 3.2582, 3.1891, 3.1563, 3.1372, 3.1433, 3.2706, 3.2241, 3.2001,
        3.2753, 3.2441, 3.1864, 3.1814, 3.1854, 3.1768, 3.1842, 3.1875, 3.2045,
        3.2059, 3.2072, 3.2757, 3.3198, 3.3186, 3.0687, 2.9276, 2.8350, 3.0606,
        3.2137, 3.1861, 3.2258, 3.2151, 3.2052, 3.2201, 3.2171, 3.3411, 3.2690,
        3.3991, 3.3057, 3.2709, 3.2695, 3.5189, 3.4021, 3.3141, 3.3674, 3.6930,
        3.4791, 3.5614, 3.4397, 3.4427, 3.4768, 3.3940, 3.3682, 3.3523, 3.3793,
        3.4011, 3.3997, 3.3586, 3.3625, 3.4775, 3.3944, 3.6758, 3.4852, 3.4296,
        3.4000, 3.3936, 3.7007, 3.5825, 3.5789, 3.4710, 3.4414, 3.4371, 3.4420,
        3.4225, 3.4542, 3.4161, 3.4521, 3.4478, 3.5586, 3.4652, 3.4365, 3.4478,
        3.4506, 3.4530, 3.5037, 3.4607, 3.4384, 3.4402, 3.4910, 3.4817, 3.4664,
        3.4443, 3.5376, 3.4731, 3.4735, 3.4617, 3.4745, 3.4495, 3.5619, 3.6771,
        3.6973, 3.5621, 3.5246, 3.4929, 3.6091, 3.5417, 3.6992, 3.6658, 3.6987,
        3.5989, 3.6533, 3.5919, 3.5646, 3.5634, 3.5803, 3.6408, 3.6898, 3.6602,
        3.6085, 3.5815, 3.6557, 3.7643, 3.6283, 3.6188, 3.7050, 3.7498, 3.6429,
        3.6149, 3.5964, 3.6000, 3.7071, 3.6231, 3.7459, 3.7138, 3.6684, 3.6190,
        3.6110, 3.6133, 3.7553, 3.6476, 3.6764, 3.6855, 3.7075, 3.6415, 3.6804,
        3.6340, 3.6581, 3.6309, 3.6171, 3.6487, 3.6083, 3.6104, 3.6365, 3.6427,
        3.6349, 3.6172, 3.6311, 3.6180, 3.7009, 3.6622, 3.6523, 3.7174, 3.7437,
        3.7212, 3.7484, 3.9185, 3.7418, 3.9679, 3.7686, 3.7732, 3.7935, 3.8020,
        3.8436, 3.9275, 3.8407, 3.8540, 3.7732, 3.8463, 3.7611, 3.8141, 3.7470,
        3.9682, 3.7800, 3.7799, 3.7954, 3.9741, 3.8081, 3.7756, 3.7936, 3.7503,
        4.0099, 3.9244, 3.9789, 3.8547, 3.7917, 3.7562, 3.7544, 3.7534, 3.7558,
        3.7551, 3.8221, 3.7732, 3.8253, 3.7766, 3.8620, 3.8625, 3.7931, 3.7841,
        3.7704, 3.7579, 3.7754, 3.7741, 3.8932, 3.9289, 3.8260, 3.8011, 3.7881,
        3.7915, 3.9377, 3.8567, 3.9815, 3.8870, 4.0294, 3.9197, 3.8663, 3.8959,
        4.1429, 3.9203, 3.8509, 3.8312, 3.9255, 3.8562, 3.9269, 3.8535, 3.8410,
        3.8271, 3.8265, 3.8168, 4.0180, 3.8933, 4.0175, 3.8909, 3.8391, 3.8569,
        3.8473, 3.9515, 3.8720, 3.8450, 3.8303, 3.8575, 3.8579, 3.9910, 4.0072,
        3.9198, 3.9208, 3.8759, 3.8616, 4.0457, 3.9211, 3.8892, 3.9060, 3.8959,
        4.0807, 3.9333, 4.0807, 4.2839, 4.0257, 3.9715, 3.9478, 3.9173, 3.9341,
        3.8951, 4.1685, 3.9855, 3.9252, 3.8828, 3.9048, 4.1869, 4.1936, 4.0549,
        4.0794, 3.9991, 3.9568, 3.9352, 3.9371, 3.9247, 3.9209, 4.0026, 3.9818,
        3.9808, 4.0362, 3.9664, 4.2106, 4.0165, 3.9765, 3.9579, 3.9562, 3.9984,
        4.0173, 4.3521, 4.0794, 4.0792, 4.0927, 4.0143, 4.1648, 4.2351, 4.1399,
        4.0617, 4.0351, 4.2213, 4.2186, 4.1834, 4.0552, 4.1183, 4.0897],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198808980
t8: 1641198808980
t9: 1641198808981
t10: 1641198808992
t11: 1641198808993
t12: 1641198808993
t1: 1641198808994
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809005
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9976, 0.9988, 0.9812, 0.9956, 1.0322, 0.9954, 1.0042, 1.0213, 0.9856,
        0.9857, 0.9743, 0.9925, 1.0506, 0.9551, 0.9139, 0.9161, 0.9399, 0.8877,
        0.9795, 0.9877, 1.0065, 1.0024, 1.0048, 0.9568, 1.0235, 0.9945, 0.9098,
        0.9995, 0.9928, 0.8273, 0.8971, 1.0600, 0.9523, 1.0177, 1.0268, 0.9902,
        1.0936, 0.9311, 1.0271, 0.9700, 0.9869, 0.9960, 0.9010, 0.9824, 0.9828,
        1.0225, 0.9109, 1.0071, 1.1091, 0.9758, 0.9497, 1.0210, 0.9529, 0.9896,
        1.0364, 0.9340, 0.9042, 1.0877, 0.9048, 0.9879, 1.0278, 0.9915, 0.9716,
        1.0759, 0.9689, 1.0383, 1.1093, 1.0645, 0.9618, 0.9421, 1.1028, 0.9640,
        1.0292, 1.1069, 0.8649, 1.0010, 0.9896, 0.9891, 0.9749, 1.0239, 0.9955,
        0.9867, 1.0219, 0.9448, 0.9293, 0.9952, 1.0360, 0.8942, 0.9949, 1.0008,
        0.8985, 1.0003, 1.0072, 1.0264, 1.0843, 0.8785, 1.0001, 0.9970, 0.9393,
        0.9918, 1.0093, 1.0481, 0.9976, 1.1500, 1.0980, 1.1300, 1.0611, 1.0336,
        1.0203, 0.9754, 0.9782, 1.0013, 1.0233, 1.1035, 1.0988, 1.2490, 0.9138,
        1.0067, 0.9844, 0.9981, 1.0127, 1.0058, 1.0075, 0.9290, 1.0012, 0.9233,
        0.9935, 1.0445, 1.1307, 0.7905, 0.9956, 1.0040, 0.9190, 0.8490, 0.9727,
        1.0099, 1.0009, 0.9936, 0.9768, 1.0003, 1.0166, 1.1348, 0.9737, 0.9754,
        0.9871, 1.1104, 1.0681, 0.9083, 1.0003, 0.9212, 0.9986, 1.0107, 1.0512,
        1.0131, 0.8435, 0.9713, 1.0199, 0.9970, 1.0127, 1.0577, 1.1121, 1.0012,
        0.9899, 1.0856, 0.9707, 1.0269, 0.9477, 1.0159, 1.0118, 0.9940, 1.0467,
        1.0406, 0.9598, 1.0083, 0.9984, 1.0009, 0.9711, 0.9942, 0.9825, 1.0270,
        0.9382, 1.1123, 1.1182, 1.0373, 0.9912, 1.0040, 0.9614, 0.9717, 1.0065,
        0.9974, 1.1204, 1.4096, 0.8536, 1.0286, 0.8990, 1.0061, 1.0084, 1.0125,
        0.9207, 1.0764, 0.9959, 1.0359, 0.9729, 0.9628, 0.9618, 0.9983, 1.0571,
        1.0594, 0.9225, 0.9379, 0.9958, 0.9875, 0.9376, 0.9708, 1.0037, 1.0103,
        1.0345, 1.0863, 0.9076, 1.0037, 0.9346, 1.0022, 0.9987, 1.0002, 1.0643,
        1.0832, 0.8679, 0.9955, 0.9714, 0.9792, 0.9731, 1.1293, 0.9232, 1.1026,
        0.9696, 1.0314, 1.0281, 0.9812, 1.0061, 1.0049, 0.9955, 0.9853, 1.0656,
        1.0298, 1.0612, 1.0123, 0.9676, 1.0129, 1.0762, 0.9385, 0.9715, 0.9899,
        0.9753, 0.9141, 0.9757, 0.8325, 1.0310, 0.9811, 0.9758, 0.9736, 0.9635,
        0.9739, 1.0024, 0.9780, 1.0908, 0.8779, 1.0231, 0.9424, 1.0640, 0.8290,
        0.9819, 0.9885, 0.9763, 0.9132, 0.9781, 1.0281, 0.9618, 1.0465, 0.8414,
        0.9872, 1.0150, 0.9978, 1.0356, 1.2171, 1.0239, 1.0066, 1.0144, 1.0454,
        0.9555, 1.0105, 0.9740, 1.0298, 0.9262, 0.9895, 1.0186, 1.0757, 1.0997,
        1.0238, 1.0014, 1.0010, 1.0319, 0.9265, 1.0001, 1.0276, 1.0332, 1.0410,
        0.9076, 1.0008, 0.9500, 1.0016, 0.9365, 1.0025, 0.9976, 0.9617, 0.9001,
        1.0014, 1.1074, 1.4975, 0.8779, 1.1003, 0.9292, 1.0606, 1.0096, 1.0286,
        1.0283, 1.0397, 0.9070, 0.9943, 0.9244, 0.9900, 1.0183, 1.0978, 1.0846,
        0.9200, 1.0165, 1.0869, 1.0167, 0.9953, 0.9912, 0.9530, 1.0001, 0.9994,
        0.9868, 1.1841, 1.0452, 0.8756, 0.9989, 1.0219, 0.9688, 0.9986, 0.9096,
        0.9854, 0.8479, 1.0385, 1.0089, 1.0505, 1.0297, 0.9934, 0.9973, 1.0061,
        0.8794, 1.0074, 1.0333, 1.0071, 1.0726, 0.8338, 0.9780, 1.0019, 0.9485,
        0.9997, 1.0333, 1.2369, 0.9871, 1.0233, 1.0265, 0.9472, 0.9936, 0.9861,
        0.9596, 1.0061, 0.8650, 1.0225, 1.0443, 1.1215, 1.0511, 0.9715, 0.9775,
        0.9156, 1.1078, 0.9703, 0.9841, 1.0624, 0.8155, 1.0195, 1.0080, 0.9966,
        1.0582, 0.8257, 1.0159, 1.0179, 1.0058, 0.9553, 0.9905, 0.8986],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809009
t4: 1641198809009
surr1, surr2: tensor([-3.1540e+00, -9.7978e-01, -1.4501e+00, -2.4644e+00, -2.8348e+00,
        -2.9308e+00, -2.7947e+00, -2.8251e+00, -2.7840e+00, -2.6254e+00,
        -2.5520e+00, -2.6372e+00, -2.6151e+00, -2.4696e+00, -2.2411e+00,
        -2.3864e+00, -2.2520e+00, -2.2981e+00, -2.3620e+00, -2.2850e+00,
        -2.2318e+00, -2.1905e+00, -8.4858e-01, -1.6897e-01, -1.6736e+00,
        -1.7944e+00, -1.5117e+00, -1.6891e+00, -1.5884e+00, -1.2811e+00,
        -1.8457e+00, -1.8686e+00, -1.6530e+00, -1.9364e+00, -1.6636e+00,
        -1.5793e+00, -1.6274e+00, -1.6848e+00, -1.5771e+00, -1.5398e+00,
        -1.5012e+00, -1.4353e+00, -1.3550e+00, -1.5900e+00, -1.7148e+00,
        -1.4763e+00, -1.3981e+00, -1.5654e+00, -1.7030e+00, -1.7209e+00,
        -1.3630e+00, -1.5759e+00, -1.4677e+00, -1.8637e+00, -1.2913e+00,
        -1.2400e+00, -1.0449e+00, -1.6232e+00, -1.4193e+00, -1.3844e+00,
        -1.2791e+00, -1.5302e+00, -1.0793e+00, -1.3512e+00, -1.5152e+00,
        -1.2240e+00,  1.2704e+00,  1.3600e+00,  2.0619e+00,  2.3029e+00,
         5.1303e+00,  1.0097e-02,  2.9612e-01,  1.9223e-01, -5.5225e-02,
         1.7135e-01,  2.1915e-01,  2.7686e-01,  3.1563e-01,  3.1642e-01,
         2.7308e-01,  4.1412e-01,  3.5006e-01,  2.8847e-01,  4.3592e-01,
         1.2979e-01, -3.2098e-02,  1.9179e-01,  5.0360e-01,  2.8526e-01,
         1.5693e-01,  1.3585e-01,  2.4036e-01,  2.7980e-01,  3.8880e-01,
         8.9465e-02,  2.2262e-01,  2.9744e-01,  2.7575e-01,  2.1085e-01,
         2.8270e-01,  4.0389e-01,  2.2999e-01,  4.0328e-01, -1.2233e-01,
         9.1097e-02, -1.1443e-01,  4.3828e-02,  1.3260e-01,  1.8454e-01,
         2.4766e-01,  3.1508e-01,  1.9368e-01,  1.9116e+00,  2.9239e+00,
         4.5489e+00,  6.6536e-01,  9.8452e-01,  9.0346e-01,  9.8335e-01,
         9.9114e-01,  9.4688e-01,  8.8596e-01,  8.7427e-01,  8.2576e-01,
         7.9216e-01,  7.3966e-01,  8.0149e-01,  9.3111e-01,  4.9715e-01,
         6.0580e-01,  8.5647e-01,  4.7913e-01,  7.5175e-01,  2.9192e-01,
         8.2526e-01,  7.0091e-01,  8.8261e-01,  9.0305e-01,  8.6182e-01,
         1.0246e+00,  1.0705e+00,  5.9538e-01,  9.3544e-01,  1.0117e+00,
         1.1471e+00,  5.9836e-01,  7.5875e-01,  9.3700e-01,  1.0042e+00,
         5.4242e-01,  9.7666e-01,  7.1593e-01,  9.2151e-01,  8.2427e-01,
         4.7823e-01,  8.0419e-01,  8.4995e-01,  8.3835e-01,  9.4383e-01,
         8.0431e-01,  5.3512e-01,  9.4955e-01,  9.5921e-01,  4.9632e-01,
         9.3400e-01,  6.5916e-01,  6.7389e-01,  8.1005e-01,  9.3118e-01,
         9.8067e-01,  6.7688e-01,  6.8093e-01,  8.7442e-01,  8.5481e-01,
         1.0293e+00,  9.3599e-01,  9.3931e-01,  8.6985e-01,  1.0888e+00,
         8.3939e-01,  8.9687e-01,  4.1818e-01,  5.4940e-01,  8.1950e-01,
         9.5317e-01,  9.6304e-01,  8.0399e-01,  6.8762e-01,  5.9119e-01,
         7.9123e-01,  6.1078e-01,  3.4733e-01,  8.2996e-01,  4.7262e-01,
         6.6783e-01,  7.8293e-01,  6.5678e-01,  3.8388e-01,  7.2130e-01,
         3.2449e-01,  8.4159e-01,  6.6429e-01,  8.6315e-01,  7.3769e-01,
         7.3162e-01,  7.5388e-01,  3.4164e-01,  5.9243e-01,  6.8305e-01,
         5.7244e-01,  6.1108e-01,  6.8836e-01,  5.8373e-01,  5.5200e-01,
         6.1492e-01,  7.0119e-01,  7.6396e-01,  4.3291e-01,  5.9957e-01,
         6.9358e-01,  6.0923e-01,  6.8845e-01,  6.1012e-01,  8.3284e-01,
         5.7982e-01,  4.3481e-01,  6.1940e-01,  6.1036e-01,  7.1224e-01,
         6.3450e-01,  6.8042e-01,  1.9129e-01,  8.0824e-01,  3.0046e-01,
         7.3852e-01,  6.2935e-01,  6.6569e-01,  7.3976e-01,  8.1709e-01,
         7.2196e-01,  6.3784e-01,  8.0101e-01,  2.8530e-01,  6.7922e-01,
         4.0840e-01,  6.4495e-01,  5.7578e-01,  4.9002e-01,  1.5951e-01,
         4.9817e-01,  5.3771e-01,  5.7854e-01,  4.2500e-01,  1.8377e-01,
         9.9415e-02,  2.0422e-01,  1.1205e-03,  4.3018e-01,  4.0623e-01,
         4.3376e-01,  3.5679e-01,  2.6904e-01,  3.1942e-01,  3.0958e-01,
        -7.8867e-02,  3.4765e-01,  3.1954e-01,  5.0692e-01,  4.8510e-02,
         1.1355e-01,  1.6576e-01,  3.9735e-01,  3.7298e-01,  1.7320e-01,
        -2.7615e-02,  2.2455e-01,  4.4789e-01,  2.4189e-01,  1.3563e-01,
         1.9626e-01,  1.6126e-01,  1.9093e-01,  2.3605e-01, -1.2851e-02,
         3.7320e-01,  3.8668e-01,  3.1056e-01,  1.4826e-01,  3.1476e-01,
         2.7485e-01,  2.7988e-01,  1.7682e-01,  2.9768e-01,  2.4434e-01,
         1.7821e-01,  1.2760e-02,  5.8595e-03,  2.4309e-01,  1.7543e-01,
         1.9909e-01, -2.9704e-01,  2.5953e-01,  5.1073e-01,  3.2026e-01,
         4.0881e-01,  2.8244e-01,  2.8611e-01,  4.2309e-01,  2.7353e-01,
         3.4012e-01,  1.4243e-01,  2.8265e-01,  3.3609e-01,  3.4828e-01,
        -4.5446e-03,  9.2831e-02, -1.3686e-01, -1.0672e-01,  3.6251e-01,
        -1.4511e-01,  3.6106e-01, -1.2880e-01,  2.7375e-01,  2.8686e-01,
         2.5277e-01,  2.1567e-01,  8.7354e-02,  7.4520e-02,  2.3676e-01,
         1.8523e-01,  3.5439e-01, -8.3230e-02,  5.4658e-02,  1.7665e-01,
         1.4337e-01, -1.1735e-01,  2.7783e-01,  2.0549e-01,  2.8611e-01,
         4.9122e-02,  3.9429e-02,  1.2983e-01,  6.0409e-02, -3.8651e-01,
         6.0343e-02, -4.5091e-02,  7.7247e-02, -7.1216e-02,  8.3355e-02,
         1.5499e-02, -2.5994e-01, -4.0810e-01, -1.7009e-01, -4.6417e-01,
        -2.6173e-01, -3.5785e-01, -1.8793e-01,  5.6698e-02, -9.4959e-02,
         2.5648e-02, -3.5694e-01, -5.1670e-01, -1.6821e-01,  1.0178e-01,
        -2.9358e-01, -4.3439e-01, -4.2711e-01, -2.8197e-01, -2.1063e-01,
        -1.6480e-01, -4.3233e-01, -6.3791e-01, -1.8502e-01, -2.3823e-01,
        -2.6385e-01, -1.9316e-01, -2.3879e-01, -2.4799e-01, -3.8456e-01,
        -3.2022e-01, -5.8876e-01, -8.8497e-01, -6.0514e-01, -7.0576e-01,
        -5.0134e-01, -4.1782e-01, -3.5238e-01, -1.0974e+00, -8.5701e-01,
        -5.2398e-01, -6.6314e-01, -8.3809e-01, -7.1965e-01, -8.1886e-01,
        -7.0324e-01, -7.1060e-01, -8.6794e-01, -9.7337e-01, -9.5068e-01,
        -9.2362e-01, -7.2017e-01, -8.3695e-01, -7.7075e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1540e+00, -9.7978e-01, -1.4501e+00, -2.4644e+00, -2.8348e+00,
        -2.9308e+00, -2.7947e+00, -2.8251e+00, -2.7840e+00, -2.6254e+00,
        -2.5520e+00, -2.6372e+00, -2.6151e+00, -2.4696e+00, -2.2411e+00,
        -2.3864e+00, -2.2520e+00, -2.3300e+00, -2.3620e+00, -2.2850e+00,
        -2.2318e+00, -2.1905e+00, -8.4858e-01, -1.6897e-01, -1.6736e+00,
        -1.7944e+00, -1.5117e+00, -1.6891e+00, -1.5884e+00, -1.3938e+00,
        -1.8517e+00, -1.8686e+00, -1.6530e+00, -1.9364e+00, -1.6636e+00,
        -1.5793e+00, -1.6274e+00, -1.6848e+00, -1.5771e+00, -1.5398e+00,
        -1.5012e+00, -1.4353e+00, -1.3550e+00, -1.5900e+00, -1.7148e+00,
        -1.4763e+00, -1.3981e+00, -1.5654e+00, -1.6891e+00, -1.7209e+00,
        -1.3630e+00, -1.5759e+00, -1.4677e+00, -1.8637e+00, -1.2913e+00,
        -1.2400e+00, -1.0449e+00, -1.6232e+00, -1.4193e+00, -1.3844e+00,
        -1.2791e+00, -1.5302e+00, -1.0793e+00, -1.3512e+00, -1.5152e+00,
        -1.2240e+00,  1.2598e+00,  1.3600e+00,  2.0619e+00,  2.3029e+00,
         5.1175e+00,  1.0097e-02,  2.9612e-01,  1.9103e-01, -5.7464e-02,
         1.7135e-01,  2.1915e-01,  2.7686e-01,  3.1563e-01,  3.1642e-01,
         2.7308e-01,  4.1412e-01,  3.5006e-01,  2.8847e-01,  4.3592e-01,
         1.2979e-01, -3.2098e-02,  1.9303e-01,  5.0360e-01,  2.8526e-01,
         1.5720e-01,  1.3585e-01,  2.4036e-01,  2.7980e-01,  3.8880e-01,
         9.1655e-02,  2.2262e-01,  2.9744e-01,  2.7575e-01,  2.1085e-01,
         2.8270e-01,  4.0389e-01,  2.2999e-01,  3.8576e-01, -1.2233e-01,
         8.8676e-02, -1.1443e-01,  4.3828e-02,  1.3260e-01,  1.8454e-01,
         2.4766e-01,  3.1508e-01,  1.9368e-01,  1.9055e+00,  2.9239e+00,
         4.0062e+00,  6.6536e-01,  9.8452e-01,  9.0346e-01,  9.8335e-01,
         9.9114e-01,  9.4688e-01,  8.8596e-01,  8.7427e-01,  8.2576e-01,
         7.9216e-01,  7.3966e-01,  8.0149e-01,  9.0579e-01,  5.6602e-01,
         6.0580e-01,  8.5647e-01,  4.7913e-01,  7.9688e-01,  2.9192e-01,
         8.2526e-01,  7.0091e-01,  8.8261e-01,  9.0305e-01,  8.6182e-01,
         1.0246e+00,  1.0376e+00,  5.9538e-01,  9.3544e-01,  1.0117e+00,
         1.1363e+00,  5.9836e-01,  7.5875e-01,  9.3700e-01,  1.0042e+00,
         5.4242e-01,  9.7666e-01,  7.1593e-01,  9.2151e-01,  8.7946e-01,
         4.7823e-01,  8.0419e-01,  8.4995e-01,  8.3835e-01,  9.4383e-01,
         7.9559e-01,  5.3512e-01,  9.4955e-01,  9.5921e-01,  4.9632e-01,
         9.3400e-01,  6.5916e-01,  6.7389e-01,  8.1005e-01,  9.3118e-01,
         9.8067e-01,  6.7688e-01,  6.8093e-01,  8.7442e-01,  8.5481e-01,
         1.0293e+00,  9.3599e-01,  9.3931e-01,  8.6985e-01,  1.0888e+00,
         8.3939e-01,  8.8694e-01,  4.1136e-01,  5.4940e-01,  8.1950e-01,
         9.5317e-01,  9.6304e-01,  8.0399e-01,  6.8762e-01,  5.9119e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.2996e-01,  4.7313e-01,
         6.6783e-01,  7.8293e-01,  6.5678e-01,  3.8388e-01,  7.2130e-01,
         3.2449e-01,  8.4159e-01,  6.6429e-01,  8.6315e-01,  7.3769e-01,
         7.3162e-01,  7.5388e-01,  3.4164e-01,  5.9243e-01,  6.8305e-01,
         5.7244e-01,  6.1108e-01,  6.8836e-01,  5.8373e-01,  5.5200e-01,
         6.1492e-01,  7.0119e-01,  7.6396e-01,  4.3291e-01,  5.9957e-01,
         6.9358e-01,  6.0923e-01,  6.8845e-01,  6.1012e-01,  8.3284e-01,
         5.7982e-01,  4.5092e-01,  6.1940e-01,  6.1036e-01,  7.1224e-01,
         6.3450e-01,  6.6277e-01,  1.9129e-01,  8.0634e-01,  3.0046e-01,
         7.3852e-01,  6.2935e-01,  6.6569e-01,  7.3976e-01,  8.1709e-01,
         7.2196e-01,  6.3784e-01,  8.0101e-01,  2.8530e-01,  6.7922e-01,
         4.0840e-01,  6.4495e-01,  5.7578e-01,  4.9002e-01,  1.5951e-01,
         4.9817e-01,  5.3771e-01,  5.7854e-01,  4.2500e-01,  1.8377e-01,
         1.0747e-01,  2.0422e-01,  1.1205e-03,  4.3018e-01,  4.0623e-01,
         4.3376e-01,  3.5679e-01,  2.6904e-01,  3.1942e-01,  3.0958e-01,
        -8.0855e-02,  3.4765e-01,  3.1954e-01,  5.0692e-01,  5.2668e-02,
         1.1355e-01,  1.6576e-01,  3.9735e-01,  3.7298e-01,  1.7320e-01,
        -2.7615e-02,  2.2455e-01,  4.4789e-01,  2.5872e-01,  1.3563e-01,
         1.9626e-01,  1.6126e-01,  1.9093e-01,  2.1334e-01, -1.2851e-02,
         3.7320e-01,  3.8668e-01,  3.1056e-01,  1.4826e-01,  3.1476e-01,
         2.7485e-01,  2.7988e-01,  1.7682e-01,  2.9768e-01,  2.4434e-01,
         1.7821e-01,  1.2760e-02,  5.8595e-03,  2.4309e-01,  1.7543e-01,
         1.9909e-01, -2.9704e-01,  2.5953e-01,  5.1073e-01,  3.2026e-01,
         4.0881e-01,  2.8244e-01,  2.8611e-01,  4.2309e-01,  2.7353e-01,
         3.4012e-01,  1.4243e-01,  2.8265e-01,  3.3609e-01,  3.4828e-01,
        -4.5446e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.6243e-01,
        -1.4511e-01,  3.6106e-01, -1.2880e-01,  2.7375e-01,  2.8686e-01,
         2.5277e-01,  2.1567e-01,  8.7354e-02,  7.4520e-02,  2.3676e-01,
         1.8523e-01,  3.5439e-01, -8.3230e-02,  5.4658e-02,  1.7665e-01,
         1.4337e-01, -1.1735e-01,  2.7783e-01,  2.0549e-01,  2.8611e-01,
         4.9122e-02,  3.9429e-02,  1.2983e-01,  5.6121e-02, -3.8651e-01,
         6.2027e-02, -4.5091e-02,  7.7247e-02, -7.1216e-02,  8.3355e-02,
         1.5499e-02, -2.5994e-01, -4.3320e-01, -1.7009e-01, -4.6417e-01,
        -2.6173e-01, -3.5785e-01, -1.8793e-01,  5.6698e-02, -9.4959e-02,
         2.6249e-02, -3.5694e-01, -5.1670e-01, -1.6821e-01,  1.0178e-01,
        -3.1689e-01, -4.3439e-01, -4.2711e-01, -2.8197e-01, -2.1063e-01,
        -1.6480e-01, -3.8447e-01, -6.3791e-01, -1.8502e-01, -2.3823e-01,
        -2.6385e-01, -1.9316e-01, -2.3879e-01, -2.4799e-01, -3.8456e-01,
        -3.3317e-01, -5.8876e-01, -8.8497e-01, -5.9353e-01, -7.0576e-01,
        -5.0134e-01, -4.1782e-01, -3.5238e-01, -1.0896e+00, -8.5701e-01,
        -5.2398e-01, -6.6314e-01, -9.2498e-01, -7.1965e-01, -8.1886e-01,
        -7.0324e-01, -7.1060e-01, -9.4607e-01, -9.7337e-01, -9.5068e-01,
        -9.2362e-01, -7.2017e-01, -8.3695e-01, -7.7199e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809019
t6: 1641198809019
state_values: tensor([1.7593, 1.5507, 1.6854, 1.8909, 2.1033, 2.2585, 2.3636, 2.4278, 2.4714,
        2.5509, 2.6800, 2.6260, 2.6471, 2.6576, 2.7364, 3.0044, 2.9076, 2.8266,
        2.9111, 2.9409, 2.9896, 2.9813, 2.7270, 2.6328, 2.8403, 2.8643, 2.8801,
        3.0039, 2.9338, 2.9339, 3.2681, 3.1649, 3.1756, 3.0570, 3.0315, 3.0286,
        3.0382, 3.0362, 3.1024, 3.0710, 3.1176, 3.1091, 3.1113, 3.2712, 3.1517,
        3.1484, 3.1377, 3.2493, 3.1693, 3.1507, 3.1717, 3.2583, 3.1927, 3.4019,
        3.2859, 3.2461, 3.3029, 3.5275, 3.3318, 3.4546, 3.3291, 3.2992, 3.2844,
        3.3423, 3.3046, 3.3251, 3.0327, 2.9577, 2.8943, 2.9252, 2.8287, 3.0893,
        3.2168, 3.2266, 3.2396, 3.3894, 3.3574, 3.3676, 3.3533, 3.3812, 3.3447,
        3.3401, 3.3700, 3.3560, 3.4114, 3.5837, 3.4322, 3.4096, 3.4746, 3.5245,
        3.4489, 3.5605, 3.4917, 3.4588, 3.4382, 3.4441, 3.5761, 3.5295, 3.5059,
        3.5831, 3.5512, 3.4934, 3.4882, 3.4930, 3.4820, 3.4908, 3.4935, 3.5124,
        3.5150, 3.5171, 3.5881, 3.6370, 3.6359, 3.3671, 3.2226, 3.1212, 3.3541,
        3.5258, 3.4977, 3.5407, 3.5304, 3.5202, 3.5366, 3.5337, 3.6662, 3.5893,
        3.7302, 3.6302, 3.5928, 3.5900, 3.8512, 3.7355, 3.6391, 3.6977, 4.0367,
        3.8147, 3.8988, 3.7764, 3.7790, 3.8133, 3.7297, 3.7018, 3.6827, 3.7132,
        3.7377, 3.7368, 3.6905, 3.6961, 3.8156, 3.7320, 4.0225, 3.8250, 3.7678,
        3.7391, 3.7326, 4.0512, 3.9250, 3.9222, 3.8128, 3.7832, 3.7784, 3.7819,
        3.7640, 3.7961, 3.7557, 3.7942, 3.7902, 3.9023, 3.8087, 3.7802, 3.7921,
        3.7940, 3.7971, 3.8484, 3.8058, 3.7838, 3.7861, 3.8381, 3.8287, 3.8138,
        3.7913, 3.8854, 3.8183, 3.8190, 3.8088, 3.8226, 3.7978, 3.9111, 4.0335,
        4.0574, 3.9131, 3.8731, 3.8400, 3.9607, 3.8902, 4.0599, 4.0241, 4.0606,
        3.9510, 4.0105, 3.9430, 3.9166, 3.9153, 3.9334, 3.9970, 4.0513, 4.0189,
        3.9617, 3.9346, 4.0142, 4.1352, 3.9856, 3.9755, 4.0698, 4.1201, 4.0026,
        3.9722, 3.9522, 3.9545, 4.0722, 3.9808, 4.1164, 4.0812, 4.0310, 3.9775,
        3.9680, 3.9701, 4.1274, 4.0088, 4.0410, 4.0507, 4.0747, 4.0006, 4.0455,
        3.9927, 4.0208, 3.9909, 3.9761, 4.0108, 3.9672, 3.9695, 3.9973, 4.0046,
        3.9943, 3.9765, 3.9908, 3.9776, 4.0679, 4.0263, 4.0141, 4.0867, 4.1164,
        4.0930, 4.1226, 4.3033, 4.1151, 4.3556, 4.1447, 4.1513, 4.1731, 4.1824,
        4.2264, 4.3143, 4.2239, 4.2376, 4.1501, 4.2290, 4.1384, 4.1965, 4.1211,
        4.3553, 4.1576, 4.1589, 4.1761, 4.3624, 4.1882, 4.1545, 4.1749, 4.1269,
        4.4015, 4.3107, 4.3696, 4.2400, 4.1733, 4.1324, 4.1317, 4.1309, 4.1334,
        4.1317, 4.2052, 4.1524, 4.2090, 4.1564, 4.2476, 4.2480, 4.1751, 4.1645,
        4.1489, 4.1362, 4.1558, 4.1547, 4.2781, 4.3169, 4.2120, 4.1846, 4.1704,
        4.1741, 4.3273, 4.2446, 4.3752, 4.2762, 4.4279, 4.3096, 4.2554, 4.2857,
        4.5492, 4.3091, 4.2366, 4.2154, 4.3137, 4.2409, 4.3152, 4.2382, 4.2274,
        4.2130, 4.2123, 4.2016, 4.4143, 4.2820, 4.4147, 4.2804, 4.2272, 4.2436,
        4.2343, 4.3421, 4.2606, 4.2308, 4.2171, 4.2458, 4.2468, 4.3860, 4.4038,
        4.3109, 4.3116, 4.2631, 4.2503, 4.4461, 4.3123, 4.2794, 4.2972, 4.2869,
        4.4848, 4.3223, 4.4845, 4.7024, 4.4250, 4.3650, 4.3402, 4.3087, 4.3255,
        4.2857, 4.5795, 4.3784, 4.3157, 4.2731, 4.2939, 4.5991, 4.6084, 4.4568,
        4.4838, 4.3966, 4.3497, 4.3247, 4.3286, 4.3157, 4.3118, 4.3995, 4.3773,
        4.3766, 4.4363, 4.3607, 4.6265, 4.4127, 4.3711, 4.3495, 4.3489, 4.3950,
        4.4163, 4.7748, 4.4818, 4.4836, 4.4981, 4.4104, 4.5783, 4.6538, 4.5510,
        4.4652, 4.4338, 4.6373, 4.6357, 4.5986, 4.4575, 4.5258, 4.4946],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809023
t8: 1641198809023
t9: 1641198809024
t10: 1641198809034
t11: 1641198809036
t12: 1641198809036
t1: 1641198809036
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809046
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9976, 1.0044, 0.9681, 0.9907, 1.0148, 0.9955, 0.9999, 1.0063, 0.9861,
        0.9996, 0.9647, 0.9936, 1.0387, 0.9569, 0.9447, 0.9103, 0.9231, 0.8885,
        0.9961, 1.0130, 1.0168, 0.9950, 1.0048, 0.9661, 1.0128, 0.9954, 0.9263,
        0.9976, 0.9952, 0.8648, 0.8921, 1.0740, 0.9267, 1.0168, 1.0225, 0.9920,
        1.0725, 0.9308, 1.0197, 0.9734, 0.9901, 0.9971, 0.9164, 0.9823, 0.9841,
        1.0177, 0.9232, 0.9922, 1.0932, 0.9753, 0.9570, 1.0111, 0.9570, 0.9921,
        1.0297, 0.9369, 0.9428, 1.0961, 0.9072, 0.9862, 1.0071, 0.9920, 0.9771,
        1.0540, 0.9689, 1.0314, 1.1038, 1.0645, 0.9641, 0.9590, 1.1210, 0.9648,
        1.0197, 1.0927, 0.8663, 1.0033, 0.9966, 0.9926, 0.9812, 1.0177, 0.9965,
        0.9891, 1.0186, 0.9495, 0.9599, 0.9965, 1.0350, 0.9055, 1.0132, 0.9863,
        0.9076, 0.9989, 1.0053, 1.0230, 1.0739, 0.8793, 1.0028, 0.9984, 0.9506,
        0.9969, 1.0071, 1.0413, 0.9980, 1.1323, 1.1001, 1.1269, 1.0605, 1.0311,
        1.0173, 0.9804, 0.9912, 1.0087, 1.0140, 1.0998, 1.0992, 1.2280, 0.9133,
        0.9994, 0.9883, 0.9988, 1.0105, 1.0054, 1.0068, 0.9379, 0.9960, 0.9384,
        0.9862, 1.0377, 1.1215, 0.7905, 0.9933, 0.9735, 0.9191, 0.8869, 0.9711,
        1.0269, 1.0010, 1.0021, 0.9907, 0.9998, 1.0125, 1.1169, 0.9730, 0.9789,
        0.9906, 1.0862, 1.0698, 0.9124, 0.9995, 0.9439, 0.9982, 0.9980, 1.0488,
        1.0122, 0.8611, 0.9671, 1.0366, 0.9833, 1.0109, 1.0489, 1.1045, 1.0012,
        0.9916, 1.0656, 0.9703, 1.0213, 0.9539, 1.0066, 1.0107, 0.9948, 1.0391,
        1.0396, 0.9625, 1.0060, 0.9988, 1.0009, 0.9756, 0.9958, 0.9854, 1.0212,
        0.9428, 1.0706, 1.1214, 1.0372, 0.9924, 1.0030, 0.9698, 0.9965, 1.0074,
        0.9923, 1.1050, 1.3988, 0.8501, 1.0065, 0.9069, 1.0066, 1.0288, 0.9860,
        0.9243, 1.0485, 0.9963, 1.0319, 0.9755, 0.9710, 0.9788, 1.0062, 1.0373,
        1.0581, 0.9261, 0.9613, 0.9883, 0.9891, 0.9502, 0.9897, 0.9933, 1.0094,
        1.0300, 1.0778, 0.9085, 1.0015, 0.9462, 1.0104, 0.9997, 1.0006, 1.0538,
        1.0802, 0.8696, 0.9900, 0.9773, 0.9853, 0.9805, 1.0939, 0.9205, 1.0822,
        0.9700, 1.0257, 1.0254, 0.9832, 1.0049, 1.0043, 0.9963, 0.9878, 1.0527,
        1.0298, 1.0562, 1.0119, 0.9726, 1.0075, 1.0675, 0.9386, 0.9801, 0.9957,
        0.9820, 0.9381, 0.9699, 0.8391, 1.0516, 0.9838, 0.9842, 0.9836, 0.9784,
        1.0019, 1.0058, 0.9872, 1.0516, 0.8785, 1.0109, 0.9513, 1.0343, 0.8287,
        0.9823, 0.9903, 0.9819, 0.9372, 0.9794, 1.0269, 0.9664, 1.0347, 0.8477,
        0.9827, 1.0377, 0.9945, 1.0253, 1.2026, 1.0242, 1.0061, 1.0118, 1.0390,
        0.9576, 1.0072, 0.9779, 1.0226, 0.9310, 1.0008, 1.0100, 1.0682, 1.0990,
        1.0236, 1.0014, 1.0010, 1.0260, 0.9287, 1.0000, 1.0210, 1.0318, 1.0383,
        0.9140, 1.0027, 0.9690, 1.0030, 0.9574, 1.0036, 0.9988, 0.9698, 0.9314,
        1.0047, 1.0961, 1.4983, 0.8754, 1.0506, 0.9315, 1.0242, 1.0095, 1.0248,
        1.0246, 1.0351, 0.9152, 0.9933, 0.9348, 0.9886, 1.0143, 1.0854, 1.0842,
        0.9223, 1.0064, 1.0764, 1.0166, 0.9961, 0.9934, 0.9653, 1.0136, 1.0011,
        0.9897, 1.1460, 1.0464, 0.8836, 0.9982, 1.0143, 0.9719, 0.9992, 0.9232,
        0.9572, 0.8503, 1.0582, 1.0092, 1.0448, 1.0281, 0.9946, 0.9982, 1.0054,
        0.8950, 1.0136, 1.0311, 1.0069, 1.0602, 0.8377, 0.9668, 1.0027, 0.9615,
        1.0008, 1.0220, 1.2210, 0.9864, 1.0204, 1.0228, 0.9531, 0.9976, 0.9899,
        0.9696, 1.0040, 0.8861, 1.0361, 1.0433, 1.1109, 1.0502, 0.9742, 0.9837,
        0.9467, 1.1166, 0.9728, 0.9965, 1.0288, 0.8164, 1.0315, 1.0125, 0.9988,
        1.0411, 0.8325, 1.0246, 1.0279, 1.0018, 0.9644, 0.9952, 0.9274],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809051
t4: 1641198809051
surr1, surr2: tensor([-3.1541e+00, -9.8524e-01, -1.4307e+00, -2.4521e+00, -2.7870e+00,
        -2.9311e+00, -2.7829e+00, -2.7838e+00, -2.7854e+00, -2.6624e+00,
        -2.5269e+00, -2.6399e+00, -2.5855e+00, -2.4741e+00, -2.3166e+00,
        -2.3710e+00, -2.2117e+00, -2.3002e+00, -2.4021e+00, -2.3437e+00,
        -2.2546e+00, -2.1743e+00, -8.4852e-01, -1.7061e-01, -1.6561e+00,
        -1.7960e+00, -1.5391e+00, -1.6859e+00, -1.5923e+00, -1.3392e+00,
        -1.8355e+00, -1.8933e+00, -1.6085e+00, -1.9347e+00, -1.6567e+00,
        -1.5821e+00, -1.5960e+00, -1.6844e+00, -1.5657e+00, -1.5451e+00,
        -1.5060e+00, -1.4368e+00, -1.3780e+00, -1.5898e+00, -1.7170e+00,
        -1.4694e+00, -1.4169e+00, -1.5423e+00, -1.6787e+00, -1.7200e+00,
        -1.3734e+00, -1.5605e+00, -1.4739e+00, -1.8683e+00, -1.2830e+00,
        -1.2437e+00, -1.0895e+00, -1.6358e+00, -1.4231e+00, -1.3821e+00,
        -1.2534e+00, -1.5310e+00, -1.0854e+00, -1.3238e+00, -1.5153e+00,
        -1.2158e+00,  1.2641e+00,  1.3600e+00,  2.0669e+00,  2.3442e+00,
         5.2154e+00,  1.0106e-02,  2.9337e-01,  1.8977e-01, -5.5314e-02,
         1.7173e-01,  2.2070e-01,  2.7782e-01,  3.1769e-01,  3.1451e-01,
         2.7333e-01,  4.1514e-01,  3.4890e-01,  2.8991e-01,  4.5029e-01,
         1.2997e-01, -3.2067e-02,  1.9420e-01,  5.1288e-01,  2.8114e-01,
         1.5852e-01,  1.3566e-01,  2.3991e-01,  2.7888e-01,  3.8510e-01,
         8.9547e-02,  2.2321e-01,  2.9785e-01,  2.7906e-01,  2.1194e-01,
         2.8209e-01,  4.0128e-01,  2.3008e-01,  3.9710e-01, -1.2256e-01,
         9.0843e-02, -1.1437e-01,  4.3722e-02,  1.3220e-01,  1.8549e-01,
         2.5097e-01,  3.1740e-01,  1.9193e-01,  1.9053e+00,  2.9249e+00,
         4.4723e+00,  6.6501e-01,  9.7743e-01,  9.0704e-01,  9.8407e-01,
         9.8905e-01,  9.4650e-01,  8.8536e-01,  8.8264e-01,  8.2153e-01,
         8.0517e-01,  7.3426e-01,  7.9627e-01,  9.2349e-01,  4.9714e-01,
         6.0441e-01,  8.3044e-01,  4.7919e-01,  7.8530e-01,  2.9143e-01,
         8.3907e-01,  7.0099e-01,  8.9010e-01,  9.1585e-01,  8.6145e-01,
         1.0204e+00,  1.0535e+00,  5.9497e-01,  9.3881e-01,  1.0153e+00,
         1.1220e+00,  5.9931e-01,  7.6222e-01,  9.3626e-01,  1.0290e+00,
         5.4220e-01,  9.6442e-01,  7.1433e-01,  9.2065e-01,  8.4146e-01,
         4.7617e-01,  8.1740e-01,  8.3830e-01,  8.3684e-01,  9.3591e-01,
         7.9887e-01,  5.3513e-01,  9.5117e-01,  9.4159e-01,  4.9611e-01,
         9.2891e-01,  6.6345e-01,  6.6771e-01,  8.0918e-01,  9.3199e-01,
         9.7354e-01,  6.7624e-01,  6.8287e-01,  8.7240e-01,  8.5517e-01,
         1.0294e+00,  9.4036e-01,  9.4078e-01,  8.7241e-01,  1.0827e+00,
         8.4343e-01,  8.6322e-01,  4.1935e-01,  5.4934e-01,  8.2045e-01,
         9.5225e-01,  9.7148e-01,  8.2457e-01,  6.8826e-01,  5.8817e-01,
         7.8035e-01,  6.0610e-01,  3.4594e-01,  8.1211e-01,  4.7676e-01,
         6.6822e-01,  7.9874e-01,  6.3962e-01,  3.8539e-01,  7.0257e-01,
         3.2464e-01,  8.3839e-01,  6.6600e-01,  8.7054e-01,  7.5079e-01,
         7.3737e-01,  7.3978e-01,  3.4120e-01,  5.9478e-01,  7.0014e-01,
         5.6816e-01,  6.1205e-01,  6.9758e-01,  5.9511e-01,  5.4631e-01,
         6.1436e-01,  6.9817e-01,  7.5795e-01,  4.3335e-01,  5.9825e-01,
         7.0222e-01,  6.1419e-01,  6.8911e-01,  6.1031e-01,  8.2465e-01,
         5.7821e-01,  4.3566e-01,  6.1596e-01,  6.1404e-01,  7.1672e-01,
         6.3929e-01,  6.5907e-01,  1.9075e-01,  7.9329e-01,  3.0058e-01,
         7.3441e-01,  6.2766e-01,  6.6707e-01,  7.3886e-01,  8.1657e-01,
         7.2254e-01,  6.3946e-01,  7.9129e-01,  2.8530e-01,  6.7602e-01,
         4.0824e-01,  6.4832e-01,  5.7268e-01,  4.8608e-01,  1.5954e-01,
         5.0261e-01,  5.4087e-01,  5.8248e-01,  4.3617e-01,  1.8267e-01,
         1.0020e-01,  2.0831e-01,  1.1236e-03,  4.3390e-01,  4.1043e-01,
         4.4048e-01,  3.6708e-01,  2.6995e-01,  3.2243e-01,  2.9845e-01,
        -7.8923e-02,  3.4348e-01,  3.2258e-01,  4.9276e-01,  4.8497e-02,
         1.1360e-01,  1.6607e-01,  3.9966e-01,  3.8278e-01,  1.7345e-01,
        -2.7582e-02,  2.2564e-01,  4.4285e-01,  2.4368e-01,  1.3502e-01,
         2.0065e-01,  1.6074e-01,  1.8903e-01,  2.3324e-01, -1.2856e-02,
         3.7303e-01,  3.8568e-01,  3.0865e-01,  1.4859e-01,  3.1375e-01,
         2.7597e-01,  2.7791e-01,  1.7773e-01,  3.0107e-01,  2.4227e-01,
         1.7696e-01,  1.2753e-02,  5.8581e-03,  2.4310e-01,  1.7542e-01,
         1.9796e-01, -2.9774e-01,  2.5952e-01,  5.0748e-01,  3.1984e-01,
         4.0774e-01,  2.8443e-01,  2.8666e-01,  4.3155e-01,  2.7391e-01,
         3.4768e-01,  1.4258e-01,  2.8299e-01,  3.3892e-01,  3.6037e-01,
        -4.5596e-03,  9.1881e-02, -1.3694e-01, -1.0642e-01,  3.4614e-01,
        -1.4547e-01,  3.4866e-01, -1.2878e-01,  2.7274e-01,  2.8584e-01,
         2.5165e-01,  2.1762e-01,  8.7269e-02,  7.5355e-02,  2.3643e-01,
         1.8452e-01,  3.5039e-01, -8.3200e-02,  5.4795e-02,  1.7488e-01,
         1.4198e-01, -1.1734e-01,  2.7805e-01,  2.0593e-01,  2.8981e-01,
         4.9782e-02,  3.9493e-02,  1.3022e-01,  5.8469e-02, -3.8694e-01,
         6.0894e-02, -4.5058e-02,  7.6670e-02, -7.1445e-02,  8.3413e-02,
         1.5730e-02, -2.5251e-01, -4.0929e-01, -1.7333e-01, -4.6430e-01,
        -2.6031e-01, -3.5729e-01, -1.8816e-01,  5.6749e-02, -9.4891e-02,
         2.6104e-02, -3.5911e-01, -5.1557e-01, -1.6817e-01,  1.0059e-01,
        -2.9496e-01, -4.2939e-01, -4.2744e-01, -2.8581e-01, -2.1087e-01,
        -1.6300e-01, -4.2675e-01, -6.3748e-01, -1.8450e-01, -2.3737e-01,
        -2.6549e-01, -1.9394e-01, -2.3969e-01, -2.5058e-01, -3.8377e-01,
        -3.2803e-01, -5.9654e-01, -8.8406e-01, -5.9942e-01, -7.0516e-01,
        -5.0274e-01, -4.2050e-01, -3.6434e-01, -1.1061e+00, -8.5918e-01,
        -5.3057e-01, -6.4216e-01, -8.3911e-01, -7.2813e-01, -8.2252e-01,
        -7.0473e-01, -6.9911e-01, -8.7510e-01, -9.8166e-01, -9.6003e-01,
        -9.2002e-01, -7.2706e-01, -8.4087e-01, -7.9552e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1541e+00, -9.8524e-01, -1.4307e+00, -2.4521e+00, -2.7870e+00,
        -2.9311e+00, -2.7829e+00, -2.7838e+00, -2.7854e+00, -2.6624e+00,
        -2.5269e+00, -2.6399e+00, -2.5855e+00, -2.4741e+00, -2.3166e+00,
        -2.3710e+00, -2.2117e+00, -2.3300e+00, -2.4021e+00, -2.3437e+00,
        -2.2546e+00, -2.1743e+00, -8.4852e-01, -1.7061e-01, -1.6561e+00,
        -1.7960e+00, -1.5391e+00, -1.6859e+00, -1.5923e+00, -1.3938e+00,
        -1.8517e+00, -1.8933e+00, -1.6085e+00, -1.9347e+00, -1.6567e+00,
        -1.5821e+00, -1.5960e+00, -1.6844e+00, -1.5657e+00, -1.5451e+00,
        -1.5060e+00, -1.4368e+00, -1.3780e+00, -1.5898e+00, -1.7170e+00,
        -1.4694e+00, -1.4169e+00, -1.5423e+00, -1.6787e+00, -1.7200e+00,
        -1.3734e+00, -1.5605e+00, -1.4739e+00, -1.8683e+00, -1.2830e+00,
        -1.2437e+00, -1.0895e+00, -1.6358e+00, -1.4231e+00, -1.3821e+00,
        -1.2534e+00, -1.5310e+00, -1.0854e+00, -1.3238e+00, -1.5153e+00,
        -1.2158e+00,  1.2598e+00,  1.3600e+00,  2.0669e+00,  2.3442e+00,
         5.1175e+00,  1.0106e-02,  2.9337e-01,  1.8977e-01, -5.7464e-02,
         1.7173e-01,  2.2070e-01,  2.7782e-01,  3.1769e-01,  3.1451e-01,
         2.7333e-01,  4.1514e-01,  3.4890e-01,  2.8991e-01,  4.5029e-01,
         1.2997e-01, -3.2067e-02,  1.9420e-01,  5.1288e-01,  2.8114e-01,
         1.5852e-01,  1.3566e-01,  2.3991e-01,  2.7888e-01,  3.8510e-01,
         9.1655e-02,  2.2321e-01,  2.9785e-01,  2.7906e-01,  2.1194e-01,
         2.8209e-01,  4.0128e-01,  2.3008e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1437e-01,  4.3722e-02,  1.3220e-01,  1.8549e-01,
         2.5097e-01,  3.1740e-01,  1.9193e-01,  1.9053e+00,  2.9249e+00,
         4.0062e+00,  6.6501e-01,  9.7743e-01,  9.0704e-01,  9.8407e-01,
         9.8905e-01,  9.4650e-01,  8.8536e-01,  8.8264e-01,  8.2153e-01,
         8.0517e-01,  7.3426e-01,  7.9627e-01,  9.0579e-01,  5.6602e-01,
         6.0441e-01,  8.3044e-01,  4.7919e-01,  7.9688e-01,  2.9143e-01,
         8.3907e-01,  7.0099e-01,  8.9010e-01,  9.1585e-01,  8.6145e-01,
         1.0204e+00,  1.0376e+00,  5.9497e-01,  9.3881e-01,  1.0153e+00,
         1.1220e+00,  5.9931e-01,  7.6222e-01,  9.3626e-01,  1.0290e+00,
         5.4220e-01,  9.6442e-01,  7.1433e-01,  9.2065e-01,  8.7946e-01,
         4.7617e-01,  8.1740e-01,  8.3830e-01,  8.3684e-01,  9.3591e-01,
         7.9559e-01,  5.3513e-01,  9.5117e-01,  9.4159e-01,  4.9611e-01,
         9.2891e-01,  6.6345e-01,  6.6771e-01,  8.0918e-01,  9.3199e-01,
         9.7354e-01,  6.7624e-01,  6.8287e-01,  8.7240e-01,  8.5517e-01,
         1.0294e+00,  9.4036e-01,  9.4078e-01,  8.7241e-01,  1.0827e+00,
         8.4343e-01,  8.6322e-01,  4.1136e-01,  5.4934e-01,  8.2045e-01,
         9.5225e-01,  9.7148e-01,  8.2457e-01,  6.8826e-01,  5.8817e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.1211e-01,  4.7676e-01,
         6.6822e-01,  7.9874e-01,  6.3962e-01,  3.8539e-01,  7.0257e-01,
         3.2464e-01,  8.3839e-01,  6.6600e-01,  8.7054e-01,  7.5079e-01,
         7.3737e-01,  7.3978e-01,  3.4120e-01,  5.9478e-01,  7.0014e-01,
         5.6816e-01,  6.1205e-01,  6.9758e-01,  5.9511e-01,  5.4631e-01,
         6.1436e-01,  6.9817e-01,  7.5795e-01,  4.3335e-01,  5.9825e-01,
         7.0222e-01,  6.1419e-01,  6.8911e-01,  6.1031e-01,  8.2465e-01,
         5.7821e-01,  4.5092e-01,  6.1596e-01,  6.1404e-01,  7.1672e-01,
         6.3929e-01,  6.5907e-01,  1.9075e-01,  7.9329e-01,  3.0058e-01,
         7.3441e-01,  6.2766e-01,  6.6707e-01,  7.3886e-01,  8.1657e-01,
         7.2254e-01,  6.3946e-01,  7.9129e-01,  2.8530e-01,  6.7602e-01,
         4.0824e-01,  6.4832e-01,  5.7268e-01,  4.8608e-01,  1.5954e-01,
         5.0261e-01,  5.4087e-01,  5.8248e-01,  4.3617e-01,  1.8267e-01,
         1.0747e-01,  2.0831e-01,  1.1236e-03,  4.3390e-01,  4.1043e-01,
         4.4048e-01,  3.6708e-01,  2.6995e-01,  3.2243e-01,  2.9845e-01,
        -8.0855e-02,  3.4348e-01,  3.2258e-01,  4.9276e-01,  5.2668e-02,
         1.1360e-01,  1.6607e-01,  3.9966e-01,  3.8278e-01,  1.7345e-01,
        -2.7582e-02,  2.2564e-01,  4.4285e-01,  2.5872e-01,  1.3502e-01,
         2.0065e-01,  1.6074e-01,  1.8903e-01,  2.1334e-01, -1.2856e-02,
         3.7303e-01,  3.8568e-01,  3.0865e-01,  1.4859e-01,  3.1375e-01,
         2.7597e-01,  2.7791e-01,  1.7773e-01,  3.0107e-01,  2.4227e-01,
         1.7696e-01,  1.2753e-02,  5.8581e-03,  2.4310e-01,  1.7542e-01,
         1.9796e-01, -2.9774e-01,  2.5952e-01,  5.0748e-01,  3.1984e-01,
         4.0774e-01,  2.8443e-01,  2.8666e-01,  4.3155e-01,  2.7391e-01,
         3.4768e-01,  1.4258e-01,  2.8299e-01,  3.3892e-01,  3.6037e-01,
        -4.5596e-03,  9.1881e-02, -1.0053e-01, -1.0941e-01,  3.4614e-01,
        -1.4547e-01,  3.4866e-01, -1.2878e-01,  2.7274e-01,  2.8584e-01,
         2.5165e-01,  2.1762e-01,  8.7269e-02,  7.5355e-02,  2.3643e-01,
         1.8452e-01,  3.5039e-01, -8.3200e-02,  5.4795e-02,  1.7488e-01,
         1.4198e-01, -1.1734e-01,  2.7805e-01,  2.0593e-01,  2.8981e-01,
         4.9782e-02,  3.9493e-02,  1.3022e-01,  5.6121e-02, -3.8694e-01,
         6.2027e-02, -4.5058e-02,  7.6670e-02, -7.1445e-02,  8.3413e-02,
         1.5730e-02, -2.5251e-01, -4.3320e-01, -1.7333e-01, -4.6430e-01,
        -2.6031e-01, -3.5729e-01, -1.8816e-01,  5.6749e-02, -9.4891e-02,
         2.6249e-02, -3.5911e-01, -5.1557e-01, -1.6817e-01,  1.0059e-01,
        -3.1689e-01, -4.2939e-01, -4.2744e-01, -2.8581e-01, -2.1087e-01,
        -1.6300e-01, -3.8447e-01, -6.3748e-01, -1.8450e-01, -2.3737e-01,
        -2.6549e-01, -1.9394e-01, -2.3969e-01, -2.5058e-01, -3.8377e-01,
        -3.3317e-01, -5.9654e-01, -8.8406e-01, -5.9353e-01, -7.0516e-01,
        -5.0274e-01, -4.2050e-01, -3.6434e-01, -1.0896e+00, -8.5918e-01,
        -5.3057e-01, -6.4216e-01, -9.2498e-01, -7.2813e-01, -8.2252e-01,
        -7.0473e-01, -6.9911e-01, -9.4607e-01, -9.8166e-01, -9.6003e-01,
        -9.2002e-01, -7.2706e-01, -8.4087e-01, -7.9552e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809060
t6: 1641198809060
state_values: tensor([1.7813, 1.5925, 1.7294, 1.9347, 2.1440, 2.2988, 2.4060, 2.4733, 2.5197,
        2.6021, 2.7315, 2.6835, 2.7085, 2.7222, 2.8029, 3.0685, 2.9781, 2.9019,
        2.9882, 3.0208, 3.0727, 3.0676, 2.8224, 2.7311, 2.9375, 2.9643, 2.9828,
        3.1080, 3.0420, 3.0447, 3.3750, 3.2780, 3.2912, 3.1741, 3.1513, 3.1504,
        3.1620, 3.1605, 3.2285, 3.1995, 3.2474, 3.2413, 3.2447, 3.4057, 3.2875,
        3.2865, 3.2773, 3.3910, 3.3124, 3.2931, 3.3167, 3.4054, 3.3414, 3.5503,
        3.4375, 3.3990, 3.4565, 3.6822, 3.4867, 3.6111, 3.4877, 3.4574, 3.4446,
        3.5026, 3.4652, 3.4876, 3.1927, 3.1165, 3.0543, 3.0864, 2.9895, 3.2523,
        3.3811, 3.3929, 3.4064, 3.5611, 3.5304, 3.5417, 3.5285, 3.5568, 3.5217,
        3.5180, 3.5481, 3.5350, 3.5910, 3.7657, 3.6118, 3.5908, 3.6567, 3.7078,
        3.6320, 3.7472, 3.6757, 3.6436, 3.6234, 3.6285, 3.7650, 3.7173, 3.6928,
        3.7738, 3.7420, 3.6821, 3.6771, 3.6827, 3.6704, 3.6796, 3.6824, 3.7023,
        3.7065, 3.7096, 3.7840, 3.8336, 3.8329, 3.5605, 3.4125, 3.3105, 3.5452,
        3.7205, 3.6934, 3.7391, 3.7292, 3.7190, 3.7370, 3.7345, 3.8694, 3.7936,
        3.9345, 3.8354, 3.7982, 3.7943, 4.0553, 3.9410, 3.8440, 3.9035, 4.2451,
        4.0217, 4.1092, 3.9844, 3.9867, 4.0212, 3.9382, 3.9104, 3.8899, 3.9214,
        3.9466, 3.9462, 3.8985, 3.9055, 4.0255, 3.9429, 4.2362, 4.0367, 3.9787,
        3.9509, 3.9449, 4.2670, 4.1413, 4.1391, 4.0264, 3.9971, 3.9917, 3.9945,
        3.9780, 4.0102, 3.9687, 4.0086, 4.0051, 4.1194, 4.0246, 3.9966, 4.0088,
        4.0097, 4.0138, 4.0651, 4.0234, 4.0019, 4.0045, 4.0561, 4.0472, 4.0332,
        4.0107, 4.1047, 4.0364, 4.0374, 4.0285, 4.0430, 4.0186, 4.1331, 4.2603,
        4.2853, 4.1379, 4.0945, 4.0601, 4.1872, 4.1123, 4.2883, 4.2524, 4.2899,
        4.1774, 4.2388, 4.1690, 4.1422, 4.1409, 4.1605, 4.2259, 4.2814, 4.2486,
        4.1895, 4.1623, 4.2442, 4.3666, 4.2162, 4.2059, 4.3016, 4.3523, 4.2344,
        4.2035, 4.1828, 4.1841, 4.3046, 4.2128, 4.3498, 4.3146, 4.2643, 4.2103,
        4.1997, 4.2017, 4.3614, 4.2426, 4.2751, 4.2848, 4.3092, 4.2330, 4.2798,
        4.2251, 4.2551, 4.2251, 4.2102, 4.2457, 4.2012, 4.2039, 4.2324, 4.2406,
        4.2286, 4.2117, 4.2254, 4.2130, 4.3050, 4.2634, 4.2497, 4.3241, 4.3543,
        4.3311, 4.3608, 4.5468, 4.3524, 4.6008, 4.3818, 4.3893, 4.4114, 4.4209,
        4.4668, 4.5602, 4.4642, 4.4786, 4.3876, 4.4692, 4.3775, 4.4355, 4.3587,
        4.6011, 4.3953, 4.3979, 4.4154, 4.6092, 4.4264, 4.3941, 4.4149, 4.3668,
        4.6491, 4.5579, 4.6190, 4.4830, 4.4138, 4.3714, 4.3721, 4.3717, 4.3744,
        4.3720, 4.4459, 4.3938, 4.4503, 4.3980, 4.4919, 4.4924, 4.4176, 4.4059,
        4.3898, 4.3785, 4.3982, 4.3976, 4.5241, 4.5670, 4.4556, 4.4281, 4.4140,
        4.4177, 4.5789, 4.4909, 4.6282, 4.5252, 4.6814, 4.5616, 4.5030, 4.5352,
        4.8063, 4.5598, 4.4806, 4.4578, 4.5645, 4.4847, 4.5661, 4.4818, 4.4719,
        4.4573, 4.4567, 4.4457, 4.6676, 4.5318, 4.6688, 4.5307, 4.4735, 4.4894,
        4.4797, 4.5956, 4.5090, 4.4757, 4.4629, 4.4928, 4.4945, 4.6405, 4.6586,
        4.5643, 4.5650, 4.5109, 4.4985, 4.7026, 4.5663, 4.5306, 4.5497, 4.5382,
        4.7435, 4.5753, 4.7433, 4.9641, 4.6816, 4.6196, 4.5954, 4.5626, 4.5796,
        4.5373, 4.8396, 4.6321, 4.5693, 4.5238, 4.5448, 4.8590, 4.8700, 4.7151,
        4.7434, 4.6530, 4.6054, 4.5773, 4.5830, 4.5697, 4.5656, 4.6553, 4.6334,
        4.6329, 4.6936, 4.6174, 4.8883, 4.6678, 4.6274, 4.6042, 4.6047, 4.6510,
        4.6727, 5.0390, 4.7406, 4.7437, 4.7590, 4.6655, 4.8407, 4.9169, 4.8137,
        4.7251, 4.6906, 4.8989, 4.8984, 4.8613, 4.7170, 4.7881, 4.7555],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809064
t8: 1641198809064
t9: 1641198809064
t10: 1641198809075
t11: 1641198809076
t12: 1641198809076
t1: 1641198809076
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809087
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9978, 1.0092, 0.9558, 0.9852, 0.9965, 0.9957, 0.9958, 0.9911, 0.9871,
        1.0126, 0.9586, 0.9948, 1.0269, 0.9598, 0.9733, 0.9109, 0.9054, 0.8924,
        1.0093, 1.0363, 1.0259, 0.9877, 1.0046, 0.9743, 1.0028, 0.9962, 0.9415,
        0.9959, 0.9978, 0.9026, 0.8941, 1.0898, 0.9033, 1.0156, 1.0186, 0.9935,
        1.0532, 0.9317, 1.0130, 0.9773, 0.9931, 0.9983, 0.9359, 0.9780, 0.9856,
        1.0131, 0.9353, 0.9817, 1.0771, 0.9752, 0.9643, 1.0008, 0.9618, 0.9945,
        1.0215, 0.9414, 0.9764, 1.0937, 0.9128, 0.9850, 0.9863, 0.9926, 0.9818,
        1.0339, 0.9695, 1.0246, 1.0975, 1.0638, 0.9665, 0.9749, 1.1270, 0.9663,
        1.0108, 1.0769, 0.8690, 1.0047, 1.0034, 0.9957, 0.9872, 1.0118, 0.9973,
        0.9916, 1.0142, 0.9548, 0.9864, 0.9915, 1.0331, 0.9161, 1.0244, 0.9753,
        0.9179, 0.9977, 1.0032, 1.0196, 1.0640, 0.8816, 1.0045, 0.9996, 0.9618,
        1.0013, 1.0047, 1.0345, 0.9986, 1.1139, 1.1013, 1.1229, 1.0590, 1.0277,
        1.0138, 0.9854, 1.0033, 1.0129, 1.0045, 1.0955, 1.0985, 1.2076, 0.9141,
        0.9933, 0.9927, 0.9994, 1.0082, 1.0049, 1.0060, 0.9474, 0.9929, 0.9539,
        0.9802, 1.0304, 1.1109, 0.7920, 0.9928, 0.9437, 0.9201, 0.9202, 0.9715,
        1.0463, 1.0010, 1.0109, 1.0034, 0.9991, 1.0078, 1.0987, 0.9727, 0.9825,
        0.9939, 1.0568, 1.0707, 0.9178, 0.9989, 0.9695, 0.9982, 0.9846, 1.0459,
        1.0112, 0.8833, 0.9670, 1.0523, 0.9739, 1.0087, 1.0404, 1.0955, 1.0013,
        0.9931, 1.0445, 0.9703, 1.0160, 0.9606, 0.9984, 1.0093, 0.9958, 1.0313,
        1.0378, 0.9655, 1.0036, 0.9992, 1.0009, 0.9803, 0.9972, 0.9889, 1.0153,
        0.9479, 1.0356, 1.1231, 1.0369, 0.9935, 1.0020, 0.9789, 1.0188, 1.0111,
        0.9879, 1.0873, 1.3873, 0.8475, 0.9893, 0.9174, 1.0081, 1.0424, 0.9641,
        0.9290, 1.0257, 0.9968, 1.0281, 0.9783, 0.9789, 0.9951, 1.0111, 1.0175,
        1.0564, 0.9305, 0.9817, 0.9822, 0.9909, 0.9618, 1.0065, 0.9848, 1.0083,
        1.0257, 1.0685, 0.9102, 0.9993, 0.9614, 1.0171, 1.0003, 1.0007, 1.0438,
        1.0751, 0.8732, 0.9854, 0.9832, 0.9907, 0.9874, 1.0616, 0.9191, 1.0612,
        0.9710, 1.0198, 1.0223, 0.9853, 1.0035, 1.0035, 0.9971, 0.9903, 1.0400,
        1.0295, 1.0511, 1.0114, 0.9777, 1.0022, 1.0564, 0.9394, 0.9878, 1.0009,
        0.9895, 0.9610, 0.9642, 0.8477, 1.0567, 0.9868, 0.9918, 0.9928, 0.9925,
        1.0267, 1.0082, 0.9969, 1.0166, 0.8805, 1.0013, 0.9600, 1.0098, 0.8320,
        0.9804, 0.9921, 0.9870, 0.9601, 0.9749, 1.0251, 0.9708, 1.0240, 0.8592,
        0.9820, 1.0601, 0.9916, 1.0140, 1.1850, 1.0245, 1.0056, 1.0090, 1.0321,
        0.9607, 1.0040, 0.9826, 1.0156, 0.9375, 1.0096, 1.0013, 1.0603, 1.0960,
        1.0232, 1.0014, 1.0009, 1.0197, 0.9321, 0.9998, 1.0142, 1.0297, 1.0351,
        0.9209, 1.0040, 0.9880, 1.0039, 0.9792, 1.0042, 0.9998, 0.9783, 0.9617,
        1.0035, 1.0803, 1.4959, 0.8735, 1.0120, 0.9347, 0.9943, 1.0093, 1.0209,
        1.0207, 1.0301, 0.9251, 0.9911, 0.9494, 0.9870, 1.0097, 1.0733, 1.0824,
        0.9259, 0.9977, 1.0641, 1.0164, 0.9968, 0.9955, 0.9782, 1.0238, 1.0022,
        0.9929, 1.1107, 1.0471, 0.8913, 0.9977, 1.0060, 0.9752, 0.9997, 0.9401,
        0.9347, 0.8539, 1.0734, 1.0066, 1.0364, 1.0263, 0.9956, 0.9988, 1.0044,
        0.9114, 1.0149, 1.0281, 1.0065, 1.0486, 0.8434, 0.9642, 1.0035, 0.9757,
        1.0014, 1.0104, 1.2003, 0.9860, 1.0178, 1.0189, 0.9594, 1.0009, 0.9943,
        0.9791, 1.0019, 0.9075, 1.0370, 1.0415, 1.1000, 1.0489, 0.9769, 0.9902,
        0.9776, 1.1160, 0.9761, 1.0072, 0.9967, 0.8187, 1.0429, 1.0163, 1.0007,
        1.0242, 0.8402, 1.0313, 1.0368, 0.9978, 0.9741, 0.9991, 0.9573],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809091
t4: 1641198809091
surr1, surr2: tensor([-3.1546e+00, -9.8995e-01, -1.4126e+00, -2.4386e+00, -2.7367e+00,
        -2.9317e+00, -2.7714e+00, -2.7416e+00, -2.7881e+00, -2.6973e+00,
        -2.5108e+00, -2.6433e+00, -2.5562e+00, -2.4817e+00, -2.3869e+00,
        -2.3727e+00, -2.1693e+00, -2.3104e+00, -2.4339e+00, -2.3974e+00,
        -2.2748e+00, -2.1582e+00, -8.4835e-01, -1.7207e-01, -1.6398e+00,
        -1.7975e+00, -1.5645e+00, -1.6830e+00, -1.5965e+00, -1.3978e+00,
        -1.8395e+00, -1.9211e+00, -1.5679e+00, -1.9324e+00, -1.6503e+00,
        -1.5845e+00, -1.5673e+00, -1.6860e+00, -1.5553e+00, -1.5514e+00,
        -1.5106e+00, -1.4385e+00, -1.4074e+00, -1.5828e+00, -1.7197e+00,
        -1.4628e+00, -1.4355e+00, -1.5259e+00, -1.6540e+00, -1.7198e+00,
        -1.3839e+00, -1.5446e+00, -1.4813e+00, -1.8730e+00, -1.2728e+00,
        -1.2497e+00, -1.1284e+00, -1.6322e+00, -1.4319e+00, -1.3805e+00,
        -1.2274e+00, -1.5319e+00, -1.0907e+00, -1.2986e+00, -1.5161e+00,
        -1.2079e+00,  1.2569e+00,  1.3591e+00,  2.0720e+00,  2.3830e+00,
         5.2431e+00,  1.0121e-02,  2.9081e-01,  1.8702e-01, -5.5484e-02,
         1.7198e-01,  2.2221e-01,  2.7869e-01,  3.1964e-01,  3.1268e-01,
         2.7356e-01,  4.1615e-01,  3.4741e-01,  2.9151e-01,  4.6273e-01,
         1.2932e-01, -3.2007e-02,  1.9648e-01,  5.1852e-01,  2.7800e-01,
         1.6033e-01,  1.3549e-01,  2.3941e-01,  2.7795e-01,  3.8153e-01,
         8.9776e-02,  2.2360e-01,  2.9822e-01,  2.8236e-01,  2.1286e-01,
         2.8142e-01,  3.9865e-01,  2.3021e-01,  3.9062e-01, -1.2269e-01,
         9.0525e-02, -1.1421e-01,  4.3576e-02,  1.3175e-01,  1.8645e-01,
         2.5403e-01,  3.1872e-01,  1.9013e-01,  1.8978e+00,  2.9232e+00,
         4.3982e+00,  6.6559e-01,  9.7145e-01,  9.1110e-01,  9.8462e-01,
         9.8678e-01,  9.4603e-01,  8.8468e-01,  8.9158e-01,  8.1897e-01,
         8.1846e-01,  7.2980e-01,  7.9066e-01,  9.1480e-01,  4.9808e-01,
         6.0412e-01,  8.0500e-01,  4.7971e-01,  8.1474e-01,  2.9155e-01,
         8.5494e-01,  7.0099e-01,  8.9794e-01,  9.2757e-01,  8.6084e-01,
         1.0157e+00,  1.0364e+00,  5.9479e-01,  9.4220e-01,  1.0187e+00,
         1.0917e+00,  5.9980e-01,  7.6669e-01,  9.3567e-01,  1.0569e+00,
         5.4220e-01,  9.5142e-01,  7.1236e-01,  9.1977e-01,  8.6314e-01,
         4.7610e-01,  8.2977e-01,  8.3026e-01,  8.3505e-01,  9.2836e-01,
         7.9237e-01,  5.3516e-01,  9.5265e-01,  9.2291e-01,  4.9614e-01,
         9.2411e-01,  6.6817e-01,  6.6222e-01,  8.0806e-01,  9.3285e-01,
         9.6625e-01,  6.7507e-01,  6.8499e-01,  8.7038e-01,  8.5552e-01,
         1.0293e+00,  9.4481e-01,  9.4215e-01,  8.7555e-01,  1.0765e+00,
         8.4801e-01,  8.3503e-01,  4.2001e-01,  5.4917e-01,  8.2137e-01,
         9.5125e-01,  9.8058e-01,  8.4299e-01,  6.9075e-01,  5.8561e-01,
         7.6785e-01,  6.0112e-01,  3.4485e-01,  7.9822e-01,  4.8227e-01,
         6.6917e-01,  8.0930e-01,  6.2542e-01,  3.8735e-01,  6.8732e-01,
         3.2479e-01,  8.3524e-01,  6.6793e-01,  8.7764e-01,  7.6328e-01,
         7.4096e-01,  7.2566e-01,  3.4065e-01,  5.9759e-01,  7.1495e-01,
         5.6462e-01,  6.1320e-01,  7.0610e-01,  6.0523e-01,  5.4160e-01,
         6.1369e-01,  6.9527e-01,  7.5141e-01,  4.3416e-01,  5.9698e-01,
         7.1352e-01,  6.1828e-01,  6.8954e-01,  6.1041e-01,  8.1682e-01,
         5.7548e-01,  4.3748e-01,  6.1309e-01,  6.1776e-01,  7.2067e-01,
         6.4379e-01,  6.3962e-01,  1.9044e-01,  7.7790e-01,  3.0088e-01,
         7.3017e-01,  6.2581e-01,  6.6850e-01,  7.3783e-01,  8.1592e-01,
         7.2313e-01,  6.4110e-01,  7.8173e-01,  2.8522e-01,  6.7277e-01,
         4.0802e-01,  6.5172e-01,  5.6971e-01,  4.8104e-01,  1.5967e-01,
         5.0655e-01,  5.4366e-01,  5.8694e-01,  4.4680e-01,  1.8161e-01,
         1.0122e-01,  2.0933e-01,  1.1270e-03,  4.3724e-01,  4.1428e-01,
         4.4683e-01,  3.7617e-01,  2.7060e-01,  3.2560e-01,  2.8853e-01,
        -7.9100e-02,  3.4022e-01,  3.2553e-01,  4.8109e-01,  4.8686e-02,
         1.1338e-01,  1.6637e-01,  4.0173e-01,  3.9216e-01,  1.7265e-01,
        -2.7534e-02,  2.2665e-01,  4.3825e-01,  2.4698e-01,  1.3491e-01,
         2.0498e-01,  1.6027e-01,  1.8695e-01,  2.2983e-01, -1.2859e-02,
         3.7285e-01,  3.8465e-01,  3.0662e-01,  1.4907e-01,  3.1273e-01,
         2.7729e-01,  2.7602e-01,  1.7897e-01,  3.0373e-01,  2.4019e-01,
         1.7565e-01,  1.2717e-02,  5.8559e-03,  2.4310e-01,  1.7540e-01,
         1.9673e-01, -2.9885e-01,  2.5947e-01,  5.0410e-01,  3.1919e-01,
         4.0647e-01,  2.8658e-01,  2.8702e-01,  4.4000e-01,  2.7415e-01,
         3.5562e-01,  1.4266e-01,  2.8328e-01,  3.4188e-01,  3.7210e-01,
        -4.5538e-03,  9.0555e-02, -1.3671e-01, -1.0618e-01,  3.3342e-01,
        -1.4597e-01,  3.3849e-01, -1.2875e-01,  2.7172e-01,  2.8474e-01,
         2.5043e-01,  2.1998e-01,  8.7076e-02,  7.6536e-02,  2.3604e-01,
         1.8367e-01,  3.4648e-01, -8.3059e-02,  5.5010e-02,  1.7337e-01,
         1.4036e-01, -1.1732e-01,  2.7826e-01,  2.0637e-01,  2.9366e-01,
         5.0282e-02,  3.9536e-02,  1.3063e-01,  5.6665e-02, -3.8720e-01,
         6.1430e-02, -4.5035e-02,  7.6049e-02, -7.1686e-02,  8.3450e-02,
         1.6018e-02, -2.4657e-01, -4.1103e-01, -1.7581e-01, -4.6310e-01,
        -2.5822e-01, -3.5668e-01, -1.8836e-01,  5.6787e-02, -9.4798e-02,
         2.6582e-02, -3.5958e-01, -5.1408e-01, -1.6811e-01,  9.9497e-02,
        -2.9695e-01, -4.2827e-01, -4.2780e-01, -2.9005e-01, -2.1098e-01,
        -1.6114e-01, -4.1953e-01, -6.3717e-01, -1.8401e-01, -2.3646e-01,
        -2.6726e-01, -1.9457e-01, -2.4077e-01, -2.5304e-01, -3.8296e-01,
        -3.3595e-01, -5.9707e-01, -8.8254e-01, -5.9353e-01, -7.0433e-01,
        -5.0416e-01, -4.2327e-01, -3.7624e-01, -1.1054e+00, -8.6218e-01,
        -5.3628e-01, -6.2209e-01, -8.4145e-01, -7.3613e-01, -8.2563e-01,
        -7.0611e-01, -6.8779e-01, -8.8322e-01, -9.8811e-01, -9.6838e-01,
        -9.1627e-01, -7.3436e-01, -8.4419e-01, -8.2111e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1546e+00, -9.8995e-01, -1.4126e+00, -2.4386e+00, -2.7367e+00,
        -2.9317e+00, -2.7714e+00, -2.7416e+00, -2.7881e+00, -2.6973e+00,
        -2.5108e+00, -2.6433e+00, -2.5562e+00, -2.4817e+00, -2.3869e+00,
        -2.3727e+00, -2.1693e+00, -2.3300e+00, -2.4339e+00, -2.3974e+00,
        -2.2748e+00, -2.1582e+00, -8.4835e-01, -1.7207e-01, -1.6398e+00,
        -1.7975e+00, -1.5645e+00, -1.6830e+00, -1.5965e+00, -1.3978e+00,
        -1.8517e+00, -1.9211e+00, -1.5679e+00, -1.9324e+00, -1.6503e+00,
        -1.5845e+00, -1.5673e+00, -1.6860e+00, -1.5553e+00, -1.5514e+00,
        -1.5106e+00, -1.4385e+00, -1.4074e+00, -1.5828e+00, -1.7197e+00,
        -1.4628e+00, -1.4355e+00, -1.5259e+00, -1.6540e+00, -1.7198e+00,
        -1.3839e+00, -1.5446e+00, -1.4813e+00, -1.8730e+00, -1.2728e+00,
        -1.2497e+00, -1.1284e+00, -1.6322e+00, -1.4319e+00, -1.3805e+00,
        -1.2274e+00, -1.5319e+00, -1.0907e+00, -1.2986e+00, -1.5161e+00,
        -1.2079e+00,  1.2569e+00,  1.3591e+00,  2.0720e+00,  2.3830e+00,
         5.1175e+00,  1.0121e-02,  2.9081e-01,  1.8702e-01, -5.7464e-02,
         1.7198e-01,  2.2221e-01,  2.7869e-01,  3.1964e-01,  3.1268e-01,
         2.7356e-01,  4.1615e-01,  3.4741e-01,  2.9151e-01,  4.6273e-01,
         1.2932e-01, -3.2007e-02,  1.9648e-01,  5.1852e-01,  2.7800e-01,
         1.6033e-01,  1.3549e-01,  2.3941e-01,  2.7795e-01,  3.8153e-01,
         9.1655e-02,  2.2360e-01,  2.9822e-01,  2.8236e-01,  2.1286e-01,
         2.8142e-01,  3.9865e-01,  2.3021e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1421e-01,  4.3576e-02,  1.3175e-01,  1.8645e-01,
         2.5403e-01,  3.1872e-01,  1.9013e-01,  1.8978e+00,  2.9232e+00,
         4.0062e+00,  6.6559e-01,  9.7145e-01,  9.1110e-01,  9.8462e-01,
         9.8678e-01,  9.4603e-01,  8.8468e-01,  8.9158e-01,  8.1897e-01,
         8.1846e-01,  7.2980e-01,  7.9066e-01,  9.0579e-01,  5.6602e-01,
         6.0412e-01,  8.0500e-01,  4.7971e-01,  8.1474e-01,  2.9155e-01,
         8.5494e-01,  7.0099e-01,  8.9794e-01,  9.2757e-01,  8.6084e-01,
         1.0157e+00,  1.0364e+00,  5.9479e-01,  9.4220e-01,  1.0187e+00,
         1.0917e+00,  5.9980e-01,  7.6669e-01,  9.3567e-01,  1.0569e+00,
         5.4220e-01,  9.5142e-01,  7.1236e-01,  9.1977e-01,  8.7946e-01,
         4.7610e-01,  8.2977e-01,  8.3026e-01,  8.3505e-01,  9.2836e-01,
         7.9237e-01,  5.3516e-01,  9.5265e-01,  9.2291e-01,  4.9614e-01,
         9.2411e-01,  6.6817e-01,  6.6222e-01,  8.0806e-01,  9.3285e-01,
         9.6625e-01,  6.7507e-01,  6.8499e-01,  8.7038e-01,  8.5552e-01,
         1.0293e+00,  9.4481e-01,  9.4215e-01,  8.7555e-01,  1.0765e+00,
         8.4801e-01,  8.3503e-01,  4.1136e-01,  5.4917e-01,  8.2137e-01,
         9.5125e-01,  9.8058e-01,  8.4299e-01,  6.9075e-01,  5.8561e-01,
         7.6785e-01,  4.7662e-01,  3.6623e-01,  7.9822e-01,  4.8227e-01,
         6.6917e-01,  8.0930e-01,  6.2542e-01,  3.8735e-01,  6.8732e-01,
         3.2479e-01,  8.3524e-01,  6.6793e-01,  8.7764e-01,  7.6328e-01,
         7.4096e-01,  7.2566e-01,  3.4065e-01,  5.9759e-01,  7.1495e-01,
         5.6462e-01,  6.1320e-01,  7.0610e-01,  6.0523e-01,  5.4160e-01,
         6.1369e-01,  6.9527e-01,  7.5141e-01,  4.3416e-01,  5.9698e-01,
         7.1352e-01,  6.1828e-01,  6.8954e-01,  6.1041e-01,  8.1682e-01,
         5.7548e-01,  4.5092e-01,  6.1309e-01,  6.1776e-01,  7.2067e-01,
         6.4379e-01,  6.3962e-01,  1.9044e-01,  7.7790e-01,  3.0088e-01,
         7.3017e-01,  6.2581e-01,  6.6850e-01,  7.3783e-01,  8.1592e-01,
         7.2313e-01,  6.4110e-01,  7.8173e-01,  2.8522e-01,  6.7277e-01,
         4.0802e-01,  6.5172e-01,  5.6971e-01,  4.8104e-01,  1.5967e-01,
         5.0655e-01,  5.4366e-01,  5.8694e-01,  4.4680e-01,  1.8161e-01,
         1.0747e-01,  2.0933e-01,  1.1270e-03,  4.3724e-01,  4.1428e-01,
         4.4683e-01,  3.7617e-01,  2.7060e-01,  3.2560e-01,  2.8853e-01,
        -8.0855e-02,  3.4022e-01,  3.2553e-01,  4.8109e-01,  5.2668e-02,
         1.1338e-01,  1.6637e-01,  4.0173e-01,  3.9216e-01,  1.7265e-01,
        -2.7534e-02,  2.2665e-01,  4.3825e-01,  2.5872e-01,  1.3491e-01,
         2.0498e-01,  1.6027e-01,  1.8695e-01,  2.1334e-01, -1.2859e-02,
         3.7285e-01,  3.8465e-01,  3.0662e-01,  1.4907e-01,  3.1273e-01,
         2.7729e-01,  2.7602e-01,  1.7897e-01,  3.0373e-01,  2.4019e-01,
         1.7565e-01,  1.2717e-02,  5.8559e-03,  2.4310e-01,  1.7540e-01,
         1.9673e-01, -2.9885e-01,  2.5947e-01,  5.0410e-01,  3.1919e-01,
         4.0647e-01,  2.8658e-01,  2.8702e-01,  4.4000e-01,  2.7415e-01,
         3.5562e-01,  1.4266e-01,  2.8328e-01,  3.4188e-01,  3.7210e-01,
        -4.5538e-03,  9.0555e-02, -1.0053e-01, -1.0941e-01,  3.3342e-01,
        -1.4597e-01,  3.3849e-01, -1.2875e-01,  2.7172e-01,  2.8474e-01,
         2.5043e-01,  2.1998e-01,  8.7076e-02,  7.6536e-02,  2.3604e-01,
         1.8367e-01,  3.4648e-01, -8.3059e-02,  5.5010e-02,  1.7337e-01,
         1.4036e-01, -1.1732e-01,  2.7826e-01,  2.0637e-01,  2.9366e-01,
         5.0282e-02,  3.9536e-02,  1.3063e-01,  5.6121e-02, -3.8720e-01,
         6.2027e-02, -4.5035e-02,  7.6049e-02, -7.1686e-02,  8.3450e-02,
         1.6018e-02, -2.4657e-01, -4.3320e-01, -1.7581e-01, -4.6310e-01,
        -2.5822e-01, -3.5668e-01, -1.8836e-01,  5.6787e-02, -9.4798e-02,
         2.6582e-02, -3.5958e-01, -5.1408e-01, -1.6811e-01,  9.9497e-02,
        -3.1689e-01, -4.2827e-01, -4.2780e-01, -2.9005e-01, -2.1098e-01,
        -1.6114e-01, -3.8447e-01, -6.3717e-01, -1.8401e-01, -2.3646e-01,
        -2.6726e-01, -1.9457e-01, -2.4077e-01, -2.5304e-01, -3.8296e-01,
        -3.3595e-01, -5.9707e-01, -8.8254e-01, -5.9353e-01, -7.0433e-01,
        -5.0416e-01, -4.2327e-01, -3.7624e-01, -1.0896e+00, -8.6218e-01,
        -5.3628e-01, -6.2209e-01, -9.2498e-01, -7.3613e-01, -8.2563e-01,
        -7.0611e-01, -6.8779e-01, -9.4607e-01, -9.8811e-01, -9.6838e-01,
        -9.1627e-01, -7.3436e-01, -8.4419e-01, -8.2111e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809100
t6: 1641198809100
state_values: tensor([1.7419, 1.5819, 1.7185, 1.9195, 2.1215, 2.2648, 2.3717, 2.4399, 2.4872,
        2.5692, 2.6903, 2.6551, 2.6836, 2.7002, 2.7794, 3.0195, 2.9429, 2.8813,
        2.9603, 2.9947, 3.0455, 3.0449, 2.8228, 2.7368, 2.9342, 2.9631, 2.9837,
        3.0999, 3.0446, 3.0495, 3.3592, 3.2678, 3.2826, 3.1774, 3.1588, 3.1600,
        3.1730, 3.1728, 3.2387, 3.2144, 3.2607, 3.2573, 3.2623, 3.4126, 3.3052,
        3.3067, 3.3002, 3.4048, 3.3359, 3.3193, 3.3429, 3.4242, 3.3687, 3.5693,
        3.4603, 3.4243, 3.4809, 3.7011, 3.5126, 3.6366, 3.5165, 3.4865, 3.4755,
        3.5332, 3.4969, 3.5204, 3.2415, 3.1678, 3.1091, 3.1402, 3.0502, 3.3007,
        3.4296, 3.4431, 3.4566, 3.6031, 3.5739, 3.5858, 3.5739, 3.6022, 3.5686,
        3.5657, 3.5958, 3.5837, 3.6394, 3.8067, 3.6605, 3.6409, 3.7067, 3.7565,
        3.6834, 3.7939, 3.7275, 3.6964, 3.6769, 3.6817, 3.8128, 3.7696, 3.7472,
        3.8232, 3.7937, 3.7383, 3.7337, 3.7398, 3.7273, 3.7368, 3.7399, 3.7598,
        3.7648, 3.7685, 3.8387, 3.8883, 3.8880, 3.6273, 3.4844, 3.3834, 3.6110,
        3.7821, 3.7573, 3.8006, 3.7922, 3.7835, 3.8004, 3.7988, 3.9301, 3.8560,
        3.9954, 3.8982, 3.8615, 3.8571, 4.1147, 4.0031, 3.9069, 3.9662, 4.2940,
        4.0839, 4.1676, 4.0478, 4.0499, 4.0841, 4.0027, 3.9753, 3.9543, 3.9862,
        4.0112, 4.0115, 3.9639, 3.9719, 4.0909, 4.0101, 4.2902, 4.1035, 4.0456,
        4.0188, 4.0133, 4.3214, 4.2029, 4.2015, 4.0952, 4.0664, 4.0604, 4.0631,
        4.0479, 4.0799, 4.0384, 4.0786, 4.0756, 4.1871, 4.0959, 4.0686, 4.0806,
        4.0812, 4.0861, 4.1369, 4.0964, 4.0754, 4.0784, 4.1292, 4.1207, 4.1076,
        4.0855, 4.1782, 4.1105, 4.1116, 4.1038, 4.1184, 4.0946, 4.2065, 4.3260,
        4.3506, 4.2111, 4.1695, 4.1357, 4.2558, 4.1868, 4.3539, 4.3193, 4.3560,
        4.2468, 4.3059, 4.2390, 4.2160, 4.2152, 4.2335, 4.2944, 4.3487, 4.3168,
        4.2595, 4.2355, 4.3129, 4.4332, 4.2866, 4.2767, 4.3696, 4.4198, 4.3049,
        4.2753, 4.2563, 4.2567, 4.3735, 4.2850, 4.4186, 4.3841, 4.3352, 4.2833,
        4.2725, 4.2744, 4.4309, 4.3152, 4.3466, 4.3560, 4.3800, 4.3053, 4.3514,
        4.2979, 4.3279, 4.2991, 4.2853, 4.3197, 4.2778, 4.2807, 4.3074, 4.3159,
        4.3035, 4.2883, 4.3009, 4.2900, 4.3791, 4.3392, 4.3251, 4.3980, 4.4282,
        4.4053, 4.4348, 4.6159, 4.4260, 4.6693, 4.4551, 4.4629, 4.4849, 4.4944,
        4.5390, 4.6306, 4.5365, 4.5508, 4.4611, 4.5414, 4.4522, 4.5095, 4.4329,
        4.6703, 4.4692, 4.4725, 4.4898, 4.6788, 4.5004, 4.4694, 4.4899, 4.4426,
        4.7191, 4.6298, 4.6908, 4.5569, 4.4894, 4.4469, 4.4486, 4.4487, 4.4515,
        4.4485, 4.5221, 4.4712, 4.5270, 4.4758, 4.5672, 4.5677, 4.4957, 4.4836,
        4.4675, 4.4572, 4.4767, 4.4763, 4.5985, 4.6422, 4.5346, 4.5071, 4.4935,
        4.4972, 4.6548, 4.5681, 4.7039, 4.6019, 4.7567, 4.6378, 4.5798, 4.6115,
        4.8780, 4.6353, 4.5573, 4.5356, 4.6404, 4.5613, 4.6422, 4.5589, 4.5508,
        4.5370, 4.5367, 4.5262, 4.7440, 4.6099, 4.7457, 4.6092, 4.5538, 4.5679,
        4.5588, 4.6730, 4.5878, 4.5552, 4.5438, 4.5722, 4.5743, 4.7186, 4.7367,
        4.6434, 4.6437, 4.5894, 4.5786, 4.7798, 4.6461, 4.6104, 4.6293, 4.6178,
        4.8207, 4.6541, 4.8206, 5.0395, 4.7598, 4.6983, 4.6748, 4.6424, 4.6589,
        4.6173, 4.9149, 4.7103, 4.6490, 4.6041, 4.6241, 4.9342, 4.9467, 4.7933,
        4.8213, 4.7325, 4.6848, 4.6563, 4.6629, 4.6501, 4.6462, 4.7349, 4.7135,
        4.7130, 4.7725, 4.6982, 4.9648, 4.7469, 4.7079, 4.6843, 4.6857, 4.7312,
        4.7526, 5.1152, 4.8190, 4.8226, 4.8378, 4.7449, 4.9189, 4.9949, 4.8924,
        4.8046, 4.7694, 4.9767, 4.9765, 4.9397, 4.7969, 4.8669, 4.8342],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809105
t8: 1641198809105
t9: 1641198809106
t10: 1641198809117
t11: 1641198809118
t12: 1641198809118
t1: 1641198809118
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809129
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9976, 1.0109, 0.9484, 0.9812, 0.9850, 0.9957, 0.9935, 0.9811, 0.9876,
        1.0203, 0.9517, 0.9957, 1.0206, 0.9611, 0.9883, 0.9176, 0.8930, 0.8930,
        1.0131, 1.0480, 1.0318, 0.9834, 1.0045, 0.9772, 0.9978, 0.9965, 0.9472,
        0.9949, 0.9992, 0.9216, 0.9024, 1.1016, 0.8887, 1.0153, 1.0180, 0.9939,
        1.0454, 0.9311, 1.0100, 0.9797, 0.9947, 0.9988, 0.9455, 0.9691, 0.9862,
        1.0113, 0.9401, 0.9774, 1.0698, 0.9747, 0.9672, 0.9957, 0.9646, 0.9955,
        1.0174, 0.9433, 0.9908, 1.0816, 0.9139, 0.9833, 0.9748, 0.9925, 0.9832,
        1.0246, 0.9695, 1.0214, 1.0956, 1.0647, 0.9672, 0.9848, 1.1199, 0.9665,
        1.0068, 1.0696, 0.8689, 1.0056, 1.0076, 0.9973, 0.9902, 1.0090, 0.9976,
        0.9925, 1.0121, 0.9573, 0.9988, 0.9813, 1.0324, 0.9187, 1.0293, 0.9693,
        0.9230, 0.9972, 1.0020, 1.0182, 1.0606, 0.8816, 1.0054, 1.0003, 0.9673,
        1.0032, 1.0033, 1.0316, 0.9987, 1.1049, 1.1038, 1.1228, 1.0588, 1.0257,
        1.0117, 0.9886, 1.0110, 1.0157, 0.9986, 1.0940, 1.1000, 1.1994, 0.9135,
        0.9901, 0.9957, 0.9996, 1.0067, 1.0046, 1.0056, 0.9523, 0.9911, 0.9624,
        0.9764, 1.0264, 1.1081, 0.7897, 0.9942, 0.9259, 0.9189, 0.9309, 0.9738,
        1.0590, 1.0010, 1.0159, 1.0087, 0.9988, 1.0057, 1.0928, 0.9719, 0.9838,
        0.9957, 1.0415, 1.0723, 0.9189, 0.9985, 0.9867, 0.9985, 0.9764, 1.0452,
        1.0110, 0.8914, 0.9712, 1.0609, 0.9684, 1.0076, 1.0379, 1.0935, 1.0012,
        0.9938, 1.0320, 0.9699, 1.0134, 0.9645, 0.9939, 1.0084, 0.9962, 1.0272,
        1.0368, 0.9666, 1.0023, 0.9994, 1.0009, 0.9827, 0.9980, 0.9909, 1.0124,
        0.9511, 1.0210, 1.1258, 1.0374, 0.9940, 1.0013, 0.9851, 1.0320, 1.0192,
        0.9844, 1.0780, 1.3949, 0.8420, 0.9822, 0.9237, 1.0113, 1.0518, 0.9538,
        0.9303, 1.0182, 0.9968, 1.0265, 0.9798, 0.9833, 1.0034, 1.0140, 1.0063,
        1.0565, 0.9312, 0.9900, 0.9772, 0.9920, 0.9665, 1.0143, 0.9809, 1.0077,
        1.0243, 1.0655, 0.9100, 0.9984, 0.9701, 1.0208, 1.0005, 1.0007, 1.0398,
        1.0737, 0.8736, 0.9822, 0.9863, 0.9933, 0.9908, 1.0461, 0.9168, 1.0542,
        0.9712, 1.0167, 1.0205, 0.9864, 1.0026, 1.0030, 0.9976, 0.9916, 1.0329,
        1.0297, 1.0493, 1.0111, 0.9804, 0.9991, 1.0500, 0.9389, 0.9915, 1.0039,
        0.9936, 0.9728, 0.9540, 0.8501, 1.0445, 0.9873, 0.9948, 0.9975, 0.9996,
        1.0396, 1.0098, 1.0025, 1.0018, 0.8777, 0.9986, 0.9637, 0.9989, 0.8321,
        0.9741, 0.9928, 0.9891, 0.9714, 0.9652, 1.0245, 0.9719, 1.0195, 0.8650,
        0.9850, 1.0736, 0.9897, 1.0079, 1.1806, 1.0249, 1.0055, 1.0074, 1.0281,
        0.9625, 1.0022, 0.9854, 1.0118, 0.9407, 1.0139, 0.9962, 1.0569, 1.0954,
        1.0233, 1.0014, 1.0008, 1.0157, 0.9334, 0.9996, 1.0099, 1.0288, 1.0337,
        0.9237, 1.0048, 0.9998, 1.0046, 0.9920, 1.0046, 1.0003, 0.9819, 0.9751,
        0.9961, 1.0724, 1.5150, 0.8689, 0.9956, 0.9361, 0.9789, 1.0092, 1.0189,
        1.0183, 1.0272, 0.9305, 0.9874, 0.9579, 0.9833, 1.0071, 1.0688, 1.0829,
        0.9269, 0.9932, 1.0567, 1.0165, 0.9972, 0.9968, 0.9861, 1.0299, 1.0026,
        0.9945, 1.0955, 1.0482, 0.8929, 0.9968, 1.0007, 0.9765, 0.9998, 0.9481,
        0.9219, 0.8526, 1.0852, 1.0012, 1.0312, 1.0259, 0.9958, 0.9991, 1.0038,
        0.9206, 1.0075, 1.0271, 1.0064, 1.0443, 0.8448, 0.9708, 1.0041, 0.9842,
        1.0015, 1.0045, 1.1948, 0.9851, 1.0167, 1.0166, 0.9629, 1.0026, 0.9968,
        0.9840, 1.0007, 0.9177, 1.0238, 1.0420, 1.0979, 1.0490, 0.9781, 0.9939,
        0.9974, 1.1055, 0.9771, 1.0118, 0.9803, 0.8154, 1.0500, 1.0195, 1.0018,
        1.0165, 0.8396, 1.0365, 1.0429, 0.9955, 0.9783, 1.0005, 0.9718],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809134
t4: 1641198809134
surr1, surr2: tensor([-3.1539e+00, -9.9162e-01, -1.4015e+00, -2.4286e+00, -2.7051e+00,
        -2.9318e+00, -2.7649e+00, -2.7138e+00, -2.7895e+00, -2.7176e+00,
        -2.4927e+00, -2.6455e+00, -2.5403e+00, -2.4848e+00, -2.4236e+00,
        -2.3902e+00, -2.1396e+00, -2.3119e+00, -2.4430e+00, -2.4246e+00,
        -2.2879e+00, -2.1490e+00, -8.4827e-01, -1.7257e-01, -1.6316e+00,
        -1.7980e+00, -1.5739e+00, -1.6813e+00, -1.5987e+00, -1.4273e+00,
        -1.8566e+00, -1.9420e+00, -1.5425e+00, -1.9318e+00, -1.6494e+00,
        -1.5851e+00, -1.5557e+00, -1.6848e+00, -1.5507e+00, -1.5551e+00,
        -1.5131e+00, -1.4393e+00, -1.4219e+00, -1.5684e+00, -1.7207e+00,
        -1.4602e+00, -1.4429e+00, -1.5193e+00, -1.6428e+00, -1.7188e+00,
        -1.3881e+00, -1.5368e+00, -1.4856e+00, -1.8748e+00, -1.2678e+00,
        -1.2522e+00, -1.1450e+00, -1.6141e+00, -1.4336e+00, -1.3781e+00,
        -1.2132e+00, -1.5318e+00, -1.0922e+00, -1.2868e+00, -1.5161e+00,
        -1.2041e+00,  1.2547e+00,  1.3603e+00,  2.0736e+00,  2.4072e+00,
         5.2102e+00,  1.0123e-02,  2.8966e-01,  1.8575e-01, -5.5476e-02,
         1.7213e-01,  2.2314e-01,  2.7913e-01,  3.2059e-01,  3.1181e-01,
         2.7364e-01,  4.1655e-01,  3.4670e-01,  2.9228e-01,  4.6851e-01,
         1.2798e-01, -3.1987e-02,  1.9705e-01,  5.2103e-01,  2.7629e-01,
         1.6122e-01,  1.3542e-01,  2.3912e-01,  2.7758e-01,  3.8032e-01,
         8.9778e-02,  2.2380e-01,  2.9840e-01,  2.8397e-01,  2.1327e-01,
         2.8103e-01,  3.9752e-01,  2.3025e-01,  3.8749e-01, -1.2297e-01,
         9.0510e-02, -1.1419e-01,  4.3493e-02,  1.3147e-01,  1.8704e-01,
         2.5599e-01,  3.1963e-01,  1.8901e-01,  1.8952e+00,  2.9271e+00,
         4.3684e+00,  6.6513e-01,  9.6831e-01,  9.1383e-01,  9.8489e-01,
         9.8533e-01,  9.4575e-01,  8.8431e-01,  8.9625e-01,  8.1748e-01,
         8.2577e-01,  7.2697e-01,  7.8757e-01,  9.1245e-01,  4.9666e-01,
         6.0494e-01,  7.8984e-01,  4.7905e-01,  8.2424e-01,  2.9224e-01,
         8.6530e-01,  7.0097e-01,  9.0243e-01,  9.3256e-01,  8.6050e-01,
         1.0136e+00,  1.0309e+00,  5.9431e-01,  9.4351e-01,  1.0206e+00,
         1.0758e+00,  6.0072e-01,  7.6760e-01,  9.3531e-01,  1.0756e+00,
         5.4240e-01,  9.4353e-01,  7.1183e-01,  9.1959e-01,  8.7102e-01,
         4.7820e-01,  8.3654e-01,  8.2558e-01,  8.3417e-01,  9.2615e-01,
         7.9092e-01,  5.3513e-01,  9.5330e-01,  9.1189e-01,  4.9590e-01,
         9.2171e-01,  6.7084e-01,  6.5925e-01,  8.0738e-01,  9.3326e-01,
         9.6234e-01,  6.7440e-01,  6.8580e-01,  8.6927e-01,  8.5570e-01,
         1.0293e+00,  9.4721e-01,  9.4292e-01,  8.7733e-01,  1.0734e+00,
         8.5094e-01,  8.2326e-01,  4.2100e-01,  5.4945e-01,  8.2179e-01,
         9.5057e-01,  9.8675e-01,  8.5388e-01,  6.9629e-01,  5.8350e-01,
         7.6133e-01,  6.0441e-01,  3.4263e-01,  7.9250e-01,  4.8560e-01,
         6.7131e-01,  8.1661e-01,  6.1874e-01,  3.8787e-01,  6.8226e-01,
         3.2480e-01,  8.3400e-01,  6.6899e-01,  8.8157e-01,  7.6959e-01,
         7.4310e-01,  7.1771e-01,  3.4071e-01,  5.9801e-01,  7.2099e-01,
         5.6174e-01,  6.1383e-01,  7.0955e-01,  6.0992e-01,  5.3944e-01,
         6.1333e-01,  6.9430e-01,  7.4928e-01,  4.3408e-01,  5.9642e-01,
         7.1997e-01,  6.2052e-01,  6.8969e-01,  6.1043e-01,  8.1370e-01,
         5.7470e-01,  4.3767e-01,  6.1108e-01,  6.1967e-01,  7.2253e-01,
         6.4602e-01,  6.3030e-01,  1.8996e-01,  7.7277e-01,  3.0095e-01,
         7.2801e-01,  6.2469e-01,  6.6923e-01,  7.3721e-01,  8.1549e-01,
         7.2344e-01,  6.4195e-01,  7.7643e-01,  2.8527e-01,  6.7158e-01,
         4.0789e-01,  6.5352e-01,  5.6794e-01,  4.7811e-01,  1.5958e-01,
         5.0844e-01,  5.4531e-01,  5.8938e-01,  4.5231e-01,  1.7968e-01,
         1.0151e-01,  2.0690e-01,  1.1276e-03,  4.3858e-01,  4.1622e-01,
         4.5005e-01,  3.8087e-01,  2.7102e-01,  3.2742e-01,  2.8432e-01,
        -7.8851e-02,  3.3931e-01,  3.2678e-01,  4.7589e-01,  4.8695e-02,
         1.1265e-01,  1.6649e-01,  4.0258e-01,  3.9676e-01,  1.7093e-01,
        -2.7519e-02,  2.2690e-01,  4.3632e-01,  2.4866e-01,  1.3534e-01,
         2.0760e-01,  1.5996e-01,  1.8583e-01,  2.2897e-01, -1.2864e-02,
         3.7280e-01,  3.8404e-01,  3.0543e-01,  1.4934e-01,  3.1217e-01,
         2.7807e-01,  2.7498e-01,  1.7958e-01,  3.0500e-01,  2.3897e-01,
         1.7509e-01,  1.2711e-02,  5.8565e-03,  2.4309e-01,  1.7538e-01,
         1.9597e-01, -2.9924e-01,  2.5942e-01,  5.0192e-01,  3.1890e-01,
         4.0594e-01,  2.8746e-01,  2.8724e-01,  4.4526e-01,  2.7433e-01,
         3.6027e-01,  1.4272e-01,  2.8342e-01,  3.4313e-01,  3.7729e-01,
        -4.5202e-03,  8.9894e-02, -1.3846e-01, -1.0562e-01,  3.2803e-01,
        -1.4620e-01,  3.3323e-01, -1.2874e-01,  2.7118e-01,  2.8407e-01,
         2.4973e-01,  2.2126e-01,  8.6749e-02,  7.7219e-02,  2.3516e-01,
         1.8321e-01,  3.4505e-01, -8.3099e-02,  5.5068e-02,  1.7259e-01,
         1.3938e-01, -1.1733e-01,  2.7835e-01,  2.0664e-01,  2.9605e-01,
         5.0585e-02,  3.9553e-02,  1.3085e-01,  5.5891e-02, -3.8761e-01,
         6.1538e-02, -4.4997e-02,  7.5645e-02, -7.1780e-02,  8.3461e-02,
         1.6154e-02, -2.4321e-01, -4.1040e-01, -1.7774e-01, -4.6063e-01,
        -2.5692e-01, -3.5654e-01, -1.8840e-01,  5.6800e-02, -9.4748e-02,
         2.6849e-02, -3.5697e-01, -5.1362e-01, -1.6809e-01,  9.9087e-02,
        -2.9746e-01, -4.3120e-01, -4.2804e-01, -2.9257e-01, -2.1101e-01,
        -1.6020e-01, -4.1760e-01, -6.3659e-01, -1.8383e-01, -2.3594e-01,
        -2.6822e-01, -1.9490e-01, -2.4139e-01, -2.5430e-01, -3.8251e-01,
        -3.3973e-01, -5.8946e-01, -8.8295e-01, -5.9238e-01, -7.0436e-01,
        -5.0477e-01, -4.2485e-01, -3.8387e-01, -1.0951e+00, -8.6301e-01,
        -5.3874e-01, -6.1189e-01, -8.3806e-01, -7.4118e-01, -8.2821e-01,
        -7.0685e-01, -6.8260e-01, -8.8263e-01, -9.9307e-01, -9.7407e-01,
        -9.1419e-01, -7.3755e-01, -8.4540e-01, -8.3361e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1539e+00, -9.9162e-01, -1.4015e+00, -2.4286e+00, -2.7051e+00,
        -2.9318e+00, -2.7649e+00, -2.7138e+00, -2.7895e+00, -2.7176e+00,
        -2.4927e+00, -2.6455e+00, -2.5403e+00, -2.4848e+00, -2.4236e+00,
        -2.3902e+00, -2.1565e+00, -2.3300e+00, -2.4430e+00, -2.4246e+00,
        -2.2879e+00, -2.1490e+00, -8.4827e-01, -1.7257e-01, -1.6316e+00,
        -1.7980e+00, -1.5739e+00, -1.6813e+00, -1.5987e+00, -1.4273e+00,
        -1.8566e+00, -1.9391e+00, -1.5622e+00, -1.9318e+00, -1.6494e+00,
        -1.5851e+00, -1.5557e+00, -1.6848e+00, -1.5507e+00, -1.5551e+00,
        -1.5131e+00, -1.4393e+00, -1.4219e+00, -1.5684e+00, -1.7207e+00,
        -1.4602e+00, -1.4429e+00, -1.5193e+00, -1.6428e+00, -1.7188e+00,
        -1.3881e+00, -1.5368e+00, -1.4856e+00, -1.8748e+00, -1.2678e+00,
        -1.2522e+00, -1.1450e+00, -1.6141e+00, -1.4336e+00, -1.3781e+00,
        -1.2132e+00, -1.5318e+00, -1.0922e+00, -1.2868e+00, -1.5161e+00,
        -1.2041e+00,  1.2547e+00,  1.3603e+00,  2.0736e+00,  2.4072e+00,
         5.1175e+00,  1.0123e-02,  2.8966e-01,  1.8575e-01, -5.7464e-02,
         1.7213e-01,  2.2314e-01,  2.7913e-01,  3.2059e-01,  3.1181e-01,
         2.7364e-01,  4.1655e-01,  3.4670e-01,  2.9228e-01,  4.6851e-01,
         1.2798e-01, -3.1987e-02,  1.9705e-01,  5.2103e-01,  2.7629e-01,
         1.6122e-01,  1.3542e-01,  2.3912e-01,  2.7758e-01,  3.8032e-01,
         9.1655e-02,  2.2380e-01,  2.9840e-01,  2.8397e-01,  2.1327e-01,
         2.8103e-01,  3.9752e-01,  2.3025e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1419e-01,  4.3493e-02,  1.3147e-01,  1.8704e-01,
         2.5599e-01,  3.1963e-01,  1.8901e-01,  1.8952e+00,  2.9271e+00,
         4.0062e+00,  6.6513e-01,  9.6831e-01,  9.1383e-01,  9.8489e-01,
         9.8533e-01,  9.4575e-01,  8.8431e-01,  8.9625e-01,  8.1748e-01,
         8.2577e-01,  7.2697e-01,  7.8757e-01,  9.0579e-01,  5.6602e-01,
         6.0494e-01,  7.8984e-01,  4.7905e-01,  8.2424e-01,  2.9224e-01,
         8.6530e-01,  7.0097e-01,  9.0243e-01,  9.3256e-01,  8.6050e-01,
         1.0136e+00,  1.0309e+00,  5.9431e-01,  9.4351e-01,  1.0206e+00,
         1.0758e+00,  6.0072e-01,  7.6760e-01,  9.3531e-01,  1.0756e+00,
         5.4240e-01,  9.4353e-01,  7.1183e-01,  9.1959e-01,  8.7946e-01,
         4.7820e-01,  8.3654e-01,  8.2558e-01,  8.3417e-01,  9.2615e-01,
         7.9092e-01,  5.3513e-01,  9.5330e-01,  9.1189e-01,  4.9590e-01,
         9.2171e-01,  6.7084e-01,  6.5925e-01,  8.0738e-01,  9.3326e-01,
         9.6234e-01,  6.7440e-01,  6.8580e-01,  8.6927e-01,  8.5570e-01,
         1.0293e+00,  9.4721e-01,  9.4292e-01,  8.7733e-01,  1.0734e+00,
         8.5094e-01,  8.2326e-01,  4.1136e-01,  5.4945e-01,  8.2179e-01,
         9.5057e-01,  9.8675e-01,  8.5388e-01,  6.9629e-01,  5.8350e-01,
         7.6133e-01,  4.7662e-01,  3.6623e-01,  7.9250e-01,  4.8560e-01,
         6.7131e-01,  8.1661e-01,  6.1874e-01,  3.8787e-01,  6.8226e-01,
         3.2480e-01,  8.3400e-01,  6.6899e-01,  8.8157e-01,  7.6959e-01,
         7.4310e-01,  7.1771e-01,  3.4071e-01,  5.9801e-01,  7.2099e-01,
         5.6174e-01,  6.1383e-01,  7.0955e-01,  6.0992e-01,  5.3944e-01,
         6.1333e-01,  6.9430e-01,  7.4928e-01,  4.3408e-01,  5.9642e-01,
         7.1997e-01,  6.2052e-01,  6.8969e-01,  6.1043e-01,  8.1370e-01,
         5.7470e-01,  4.5092e-01,  6.1108e-01,  6.1967e-01,  7.2253e-01,
         6.4602e-01,  6.3030e-01,  1.8996e-01,  7.7277e-01,  3.0095e-01,
         7.2801e-01,  6.2469e-01,  6.6923e-01,  7.3721e-01,  8.1549e-01,
         7.2344e-01,  6.4195e-01,  7.7643e-01,  2.8527e-01,  6.7158e-01,
         4.0789e-01,  6.5352e-01,  5.6794e-01,  4.7811e-01,  1.5958e-01,
         5.0844e-01,  5.4531e-01,  5.8938e-01,  4.5231e-01,  1.7968e-01,
         1.0747e-01,  2.0690e-01,  1.1276e-03,  4.3858e-01,  4.1622e-01,
         4.5005e-01,  3.8087e-01,  2.7102e-01,  3.2742e-01,  2.8432e-01,
        -8.0855e-02,  3.3931e-01,  3.2678e-01,  4.7589e-01,  5.2668e-02,
         1.1265e-01,  1.6649e-01,  4.0258e-01,  3.9676e-01,  1.7093e-01,
        -2.7519e-02,  2.2690e-01,  4.3632e-01,  2.5872e-01,  1.3534e-01,
         2.0760e-01,  1.5996e-01,  1.8583e-01,  2.1334e-01, -1.2864e-02,
         3.7280e-01,  3.8404e-01,  3.0543e-01,  1.4934e-01,  3.1217e-01,
         2.7807e-01,  2.7498e-01,  1.7958e-01,  3.0500e-01,  2.3897e-01,
         1.7509e-01,  1.2711e-02,  5.8565e-03,  2.4309e-01,  1.7538e-01,
         1.9597e-01, -2.9924e-01,  2.5942e-01,  5.0192e-01,  3.1890e-01,
         4.0594e-01,  2.8746e-01,  2.8724e-01,  4.4526e-01,  2.7433e-01,
         3.6027e-01,  1.4272e-01,  2.8342e-01,  3.4313e-01,  3.7729e-01,
        -4.5202e-03,  8.9894e-02, -1.0053e-01, -1.0941e-01,  3.2803e-01,
        -1.4620e-01,  3.3323e-01, -1.2874e-01,  2.7118e-01,  2.8407e-01,
         2.4973e-01,  2.2126e-01,  8.6749e-02,  7.7219e-02,  2.3516e-01,
         1.8321e-01,  3.4505e-01, -8.3099e-02,  5.5068e-02,  1.7259e-01,
         1.3938e-01, -1.1733e-01,  2.7835e-01,  2.0664e-01,  2.9605e-01,
         5.0585e-02,  3.9553e-02,  1.3085e-01,  5.5891e-02, -3.8761e-01,
         6.2027e-02, -4.4997e-02,  7.5645e-02, -7.1780e-02,  8.3461e-02,
         1.6154e-02, -2.4321e-01, -4.3320e-01, -1.7774e-01, -4.6063e-01,
        -2.5692e-01, -3.5654e-01, -1.8840e-01,  5.6800e-02, -9.4748e-02,
         2.6849e-02, -3.5697e-01, -5.1362e-01, -1.6809e-01,  9.9087e-02,
        -3.1689e-01, -4.3120e-01, -4.2804e-01, -2.9257e-01, -2.1101e-01,
        -1.6020e-01, -3.8447e-01, -6.3659e-01, -1.8383e-01, -2.3594e-01,
        -2.6822e-01, -1.9490e-01, -2.4139e-01, -2.5430e-01, -3.8251e-01,
        -3.3973e-01, -5.8946e-01, -8.8295e-01, -5.9238e-01, -7.0436e-01,
        -5.0477e-01, -4.2485e-01, -3.8387e-01, -1.0896e+00, -8.6301e-01,
        -5.3874e-01, -6.1189e-01, -9.2498e-01, -7.4118e-01, -8.2821e-01,
        -7.0685e-01, -6.8260e-01, -9.4607e-01, -9.9307e-01, -9.7407e-01,
        -9.1419e-01, -7.3755e-01, -8.4540e-01, -8.3361e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809143
t6: 1641198809143
state_values: tensor([1.6532, 1.5311, 1.6637, 1.8575, 2.0521, 2.1896, 2.2884, 2.3541, 2.4006,
        2.4749, 2.5880, 2.5606, 2.5898, 2.6081, 2.6829, 2.8910, 2.8339, 2.7839,
        2.8573, 2.8898, 2.9367, 2.9400, 2.7406, 2.6578, 2.8485, 2.8778, 2.8990,
        3.0047, 2.9591, 2.9653, 3.2358, 3.1617, 3.1769, 3.0865, 3.0720, 3.0753,
        3.0883, 3.0901, 3.1503, 3.1306, 3.1737, 3.1726, 3.1786, 3.3143, 3.2205,
        3.2232, 3.2189, 3.3162, 3.2543, 3.2402, 3.2629, 3.3406, 3.2904, 3.4689,
        3.3779, 3.3477, 3.3982, 3.5991, 3.4291, 3.5388, 3.4357, 3.4109, 3.4029,
        3.4535, 3.4240, 3.4457, 3.1914, 3.1248, 3.0708, 3.1021, 3.0156, 3.2466,
        3.3682, 3.3830, 3.3971, 3.5313, 3.5078, 3.5190, 3.5097, 3.5350, 3.5075,
        3.5059, 3.5325, 3.5233, 3.5718, 3.7268, 3.5920, 3.5762, 3.6342, 3.6812,
        3.6157, 3.7188, 3.6566, 3.6292, 3.6132, 3.6178, 3.7396, 3.6989, 3.6789,
        3.7508, 3.7233, 3.6727, 3.6690, 3.6753, 3.6643, 3.6734, 3.6769, 3.6962,
        3.7016, 3.7060, 3.7710, 3.8168, 3.8169, 3.5823, 3.4445, 3.3488, 3.5683,
        3.7222, 3.7009, 3.7415, 3.7344, 3.7272, 3.7432, 3.7424, 3.8616, 3.7949,
        3.9217, 3.8336, 3.8010, 3.7971, 4.0377, 3.9298, 3.8418, 3.8965, 4.2094,
        4.0097, 4.0917, 3.9748, 3.9765, 4.0102, 3.9314, 3.9067, 3.8873, 3.9171,
        3.9403, 3.9409, 3.8974, 3.9053, 4.0199, 3.9414, 4.2115, 4.0333, 3.9765,
        3.9508, 3.9457, 4.2408, 4.1318, 4.1306, 4.0270, 3.9989, 3.9928, 3.9956,
        3.9815, 4.0128, 3.9725, 4.0121, 4.0097, 4.1192, 4.0307, 4.0041, 4.0158,
        4.0165, 4.0216, 4.0716, 4.0326, 4.0123, 4.0154, 4.0651, 4.0572, 4.0449,
        4.0234, 4.1142, 4.0481, 4.0494, 4.0424, 4.0566, 4.0338, 4.1434, 4.2571,
        4.2791, 4.1489, 4.1077, 4.0747, 4.1926, 4.1252, 4.2828, 4.2518, 4.2849,
        4.1844, 4.2395, 4.1770, 4.1548, 4.1541, 4.1720, 4.2305, 4.2798, 4.2510,
        4.1980, 4.1751, 4.2481, 4.3561, 4.2246, 4.2156, 4.2999, 4.3448, 4.2420,
        4.2150, 4.1970, 4.1973, 4.3046, 4.2253, 4.3455, 4.3148, 4.2709, 4.2243,
        4.2138, 4.2157, 4.3574, 4.2542, 4.2824, 4.2910, 4.3128, 4.2450, 4.2873,
        4.2389, 4.2665, 4.2413, 4.2288, 4.2604, 4.2220, 4.2251, 4.2503, 4.2584,
        4.2467, 4.2333, 4.2448, 4.2353, 4.3163, 4.2807, 4.2674, 4.3336, 4.3606,
        4.3402, 4.3664, 4.5272, 4.3573, 4.5742, 4.3823, 4.3897, 4.4097, 4.4183,
        4.4588, 4.5406, 4.4565, 4.4693, 4.3881, 4.4611, 4.3813, 4.4324, 4.3641,
        4.5759, 4.3960, 4.3998, 4.4154, 4.5842, 4.4241, 4.3976, 4.4160, 4.3742,
        4.6233, 4.5421, 4.5976, 4.4776, 4.4155, 4.3776, 4.3804, 4.3813, 4.3843,
        4.3814, 4.4472, 4.4026, 4.4522, 4.4068, 4.4899, 4.4904, 4.4250, 4.4139,
        4.3997, 4.3914, 4.4091, 4.4090, 4.5187, 4.5578, 4.4611, 4.4361, 4.4246,
        4.4278, 4.5696, 4.4930, 4.6144, 4.5227, 4.6646, 4.5544, 4.5031, 4.5309,
        4.7770, 4.5518, 4.4818, 4.4614, 4.5568, 4.4864, 4.5589, 4.4845, 4.4773,
        4.4648, 4.4648, 4.4556, 4.6542, 4.5320, 4.6559, 4.5316, 4.4816, 4.4946,
        4.4861, 4.5890, 4.5136, 4.4832, 4.4730, 4.4999, 4.5020, 4.6315, 4.6488,
        4.5635, 4.5634, 4.5151, 4.5060, 4.6897, 4.5665, 4.5344, 4.5512, 4.5412,
        4.7284, 4.5737, 4.7284, 4.9353, 4.6713, 4.6136, 4.5927, 4.5634, 4.5783,
        4.5412, 4.8158, 4.6250, 4.5698, 4.5298, 4.5475, 4.8350, 4.8486, 4.7033,
        4.7295, 4.6461, 4.6024, 4.5766, 4.5830, 4.5719, 4.5688, 4.6494, 4.6299,
        4.6295, 4.6850, 4.6164, 4.8664, 4.6604, 4.6252, 4.6037, 4.6053, 4.6467,
        4.6665, 5.0103, 4.7289, 4.7323, 4.7466, 4.6593, 4.8232, 4.8980, 4.7981,
        4.7156, 4.6823, 4.8800, 4.8801, 4.8438, 4.7090, 4.7742, 4.7434],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809148
t8: 1641198809148
t9: 1641198809148
t10: 1641198809159
t11: 1641198809160
t12: 1641198809160
t1: 1641198809160
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809171
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9970, 1.0097, 0.9451, 0.9788, 0.9799, 0.9957, 0.9927, 0.9755, 0.9878,
        1.0230, 0.9437, 0.9961, 1.0194, 0.9608, 0.9898, 0.9255, 0.8879, 0.8871,
        1.0057, 1.0462, 1.0324, 0.9836, 1.0046, 0.9737, 0.9994, 0.9962, 0.9423,
        0.9950, 0.9992, 0.9141, 0.9119, 1.1036, 0.8884, 1.0165, 1.0212, 0.9932,
        1.0500, 0.9295, 1.0104, 0.9803, 0.9946, 0.9987, 0.9431, 0.9608, 0.9850,
        1.0128, 0.9367, 0.9787, 1.0732, 0.9736, 0.9663, 0.9958, 0.9653, 0.9950,
        1.0183, 0.9428, 0.9839, 1.0673, 0.9057, 0.9818, 0.9772, 0.9915, 0.9809,
        1.0271, 0.9689, 1.0216, 1.0970, 1.0663, 0.9670, 0.9897, 1.1116, 0.9647,
        1.0082, 1.0718, 0.8669, 1.0057, 1.0082, 0.9970, 0.9895, 1.0099, 0.9974,
        0.9919, 1.0128, 0.9572, 0.9952, 0.9711, 1.0343, 0.9127, 1.0273, 0.9685,
        0.9199, 0.9974, 1.0023, 1.0194, 1.0644, 0.8793, 1.0051, 1.0003, 0.9658,
        1.0025, 1.0036, 1.0333, 0.9984, 1.1101, 1.1067, 1.1254, 1.0591, 1.0248,
        1.0104, 0.9905, 1.0137, 1.0178, 0.9981, 1.0975, 1.1033, 1.2018, 0.9131,
        0.9897, 0.9973, 0.9997, 1.0062, 1.0045, 1.0056, 0.9530, 0.9912, 0.9617,
        0.9762, 1.0277, 1.1139, 0.7836, 0.9963, 0.9244, 0.9148, 0.9170, 0.9763,
        1.0585, 1.0010, 1.0152, 1.0049, 0.9990, 1.0070, 1.1020, 0.9704, 0.9833,
        0.9963, 1.0427, 1.0742, 0.9168, 0.9984, 0.9931, 0.9990, 0.9771, 1.0477,
        1.0116, 0.8848, 0.9766, 1.0622, 0.9688, 1.0083, 1.0426, 1.0995, 1.0011,
        0.9938, 1.0281, 0.9692, 1.0134, 0.9659, 0.9933, 1.0082, 0.9962, 1.0265,
        1.0370, 0.9663, 1.0022, 0.9995, 1.0008, 0.9832, 0.9979, 0.9906, 1.0131,
        0.9495, 1.0280, 1.1296, 1.0385, 0.9941, 1.0009, 0.9878, 1.0356, 1.0271,
        0.9828, 1.0817, 1.4235, 0.8355, 0.9840, 0.9259, 1.0146, 1.0542, 0.9531,
        0.9259, 1.0278, 0.9963, 1.0270, 0.9801, 0.9845, 1.0026, 1.0144, 1.0069,
        1.0591, 0.9286, 0.9854, 0.9757, 0.9917, 0.9633, 1.0115, 0.9808, 1.0080,
        1.0261, 1.0692, 0.9085, 0.9989, 0.9696, 1.0207, 1.0005, 1.0007, 1.0429,
        1.0773, 0.8707, 0.9811, 0.9857, 0.9924, 0.9902, 1.0509, 0.9137, 1.0606,
        0.9708, 1.0164, 1.0198, 0.9867, 1.0024, 1.0028, 0.9977, 0.9919, 1.0315,
        1.0302, 1.0501, 1.0110, 0.9811, 0.9982, 1.0478, 0.9377, 0.9914, 1.0050,
        0.9936, 0.9708, 0.9462, 0.8419, 1.0272, 0.9841, 0.9921, 0.9963, 0.9980,
        1.0373, 1.0102, 1.0017, 1.0116, 0.8693, 1.0032, 0.9617, 1.0037, 0.8288,
        0.9673, 0.9919, 0.9879, 0.9686, 0.9562, 1.0262, 0.9695, 1.0219, 0.8653,
        0.9896, 1.0738, 0.9897, 1.0099, 1.1953, 1.0255, 1.0056, 1.0068, 1.0265,
        0.9627, 1.0019, 0.9863, 1.0111, 0.9407, 1.0125, 0.9963, 1.0590, 1.0978,
        1.0237, 1.0014, 1.0007, 1.0144, 0.9329, 0.9996, 1.0081, 1.0289, 1.0342,
        0.9235, 1.0047, 1.0011, 1.0048, 0.9915, 1.0048, 1.0002, 0.9793, 0.9658,
        0.9869, 1.0827, 1.5592, 0.8633, 0.9985, 0.9366, 0.9769, 1.0092, 1.0185,
        1.0173, 1.0261, 0.9322, 0.9845, 0.9577, 0.9799, 1.0077, 1.0731, 1.0854,
        0.9255, 0.9927, 1.0538, 1.0167, 0.9972, 0.9973, 0.9892, 1.0305, 1.0026,
        0.9943, 1.1039, 1.0496, 0.8895, 0.9962, 1.0001, 0.9755, 0.9997, 0.9458,
        0.9210, 0.8445, 1.0925, 0.9958, 1.0328, 1.0278, 0.9953, 0.9989, 1.0039,
        0.9214, 0.9969, 1.0298, 1.0066, 1.0478, 0.8437, 0.9812, 1.0043, 0.9838,
        1.0014, 1.0067, 1.2092, 0.9840, 1.0172, 1.0157, 0.9641, 1.0030, 0.9969,
        0.9833, 1.0010, 0.9138, 1.0047, 1.0468, 1.1053, 1.0500, 0.9781, 0.9951,
        1.0050, 1.0930, 0.9733, 1.0096, 0.9831, 0.8059, 1.0519, 1.0211, 1.0018,
        1.0209, 0.8287, 1.0384, 1.0443, 0.9961, 0.9753, 0.9992, 0.9659],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809176
t4: 1641198809176
surr1, surr2: tensor([-3.1521e+00, -9.9040e-01, -1.3967e+00, -2.4227e+00, -2.6909e+00,
        -2.9318e+00, -2.7627e+00, -2.6984e+00, -2.7901e+00, -2.7249e+00,
        -2.4717e+00, -2.6468e+00, -2.5374e+00, -2.4842e+00, -2.4273e+00,
        -2.4107e+00, -2.1273e+00, -2.2967e+00, -2.4252e+00, -2.4204e+00,
        -2.2893e+00, -2.1495e+00, -8.4840e-01, -1.7195e-01, -1.6342e+00,
        -1.7975e+00, -1.5658e+00, -1.6815e+00, -1.5986e+00, -1.4156e+00,
        -1.8763e+00, -1.9454e+00, -1.5420e+00, -1.9341e+00, -1.6546e+00,
        -1.5840e+00, -1.5625e+00, -1.6819e+00, -1.5514e+00, -1.5560e+00,
        -1.5130e+00, -1.4392e+00, -1.4182e+00, -1.5550e+00, -1.7186e+00,
        -1.4623e+00, -1.4376e+00, -1.5213e+00, -1.6479e+00, -1.7170e+00,
        -1.3867e+00, -1.5369e+00, -1.4867e+00, -1.8739e+00, -1.2688e+00,
        -1.2516e+00, -1.1370e+00, -1.5928e+00, -1.4208e+00, -1.3760e+00,
        -1.2161e+00, -1.5303e+00, -1.0897e+00, -1.2900e+00, -1.5153e+00,
        -1.2043e+00,  1.2564e+00,  1.3623e+00,  2.0731e+00,  2.4193e+00,
         5.1716e+00,  1.0104e-02,  2.9007e-01,  1.8614e-01, -5.5353e-02,
         1.7215e-01,  2.2327e-01,  2.7908e-01,  3.2037e-01,  3.1209e-01,
         2.7359e-01,  4.1631e-01,  3.4693e-01,  2.9226e-01,  4.6685e-01,
         1.2665e-01, -3.2044e-02,  1.9576e-01,  5.2003e-01,  2.7607e-01,
         1.6067e-01,  1.3545e-01,  2.3919e-01,  2.7789e-01,  3.8168e-01,
         8.9542e-02,  2.2373e-01,  2.9841e-01,  2.8352e-01,  2.1313e-01,
         2.8110e-01,  3.9819e-01,  2.3019e-01,  3.8931e-01, -1.2330e-01,
         9.0726e-02, -1.1422e-01,  4.3455e-02,  1.3131e-01,  1.8740e-01,
         2.5667e-01,  3.2027e-01,  1.8890e-01,  1.9013e+00,  2.9359e+00,
         4.3768e+00,  6.6484e-01,  9.6790e-01,  9.1533e-01,  9.8499e-01,
         9.8478e-01,  9.4569e-01,  8.8428e-01,  8.9686e-01,  8.1754e-01,
         8.2516e-01,  7.2679e-01,  7.8861e-01,  9.1721e-01,  4.9279e-01,
         6.0620e-01,  7.8854e-01,  4.7694e-01,  8.1194e-01,  2.9300e-01,
         8.6493e-01,  7.0096e-01,  9.0174e-01,  9.2902e-01,  8.6071e-01,
         1.0149e+00,  1.0395e+00,  5.9340e-01,  9.4295e-01,  1.0211e+00,
         1.0771e+00,  6.0179e-01,  7.6584e-01,  9.3517e-01,  1.0826e+00,
         5.4264e-01,  9.4417e-01,  7.1353e-01,  9.2017e-01,  8.6463e-01,
         4.8083e-01,  8.3758e-01,  8.2589e-01,  8.3474e-01,  9.3031e-01,
         7.9524e-01,  5.3506e-01,  9.5328e-01,  9.0842e-01,  4.9556e-01,
         9.2170e-01,  6.7183e-01,  6.5885e-01,  8.0718e-01,  9.3328e-01,
         9.6173e-01,  6.7453e-01,  6.8557e-01,  8.6911e-01,  8.5575e-01,
         1.0293e+00,  9.4764e-01,  9.4281e-01,  8.7706e-01,  1.0741e+00,
         8.4950e-01,  8.2887e-01,  4.2243e-01,  5.5002e-01,  8.2184e-01,
         9.5025e-01,  9.8950e-01,  8.5690e-01,  7.0175e-01,  5.8258e-01,
         7.6394e-01,  6.1679e-01,  3.4000e-01,  7.9396e-01,  4.8676e-01,
         6.7348e-01,  8.1847e-01,  6.1827e-01,  3.8605e-01,  6.8873e-01,
         3.2463e-01,  8.3440e-01,  6.6918e-01,  8.8261e-01,  7.6899e-01,
         7.4338e-01,  7.1813e-01,  3.4153e-01,  5.9638e-01,  7.1767e-01,
         5.6089e-01,  6.1365e-01,  7.0718e-01,  6.0826e-01,  5.3941e-01,
         6.1351e-01,  6.9549e-01,  7.5194e-01,  4.3334e-01,  5.9673e-01,
         7.1960e-01,  6.2047e-01,  6.8969e-01,  6.1042e-01,  8.1611e-01,
         5.7662e-01,  4.3624e-01,  6.1042e-01,  6.1932e-01,  7.2190e-01,
         6.4560e-01,  6.3317e-01,  1.8933e-01,  7.7742e-01,  3.0083e-01,
         7.2780e-01,  6.2423e-01,  6.6943e-01,  7.3704e-01,  8.1535e-01,
         7.2350e-01,  6.4209e-01,  7.7535e-01,  2.8542e-01,  6.7212e-01,
         4.0788e-01,  6.5398e-01,  5.6739e-01,  4.7710e-01,  1.5937e-01,
         5.0840e-01,  5.4589e-01,  5.8939e-01,  4.5137e-01,  1.7822e-01,
         1.0053e-01,  2.0347e-01,  1.1239e-03,  4.3738e-01,  4.1571e-01,
         4.4932e-01,  3.8003e-01,  2.7114e-01,  3.2716e-01,  2.8712e-01,
        -7.8099e-02,  3.4087e-01,  3.2610e-01,  4.7819e-01,  4.8502e-02,
         1.1186e-01,  1.6634e-01,  4.0209e-01,  3.9562e-01,  1.6932e-01,
        -2.7564e-02,  2.2635e-01,  4.3737e-01,  2.4874e-01,  1.3597e-01,
         2.0764e-01,  1.5996e-01,  1.8619e-01,  2.3183e-01, -1.2871e-02,
         3.7284e-01,  3.8381e-01,  3.0495e-01,  1.4939e-01,  3.1209e-01,
         2.7832e-01,  2.7481e-01,  1.7959e-01,  3.0458e-01,  2.3900e-01,
         1.7544e-01,  1.2739e-02,  5.8588e-03,  2.4309e-01,  1.7538e-01,
         1.9572e-01, -2.9911e-01,  2.5942e-01,  5.0106e-01,  3.1893e-01,
         4.0613e-01,  2.8738e-01,  2.8724e-01,  4.4581e-01,  2.7439e-01,
         3.6007e-01,  1.4274e-01,  2.8340e-01,  3.4222e-01,  3.7370e-01,
        -4.4787e-03,  9.0763e-02, -1.4250e-01, -1.0494e-01,  3.2900e-01,
        -1.4627e-01,  3.3257e-01, -1.2874e-01,  2.7108e-01,  2.8379e-01,
         2.4946e-01,  2.2166e-01,  8.6497e-02,  7.7201e-02,  2.3434e-01,
         1.8330e-01,  3.4644e-01, -8.3288e-02,  5.4984e-02,  1.7252e-01,
         1.3900e-01, -1.1735e-01,  2.7835e-01,  2.0673e-01,  2.9697e-01,
         5.0612e-02,  3.9555e-02,  1.3082e-01,  5.6322e-02, -3.8813e-01,
         6.1305e-02, -4.4968e-02,  7.5603e-02, -7.1706e-02,  8.3455e-02,
         1.6115e-02, -2.4296e-01, -4.0650e-01, -1.7894e-01, -4.5814e-01,
        -2.5731e-01, -3.5719e-01, -1.8829e-01,  5.6793e-02, -9.4757e-02,
         2.6873e-02, -3.5320e-01, -5.1496e-01, -1.6813e-01,  9.9421e-02,
        -2.9708e-01, -4.3578e-01, -4.2815e-01, -2.9245e-01, -2.1100e-01,
        -1.6056e-01, -4.2264e-01, -6.3591e-01, -1.8392e-01, -2.3573e-01,
        -2.6855e-01, -1.9497e-01, -2.4141e-01, -2.5413e-01, -3.8263e-01,
        -3.3829e-01, -5.7846e-01, -8.8704e-01, -5.9639e-01, -7.0501e-01,
        -5.0477e-01, -4.2534e-01, -3.8679e-01, -1.0827e+00, -8.5964e-01,
        -5.3755e-01, -6.1364e-01, -8.2830e-01, -7.4250e-01, -8.2947e-01,
        -7.0686e-01, -6.8555e-01, -8.7109e-01, -9.9492e-01, -9.7537e-01,
        -9.1476e-01, -7.3525e-01, -8.4428e-01, -8.2854e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1521e+00, -9.9040e-01, -1.3967e+00, -2.4227e+00, -2.6909e+00,
        -2.9318e+00, -2.7627e+00, -2.6984e+00, -2.7901e+00, -2.7249e+00,
        -2.4717e+00, -2.6468e+00, -2.5374e+00, -2.4842e+00, -2.4273e+00,
        -2.4107e+00, -2.1565e+00, -2.3300e+00, -2.4252e+00, -2.4204e+00,
        -2.2893e+00, -2.1495e+00, -8.4840e-01, -1.7195e-01, -1.6342e+00,
        -1.7975e+00, -1.5658e+00, -1.6815e+00, -1.5986e+00, -1.4156e+00,
        -1.8763e+00, -1.9391e+00, -1.5622e+00, -1.9341e+00, -1.6546e+00,
        -1.5840e+00, -1.5625e+00, -1.6819e+00, -1.5514e+00, -1.5560e+00,
        -1.5130e+00, -1.4392e+00, -1.4182e+00, -1.5550e+00, -1.7186e+00,
        -1.4623e+00, -1.4376e+00, -1.5213e+00, -1.6479e+00, -1.7170e+00,
        -1.3867e+00, -1.5369e+00, -1.4867e+00, -1.8739e+00, -1.2688e+00,
        -1.2516e+00, -1.1370e+00, -1.5928e+00, -1.4208e+00, -1.3760e+00,
        -1.2161e+00, -1.5303e+00, -1.0897e+00, -1.2900e+00, -1.5153e+00,
        -1.2043e+00,  1.2564e+00,  1.3623e+00,  2.0731e+00,  2.4193e+00,
         5.1175e+00,  1.0104e-02,  2.9007e-01,  1.8614e-01, -5.7464e-02,
         1.7215e-01,  2.2327e-01,  2.7908e-01,  3.2037e-01,  3.1209e-01,
         2.7359e-01,  4.1631e-01,  3.4693e-01,  2.9226e-01,  4.6685e-01,
         1.2665e-01, -3.2044e-02,  1.9576e-01,  5.2003e-01,  2.7607e-01,
         1.6067e-01,  1.3545e-01,  2.3919e-01,  2.7789e-01,  3.8168e-01,
         9.1655e-02,  2.2373e-01,  2.9841e-01,  2.8352e-01,  2.1313e-01,
         2.8110e-01,  3.9819e-01,  2.3019e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1422e-01,  4.3455e-02,  1.3131e-01,  1.8740e-01,
         2.5667e-01,  3.2027e-01,  1.8890e-01,  1.9013e+00,  2.9271e+00,
         4.0062e+00,  6.6484e-01,  9.6790e-01,  9.1533e-01,  9.8499e-01,
         9.8478e-01,  9.4569e-01,  8.8428e-01,  8.9686e-01,  8.1754e-01,
         8.2516e-01,  7.2679e-01,  7.8861e-01,  9.0579e-01,  5.6602e-01,
         6.0620e-01,  7.8854e-01,  4.7694e-01,  8.1194e-01,  2.9300e-01,
         8.6493e-01,  7.0096e-01,  9.0174e-01,  9.2902e-01,  8.6071e-01,
         1.0149e+00,  1.0376e+00,  5.9340e-01,  9.4295e-01,  1.0211e+00,
         1.0771e+00,  6.0179e-01,  7.6584e-01,  9.3517e-01,  1.0826e+00,
         5.4264e-01,  9.4417e-01,  7.1353e-01,  9.2017e-01,  8.7946e-01,
         4.8083e-01,  8.3758e-01,  8.2589e-01,  8.3474e-01,  9.3031e-01,
         7.9524e-01,  5.3506e-01,  9.5328e-01,  9.0842e-01,  4.9556e-01,
         9.2170e-01,  6.7183e-01,  6.5885e-01,  8.0718e-01,  9.3328e-01,
         9.6173e-01,  6.7453e-01,  6.8557e-01,  8.6911e-01,  8.5575e-01,
         1.0293e+00,  9.4764e-01,  9.4281e-01,  8.7706e-01,  1.0741e+00,
         8.4950e-01,  8.2887e-01,  4.1136e-01,  5.5002e-01,  8.2184e-01,
         9.5025e-01,  9.8950e-01,  8.5690e-01,  7.0175e-01,  5.8258e-01,
         7.6394e-01,  4.7662e-01,  3.6623e-01,  7.9396e-01,  4.8676e-01,
         6.7348e-01,  8.1847e-01,  6.1827e-01,  3.8605e-01,  6.8873e-01,
         3.2463e-01,  8.3440e-01,  6.6918e-01,  8.8261e-01,  7.6899e-01,
         7.4338e-01,  7.1813e-01,  3.4153e-01,  5.9638e-01,  7.1767e-01,
         5.6089e-01,  6.1365e-01,  7.0718e-01,  6.0826e-01,  5.3941e-01,
         6.1351e-01,  6.9549e-01,  7.5194e-01,  4.3334e-01,  5.9673e-01,
         7.1960e-01,  6.2047e-01,  6.8969e-01,  6.1042e-01,  8.1611e-01,
         5.7662e-01,  4.5092e-01,  6.1042e-01,  6.1932e-01,  7.2190e-01,
         6.4560e-01,  6.3317e-01,  1.8933e-01,  7.7742e-01,  3.0083e-01,
         7.2780e-01,  6.2423e-01,  6.6943e-01,  7.3704e-01,  8.1535e-01,
         7.2350e-01,  6.4209e-01,  7.7535e-01,  2.8542e-01,  6.7212e-01,
         4.0788e-01,  6.5398e-01,  5.6739e-01,  4.7710e-01,  1.5937e-01,
         5.0840e-01,  5.4589e-01,  5.8939e-01,  4.5137e-01,  1.7822e-01,
         1.0747e-01,  2.0347e-01,  1.1239e-03,  4.3738e-01,  4.1571e-01,
         4.4932e-01,  3.8003e-01,  2.7114e-01,  3.2716e-01,  2.8712e-01,
        -8.0855e-02,  3.4087e-01,  3.2610e-01,  4.7819e-01,  5.2668e-02,
         1.1186e-01,  1.6634e-01,  4.0209e-01,  3.9562e-01,  1.6932e-01,
        -2.7564e-02,  2.2635e-01,  4.3737e-01,  2.5872e-01,  1.3597e-01,
         2.0764e-01,  1.5996e-01,  1.8619e-01,  2.1334e-01, -1.2871e-02,
         3.7284e-01,  3.8381e-01,  3.0495e-01,  1.4939e-01,  3.1209e-01,
         2.7832e-01,  2.7481e-01,  1.7959e-01,  3.0458e-01,  2.3900e-01,
         1.7544e-01,  1.2739e-02,  5.8588e-03,  2.4309e-01,  1.7538e-01,
         1.9572e-01, -2.9911e-01,  2.5942e-01,  5.0106e-01,  3.1893e-01,
         4.0613e-01,  2.8738e-01,  2.8724e-01,  4.4581e-01,  2.7439e-01,
         3.6007e-01,  1.4274e-01,  2.8340e-01,  3.4222e-01,  3.7370e-01,
        -4.4787e-03,  9.0763e-02, -1.0053e-01, -1.0941e-01,  3.2900e-01,
        -1.4627e-01,  3.3257e-01, -1.2874e-01,  2.7108e-01,  2.8379e-01,
         2.4946e-01,  2.2166e-01,  8.6497e-02,  7.7201e-02,  2.3434e-01,
         1.8330e-01,  3.4644e-01, -8.3288e-02,  5.4984e-02,  1.7252e-01,
         1.3900e-01, -1.1735e-01,  2.7835e-01,  2.0673e-01,  2.9697e-01,
         5.0612e-02,  3.9555e-02,  1.3082e-01,  5.6121e-02, -3.8813e-01,
         6.2027e-02, -4.4968e-02,  7.5603e-02, -7.1706e-02,  8.3455e-02,
         1.6115e-02, -2.4296e-01, -4.3320e-01, -1.7894e-01, -4.5814e-01,
        -2.5731e-01, -3.5719e-01, -1.8829e-01,  5.6793e-02, -9.4757e-02,
         2.6873e-02, -3.5320e-01, -5.1496e-01, -1.6813e-01,  9.9421e-02,
        -3.1689e-01, -4.3578e-01, -4.2815e-01, -2.9245e-01, -2.1100e-01,
        -1.6056e-01, -3.8447e-01, -6.3591e-01, -1.8392e-01, -2.3573e-01,
        -2.6855e-01, -1.9497e-01, -2.4141e-01, -2.5413e-01, -3.8263e-01,
        -3.3829e-01, -5.7846e-01, -8.8704e-01, -5.9353e-01, -7.0501e-01,
        -5.0477e-01, -4.2534e-01, -3.8679e-01, -1.0827e+00, -8.5964e-01,
        -5.3755e-01, -6.1364e-01, -9.2498e-01, -7.4250e-01, -8.2947e-01,
        -7.0686e-01, -6.8555e-01, -9.4607e-01, -9.9492e-01, -9.7537e-01,
        -9.1476e-01, -7.3525e-01, -8.4428e-01, -8.2854e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809185
t6: 1641198809185
state_values: tensor([1.5381, 1.4504, 1.5731, 1.7566, 1.9424, 2.0741, 2.1685, 2.2314, 2.2761,
        2.3466, 2.4473, 2.4258, 2.4527, 2.4703, 2.5369, 2.7265, 2.6784, 2.6313,
        2.7021, 2.7343, 2.7784, 2.7833, 2.5979, 2.5257, 2.6976, 2.7279, 2.7495,
        2.8524, 2.8100, 2.8167, 3.0583, 3.0005, 3.0150, 2.9365, 2.9242, 2.9284,
        2.9418, 2.9448, 3.0012, 2.9854, 3.0254, 3.0260, 3.0328, 3.1507, 3.0735,
        3.0769, 3.0745, 3.1590, 3.1093, 3.0976, 3.1191, 3.1856, 3.1458, 3.2984,
        3.2216, 3.1976, 3.2408, 3.4166, 3.2696, 3.3691, 3.2776, 3.2578, 3.2524,
        3.2957, 3.2725, 3.2916, 3.0702, 3.0061, 2.9546, 2.9857, 2.9034, 3.1275,
        3.2353, 3.2491, 3.2619, 3.3795, 3.3580, 3.3690, 3.3610, 3.3853, 3.3606,
        3.3598, 3.3845, 3.3769, 3.4238, 3.5628, 3.4449, 3.4300, 3.4857, 3.5287,
        3.4698, 3.5625, 3.5090, 3.4843, 3.4694, 3.4743, 3.5845, 3.5504, 3.5330,
        3.5968, 3.5740, 3.5295, 3.5268, 3.5329, 3.5239, 3.5330, 3.5371, 3.5550,
        3.5606, 3.5652, 3.6234, 3.6632, 3.6641, 3.4488, 3.3285, 3.2466, 3.4358,
        3.5843, 3.5668, 3.6036, 3.5979, 3.5920, 3.6067, 3.6065, 3.7109, 3.6546,
        3.7632, 3.6893, 3.6611, 3.6579, 3.8642, 3.7730, 3.6988, 3.7453, 4.0205,
        3.8432, 3.9148, 3.8139, 3.8155, 3.8448, 3.7783, 3.7578, 3.7418, 3.7677,
        3.7879, 3.7890, 3.7529, 3.7601, 3.8574, 3.7923, 4.0272, 3.8704, 3.8216,
        3.8012, 3.7979, 4.0567, 3.9573, 3.9563, 3.8675, 3.8437, 3.8385, 3.8413,
        3.8300, 3.8571, 3.8238, 3.8572, 3.8557, 3.9503, 3.8747, 3.8523, 3.8626,
        3.8633, 3.8682, 3.9115, 3.8786, 3.8616, 3.8644, 3.9073, 3.9010, 3.8910,
        3.8728, 3.9509, 3.8943, 3.8957, 3.8904, 3.9030, 3.8838, 3.9780, 4.0820,
        4.1036, 3.9838, 3.9481, 3.9199, 4.0218, 3.9641, 4.1080, 4.0781, 4.1103,
        4.0159, 4.0662, 4.0099, 3.9912, 3.9909, 4.0064, 4.0582, 4.1065, 4.0786,
        4.0298, 4.0104, 4.0762, 4.1815, 4.0543, 4.0463, 4.1272, 4.1712, 4.0717,
        4.0468, 4.0317, 4.0319, 4.1329, 4.0573, 4.1730, 4.1434, 4.1008, 4.0570,
        4.0478, 4.0497, 4.1856, 4.0861, 4.1131, 4.1215, 4.1428, 4.0783, 4.1185,
        4.0731, 4.0987, 4.0758, 4.0646, 4.0937, 4.0588, 4.0617, 4.0852, 4.0930,
        4.0825, 4.0705, 4.0813, 4.0729, 4.1496, 4.1154, 4.1033, 4.1669, 4.1933,
        4.1735, 4.1989, 4.3489, 4.1903, 4.3906, 4.2148, 4.2216, 4.2410, 4.2495,
        4.2878, 4.3623, 4.2857, 4.2974, 4.2209, 4.2902, 4.2145, 4.2639, 4.1981,
        4.3937, 4.2293, 4.2328, 4.2480, 4.4016, 4.2570, 4.2315, 4.2491, 4.2089,
        4.4356, 4.3660, 4.4149, 4.3076, 4.2494, 4.2131, 4.2161, 4.2170, 4.2204,
        4.2177, 4.2816, 4.2389, 4.2867, 4.2431, 4.3215, 4.3221, 4.2612, 4.2504,
        4.2368, 4.2292, 4.2463, 4.2464, 4.3489, 4.3849, 4.2971, 4.2732, 4.2623,
        4.2655, 4.3961, 4.3267, 4.4355, 4.3537, 4.4774, 4.3821, 4.3358, 4.3611,
        4.5685, 4.3794, 4.3164, 4.2982, 4.3849, 4.3217, 4.3874, 4.3204, 4.3141,
        4.3031, 4.3034, 4.2946, 4.4719, 4.3650, 4.4733, 4.3648, 4.3194, 4.3311,
        4.3236, 4.4165, 4.3491, 4.3214, 4.3126, 4.3372, 4.3394, 4.4554, 4.4699,
        4.3952, 4.3949, 4.3514, 4.3435, 4.5044, 4.3984, 4.3696, 4.3847, 4.3760,
        4.5365, 4.4049, 4.5362, 4.7088, 4.4885, 4.4387, 4.4209, 4.3957, 4.4090,
        4.3767, 4.6074, 4.4500, 4.4020, 4.3669, 4.3827, 4.6240, 4.6357, 4.5165,
        4.5378, 4.4692, 4.4308, 4.4083, 4.4145, 4.4055, 4.4032, 4.4741, 4.4571,
        4.4569, 4.5045, 4.4457, 4.6515, 4.4834, 4.4527, 4.4340, 4.4359, 4.4726,
        4.4902, 4.7775, 4.5398, 4.5426, 4.5545, 4.4830, 4.6170, 4.6794, 4.5965,
        4.5294, 4.5021, 4.6639, 4.6642, 4.6339, 4.5243, 4.5774, 4.5525],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809190
t8: 1641198809190
t9: 1641198809190
t10: 1641198809200
t11: 1641198809202
t12: 1641198809202
t1: 1641198809202
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809213
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9962, 1.0060, 0.9457, 0.9775, 0.9788, 0.9958, 0.9929, 0.9722, 0.9884,
        1.0233, 0.9369, 0.9965, 1.0217, 0.9599, 0.9818, 0.9336, 0.8887, 0.8759,
        0.9904, 1.0320, 1.0282, 0.9880, 1.0048, 0.9656, 1.0065, 0.9954, 0.9287,
        0.9961, 0.9979, 0.8847, 0.9217, 1.0960, 0.9008, 1.0188, 1.0276, 0.9915,
        1.0653, 0.9280, 1.0132, 0.9799, 0.9941, 0.9985, 0.9342, 0.9551, 0.9823,
        1.0169, 0.9264, 0.9856, 1.0852, 0.9725, 0.9626, 0.9975, 0.9652, 0.9941,
        1.0211, 0.9412, 0.9594, 1.0533, 0.8909, 0.9819, 0.9911, 0.9897, 0.9755,
        1.0385, 0.9686, 1.0239, 1.0999, 1.0671, 0.9665, 0.9916, 1.0965, 0.9639,
        1.0143, 1.0746, 0.8651, 1.0045, 1.0057, 0.9954, 0.9858, 1.0137, 0.9967,
        0.9902, 1.0146, 0.9558, 0.9768, 0.9627, 1.0379, 0.8995, 1.0195, 0.9725,
        0.9102, 0.9985, 1.0038, 1.0225, 1.0742, 0.8759, 1.0037, 0.9998, 0.9585,
        0.9994, 1.0052, 1.0390, 0.9979, 1.1178, 1.1086, 1.1291, 1.0591, 1.0241,
        1.0094, 0.9920, 1.0137, 1.0192, 1.0021, 1.1019, 1.1075, 1.2093, 0.9148,
        0.9914, 0.9983, 0.9997, 1.0062, 1.0046, 1.0058, 0.9511, 0.9930, 0.9535,
        0.9797, 1.0335, 1.1267, 0.7773, 0.9985, 0.9361, 0.9088, 0.8845, 0.9787,
        1.0473, 1.0010, 1.0093, 0.9930, 0.9997, 1.0111, 1.1211, 0.9691, 0.9814,
        0.9961, 1.0443, 1.0755, 0.9132, 0.9984, 0.9958, 0.9994, 0.9851, 1.0532,
        1.0129, 0.8775, 0.9818, 1.0561, 0.9742, 1.0104, 1.0532, 1.1121, 1.0009,
        0.9933, 1.0296, 0.9689, 1.0148, 0.9662, 0.9955, 1.0082, 0.9960, 1.0281,
        1.0375, 0.9654, 1.0027, 0.9995, 1.0009, 0.9824, 0.9976, 0.9897, 1.0168,
        0.9477, 1.0543, 1.1325, 1.0395, 0.9940, 1.0008, 0.9885, 1.0316, 1.0334,
        0.9832, 1.0967, 1.4697, 0.8296, 0.9929, 0.9268, 1.0171, 1.0502, 0.9644,
        0.9171, 1.0517, 0.9956, 1.0287, 0.9798, 0.9835, 0.9977, 1.0126, 1.0172,
        1.0634, 0.9242, 0.9761, 0.9776, 0.9903, 0.9534, 0.9991, 0.9850, 1.0089,
        1.0305, 1.0775, 0.9065, 1.0000, 0.9693, 1.0173, 1.0004, 1.0007, 1.0519,
        1.0838, 0.8667, 0.9838, 0.9823, 0.9887, 0.9861, 1.0709, 0.9113, 1.0706,
        0.9704, 1.0179, 1.0194, 0.9867, 1.0026, 1.0028, 0.9975, 0.9914, 1.0337,
        1.0307, 1.0522, 1.0111, 0.9808, 0.9987, 1.0477, 0.9367, 0.9887, 1.0046,
        0.9921, 0.9576, 0.9436, 0.8257, 1.0074, 0.9778, 0.9844, 0.9905, 0.9890,
        1.0213, 1.0094, 0.9954, 1.0430, 0.8569, 1.0143, 0.9550, 1.0223, 0.8251,
        0.9622, 0.9898, 0.9839, 0.9540, 0.9496, 1.0295, 0.9642, 1.0304, 0.8629,
        0.9946, 1.0633, 0.9912, 1.0187, 1.2265, 1.0259, 1.0058, 1.0068, 1.0260,
        0.9625, 1.0027, 0.9861, 1.0124, 0.9393, 1.0063, 1.0006, 1.0655, 1.1018,
        1.0242, 1.0014, 1.0007, 1.0147, 0.9324, 0.9998, 1.0080, 1.0295, 1.0357,
        0.9220, 1.0044, 0.9934, 1.0045, 0.9797, 1.0046, 0.9996, 0.9713, 0.9372,
        0.9781, 1.1083, 1.6244, 0.8582, 1.0164, 0.9374, 0.9850, 1.0091, 1.0190,
        1.0169, 1.0258, 0.9324, 0.9793, 0.9580, 0.9782, 1.0107, 1.0847, 1.0886,
        0.9231, 0.9955, 1.0531, 1.0168, 0.9970, 0.9972, 0.9892, 1.0260, 1.0023,
        0.9925, 1.1326, 1.0507, 0.8837, 0.9961, 1.0035, 0.9729, 0.9994, 0.9400,
        0.9341, 0.8316, 1.0858, 0.9917, 1.0401, 1.0313, 0.9939, 0.9986, 1.0041,
        0.9182, 0.9875, 1.0352, 1.0070, 1.0553, 0.8418, 0.9925, 1.0043, 0.9760,
        1.0010, 1.0159, 1.2402, 0.9831, 1.0186, 1.0156, 0.9643, 1.0023, 0.9968,
        0.9780, 1.0026, 0.8983, 0.9840, 1.0548, 1.1206, 1.0513, 0.9776, 0.9946,
        1.0045, 1.0800, 0.9658, 1.0009, 1.0030, 0.7922, 1.0452, 1.0211, 1.0009,
        1.0358, 0.8095, 1.0356, 1.0415, 0.9991, 0.9659, 0.9953, 0.9430],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809217
t4: 1641198809217
surr1, surr2: tensor([-3.1494e+00, -9.8678e-01, -1.3976e+00, -2.4195e+00, -2.6879e+00,
        -2.9319e+00, -2.7633e+00, -2.6892e+00, -2.7920e+00, -2.7257e+00,
        -2.4539e+00, -2.6476e+00, -2.5431e+00, -2.4819e+00, -2.4077e+00,
        -2.4317e+00, -2.1294e+00, -2.2676e+00, -2.3883e+00, -2.3875e+00,
        -2.2799e+00, -2.1589e+00, -8.4857e-01, -1.7052e-01, -1.6458e+00,
        -1.7960e+00, -1.5431e+00, -1.6832e+00, -1.5965e+00, -1.3700e+00,
        -1.8963e+00, -1.9320e+00, -1.5635e+00, -1.9385e+00, -1.6648e+00,
        -1.5813e+00, -1.5853e+00, -1.6792e+00, -1.5557e+00, -1.5555e+00,
        -1.5122e+00, -1.4388e+00, -1.4048e+00, -1.5457e+00, -1.7139e+00,
        -1.4683e+00, -1.4218e+00, -1.5320e+00, -1.6664e+00, -1.7151e+00,
        -1.3815e+00, -1.5396e+00, -1.4866e+00, -1.8722e+00, -1.2723e+00,
        -1.2495e+00, -1.1087e+00, -1.5720e+00, -1.3976e+00, -1.3761e+00,
        -1.2334e+00, -1.5275e+00, -1.0837e+00, -1.3043e+00, -1.5148e+00,
        -1.2071e+00,  1.2596e+00,  1.3633e+00,  2.0720e+00,  2.4239e+00,
         5.1012e+00,  1.0097e-02,  2.9184e-01,  1.8662e-01, -5.5233e-02,
         1.7195e-01,  2.2273e-01,  2.7862e-01,  3.1917e-01,  3.1327e-01,
         2.7341e-01,  4.1559e-01,  3.4755e-01,  2.9182e-01,  4.5822e-01,
         1.2556e-01, -3.2157e-02,  1.9293e-01,  5.1605e-01,  2.7720e-01,
         1.5898e-01,  1.3560e-01,  2.3954e-01,  2.7874e-01,  3.8521e-01,
         8.9199e-02,  2.2342e-01,  2.9827e-01,  2.8137e-01,  2.1247e-01,
         2.8154e-01,  4.0036e-01,  2.3006e-01,  3.9200e-01, -1.2351e-01,
         9.1018e-02, -1.1422e-01,  4.3425e-02,  1.3117e-01,  1.8770e-01,
         2.5666e-01,  3.2071e-01,  1.8966e-01,  1.9089e+00,  2.9472e+00,
         4.4043e+00,  6.6606e-01,  9.6956e-01,  9.1617e-01,  9.8499e-01,
         9.8477e-01,  9.4577e-01,  8.8445e-01,  8.9505e-01,  8.1904e-01,
         8.1810e-01,  7.2945e-01,  7.9302e-01,  9.2774e-01,  4.8885e-01,
         6.0755e-01,  7.9851e-01,  4.7379e-01,  7.8317e-01,  2.9372e-01,
         8.5580e-01,  7.0096e-01,  8.9653e-01,  9.1800e-01,  8.6130e-01,
         1.0191e+00,  1.0575e+00,  5.9260e-01,  9.4114e-01,  1.0209e+00,
         1.0787e+00,  6.0250e-01,  7.6286e-01,  9.3525e-01,  1.0856e+00,
         5.4287e-01,  9.5197e-01,  7.1729e-01,  9.2129e-01,  8.5752e-01,
         4.8340e-01,  8.3271e-01,  8.3050e-01,  8.3645e-01,  9.3982e-01,
         8.0437e-01,  5.3498e-01,  9.5288e-01,  9.0972e-01,  4.9539e-01,
         9.2296e-01,  6.7202e-01,  6.6032e-01,  8.0723e-01,  9.3307e-01,
         9.6323e-01,  6.7488e-01,  6.8489e-01,  8.6958e-01,  8.5574e-01,
         1.0293e+00,  9.4688e-01,  9.4253e-01,  8.7629e-01,  1.0780e+00,
         8.4789e-01,  8.5010e-01,  4.2350e-01,  5.5059e-01,  8.2178e-01,
         9.5012e-01,  9.9024e-01,  8.5358e-01,  7.0601e-01,  5.8281e-01,
         7.7449e-01,  6.3679e-01,  3.3757e-01,  8.0113e-01,  4.8721e-01,
         6.7518e-01,  8.1541e-01,  6.2558e-01,  3.8238e-01,  7.0476e-01,
         3.2441e-01,  8.3580e-01,  6.6899e-01,  8.8177e-01,  7.6522e-01,
         7.4206e-01,  7.2543e-01,  3.4291e-01,  5.9351e-01,  7.1086e-01,
         5.6199e-01,  6.1282e-01,  6.9996e-01,  6.0077e-01,  5.4171e-01,
         6.1410e-01,  6.9848e-01,  7.5774e-01,  4.3238e-01,  5.9738e-01,
         7.1932e-01,  6.1840e-01,  6.8956e-01,  6.1038e-01,  8.2318e-01,
         5.8011e-01,  4.3426e-01,  6.1212e-01,  6.1718e-01,  7.1921e-01,
         6.4293e-01,  6.4525e-01,  1.8883e-01,  7.8480e-01,  3.0069e-01,
         7.2887e-01,  6.2399e-01,  6.6942e-01,  7.3717e-01,  8.1537e-01,
         7.2342e-01,  6.4180e-01,  7.7697e-01,  2.8554e-01,  6.7343e-01,
         4.0792e-01,  6.5376e-01,  5.6769e-01,  4.7708e-01,  1.5920e-01,
         5.0702e-01,  5.4567e-01,  5.8849e-01,  4.4521e-01,  1.7773e-01,
         9.8599e-02,  1.9956e-01,  1.1167e-03,  4.3401e-01,  4.1330e-01,
         4.4525e-01,  3.7416e-01,  2.7093e-01,  3.2510e-01,  2.9602e-01,
        -7.6983e-02,  3.4463e-01,  3.2382e-01,  4.8701e-01,  4.8287e-02,
         1.1127e-01,  1.6599e-01,  4.0046e-01,  3.8966e-01,  1.6817e-01,
        -2.7653e-02,  2.2511e-01,  4.4101e-01,  2.4807e-01,  1.3665e-01,
         2.0560e-01,  1.6019e-01,  1.8782e-01,  2.3788e-01, -1.2877e-02,
         3.7293e-01,  3.8379e-01,  3.0480e-01,  1.4935e-01,  3.1235e-01,
         2.7827e-01,  2.7514e-01,  1.7931e-01,  3.0273e-01,  2.4004e-01,
         1.7653e-01,  1.2785e-02,  5.8620e-03,  2.4309e-01,  1.7538e-01,
         1.9578e-01, -2.9892e-01,  2.5945e-01,  5.0100e-01,  3.1910e-01,
         4.0670e-01,  2.8691e-01,  2.8715e-01,  4.4242e-01,  2.7431e-01,
         3.5581e-01,  1.4272e-01,  2.8322e-01,  3.3942e-01,  3.6264e-01,
        -4.4385e-03,  9.2902e-02, -1.4846e-01, -1.0433e-01,  3.3489e-01,
        -1.4639e-01,  3.3533e-01, -1.2873e-01,  2.7119e-01,  2.8369e-01,
         2.4938e-01,  2.2171e-01,  8.6043e-02,  7.7230e-02,  2.3392e-01,
         1.8385e-01,  3.5017e-01, -8.3538e-02,  5.4841e-02,  1.7300e-01,
         1.3890e-01, -1.1737e-01,  2.7831e-01,  2.0673e-01,  2.9699e-01,
         5.0392e-02,  3.9544e-02,  1.3058e-01,  5.7783e-02, -3.8855e-01,
         6.0906e-02, -4.4967e-02,  7.5858e-02, -7.1514e-02,  8.3430e-02,
         1.6016e-02, -2.4640e-01, -4.0027e-01, -1.7785e-01, -4.5623e-01,
        -2.5913e-01, -3.5842e-01, -1.8804e-01,  5.6774e-02, -9.4770e-02,
         2.6782e-02, -3.4986e-01, -5.1766e-01, -1.6819e-01,  1.0013e-01,
        -2.9641e-01, -4.4083e-01, -4.2816e-01, -2.9013e-01, -2.1091e-01,
        -1.6202e-01, -4.3348e-01, -6.3530e-01, -1.8416e-01, -2.3569e-01,
        -2.6860e-01, -1.9484e-01, -2.4137e-01, -2.5274e-01, -3.8324e-01,
        -3.3253e-01, -5.6655e-01, -8.9386e-01, -6.0462e-01, -7.0589e-01,
        -5.0448e-01, -4.2512e-01, -3.8659e-01, -1.0698e+00, -8.5307e-01,
        -5.3290e-01, -6.2606e-01, -8.1414e-01, -7.3774e-01, -8.2953e-01,
        -7.0625e-01, -6.9558e-01, -8.5090e-01, -9.9222e-01, -9.7274e-01,
        -9.1749e-01, -7.2822e-01, -8.4097e-01, -8.0888e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1494e+00, -9.8678e-01, -1.3976e+00, -2.4195e+00, -2.6879e+00,
        -2.9319e+00, -2.7633e+00, -2.6892e+00, -2.7920e+00, -2.7257e+00,
        -2.4539e+00, -2.6476e+00, -2.5431e+00, -2.4819e+00, -2.4077e+00,
        -2.4317e+00, -2.1565e+00, -2.3300e+00, -2.3883e+00, -2.3875e+00,
        -2.2799e+00, -2.1589e+00, -8.4857e-01, -1.7052e-01, -1.6458e+00,
        -1.7960e+00, -1.5431e+00, -1.6832e+00, -1.5965e+00, -1.3938e+00,
        -1.8963e+00, -1.9320e+00, -1.5635e+00, -1.9385e+00, -1.6648e+00,
        -1.5813e+00, -1.5853e+00, -1.6792e+00, -1.5557e+00, -1.5555e+00,
        -1.5122e+00, -1.4388e+00, -1.4048e+00, -1.5457e+00, -1.7139e+00,
        -1.4683e+00, -1.4218e+00, -1.5320e+00, -1.6664e+00, -1.7151e+00,
        -1.3815e+00, -1.5396e+00, -1.4866e+00, -1.8722e+00, -1.2723e+00,
        -1.2495e+00, -1.1087e+00, -1.5720e+00, -1.4118e+00, -1.3761e+00,
        -1.2334e+00, -1.5275e+00, -1.0837e+00, -1.3043e+00, -1.5148e+00,
        -1.2071e+00,  1.2596e+00,  1.3633e+00,  2.0720e+00,  2.4239e+00,
         5.1012e+00,  1.0097e-02,  2.9184e-01,  1.8662e-01, -5.7464e-02,
         1.7195e-01,  2.2273e-01,  2.7862e-01,  3.1917e-01,  3.1327e-01,
         2.7341e-01,  4.1559e-01,  3.4755e-01,  2.9182e-01,  4.5822e-01,
         1.2556e-01, -3.2157e-02,  1.9303e-01,  5.1605e-01,  2.7720e-01,
         1.5898e-01,  1.3560e-01,  2.3954e-01,  2.7874e-01,  3.8521e-01,
         9.1655e-02,  2.2342e-01,  2.9827e-01,  2.8137e-01,  2.1247e-01,
         2.8154e-01,  4.0036e-01,  2.3006e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1422e-01,  4.3425e-02,  1.3117e-01,  1.8770e-01,
         2.5666e-01,  3.2071e-01,  1.8966e-01,  1.9055e+00,  2.9271e+00,
         4.0062e+00,  6.6606e-01,  9.6956e-01,  9.1617e-01,  9.8499e-01,
         9.8477e-01,  9.4577e-01,  8.8445e-01,  8.9505e-01,  8.1904e-01,
         8.1810e-01,  7.2945e-01,  7.9302e-01,  9.0579e-01,  5.6602e-01,
         6.0755e-01,  7.9851e-01,  4.7379e-01,  7.9688e-01,  2.9372e-01,
         8.5580e-01,  7.0096e-01,  8.9653e-01,  9.1800e-01,  8.6130e-01,
         1.0191e+00,  1.0376e+00,  5.9260e-01,  9.4114e-01,  1.0209e+00,
         1.0787e+00,  6.0250e-01,  7.6286e-01,  9.3525e-01,  1.0856e+00,
         5.4287e-01,  9.5197e-01,  7.1729e-01,  9.2129e-01,  8.7946e-01,
         4.8340e-01,  8.3271e-01,  8.3050e-01,  8.3645e-01,  9.3982e-01,
         7.9559e-01,  5.3498e-01,  9.5288e-01,  9.0972e-01,  4.9539e-01,
         9.2296e-01,  6.7202e-01,  6.6032e-01,  8.0723e-01,  9.3307e-01,
         9.6323e-01,  6.7488e-01,  6.8489e-01,  8.6958e-01,  8.5574e-01,
         1.0293e+00,  9.4688e-01,  9.4253e-01,  8.7629e-01,  1.0780e+00,
         8.4789e-01,  8.5010e-01,  4.1136e-01,  5.5059e-01,  8.2178e-01,
         9.5012e-01,  9.9024e-01,  8.5358e-01,  7.0601e-01,  5.8281e-01,
         7.7449e-01,  4.7662e-01,  3.6623e-01,  8.0113e-01,  4.8721e-01,
         6.7518e-01,  8.1541e-01,  6.2558e-01,  3.8238e-01,  7.0476e-01,
         3.2441e-01,  8.3580e-01,  6.6899e-01,  8.8177e-01,  7.6522e-01,
         7.4206e-01,  7.2543e-01,  3.4291e-01,  5.9351e-01,  7.1086e-01,
         5.6199e-01,  6.1282e-01,  6.9996e-01,  6.0077e-01,  5.4171e-01,
         6.1410e-01,  6.9848e-01,  7.5774e-01,  4.3238e-01,  5.9738e-01,
         7.1932e-01,  6.1840e-01,  6.8956e-01,  6.1038e-01,  8.2318e-01,
         5.8011e-01,  4.5092e-01,  6.1212e-01,  6.1718e-01,  7.1921e-01,
         6.4293e-01,  6.4525e-01,  1.8883e-01,  7.8480e-01,  3.0069e-01,
         7.2887e-01,  6.2399e-01,  6.6942e-01,  7.3717e-01,  8.1537e-01,
         7.2342e-01,  6.4180e-01,  7.7697e-01,  2.8554e-01,  6.7343e-01,
         4.0792e-01,  6.5376e-01,  5.6769e-01,  4.7708e-01,  1.5920e-01,
         5.0702e-01,  5.4567e-01,  5.8849e-01,  4.4521e-01,  1.7773e-01,
         1.0747e-01,  1.9956e-01,  1.1167e-03,  4.3401e-01,  4.1330e-01,
         4.4525e-01,  3.7416e-01,  2.7093e-01,  3.2510e-01,  2.9602e-01,
        -8.0855e-02,  3.4463e-01,  3.2382e-01,  4.8701e-01,  5.2668e-02,
         1.1127e-01,  1.6599e-01,  4.0046e-01,  3.8966e-01,  1.6817e-01,
        -2.7653e-02,  2.2511e-01,  4.4101e-01,  2.5872e-01,  1.3665e-01,
         2.0560e-01,  1.6019e-01,  1.8782e-01,  2.1334e-01, -1.2877e-02,
         3.7293e-01,  3.8379e-01,  3.0480e-01,  1.4935e-01,  3.1235e-01,
         2.7827e-01,  2.7514e-01,  1.7931e-01,  3.0273e-01,  2.4004e-01,
         1.7653e-01,  1.2764e-02,  5.8620e-03,  2.4309e-01,  1.7538e-01,
         1.9578e-01, -2.9892e-01,  2.5945e-01,  5.0100e-01,  3.1910e-01,
         4.0670e-01,  2.8691e-01,  2.8715e-01,  4.4242e-01,  2.7431e-01,
         3.5581e-01,  1.4272e-01,  2.8322e-01,  3.3942e-01,  3.6264e-01,
        -4.4385e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.3489e-01,
        -1.4639e-01,  3.3533e-01, -1.2873e-01,  2.7119e-01,  2.8369e-01,
         2.4938e-01,  2.2171e-01,  8.6043e-02,  7.7230e-02,  2.3392e-01,
         1.8385e-01,  3.5017e-01, -8.3538e-02,  5.4841e-02,  1.7300e-01,
         1.3890e-01, -1.1737e-01,  2.7831e-01,  2.0673e-01,  2.9699e-01,
         5.0392e-02,  3.9544e-02,  1.3058e-01,  5.6121e-02, -3.8855e-01,
         6.2027e-02, -4.4967e-02,  7.5858e-02, -7.1514e-02,  8.3430e-02,
         1.6016e-02, -2.4640e-01, -4.3320e-01, -1.7785e-01, -4.5623e-01,
        -2.5913e-01, -3.5842e-01, -1.8804e-01,  5.6774e-02, -9.4770e-02,
         2.6782e-02, -3.4986e-01, -5.1766e-01, -1.6819e-01,  1.0013e-01,
        -3.1689e-01, -4.4083e-01, -4.2816e-01, -2.9013e-01, -2.1091e-01,
        -1.6202e-01, -3.8447e-01, -6.3530e-01, -1.8416e-01, -2.3569e-01,
        -2.6860e-01, -1.9484e-01, -2.4137e-01, -2.5274e-01, -3.8324e-01,
        -3.3317e-01, -5.6655e-01, -8.9386e-01, -5.9353e-01, -7.0589e-01,
        -5.0448e-01, -4.2512e-01, -3.8659e-01, -1.0698e+00, -8.5307e-01,
        -5.3290e-01, -6.2606e-01, -9.2498e-01, -7.3774e-01, -8.2953e-01,
        -7.0625e-01, -6.9558e-01, -9.4607e-01, -9.9222e-01, -9.7274e-01,
        -9.1749e-01, -7.2822e-01, -8.4097e-01, -8.0888e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809227
t6: 1641198809227
state_values: tensor([1.4287, 1.3491, 1.4615, 1.6330, 1.8069, 1.9307, 2.0195, 2.0789, 2.1209,
        2.1873, 2.2830, 2.2624, 2.2875, 2.3042, 2.3677, 2.5368, 2.4968, 2.4568,
        2.5184, 2.5472, 2.5861, 2.5912, 2.4268, 2.3593, 2.5175, 2.5455, 2.5654,
        2.6554, 2.6199, 2.6261, 2.8533, 2.7994, 2.8134, 2.7388, 2.7272, 2.7318,
        2.7453, 2.7493, 2.8040, 2.7900, 2.8290, 2.8304, 2.8378, 2.9517, 2.8791,
        2.8826, 2.8812, 2.9630, 2.9165, 2.9061, 2.9273, 2.9920, 2.9550, 3.0996,
        3.0298, 3.0072, 3.0492, 3.2024, 3.0792, 3.1660, 3.0881, 3.0696, 3.0650,
        3.1073, 3.0864, 3.1052, 2.8933, 2.8320, 2.7830, 2.8136, 2.7360, 2.9525,
        3.0578, 3.0726, 3.0859, 3.1947, 3.1779, 3.1879, 3.1816, 3.2030, 3.1827,
        3.1824, 3.2048, 3.1994, 3.2391, 3.3545, 3.2586, 3.2467, 3.2935, 3.3301,
        3.2824, 3.3596, 3.3163, 3.2965, 3.2849, 3.2897, 3.3817, 3.3544, 3.3407,
        3.3941, 3.3760, 3.3399, 3.3382, 3.3439, 3.3374, 3.3455, 3.3498, 3.3652,
        3.3705, 3.3750, 3.4238, 3.4587, 3.4599, 3.2810, 3.1704, 3.0920, 3.2714,
        3.3955, 3.3823, 3.4130, 3.4089, 3.4045, 3.4170, 3.4173, 3.5107, 3.4583,
        3.5618, 3.4912, 3.4647, 3.4624, 3.6531, 3.5728, 3.5018, 3.5463, 3.7879,
        3.6374, 3.6999, 3.6112, 3.6125, 3.6390, 3.5795, 3.5601, 3.5453, 3.5703,
        3.5901, 3.5916, 3.5577, 3.5648, 3.6551, 3.5965, 3.8023, 3.6682, 3.6249,
        3.6059, 3.6030, 3.8283, 3.7470, 3.7463, 3.6678, 3.6467, 3.6426, 3.6458,
        3.6360, 3.6604, 3.6310, 3.6616, 3.6609, 3.7455, 3.6789, 3.6590, 3.6682,
        3.6696, 3.6744, 3.7131, 3.6843, 3.6694, 3.6721, 3.7105, 3.7054, 3.6969,
        3.6812, 3.7509, 3.7017, 3.7031, 3.6986, 3.7098, 3.6931, 3.7772, 3.8650,
        3.8825, 3.7835, 3.7519, 3.7273, 3.8176, 3.7672, 3.8880, 3.8649, 3.8905,
        3.8142, 3.8561, 3.8093, 3.7925, 3.7925, 3.8064, 3.8518, 3.8903, 3.8687,
        3.8289, 3.8117, 3.8678, 3.9529, 3.8514, 3.8443, 3.9093, 3.9451, 3.8665,
        3.8458, 3.8327, 3.8334, 3.9160, 3.8564, 3.9485, 3.9254, 3.8920, 3.8569,
        3.8492, 3.8514, 3.9607, 3.8826, 3.9038, 3.9107, 3.9279, 3.8771, 3.9093,
        3.8734, 3.8945, 3.8761, 3.8672, 3.8914, 3.8630, 3.8657, 3.8853, 3.8920,
        3.8838, 3.8743, 3.8835, 3.8769, 3.9389, 3.9124, 3.9028, 3.9535, 3.9745,
        3.9591, 3.9793, 4.1060, 3.9730, 4.1417, 3.9927, 3.9985, 4.0153, 4.0227,
        4.0556, 4.1194, 4.0542, 4.0642, 3.9992, 4.0585, 3.9948, 4.0365, 3.9825,
        4.1470, 4.0076, 4.0107, 4.0240, 4.1545, 4.0320, 4.0105, 4.0257, 3.9934,
        4.1869, 4.1261, 4.1692, 4.0766, 4.0270, 3.9981, 4.0008, 4.0019, 4.0050,
        4.0033, 4.0563, 4.0208, 4.0611, 4.0245, 4.0912, 4.0919, 4.0401, 4.0312,
        4.0208, 4.0152, 4.0289, 4.0292, 4.1161, 4.1473, 4.0726, 4.0522, 4.0430,
        4.0459, 4.1576, 4.0986, 4.1939, 4.1218, 4.2328, 4.1462, 4.1068, 4.1284,
        4.3201, 4.1443, 4.0906, 4.0754, 4.1497, 4.0962, 4.1524, 4.0955, 4.0903,
        4.0812, 4.0816, 4.0743, 4.2308, 4.1349, 4.2324, 4.1350, 4.0963, 4.1063,
        4.1001, 4.1810, 4.1225, 4.0989, 4.0917, 4.1128, 4.1149, 4.2179, 4.2316,
        4.1630, 4.1626, 4.1257, 4.1193, 4.2639, 4.1669, 4.1419, 4.1548, 4.1477,
        4.2948, 4.1741, 4.2947, 4.4519, 4.2511, 4.2051, 4.1884, 4.1655, 4.1778,
        4.1496, 4.3626, 4.2165, 4.1723, 4.1419, 4.1554, 4.3790, 4.3906, 4.2784,
        4.2978, 4.2349, 4.1997, 4.1795, 4.1852, 4.1772, 4.1753, 4.2405, 4.2251,
        4.2249, 4.2689, 4.2151, 4.4059, 4.2506, 4.2221, 4.2051, 4.2070, 4.2407,
        4.2570, 4.5158, 4.3037, 4.3057, 4.3166, 4.2515, 4.3762, 4.4328, 4.3568,
        4.2942, 4.2693, 4.4193, 4.4198, 4.3923, 4.2904, 4.3389, 4.3161],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809231
t8: 1641198809231
t9: 1641198809232
t10: 1641198809242
t11: 1641198809244
t12: 1641198809244
t1: 1641198809244
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809255
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9955, 1.0038, 0.9438, 0.9750, 0.9737, 0.9960, 0.9923, 0.9647, 0.9895,
        1.0268, 0.9323, 0.9971, 1.0223, 0.9605, 0.9785, 0.9336, 0.8906, 0.8677,
        0.9780, 1.0214, 1.0248, 0.9910, 1.0048, 0.9595, 1.0113, 0.9949, 0.9225,
        0.9969, 0.9971, 0.8656, 0.9232, 1.0895, 0.9101, 1.0204, 1.0322, 0.9902,
        1.0687, 0.9273, 1.0151, 0.9807, 0.9939, 0.9985, 0.9329, 0.9558, 0.9805,
        1.0198, 0.9214, 0.9921, 1.0925, 0.9718, 0.9605, 0.9985, 0.9660, 0.9938,
        1.0213, 0.9407, 0.9512, 1.0513, 0.8808, 0.9833, 0.9995, 0.9883, 0.9720,
        1.0396, 0.9687, 1.0254, 1.1007, 1.0670, 0.9669, 0.9962, 1.0872, 0.9649,
        1.0184, 1.0737, 0.8650, 1.0045, 1.0059, 0.9945, 0.9833, 1.0163, 0.9964,
        0.9898, 1.0150, 0.9553, 0.9692, 0.9622, 1.0404, 0.8902, 1.0123, 0.9761,
        0.9044, 0.9993, 1.0047, 1.0246, 1.0803, 0.8749, 1.0025, 0.9996, 0.9537,
        0.9970, 1.0062, 1.0427, 0.9977, 1.1226, 1.1096, 1.1280, 1.0581, 1.0224,
        1.0075, 0.9946, 1.0167, 1.0199, 1.0031, 1.1048, 1.1102, 1.2118, 0.9175,
        0.9919, 0.9999, 0.9998, 1.0059, 1.0046, 1.0059, 0.9506, 0.9943, 0.9496,
        0.9826, 1.0368, 1.1355, 0.7733, 0.9985, 0.9424, 0.9072, 0.8655, 0.9786,
        1.0407, 1.0010, 1.0055, 0.9844, 1.0000, 1.0139, 1.1345, 0.9682, 0.9803,
        0.9961, 1.0432, 1.0762, 0.9115, 0.9985, 1.0007, 0.9995, 0.9903, 1.0567,
        1.0137, 0.8734, 0.9823, 1.0508, 0.9800, 1.0117, 1.0608, 1.1188, 1.0008,
        0.9932, 1.0279, 0.9689, 1.0153, 0.9678, 0.9968, 1.0081, 0.9959, 1.0287,
        1.0373, 0.9652, 1.0029, 0.9995, 1.0009, 0.9822, 0.9975, 0.9896, 1.0178,
        0.9469, 1.0705, 1.1340, 1.0400, 0.9943, 1.0004, 0.9910, 1.0294, 1.0340,
        0.9838, 1.1064, 1.5061, 0.8249, 1.0002, 0.9309, 1.0157, 1.0475, 0.9740,
        0.9115, 1.0711, 0.9954, 1.0297, 0.9804, 0.9836, 0.9968, 1.0109, 1.0235,
        1.0664, 0.9214, 0.9713, 0.9795, 0.9896, 0.9464, 0.9897, 0.9890, 1.0095,
        1.0335, 1.0795, 0.9059, 1.0006, 0.9705, 1.0144, 1.0002, 1.0006, 1.0584,
        1.0856, 0.8649, 0.9871, 0.9820, 0.9863, 0.9833, 1.0845, 0.9098, 1.0784,
        0.9704, 1.0183, 1.0183, 0.9871, 1.0025, 1.0027, 0.9975, 0.9913, 1.0345,
        1.0308, 1.0529, 1.0110, 0.9815, 0.9987, 1.0460, 0.9366, 0.9870, 1.0046,
        0.9926, 0.9498, 0.9453, 0.8146, 1.0029, 0.9734, 0.9792, 0.9870, 0.9830,
        1.0095, 1.0084, 0.9913, 1.0662, 0.8481, 1.0231, 0.9552, 1.0358, 0.8232,
        0.9638, 0.9884, 0.9812, 0.9446, 0.9507, 1.0316, 0.9603, 1.0351, 0.8632,
        0.9953, 1.0565, 0.9924, 1.0243, 1.2484, 1.0263, 1.0060, 1.0063, 1.0244,
        0.9633, 1.0032, 0.9865, 1.0128, 0.9391, 1.0017, 1.0035, 1.0697, 1.1042,
        1.0245, 1.0014, 1.0007, 1.0139, 0.9329, 0.9998, 1.0073, 1.0295, 1.0362,
        0.9222, 1.0044, 0.9937, 1.0042, 0.9720, 1.0042, 0.9991, 0.9655, 0.9184,
        0.9757, 1.1250, 1.6733, 0.8543, 1.0293, 0.9405, 0.9896, 1.0090, 1.0185,
        1.0157, 1.0243, 0.9352, 0.9769, 0.9602, 0.9786, 1.0125, 1.0926, 1.0905,
        0.9221, 0.9974, 1.0504, 1.0168, 0.9970, 0.9976, 0.9907, 1.0237, 1.0021,
        0.9913, 1.1536, 1.0515, 0.8803, 0.9961, 1.0046, 0.9726, 0.9993, 0.9390,
        0.9444, 0.8259, 1.0740, 0.9917, 1.0444, 1.0337, 0.9929, 0.9985, 1.0041,
        0.9174, 0.9883, 1.0389, 1.0072, 1.0574, 0.8425, 0.9917, 1.0043, 0.9708,
        1.0005, 1.0221, 1.2603, 0.9823, 1.0194, 1.0146, 0.9657, 1.0018, 0.9970,
        0.9753, 1.0036, 0.8881, 0.9790, 1.0600, 1.1313, 1.0518, 0.9777, 0.9951,
        1.0063, 1.0782, 0.9606, 0.9943, 1.0176, 0.7840, 1.0363, 1.0208, 1.0003,
        1.0463, 0.7963, 1.0296, 1.0392, 1.0010, 0.9594, 0.9923, 0.9271],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809259
t4: 1641198809259
surr1, surr2: tensor([-3.1474e+00, -9.8464e-01, -1.3948e+00, -2.4134e+00, -2.6739e+00,
        -2.9325e+00, -2.7616e+00, -2.6685e+00, -2.7949e+00, -2.7349e+00,
        -2.4421e+00, -2.6492e+00, -2.5447e+00, -2.4834e+00, -2.3996e+00,
        -2.4319e+00, -2.1339e+00, -2.2464e+00, -2.3582e+00, -2.3630e+00,
        -2.2724e+00, -2.1657e+00, -8.4858e-01, -1.6945e-01, -1.6537e+00,
        -1.7950e+00, -1.5328e+00, -1.6846e+00, -1.5953e+00, -1.3405e+00,
        -1.8995e+00, -1.9206e+00, -1.5796e+00, -1.9414e+00, -1.6723e+00,
        -1.5793e+00, -1.5903e+00, -1.6779e+00, -1.5586e+00, -1.5567e+00,
        -1.5119e+00, -1.4388e+00, -1.4029e+00, -1.5470e+00, -1.7108e+00,
        -1.4725e+00, -1.4142e+00, -1.5421e+00, -1.6776e+00, -1.7139e+00,
        -1.3785e+00, -1.5411e+00, -1.4878e+00, -1.8716e+00, -1.2726e+00,
        -1.2488e+00, -1.0992e+00, -1.5690e+00, -1.3816e+00, -1.3780e+00,
        -1.2438e+00, -1.5254e+00, -1.0798e+00, -1.3057e+00, -1.5148e+00,
        -1.2088e+00,  1.2606e+00,  1.3631e+00,  2.0728e+00,  2.4352e+00,
         5.0579e+00,  1.0107e-02,  2.9301e-01,  1.8646e-01, -5.5226e-02,
         1.7195e-01,  2.2278e-01,  2.7835e-01,  3.1838e-01,  3.1408e-01,
         2.7331e-01,  4.1541e-01,  3.4769e-01,  2.9168e-01,  4.5466e-01,
         1.2549e-01, -3.2233e-02,  1.9093e-01,  5.1243e-01,  2.7821e-01,
         1.5797e-01,  1.3571e-01,  2.3976e-01,  2.7930e-01,  3.8739e-01,
         8.9095e-02,  2.2315e-01,  2.9819e-01,  2.7996e-01,  2.1195e-01,
         2.8183e-01,  4.0180e-01,  2.3002e-01,  3.9370e-01, -1.2362e-01,
         9.0930e-02, -1.1411e-01,  4.3352e-02,  1.3093e-01,  1.8818e-01,
         2.5741e-01,  3.2094e-01,  1.8986e-01,  1.9138e+00,  2.9543e+00,
         4.4135e+00,  6.6807e-01,  9.7012e-01,  9.1765e-01,  9.8504e-01,
         9.8449e-01,  9.4576e-01,  8.8451e-01,  8.9460e-01,  8.2014e-01,
         8.1475e-01,  7.3160e-01,  7.9559e-01,  9.3506e-01,  4.8634e-01,
         6.0756e-01,  8.0392e-01,  4.7294e-01,  7.6635e-01,  2.9368e-01,
         8.5038e-01,  7.0098e-01,  8.9316e-01,  9.1003e-01,  8.6161e-01,
         1.0219e+00,  1.0702e+00,  5.9201e-01,  9.4016e-01,  1.0210e+00,
         1.0776e+00,  6.0289e-01,  7.6145e-01,  9.3534e-01,  1.0909e+00,
         5.4291e-01,  9.5696e-01,  7.1969e-01,  9.2201e-01,  8.5343e-01,
         4.8366e-01,  8.2852e-01,  8.3543e-01,  8.3752e-01,  9.4653e-01,
         8.0917e-01,  5.3493e-01,  9.5272e-01,  9.0823e-01,  4.9542e-01,
         9.2348e-01,  6.7314e-01,  6.6121e-01,  8.0709e-01,  9.3302e-01,
         9.6377e-01,  6.7476e-01,  6.8480e-01,  8.6972e-01,  8.5577e-01,
         1.0293e+00,  9.4667e-01,  9.4240e-01,  8.7614e-01,  1.0790e+00,
         8.4713e-01,  8.6316e-01,  4.2407e-01,  5.5085e-01,  8.2203e-01,
         9.4971e-01,  9.9270e-01,  8.5173e-01,  7.0641e-01,  5.8316e-01,
         7.8140e-01,  6.5257e-01,  3.3567e-01,  8.0704e-01,  4.8939e-01,
         6.7423e-01,  8.1326e-01,  6.3186e-01,  3.8005e-01,  7.1775e-01,
         3.2434e-01,  8.3661e-01,  6.6937e-01,  8.8179e-01,  7.6454e-01,
         7.4081e-01,  7.2991e-01,  3.4388e-01,  5.9174e-01,  7.0737e-01,
         5.6308e-01,  6.1236e-01,  6.9481e-01,  5.9514e-01,  5.4393e-01,
         6.1444e-01,  7.0054e-01,  7.5915e-01,  4.3210e-01,  5.9771e-01,
         7.2028e-01,  6.1666e-01,  6.8944e-01,  6.1032e-01,  8.2824e-01,
         5.8109e-01,  4.3332e-01,  6.1414e-01,  6.1698e-01,  7.1744e-01,
         6.4114e-01,  6.5345e-01,  1.8852e-01,  7.9048e-01,  3.0071e-01,
         7.2911e-01,  6.2336e-01,  6.6973e-01,  7.3716e-01,  8.1530e-01,
         7.2343e-01,  6.4171e-01,  7.7757e-01,  2.8558e-01,  6.7392e-01,
         4.0787e-01,  6.5425e-01,  5.6770e-01,  4.7630e-01,  1.5918e-01,
         5.0613e-01,  5.4566e-01,  5.8879e-01,  4.4159e-01,  1.7804e-01,
         9.7270e-02,  1.9867e-01,  1.1117e-03,  4.3170e-01,  4.1183e-01,
         4.4256e-01,  3.6984e-01,  2.7066e-01,  3.2377e-01,  3.0260e-01,
        -7.6189e-02,  3.4762e-01,  3.2389e-01,  4.9347e-01,  4.8171e-02,
         1.1146e-01,  1.6576e-01,  3.9935e-01,  3.8583e-01,  1.6835e-01,
        -2.7710e-02,  2.2421e-01,  4.4300e-01,  2.4815e-01,  1.3674e-01,
         2.0430e-01,  1.6039e-01,  1.8885e-01,  2.4213e-01, -1.2881e-02,
         3.7298e-01,  3.8362e-01,  3.0431e-01,  1.4947e-01,  3.1248e-01,
         2.7840e-01,  2.7526e-01,  1.7928e-01,  3.0133e-01,  2.4073e-01,
         1.7722e-01,  1.2812e-02,  5.8633e-03,  2.4309e-01,  1.7537e-01,
         1.9561e-01, -2.9911e-01,  2.5947e-01,  5.0065e-01,  3.1912e-01,
         4.0689e-01,  2.8698e-01,  2.8715e-01,  4.4256e-01,  2.7422e-01,
         3.5300e-01,  1.4266e-01,  2.8309e-01,  3.3741e-01,  3.5534e-01,
        -4.4278e-03,  9.4308e-02, -1.5293e-01, -1.0386e-01,  3.3914e-01,
        -1.4687e-01,  3.3689e-01, -1.2872e-01,  2.7108e-01,  2.8336e-01,
         2.4903e-01,  2.2239e-01,  8.5827e-02,  7.7406e-02,  2.3402e-01,
         1.8418e-01,  3.5273e-01, -8.3681e-02,  5.4780e-02,  1.7332e-01,
         1.3855e-01, -1.1737e-01,  2.7830e-01,  2.0680e-01,  2.9743e-01,
         5.0279e-02,  3.9532e-02,  1.3042e-01,  5.8857e-02, -3.8883e-01,
         6.0670e-02, -4.4966e-02,  7.5939e-02, -7.1491e-02,  8.3418e-02,
         1.6000e-02, -2.4913e-01, -3.9752e-01, -1.7592e-01, -4.5622e-01,
        -2.6020e-01, -3.5923e-01, -1.8783e-01,  5.6769e-02, -9.4772e-02,
         2.6756e-02, -3.5017e-01, -5.1952e-01, -1.6822e-01,  1.0033e-01,
        -2.9666e-01, -4.4045e-01, -4.2814e-01, -2.8860e-01, -2.1081e-01,
        -1.6301e-01, -4.4049e-01, -6.3482e-01, -1.8430e-01, -2.3548e-01,
        -2.6901e-01, -1.9475e-01, -2.4143e-01, -2.5206e-01, -3.8363e-01,
        -3.2875e-01, -5.6367e-01, -8.9821e-01, -6.1041e-01, -7.0628e-01,
        -5.0454e-01, -4.2534e-01, -3.8729e-01, -1.0680e+00, -8.4847e-01,
        -5.2941e-01, -6.3516e-01, -8.0572e-01, -7.3153e-01, -8.2922e-01,
        -7.0580e-01, -7.0266e-01, -8.3710e-01, -9.8648e-01, -9.7063e-01,
        -9.1928e-01, -7.2332e-01, -8.3842e-01, -7.9521e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1474e+00, -9.8464e-01, -1.3948e+00, -2.4134e+00, -2.6739e+00,
        -2.9325e+00, -2.7616e+00, -2.6685e+00, -2.7949e+00, -2.7349e+00,
        -2.4421e+00, -2.6492e+00, -2.5447e+00, -2.4834e+00, -2.3996e+00,
        -2.4319e+00, -2.1565e+00, -2.3300e+00, -2.3582e+00, -2.3630e+00,
        -2.2724e+00, -2.1657e+00, -8.4858e-01, -1.6945e-01, -1.6537e+00,
        -1.7950e+00, -1.5328e+00, -1.6846e+00, -1.5953e+00, -1.3938e+00,
        -1.8995e+00, -1.9206e+00, -1.5796e+00, -1.9414e+00, -1.6723e+00,
        -1.5793e+00, -1.5903e+00, -1.6779e+00, -1.5586e+00, -1.5567e+00,
        -1.5119e+00, -1.4388e+00, -1.4029e+00, -1.5470e+00, -1.7108e+00,
        -1.4725e+00, -1.4142e+00, -1.5421e+00, -1.6776e+00, -1.7139e+00,
        -1.3785e+00, -1.5411e+00, -1.4878e+00, -1.8716e+00, -1.2726e+00,
        -1.2488e+00, -1.0992e+00, -1.5690e+00, -1.4118e+00, -1.3780e+00,
        -1.2438e+00, -1.5254e+00, -1.0798e+00, -1.3057e+00, -1.5148e+00,
        -1.2088e+00,  1.2598e+00,  1.3631e+00,  2.0728e+00,  2.4352e+00,
         5.0579e+00,  1.0107e-02,  2.9301e-01,  1.8646e-01, -5.7464e-02,
         1.7195e-01,  2.2278e-01,  2.7835e-01,  3.1838e-01,  3.1408e-01,
         2.7331e-01,  4.1541e-01,  3.4769e-01,  2.9168e-01,  4.5466e-01,
         1.2549e-01, -3.2233e-02,  1.9303e-01,  5.1243e-01,  2.7821e-01,
         1.5797e-01,  1.3571e-01,  2.3976e-01,  2.7930e-01,  3.8739e-01,
         9.1655e-02,  2.2315e-01,  2.9819e-01,  2.7996e-01,  2.1195e-01,
         2.8183e-01,  4.0180e-01,  2.3002e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1411e-01,  4.3352e-02,  1.3093e-01,  1.8818e-01,
         2.5741e-01,  3.2094e-01,  1.8986e-01,  1.9055e+00,  2.9271e+00,
         4.0062e+00,  6.6807e-01,  9.7012e-01,  9.1765e-01,  9.8504e-01,
         9.8449e-01,  9.4576e-01,  8.8451e-01,  8.9460e-01,  8.2014e-01,
         8.1475e-01,  7.3160e-01,  7.9559e-01,  9.0579e-01,  5.6602e-01,
         6.0756e-01,  8.0392e-01,  4.7294e-01,  7.9688e-01,  2.9368e-01,
         8.5038e-01,  7.0098e-01,  8.9316e-01,  9.1003e-01,  8.6161e-01,
         1.0219e+00,  1.0376e+00,  5.9201e-01,  9.4016e-01,  1.0210e+00,
         1.0776e+00,  6.0289e-01,  7.6145e-01,  9.3534e-01,  1.0909e+00,
         5.4291e-01,  9.5696e-01,  7.1969e-01,  9.2201e-01,  8.7946e-01,
         4.8366e-01,  8.2852e-01,  8.3543e-01,  8.3752e-01,  9.4653e-01,
         7.9559e-01,  5.3493e-01,  9.5272e-01,  9.0823e-01,  4.9542e-01,
         9.2348e-01,  6.7314e-01,  6.6121e-01,  8.0709e-01,  9.3302e-01,
         9.6377e-01,  6.7476e-01,  6.8480e-01,  8.6972e-01,  8.5577e-01,
         1.0293e+00,  9.4667e-01,  9.4240e-01,  8.7614e-01,  1.0790e+00,
         8.4713e-01,  8.6316e-01,  4.1136e-01,  5.5085e-01,  8.2203e-01,
         9.4971e-01,  9.9270e-01,  8.5173e-01,  7.0641e-01,  5.8316e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.0704e-01,  4.8939e-01,
         6.7423e-01,  8.1326e-01,  6.3186e-01,  3.8005e-01,  7.1775e-01,
         3.2434e-01,  8.3661e-01,  6.6937e-01,  8.8179e-01,  7.6454e-01,
         7.4081e-01,  7.2991e-01,  3.4388e-01,  5.9174e-01,  7.0737e-01,
         5.6308e-01,  6.1236e-01,  6.9481e-01,  5.9514e-01,  5.4393e-01,
         6.1444e-01,  7.0054e-01,  7.5915e-01,  4.3210e-01,  5.9771e-01,
         7.2028e-01,  6.1666e-01,  6.8944e-01,  6.1032e-01,  8.2824e-01,
         5.8109e-01,  4.5092e-01,  6.1414e-01,  6.1698e-01,  7.1744e-01,
         6.4114e-01,  6.5345e-01,  1.8852e-01,  7.9048e-01,  3.0071e-01,
         7.2911e-01,  6.2336e-01,  6.6973e-01,  7.3716e-01,  8.1530e-01,
         7.2343e-01,  6.4171e-01,  7.7757e-01,  2.8558e-01,  6.7392e-01,
         4.0787e-01,  6.5425e-01,  5.6770e-01,  4.7630e-01,  1.5918e-01,
         5.0613e-01,  5.4566e-01,  5.8879e-01,  4.4159e-01,  1.7804e-01,
         1.0747e-01,  1.9867e-01,  1.1117e-03,  4.3170e-01,  4.1183e-01,
         4.4256e-01,  3.6984e-01,  2.7066e-01,  3.2377e-01,  3.0260e-01,
        -8.0855e-02,  3.4762e-01,  3.2389e-01,  4.9347e-01,  5.2668e-02,
         1.1146e-01,  1.6576e-01,  3.9935e-01,  3.8583e-01,  1.6835e-01,
        -2.7710e-02,  2.2421e-01,  4.4300e-01,  2.5872e-01,  1.3674e-01,
         2.0430e-01,  1.6039e-01,  1.8885e-01,  2.1334e-01, -1.2881e-02,
         3.7298e-01,  3.8362e-01,  3.0431e-01,  1.4947e-01,  3.1248e-01,
         2.7840e-01,  2.7526e-01,  1.7928e-01,  3.0133e-01,  2.4073e-01,
         1.7722e-01,  1.2764e-02,  5.8633e-03,  2.4309e-01,  1.7537e-01,
         1.9561e-01, -2.9911e-01,  2.5947e-01,  5.0065e-01,  3.1912e-01,
         4.0689e-01,  2.8698e-01,  2.8715e-01,  4.4256e-01,  2.7422e-01,
         3.5300e-01,  1.4266e-01,  2.8309e-01,  3.3741e-01,  3.5534e-01,
        -4.4278e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.3914e-01,
        -1.4687e-01,  3.3689e-01, -1.2872e-01,  2.7108e-01,  2.8336e-01,
         2.4903e-01,  2.2239e-01,  8.5827e-02,  7.7406e-02,  2.3402e-01,
         1.8418e-01,  3.5273e-01, -8.3681e-02,  5.4780e-02,  1.7332e-01,
         1.3855e-01, -1.1737e-01,  2.7830e-01,  2.0680e-01,  2.9743e-01,
         5.0279e-02,  3.9532e-02,  1.3042e-01,  5.6121e-02, -3.8883e-01,
         6.2027e-02, -4.4966e-02,  7.5939e-02, -7.1491e-02,  8.3418e-02,
         1.6000e-02, -2.4913e-01, -4.3320e-01, -1.7592e-01, -4.5622e-01,
        -2.6020e-01, -3.5923e-01, -1.8783e-01,  5.6769e-02, -9.4772e-02,
         2.6756e-02, -3.5017e-01, -5.1952e-01, -1.6822e-01,  1.0033e-01,
        -3.1689e-01, -4.4045e-01, -4.2814e-01, -2.8860e-01, -2.1081e-01,
        -1.6301e-01, -3.8447e-01, -6.3482e-01, -1.8430e-01, -2.3548e-01,
        -2.6901e-01, -1.9475e-01, -2.4143e-01, -2.5206e-01, -3.8363e-01,
        -3.3317e-01, -5.6367e-01, -8.9821e-01, -5.9353e-01, -7.0628e-01,
        -5.0454e-01, -4.2534e-01, -3.8729e-01, -1.0680e+00, -8.4847e-01,
        -5.2941e-01, -6.3516e-01, -9.2498e-01, -7.3153e-01, -8.2922e-01,
        -7.0580e-01, -7.0266e-01, -9.4607e-01, -9.8648e-01, -9.7063e-01,
        -9.1928e-01, -7.2332e-01, -8.3842e-01, -7.9521e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809268
t6: 1641198809268
state_values: tensor([1.3230, 1.2392, 1.3383, 1.4981, 1.6615, 1.7783, 1.8609, 1.9163, 1.9556,
        2.0170, 2.1057, 2.0870, 2.1101, 2.1259, 2.1847, 2.3414, 2.3048, 2.2678,
        2.3246, 2.3515, 2.3877, 2.3925, 2.2402, 2.1760, 2.3246, 2.3520, 2.3711,
        2.4551, 2.4229, 2.4289, 2.6279, 2.5827, 2.5948, 2.5310, 2.5210, 2.5253,
        2.5375, 2.5420, 2.5897, 2.5785, 2.6126, 2.6146, 2.6215, 2.7225, 2.6592,
        2.6624, 2.6619, 2.7358, 2.6941, 2.6858, 2.7046, 2.7665, 2.7321, 2.8713,
        2.8053, 2.7842, 2.8246, 2.9718, 2.8556, 2.9385, 2.8649, 2.8478, 2.8438,
        2.8849, 2.8662, 2.8846, 2.6870, 2.6337, 2.5923, 2.6187, 2.5546, 2.7429,
        2.8453, 2.8610, 2.8747, 2.9794, 2.9641, 2.9740, 2.9683, 2.9892, 2.9705,
        2.9706, 2.9925, 2.9882, 3.0266, 3.1373, 3.0470, 3.0358, 3.0809, 3.1167,
        3.0718, 3.1461, 3.1054, 3.0868, 3.0762, 3.0815, 3.1701, 3.1448, 3.1320,
        3.1836, 3.1669, 3.1329, 3.1317, 3.1376, 3.1324, 3.1408, 3.1456, 3.1610,
        3.1666, 3.1713, 3.2184, 3.2513, 3.2530, 3.0827, 2.9764, 2.9018, 3.0746,
        3.1947, 3.1832, 3.2130, 3.2095, 3.2057, 3.2180, 3.2186, 3.3023, 3.2589,
        3.3454, 3.2879, 3.2658, 3.2641, 3.4220, 3.3571, 3.2992, 3.3358, 3.5409,
        3.4123, 3.4639, 3.3912, 3.3924, 3.4147, 3.3662, 3.3507, 3.3391, 3.3599,
        3.3769, 3.3786, 3.3515, 3.3575, 3.4324, 3.3848, 3.5595, 3.4448, 3.4093,
        3.3937, 3.3918, 3.5858, 3.5123, 3.5116, 3.4468, 3.4296, 3.4266, 3.4297,
        3.4221, 3.4424, 3.4190, 3.4442, 3.4443, 3.5144, 3.4600, 3.4438, 3.4516,
        3.4532, 3.4574, 3.4896, 3.4664, 3.4543, 3.4567, 3.4887, 3.4849, 3.4782,
        3.4657, 3.5232, 3.4835, 3.4847, 3.4813, 3.4907, 3.4773, 3.5485, 3.6304,
        3.6475, 3.5550, 3.5270, 3.5071, 3.5860, 3.5414, 3.6537, 3.6319, 3.6564,
        3.5842, 3.6235, 3.5802, 3.5650, 3.5652, 3.5779, 3.6203, 3.6576, 3.6370,
        3.5993, 3.5838, 3.6365, 3.7170, 3.6216, 3.6144, 3.6769, 3.7111, 3.6365,
        3.6165, 3.6042, 3.6052, 3.6846, 3.6277, 3.7158, 3.6941, 3.6621, 3.6284,
        3.6214, 3.6238, 3.7283, 3.6544, 3.6745, 3.6813, 3.6979, 3.6500, 3.6805,
        3.6467, 3.6667, 3.6494, 3.6411, 3.6642, 3.6373, 3.6399, 3.6589, 3.6655,
        3.6583, 3.6492, 3.6584, 3.6522, 3.7117, 3.6865, 3.6777, 3.7262, 3.7466,
        3.7318, 3.7512, 3.8636, 3.7457, 3.8955, 3.7649, 3.7693, 3.7843, 3.7910,
        3.8204, 3.8772, 3.8196, 3.8284, 3.7718, 3.8239, 3.7675, 3.8047, 3.7562,
        3.9027, 3.7806, 3.7828, 3.7947, 3.9105, 3.8032, 3.7838, 3.7971, 3.7676,
        3.9380, 3.8872, 3.9242, 3.8435, 3.7997, 3.7730, 3.7756, 3.7767, 3.7801,
        3.7787, 3.8273, 3.7959, 3.8320, 3.7998, 3.8591, 3.8601, 3.8145, 3.8068,
        3.7970, 3.7917, 3.8049, 3.8053, 3.8838, 3.9115, 3.8452, 3.8272, 3.8191,
        3.8219, 3.9210, 3.8690, 3.9513, 3.8898, 3.9834, 3.9116, 3.8765, 3.8956,
        4.0521, 3.9108, 3.8635, 3.8504, 3.9158, 3.8696, 3.9188, 3.8694, 3.8645,
        3.8566, 3.8571, 3.8509, 3.9854, 3.9055, 3.9868, 3.9056, 3.8712, 3.8804,
        3.8752, 3.9456, 3.8954, 3.8750, 3.8685, 3.8871, 3.8891, 3.9773, 3.9888,
        3.9323, 3.9318, 3.9002, 3.8943, 4.0160, 3.9365, 3.9147, 3.9260, 3.9199,
        4.0418, 3.9437, 4.0422, 4.1705, 4.0067, 3.9688, 3.9550, 3.9364, 3.9467,
        3.9229, 4.0960, 3.9796, 3.9433, 3.9168, 3.9293, 4.1102, 4.1209, 4.0312,
        4.0470, 3.9953, 3.9666, 3.9506, 3.9553, 3.9492, 3.9479, 4.0016, 3.9892,
        3.9892, 4.0255, 3.9815, 4.1352, 4.0112, 3.9876, 3.9740, 3.9757, 4.0035,
        4.0171, 4.2301, 4.0560, 4.0572, 4.0662, 4.0132, 4.1139, 4.1608, 4.0992,
        4.0482, 4.0281, 4.1496, 4.1505, 4.1278, 4.0459, 4.0857, 4.0672],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809273
t8: 1641198809273
t9: 1641198809273
t10: 1641198809284
t11: 1641198809285
t12: 1641198809286
t1: 1641198809286
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809297
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9954, 1.0051, 0.9376, 0.9708, 0.9630, 0.9964, 0.9902, 0.9537, 0.9911,
        1.0331, 0.9292, 0.9981, 1.0201, 0.9625, 0.9818, 0.9255, 0.8902, 0.8642,
        0.9699, 1.0172, 1.0233, 0.9921, 1.0048, 0.9565, 1.0129, 0.9948, 0.9226,
        0.9973, 0.9971, 0.8590, 0.9164, 1.0874, 0.9133, 1.0210, 1.0347, 0.9897,
        1.0671, 0.9278, 1.0154, 0.9826, 0.9943, 0.9986, 0.9349, 0.9594, 0.9800,
        1.0210, 0.9224, 0.9951, 1.0926, 0.9716, 0.9606, 0.9978, 0.9683, 0.9939,
        1.0202, 0.9420, 0.9490, 1.0598, 0.8768, 0.9843, 0.9993, 0.9876, 0.9713,
        1.0372, 0.9692, 1.0250, 1.0977, 1.0654, 0.9687, 1.0051, 1.1031, 0.9671,
        1.0196, 1.0693, 0.8673, 1.0046, 1.0081, 0.9944, 0.9829, 1.0170, 0.9964,
        0.9898, 1.0147, 0.9566, 0.9683, 0.9681, 1.0412, 0.8862, 1.0080, 0.9769,
        0.9035, 0.9997, 1.0049, 1.0253, 1.0804, 0.8763, 1.0019, 0.9996, 0.9527,
        0.9958, 1.0064, 1.0438, 0.9977, 1.1225, 1.1093, 1.1232, 1.0555, 1.0191,
        1.0044, 0.9986, 1.0220, 1.0209, 1.0014, 1.1054, 1.1109, 1.2044, 0.9216,
        0.9904, 1.0028, 0.9999, 1.0051, 1.0044, 1.0057, 0.9527, 0.9943, 0.9529,
        0.9840, 1.0371, 1.1387, 0.7726, 0.9964, 0.9394, 0.9068, 0.8613, 0.9758,
        1.0417, 1.0010, 1.0051, 0.9811, 1.0002, 1.0148, 1.1401, 0.9677, 0.9803,
        0.9968, 1.0374, 1.0761, 0.9125, 0.9985, 1.0107, 0.9993, 0.9905, 1.0578,
        1.0140, 0.8741, 0.9777, 1.0506, 0.9822, 1.0120, 1.0640, 1.1197, 1.0008,
        0.9935, 1.0213, 0.9698, 1.0145, 0.9719, 0.9964, 1.0074, 0.9961, 1.0273,
        1.0361, 0.9664, 1.0025, 0.9996, 1.0008, 0.9831, 0.9976, 0.9900, 1.0175,
        0.9477, 1.0743, 1.1342, 1.0396, 0.9950, 0.9996, 0.9961, 1.0316, 1.0310,
        0.9838, 1.1081, 1.5291, 0.8217, 1.0034, 0.9399, 1.0112, 1.0478, 0.9769,
        0.9098, 1.0815, 0.9954, 1.0296, 0.9818, 0.9853, 0.9983, 1.0103, 1.0241,
        1.0671, 0.9207, 0.9700, 0.9802, 0.9898, 0.9438, 0.9857, 0.9905, 1.0096,
        1.0348, 1.0794, 0.9072, 1.0007, 0.9745, 1.0132, 1.0001, 1.0005, 1.0611,
        1.0858, 0.8658, 0.9884, 0.9839, 0.9857, 0.9828, 1.0881, 0.9090, 1.0810,
        0.9712, 1.0170, 1.0163, 0.9883, 1.0022, 1.0024, 0.9977, 0.9917, 1.0325,
        1.0305, 1.0517, 1.0104, 0.9842, 0.9975, 1.0412, 0.9375, 0.9873, 1.0055,
        0.9941, 0.9488, 0.9467, 0.8109, 1.0154, 0.9721, 0.9772, 0.9863, 0.9817,
        1.0052, 1.0080, 0.9909, 1.0750, 0.8429, 1.0286, 0.9574, 1.0411, 0.8248,
        0.9713, 0.9883, 0.9804, 0.9432, 0.9553, 1.0323, 0.9588, 1.0349, 0.8678,
        0.9917, 1.0570, 0.9929, 1.0251, 1.2571, 1.0265, 1.0060, 1.0049, 1.0203,
        0.9658, 1.0027, 0.9882, 1.0119, 0.9411, 1.0007, 1.0036, 1.0692, 1.1040,
        1.0242, 1.0014, 1.0006, 1.0114, 0.9353, 0.9999, 1.0052, 1.0288, 1.0354,
        0.9250, 1.0045, 0.9983, 1.0040, 0.9711, 1.0040, 0.9991, 0.9634, 0.9106,
        0.9789, 1.1282, 1.7053, 0.8516, 1.0291, 0.9452, 0.9875, 1.0086, 1.0169,
        1.0129, 1.0208, 0.9425, 0.9780, 0.9664, 0.9802, 1.0124, 1.0950, 1.0905,
        0.9231, 0.9975, 1.0446, 1.0166, 0.9972, 0.9984, 0.9949, 1.0245, 1.0020,
        0.9910, 1.1626, 1.0520, 0.8802, 0.9962, 1.0012, 0.9732, 0.9993, 0.9411,
        0.9489, 0.8256, 1.0631, 0.9954, 1.0441, 1.0346, 0.9925, 0.9986, 1.0039,
        0.9205, 0.9958, 1.0400, 1.0072, 1.0570, 0.8470, 0.9810, 1.0043, 0.9701,
        1.0003, 1.0236, 1.2689, 0.9818, 1.0194, 1.0121, 0.9700, 1.0020, 0.9979,
        0.9761, 1.0040, 0.8858, 0.9868, 1.0624, 1.1361, 1.0513, 0.9789, 0.9970,
        1.0150, 1.0887, 0.9593, 0.9914, 1.0217, 0.7821, 1.0280, 1.0205, 1.0002,
        1.0497, 0.7899, 1.0239, 1.0385, 1.0017, 0.9574, 0.9907, 0.9214],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809301
t4: 1641198809301
surr1, surr2: tensor([-3.1469e+00, -9.8593e-01, -1.3857e+00, -2.4029e+00, -2.6447e+00,
        -2.9339e+00, -2.7557e+00, -2.6381e+00, -2.7994e+00, -2.7517e+00,
        -2.4338e+00, -2.6519e+00, -2.5393e+00, -2.4887e+00, -2.4077e+00,
        -2.4107e+00, -2.1330e+00, -2.2374e+00, -2.3388e+00, -2.3534e+00,
        -2.2691e+00, -2.1679e+00, -8.4855e-01, -1.6891e-01, -1.6562e+00,
        -1.7948e+00, -1.5329e+00, -1.6853e+00, -1.5954e+00, -1.3303e+00,
        -1.8855e+00, -1.9170e+00, -1.5852e+00, -1.9428e+00, -1.6763e+00,
        -1.5785e+00, -1.5880e+00, -1.6788e+00, -1.5591e+00, -1.5597e+00,
        -1.5125e+00, -1.4390e+00, -1.4059e+00, -1.5527e+00, -1.7099e+00,
        -1.4742e+00, -1.4158e+00, -1.5467e+00, -1.6777e+00, -1.7134e+00,
        -1.3786e+00, -1.5400e+00, -1.4913e+00, -1.8718e+00, -1.2712e+00,
        -1.2506e+00, -1.0967e+00, -1.5817e+00, -1.3753e+00, -1.3794e+00,
        -1.2436e+00, -1.5243e+00, -1.0791e+00, -1.3027e+00, -1.5156e+00,
        -1.2084e+00,  1.2572e+00,  1.3611e+00,  2.0767e+00,  2.4569e+00,
         5.1317e+00,  1.0129e-02,  2.9335e-01,  1.8570e-01, -5.5373e-02,
         1.7195e-01,  2.2325e-01,  2.7834e-01,  3.1823e-01,  3.1431e-01,
         2.7333e-01,  4.1542e-01,  3.4757e-01,  2.9205e-01,  4.5420e-01,
         1.2626e-01, -3.2258e-02,  1.9007e-01,  5.1027e-01,  2.7846e-01,
         1.5781e-01,  1.3577e-01,  2.3980e-01,  2.7950e-01,  3.8741e-01,
         8.9237e-02,  2.2301e-01,  2.9821e-01,  2.7969e-01,  2.1171e-01,
         2.8189e-01,  4.0224e-01,  2.3002e-01,  3.9364e-01, -1.2358e-01,
         9.0546e-02, -1.1383e-01,  4.3212e-02,  1.3052e-01,  1.8894e-01,
         2.5876e-01,  3.2125e-01,  1.8953e-01,  1.9149e+00,  2.9562e+00,
         4.3865e+00,  6.7106e-01,  9.6865e-01,  9.2032e-01,  9.8515e-01,
         9.8367e-01,  9.4560e-01,  8.8437e-01,  8.9655e-01,  8.2009e-01,
         8.1756e-01,  7.3263e-01,  7.9579e-01,  9.3769e-01,  4.8587e-01,
         6.0628e-01,  8.0134e-01,  4.7274e-01,  7.6264e-01,  2.9286e-01,
         8.5123e-01,  7.0099e-01,  8.9278e-01,  9.0696e-01,  8.6171e-01,
         1.0228e+00,  1.0754e+00,  5.9169e-01,  9.4008e-01,  1.0216e+00,
         1.0716e+00,  6.0281e-01,  7.6230e-01,  9.3533e-01,  1.1018e+00,
         5.4281e-01,  9.5720e-01,  7.2046e-01,  9.2229e-01,  8.5419e-01,
         4.8139e-01,  8.2838e-01,  8.3731e-01,  8.3775e-01,  9.4941e-01,
         8.0984e-01,  5.3492e-01,  9.5298e-01,  9.0242e-01,  4.9585e-01,
         9.2276e-01,  6.7597e-01,  6.6094e-01,  8.0660e-01,  9.3319e-01,
         9.6252e-01,  6.7399e-01,  6.8562e-01,  8.6940e-01,  8.5586e-01,
         1.0293e+00,  9.4759e-01,  9.4253e-01,  8.7655e-01,  1.0787e+00,
         8.4788e-01,  8.6621e-01,  4.2416e-01,  5.5060e-01,  8.2260e-01,
         9.4902e-01,  9.9779e-01,  8.5359e-01,  7.0436e-01,  5.8318e-01,
         7.8254e-01,  6.6252e-01,  3.3437e-01,  8.0959e-01,  4.9409e-01,
         6.7122e-01,  8.1354e-01,  6.3374e-01,  3.7934e-01,  7.2472e-01,
         3.2433e-01,  8.3652e-01,  6.7031e-01,  8.8334e-01,  7.6573e-01,
         7.4038e-01,  7.3038e-01,  3.4412e-01,  5.9130e-01,  7.0647e-01,
         5.6351e-01,  6.1246e-01,  6.9289e-01,  5.9272e-01,  5.4472e-01,
         6.1449e-01,  7.0140e-01,  7.5908e-01,  4.3275e-01,  5.9781e-01,
         7.2318e-01,  6.1591e-01,  6.8942e-01,  6.1029e-01,  8.3033e-01,
         5.8117e-01,  4.3379e-01,  6.1495e-01,  6.1819e-01,  7.1701e-01,
         6.4079e-01,  6.5562e-01,  1.8836e-01,  7.9242e-01,  3.0096e-01,
         7.2818e-01,  6.2211e-01,  6.7050e-01,  7.3687e-01,  8.1504e-01,
         7.2357e-01,  6.4201e-01,  7.7612e-01,  2.8549e-01,  6.7316e-01,
         4.0764e-01,  6.5607e-01,  5.6700e-01,  4.7410e-01,  1.5935e-01,
         5.0632e-01,  5.4616e-01,  5.8967e-01,  4.4113e-01,  1.7832e-01,
         9.6830e-02,  2.0115e-01,  1.1102e-03,  4.3080e-01,  4.1156e-01,
         4.4196e-01,  3.6826e-01,  2.7054e-01,  3.2361e-01,  3.0509e-01,
        -7.5721e-02,  3.4949e-01,  3.2465e-01,  4.9598e-01,  4.8267e-02,
         1.1232e-01,  1.6573e-01,  3.9904e-01,  3.8523e-01,  1.6917e-01,
        -2.7728e-02,  2.2385e-01,  4.4292e-01,  2.4947e-01,  1.3626e-01,
         2.0439e-01,  1.6047e-01,  1.8899e-01,  2.4381e-01, -1.2884e-02,
         3.7297e-01,  3.8305e-01,  3.0309e-01,  1.4986e-01,  3.1234e-01,
         2.7888e-01,  2.7501e-01,  1.7966e-01,  3.0103e-01,  2.4076e-01,
         1.7713e-01,  1.2810e-02,  5.8620e-03,  2.4309e-01,  1.7535e-01,
         1.9514e-01, -2.9986e-01,  2.5948e-01,  4.9961e-01,  3.1890e-01,
         4.0660e-01,  2.8786e-01,  2.8716e-01,  4.4456e-01,  2.7418e-01,
         3.5268e-01,  1.4263e-01,  2.8307e-01,  3.3667e-01,  3.5233e-01,
        -4.4423e-03,  9.4578e-02, -1.5585e-01, -1.0353e-01,  3.3907e-01,
        -1.4761e-01,  3.3619e-01, -1.2867e-01,  2.7065e-01,  2.8259e-01,
         2.4816e-01,  2.2412e-01,  8.5925e-02,  7.7908e-02,  2.3441e-01,
         1.8416e-01,  3.5349e-01, -8.3681e-02,  5.4842e-02,  1.7334e-01,
         1.3779e-01, -1.1735e-01,  2.7836e-01,  2.0697e-01,  2.9868e-01,
         5.0318e-02,  3.9531e-02,  1.3039e-01,  5.9313e-02, -3.8901e-01,
         6.0663e-02, -4.4968e-02,  7.5686e-02, -7.1537e-02,  8.3418e-02,
         1.6036e-02, -2.5031e-01, -3.9740e-01, -1.7413e-01, -4.5794e-01,
        -2.6014e-01, -3.5955e-01, -1.8776e-01,  5.6771e-02, -9.4755e-02,
         2.6847e-02, -3.5283e-01, -5.2007e-01, -1.6823e-01,  1.0029e-01,
        -2.9822e-01, -4.3571e-01, -4.2813e-01, -2.8837e-01, -2.1076e-01,
        -1.6325e-01, -4.4351e-01, -6.3449e-01, -1.8430e-01, -2.3490e-01,
        -2.7021e-01, -1.9479e-01, -2.4164e-01, -2.5226e-01, -3.8376e-01,
        -3.2789e-01, -5.6818e-01, -9.0031e-01, -6.1303e-01, -7.0591e-01,
        -5.0515e-01, -4.2619e-01, -3.9062e-01, -1.0784e+00, -8.4726e-01,
        -5.2786e-01, -6.3774e-01, -8.0384e-01, -7.2563e-01, -8.2902e-01,
        -7.0572e-01, -7.0491e-01, -8.3038e-01, -9.8098e-01, -9.7000e-01,
        -9.1992e-01, -7.2179e-01, -8.3706e-01, -7.9031e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1469e+00, -9.8593e-01, -1.3857e+00, -2.4029e+00, -2.6447e+00,
        -2.9339e+00, -2.7557e+00, -2.6381e+00, -2.7994e+00, -2.7517e+00,
        -2.4338e+00, -2.6519e+00, -2.5393e+00, -2.4887e+00, -2.4077e+00,
        -2.4107e+00, -2.1565e+00, -2.3300e+00, -2.3388e+00, -2.3534e+00,
        -2.2691e+00, -2.1679e+00, -8.4855e-01, -1.6891e-01, -1.6562e+00,
        -1.7948e+00, -1.5329e+00, -1.6853e+00, -1.5954e+00, -1.3938e+00,
        -1.8855e+00, -1.9170e+00, -1.5852e+00, -1.9428e+00, -1.6763e+00,
        -1.5785e+00, -1.5880e+00, -1.6788e+00, -1.5591e+00, -1.5597e+00,
        -1.5125e+00, -1.4390e+00, -1.4059e+00, -1.5527e+00, -1.7099e+00,
        -1.4742e+00, -1.4158e+00, -1.5467e+00, -1.6777e+00, -1.7134e+00,
        -1.3786e+00, -1.5400e+00, -1.4913e+00, -1.8718e+00, -1.2712e+00,
        -1.2506e+00, -1.0967e+00, -1.5817e+00, -1.4118e+00, -1.3794e+00,
        -1.2436e+00, -1.5243e+00, -1.0791e+00, -1.3027e+00, -1.5156e+00,
        -1.2084e+00,  1.2572e+00,  1.3611e+00,  2.0767e+00,  2.4569e+00,
         5.1175e+00,  1.0129e-02,  2.9335e-01,  1.8570e-01, -5.7464e-02,
         1.7195e-01,  2.2325e-01,  2.7834e-01,  3.1823e-01,  3.1431e-01,
         2.7333e-01,  4.1542e-01,  3.4757e-01,  2.9205e-01,  4.5420e-01,
         1.2626e-01, -3.2258e-02,  1.9303e-01,  5.1027e-01,  2.7846e-01,
         1.5781e-01,  1.3577e-01,  2.3980e-01,  2.7950e-01,  3.8741e-01,
         9.1655e-02,  2.2301e-01,  2.9821e-01,  2.7969e-01,  2.1171e-01,
         2.8189e-01,  4.0224e-01,  2.3002e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1383e-01,  4.3212e-02,  1.3052e-01,  1.8894e-01,
         2.5876e-01,  3.2125e-01,  1.8953e-01,  1.9055e+00,  2.9271e+00,
         4.0062e+00,  6.7106e-01,  9.6865e-01,  9.2032e-01,  9.8515e-01,
         9.8367e-01,  9.4560e-01,  8.8437e-01,  8.9655e-01,  8.2009e-01,
         8.1756e-01,  7.3263e-01,  7.9579e-01,  9.0579e-01,  5.6602e-01,
         6.0628e-01,  8.0134e-01,  4.7274e-01,  7.9688e-01,  2.9286e-01,
         8.5123e-01,  7.0099e-01,  8.9278e-01,  9.0696e-01,  8.6171e-01,
         1.0228e+00,  1.0376e+00,  5.9169e-01,  9.4008e-01,  1.0216e+00,
         1.0716e+00,  6.0281e-01,  7.6230e-01,  9.3533e-01,  1.1018e+00,
         5.4281e-01,  9.5720e-01,  7.2046e-01,  9.2229e-01,  8.7946e-01,
         4.8139e-01,  8.2838e-01,  8.3731e-01,  8.3775e-01,  9.4941e-01,
         7.9559e-01,  5.3492e-01,  9.5298e-01,  9.0242e-01,  4.9585e-01,
         9.2276e-01,  6.7597e-01,  6.6094e-01,  8.0660e-01,  9.3319e-01,
         9.6252e-01,  6.7399e-01,  6.8562e-01,  8.6940e-01,  8.5586e-01,
         1.0293e+00,  9.4759e-01,  9.4253e-01,  8.7655e-01,  1.0787e+00,
         8.4788e-01,  8.6621e-01,  4.1136e-01,  5.5060e-01,  8.2260e-01,
         9.4902e-01,  9.9779e-01,  8.5359e-01,  7.0436e-01,  5.8318e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.0959e-01,  4.9409e-01,
         6.7122e-01,  8.1354e-01,  6.3374e-01,  3.7934e-01,  7.2472e-01,
         3.2433e-01,  8.3652e-01,  6.7031e-01,  8.8334e-01,  7.6573e-01,
         7.4038e-01,  7.3038e-01,  3.4412e-01,  5.9130e-01,  7.0647e-01,
         5.6351e-01,  6.1246e-01,  6.9289e-01,  5.9272e-01,  5.4472e-01,
         6.1449e-01,  7.0140e-01,  7.5908e-01,  4.3275e-01,  5.9781e-01,
         7.2318e-01,  6.1591e-01,  6.8942e-01,  6.1029e-01,  8.3033e-01,
         5.8117e-01,  4.5092e-01,  6.1495e-01,  6.1819e-01,  7.1701e-01,
         6.4079e-01,  6.5562e-01,  1.8836e-01,  7.9242e-01,  3.0096e-01,
         7.2818e-01,  6.2211e-01,  6.7050e-01,  7.3687e-01,  8.1504e-01,
         7.2357e-01,  6.4201e-01,  7.7612e-01,  2.8549e-01,  6.7316e-01,
         4.0764e-01,  6.5607e-01,  5.6700e-01,  4.7410e-01,  1.5935e-01,
         5.0632e-01,  5.4616e-01,  5.8967e-01,  4.4113e-01,  1.7832e-01,
         1.0747e-01,  2.0115e-01,  1.1102e-03,  4.3080e-01,  4.1156e-01,
         4.4196e-01,  3.6826e-01,  2.7054e-01,  3.2361e-01,  3.0509e-01,
        -8.0855e-02,  3.4949e-01,  3.2465e-01,  4.9598e-01,  5.2668e-02,
         1.1232e-01,  1.6573e-01,  3.9904e-01,  3.8523e-01,  1.6917e-01,
        -2.7728e-02,  2.2385e-01,  4.4292e-01,  2.5872e-01,  1.3626e-01,
         2.0439e-01,  1.6047e-01,  1.8899e-01,  2.1334e-01, -1.2884e-02,
         3.7297e-01,  3.8305e-01,  3.0309e-01,  1.4986e-01,  3.1234e-01,
         2.7888e-01,  2.7501e-01,  1.7966e-01,  3.0103e-01,  2.4076e-01,
         1.7713e-01,  1.2764e-02,  5.8620e-03,  2.4309e-01,  1.7535e-01,
         1.9514e-01, -2.9986e-01,  2.5948e-01,  4.9961e-01,  3.1890e-01,
         4.0660e-01,  2.8786e-01,  2.8716e-01,  4.4456e-01,  2.7418e-01,
         3.5268e-01,  1.4263e-01,  2.8307e-01,  3.3667e-01,  3.5233e-01,
        -4.4423e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.3907e-01,
        -1.4761e-01,  3.3619e-01, -1.2867e-01,  2.7065e-01,  2.8259e-01,
         2.4816e-01,  2.2412e-01,  8.5925e-02,  7.7908e-02,  2.3441e-01,
         1.8416e-01,  3.5349e-01, -8.3681e-02,  5.4842e-02,  1.7334e-01,
         1.3779e-01, -1.1735e-01,  2.7836e-01,  2.0697e-01,  2.9868e-01,
         5.0318e-02,  3.9531e-02,  1.3039e-01,  5.6121e-02, -3.8901e-01,
         6.2027e-02, -4.4968e-02,  7.5686e-02, -7.1537e-02,  8.3418e-02,
         1.6036e-02, -2.5031e-01, -4.3320e-01, -1.7413e-01, -4.5794e-01,
        -2.6014e-01, -3.5955e-01, -1.8776e-01,  5.6771e-02, -9.4755e-02,
         2.6847e-02, -3.5283e-01, -5.2007e-01, -1.6823e-01,  1.0029e-01,
        -3.1689e-01, -4.3571e-01, -4.2813e-01, -2.8837e-01, -2.1076e-01,
        -1.6325e-01, -3.8447e-01, -6.3449e-01, -1.8430e-01, -2.3490e-01,
        -2.7021e-01, -1.9479e-01, -2.4164e-01, -2.5226e-01, -3.8376e-01,
        -3.3317e-01, -5.6818e-01, -9.0031e-01, -5.9353e-01, -7.0591e-01,
        -5.0515e-01, -4.2619e-01, -3.9062e-01, -1.0784e+00, -8.4726e-01,
        -5.2786e-01, -6.3774e-01, -9.2498e-01, -7.2563e-01, -8.2902e-01,
        -7.0572e-01, -7.0491e-01, -9.4607e-01, -9.8098e-01, -9.7000e-01,
        -9.1992e-01, -7.2179e-01, -8.3706e-01, -7.9031e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809310
t6: 1641198809311
state_values: tensor([1.2206, 1.1270, 1.2196, 1.3673, 1.5173, 1.6248, 1.7005, 1.7518, 1.7883,
        1.8453, 1.9286, 1.9104, 1.9311, 1.9456, 2.0009, 2.1496, 2.1147, 2.0791,
        2.1323, 2.1577, 2.1917, 2.1962, 2.0500, 1.9892, 2.1298, 2.1570, 2.1756,
        2.2558, 2.2255, 2.2311, 2.4177, 2.3764, 2.3874, 2.3284, 2.3190, 2.3233,
        2.3350, 2.3399, 2.3845, 2.3748, 2.4068, 2.4091, 2.4160, 2.5086, 2.4524,
        2.4553, 2.4554, 2.5221, 2.4865, 2.4793, 2.4972, 2.5504, 2.5224, 2.6413,
        2.5854, 2.5679, 2.6023, 2.7312, 2.6311, 2.7028, 2.6401, 2.6258, 2.6227,
        2.6587, 2.6437, 2.6600, 2.4900, 2.4410, 2.4027, 2.4284, 2.3681, 2.5441,
        2.6316, 2.6458, 2.6581, 2.7510, 2.7372, 2.7469, 2.7418, 2.7621, 2.7448,
        2.7452, 2.7667, 2.7632, 2.8004, 2.9064, 2.8217, 2.8110, 2.8544, 2.8894,
        2.8472, 2.9186, 2.8803, 2.8629, 2.8533, 2.8590, 2.9443, 2.9208, 2.9090,
        2.9588, 2.9435, 2.9114, 2.9107, 2.9168, 2.9129, 2.9214, 2.9268, 2.9420,
        2.9479, 2.9529, 2.9983, 3.0303, 3.0325, 2.8694, 2.7674, 2.7025, 2.8628,
        2.9789, 2.9692, 2.9980, 2.9951, 2.9920, 3.0039, 3.0050, 3.0855, 3.0446,
        3.1275, 3.0731, 3.0520, 3.0508, 3.2019, 3.1408, 3.0859, 3.1207, 3.3125,
        3.1953, 3.2445, 3.1750, 3.1762, 3.1979, 3.1519, 3.1373, 3.1269, 3.1469,
        3.1636, 3.1656, 3.1407, 3.1466, 3.2185, 3.1734, 3.3358, 3.2317, 3.1978,
        3.1829, 3.1813, 3.3595, 3.2972, 3.2967, 3.2353, 3.2189, 3.2164, 3.2199,
        3.2127, 3.2324, 3.2109, 3.2350, 3.2355, 3.3029, 3.2513, 3.2358, 3.2434,
        3.2455, 3.2498, 3.2807, 3.2589, 3.2474, 3.2499, 3.2807, 3.2774, 3.2713,
        3.2596, 3.3148, 3.2778, 3.2790, 3.2758, 3.2849, 3.2723, 3.3390, 3.4092,
        3.4237, 3.3459, 3.3211, 3.3024, 3.3735, 3.3350, 3.4303, 3.4128, 3.4332,
        3.3737, 3.4064, 3.3706, 3.3570, 3.3575, 3.3687, 3.4052, 3.4363, 3.4196,
        3.3890, 3.3754, 3.4198, 3.4864, 3.4084, 3.4023, 3.4540, 3.4825, 3.4215,
        3.4051, 3.3951, 3.3963, 3.4619, 3.4155, 3.4880, 3.4706, 3.4444, 3.4168,
        3.4113, 3.4136, 3.4996, 3.4395, 3.4559, 3.4617, 3.4756, 3.4369, 3.4617,
        3.4346, 3.4508, 3.4370, 3.4304, 3.4495, 3.4277, 3.4300, 3.4458, 3.4515,
        3.4461, 3.4387, 3.4465, 3.4417, 3.4907, 3.4703, 3.4635, 3.5033, 3.5203,
        3.5083, 3.5243, 3.6205, 3.5204, 3.6490, 3.5366, 3.5398, 3.5523, 3.5581,
        3.5827, 3.6338, 3.5823, 3.5901, 3.5435, 3.5864, 3.5401, 3.5707, 3.5314,
        3.6574, 3.5519, 3.5535, 3.5634, 3.6655, 3.5714, 3.5551, 3.5661, 3.5422,
        3.6902, 3.6455, 3.6787, 3.6064, 3.5693, 3.5477, 3.5498, 3.5509, 3.5540,
        3.5533, 3.5933, 3.5678, 3.5979, 3.5714, 3.6224, 3.6235, 3.5839, 3.5777,
        3.5699, 3.5656, 3.5766, 3.5772, 3.6462, 3.6711, 3.6117, 3.5958, 3.5893,
        3.5918, 3.6800, 3.6336, 3.7073, 3.6523, 3.7378, 3.6721, 3.6404, 3.6575,
        3.8035, 3.6719, 3.6297, 3.6182, 3.6766, 3.6360, 3.6798, 3.6362, 3.6315,
        3.6246, 3.6252, 3.6199, 3.7418, 3.6693, 3.7434, 3.6694, 3.6384, 3.6469,
        3.6425, 3.7056, 3.6610, 3.6430, 3.6371, 3.6537, 3.6557, 3.7356, 3.7468,
        3.6948, 3.6942, 3.6666, 3.6611, 3.7732, 3.6993, 3.6797, 3.6897, 3.6844,
        3.7984, 3.7068, 3.7992, 3.9172, 3.7656, 3.7290, 3.7163, 3.6997, 3.7090,
        3.6880, 3.8505, 3.7402, 3.7068, 3.6831, 3.6946, 3.8650, 3.8755, 3.7901,
        3.8048, 3.7554, 3.7284, 3.7145, 3.7186, 3.7134, 3.7124, 3.7623, 3.7506,
        3.7506, 3.7855, 3.7435, 3.8894, 3.7728, 3.7498, 3.7370, 3.7386, 3.7652,
        3.7783, 3.9752, 3.8166, 3.8170, 3.8256, 3.7755, 3.8716, 3.9148, 3.8578,
        3.8087, 3.7898, 3.9051, 3.9063, 3.8856, 3.8071, 3.8449, 3.8273],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809315
t8: 1641198809315
t9: 1641198809316
t10: 1641198809326
t11: 1641198809328
t12: 1641198809328
t1: 1641198809328
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809338
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9956, 1.0074, 0.9294, 0.9646, 0.9474, 0.9972, 0.9874, 0.9400, 0.9932,
        1.0411, 0.9261, 0.9996, 1.0156, 0.9660, 0.9884, 0.9188, 0.8873, 0.8645,
        0.9627, 1.0150, 1.0229, 0.9921, 1.0048, 0.9545, 1.0131, 0.9951, 0.9245,
        0.9975, 0.9977, 0.8592, 0.9108, 1.0881, 0.9132, 1.0211, 1.0361, 0.9899,
        1.0633, 0.9298, 1.0148, 0.9854, 0.9951, 0.9989, 0.9397, 0.9615, 0.9806,
        1.0216, 0.9254, 0.9970, 1.0871, 0.9720, 0.9617, 0.9963, 0.9721, 0.9943,
        1.0181, 0.9450, 0.9490, 1.0692, 0.8780, 0.9849, 0.9947, 0.9875, 0.9712,
        1.0328, 0.9705, 1.0235, 1.0913, 1.0619, 0.9716, 1.0170, 1.1157, 0.9712,
        1.0180, 1.0611, 0.8721, 1.0048, 1.0113, 0.9949, 0.9835, 1.0169, 0.9967,
        0.9901, 1.0139, 0.9592, 0.9689, 0.9717, 1.0407, 0.8847, 1.0045, 0.9770,
        0.9061, 1.0000, 1.0047, 1.0252, 1.0786, 0.8799, 1.0020, 0.9999, 0.9542,
        0.9952, 1.0063, 1.0436, 0.9978, 1.1193, 1.1072, 1.1157, 1.0514, 1.0146,
        1.0003, 1.0038, 1.0288, 1.0223, 0.9982, 1.1034, 1.1098, 1.1917, 0.9278,
        0.9885, 1.0066, 1.0000, 1.0037, 1.0041, 1.0053, 0.9567, 0.9940, 0.9595,
        0.9848, 1.0353, 1.1365, 0.7766, 0.9946, 0.9317, 0.9082, 0.8596, 0.9734,
        1.0480, 1.0010, 1.0066, 0.9799, 1.0002, 1.0148, 1.1417, 0.9677, 0.9806,
        0.9979, 1.0277, 1.0746, 0.9166, 0.9984, 1.0265, 0.9991, 0.9876, 1.0575,
        1.0140, 0.8782, 0.9740, 1.0527, 0.9830, 1.0117, 1.0652, 1.1185, 1.0010,
        0.9941, 1.0117, 0.9714, 1.0128, 0.9781, 0.9953, 1.0064, 0.9965, 1.0246,
        1.0340, 0.9686, 1.0018, 0.9997, 1.0008, 0.9850, 0.9979, 0.9910, 1.0165,
        0.9501, 1.0753, 1.1323, 1.0382, 0.9959, 0.9987, 1.0031, 1.0358, 1.0292,
        0.9832, 1.1043, 1.5418, 0.8206, 1.0060, 0.9540, 1.0087, 1.0498, 0.9771,
        0.9112, 1.0879, 0.9955, 1.0287, 0.9838, 0.9880, 1.0013, 1.0100, 1.0217,
        1.0666, 0.9213, 0.9702, 0.9802, 0.9905, 0.9433, 0.9830, 0.9911, 1.0094,
        1.0352, 1.0776, 0.9104, 1.0008, 0.9808, 1.0126, 1.0002, 1.0005, 1.0618,
        1.0844, 0.8690, 0.9878, 0.9873, 0.9860, 0.9833, 1.0872, 0.9096, 1.0821,
        0.9728, 1.0149, 1.0131, 0.9901, 1.0015, 1.0019, 0.9981, 0.9926, 1.0286,
        1.0296, 1.0492, 1.0096, 0.9879, 0.9956, 1.0334, 0.9399, 0.9887, 1.0068,
        0.9965, 0.9531, 0.9468, 0.8125, 1.0251, 0.9721, 0.9762, 0.9868, 0.9825,
        1.0030, 1.0078, 0.9925, 1.0780, 0.8399, 1.0336, 0.9613, 1.0416, 0.8301,
        0.9767, 0.9890, 0.9805, 0.9457, 0.9587, 1.0320, 0.9586, 1.0336, 0.8768,
        0.9889, 1.0617, 0.9930, 1.0230, 1.2580, 1.0265, 1.0058, 1.0029, 1.0144,
        0.9698, 1.0018, 0.9909, 1.0100, 0.9452, 1.0002, 1.0026, 1.0665, 1.1020,
        1.0236, 1.0013, 1.0004, 1.0078, 0.9400, 0.9999, 1.0019, 1.0272, 1.0338,
        0.9300, 1.0047, 1.0063, 1.0040, 0.9746, 1.0039, 0.9992, 0.9631, 0.9062,
        0.9804, 1.1226, 1.7254, 0.8511, 1.0260, 0.9520, 0.9814, 1.0080, 1.0140,
        1.0090, 1.0157, 0.9530, 0.9780, 0.9770, 0.9811, 1.0111, 1.0943, 1.0888,
        0.9257, 0.9971, 1.0357, 1.0162, 0.9976, 0.9996, 1.0010, 1.0264, 1.0021,
        0.9914, 1.1649, 1.0519, 0.8820, 0.9962, 0.9962, 0.9748, 0.9994, 0.9454,
        0.9516, 0.8290, 1.0544, 0.9984, 1.0403, 1.0348, 0.9925, 0.9987, 1.0036,
        0.9268, 1.0008, 1.0396, 1.0072, 1.0549, 0.8550, 0.9724, 1.0043, 0.9726,
        1.0001, 1.0229, 1.2706, 0.9817, 1.0190, 1.0088, 0.9761, 1.0025, 0.9994,
        0.9782, 1.0040, 0.8878, 0.9909, 1.0629, 1.1357, 1.0495, 0.9812, 1.0001,
        1.0279, 1.0969, 0.9604, 0.9898, 1.0222, 0.7836, 1.0210, 1.0207, 1.0004,
        1.0494, 0.7887, 1.0193, 1.0389, 1.0018, 0.9578, 0.9897, 0.9200],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809343
t4: 1641198809343
surr1, surr2: tensor([-3.1475e+00, -9.8818e-01, -1.3735e+00, -2.3876e+00, -2.6018e+00,
        -2.9361e+00, -2.7479e+00, -2.6003e+00, -2.8055e+00, -2.7730e+00,
        -2.4258e+00, -2.6560e+00, -2.5279e+00, -2.4977e+00, -2.4238e+00,
        -2.3932e+00, -2.1261e+00, -2.2380e+00, -2.3215e+00, -2.3482e+00,
        -2.2681e+00, -2.1679e+00, -8.4854e-01, -1.6857e-01, -1.6567e+00,
        -1.7955e+00, -1.5361e+00, -1.6857e+00, -1.5963e+00, -1.3306e+00,
        -1.8740e+00, -1.9181e+00, -1.5851e+00, -1.9429e+00, -1.6787e+00,
        -1.5787e+00, -1.5823e+00, -1.6824e+00, -1.5581e+00, -1.5641e+00,
        -1.5137e+00, -1.4394e+00, -1.4131e+00, -1.5562e+00, -1.7109e+00,
        -1.4750e+00, -1.4203e+00, -1.5497e+00, -1.6694e+00, -1.7141e+00,
        -1.3802e+00, -1.5377e+00, -1.4972e+00, -1.8726e+00, -1.2685e+00,
        -1.2545e+00, -1.0967e+00, -1.5956e+00, -1.3773e+00, -1.3803e+00,
        -1.2379e+00, -1.5240e+00, -1.0790e+00, -1.2971e+00, -1.5178e+00,
        -1.2066e+00,  1.2498e+00,  1.3567e+00,  2.0829e+00,  2.4860e+00,
         5.1907e+00,  1.0172e-02,  2.9290e-01,  1.8429e-01, -5.5682e-02,
         1.7200e-01,  2.2396e-01,  2.7848e-01,  3.1843e-01,  3.1428e-01,
         2.7339e-01,  4.1555e-01,  3.4729e-01,  2.9285e-01,  4.5452e-01,
         1.2673e-01, -3.2243e-02,  1.8974e-01,  5.0848e-01,  2.7847e-01,
         1.5826e-01,  1.3581e-01,  2.3975e-01,  2.7948e-01,  3.8677e-01,
         8.9610e-02,  2.2303e-01,  2.9829e-01,  2.8011e-01,  2.1156e-01,
         2.8185e-01,  4.0215e-01,  2.3005e-01,  3.9254e-01, -1.2335e-01,
         8.9940e-02, -1.1338e-01,  4.3022e-02,  1.2999e-01,  1.8992e-01,
         2.6048e-01,  3.2169e-01,  1.8894e-01,  1.9115e+00,  2.9533e+00,
         4.3403e+00,  6.7558e-01,  9.6677e-01,  9.2379e-01,  9.8528e-01,
         9.8236e-01,  9.4529e-01,  8.8406e-01,  9.0035e-01,  8.1987e-01,
         8.2321e-01,  7.3322e-01,  7.9443e-01,  9.3584e-01,  4.8841e-01,
         6.0520e-01,  7.9481e-01,  4.7346e-01,  7.6112e-01,  2.9215e-01,
         8.5638e-01,  7.0099e-01,  8.9412e-01,  9.0588e-01,  8.6176e-01,
         1.0227e+00,  1.0769e+00,  5.9171e-01,  9.4039e-01,  1.0227e+00,
         1.0616e+00,  6.0200e-01,  7.6569e-01,  9.3526e-01,  1.1190e+00,
         5.4270e-01,  9.5437e-01,  7.2019e-01,  9.2229e-01,  8.5812e-01,
         4.7956e-01,  8.3006e-01,  8.3797e-01,  8.3750e-01,  9.5048e-01,
         8.0899e-01,  5.3499e-01,  9.5356e-01,  8.9397e-01,  4.9671e-01,
         9.2122e-01,  6.8028e-01,  6.6018e-01,  8.0574e-01,  9.3356e-01,
         9.5995e-01,  6.7257e-01,  6.8717e-01,  8.6882e-01,  8.5598e-01,
         1.0292e+00,  9.4935e-01,  9.4282e-01,  8.7745e-01,  1.0777e+00,
         8.4999e-01,  8.6702e-01,  4.2342e-01,  5.4990e-01,  8.2335e-01,
         9.4810e-01,  1.0048e+00,  8.5702e-01,  7.0316e-01,  5.8277e-01,
         7.7988e-01,  6.6806e-01,  3.3391e-01,  8.1172e-01,  5.0153e-01,
         6.6959e-01,  8.1509e-01,  6.3383e-01,  3.7992e-01,  7.2900e-01,
         3.2439e-01,  8.3580e-01,  6.7167e-01,  8.8581e-01,  7.6802e-01,
         7.4021e-01,  7.2866e-01,  3.4397e-01,  5.9167e-01,  7.0661e-01,
         5.6349e-01,  6.1295e-01,  6.9250e-01,  5.9110e-01,  5.4507e-01,
         6.1438e-01,  7.0168e-01,  7.5783e-01,  4.3423e-01,  5.9785e-01,
         7.2789e-01,  6.1556e-01,  6.8944e-01,  6.1029e-01,  8.3093e-01,
         5.8043e-01,  4.3539e-01,  6.1460e-01,  6.2031e-01,  7.1719e-01,
         6.4113e-01,  6.5508e-01,  1.8848e-01,  7.9324e-01,  3.0145e-01,
         7.2668e-01,  6.2018e-01,  6.7171e-01,  7.3638e-01,  8.1462e-01,
         7.2381e-01,  6.4260e-01,  7.7320e-01,  2.8524e-01,  6.7155e-01,
         4.0728e-01,  6.5854e-01,  5.6592e-01,  4.7057e-01,  1.5975e-01,
         5.0701e-01,  5.4689e-01,  5.9110e-01,  4.4312e-01,  1.7834e-01,
         9.7023e-02,  2.0306e-01,  1.1103e-03,  4.3039e-01,  4.1176e-01,
         4.4235e-01,  3.6746e-01,  2.7049e-01,  3.2414e-01,  3.0596e-01,
        -7.5460e-02,  3.5121e-01,  3.2598e-01,  4.9623e-01,  4.8577e-02,
         1.1295e-01,  1.6585e-01,  3.9908e-01,  3.8625e-01,  1.6978e-01,
        -2.7719e-02,  2.2381e-01,  4.4236e-01,  2.5207e-01,  1.3586e-01,
         2.0529e-01,  1.6049e-01,  1.8861e-01,  2.4399e-01, -1.2885e-02,
         3.7293e-01,  3.8231e-01,  3.0134e-01,  1.5049e-01,  3.1205e-01,
         2.7964e-01,  2.7449e-01,  1.8045e-01,  3.0090e-01,  2.4051e-01,
         1.7669e-01,  1.2787e-02,  5.8585e-03,  2.4307e-01,  1.7532e-01,
         1.9444e-01, -3.0138e-01,  2.5948e-01,  4.9796e-01,  3.1841e-01,
         4.0596e-01,  2.8941e-01,  2.8722e-01,  4.4814e-01,  2.7416e-01,
         3.5395e-01,  1.4261e-01,  2.8312e-01,  3.3659e-01,  3.5065e-01,
        -4.4493e-03,  9.4103e-02, -1.5769e-01, -1.0346e-01,  3.3804e-01,
        -1.4868e-01,  3.3410e-01, -1.2859e-01,  2.6987e-01,  2.8148e-01,
         2.4693e-01,  2.2661e-01,  8.5923e-02,  7.8760e-02,  2.3463e-01,
         1.8394e-01,  3.5328e-01, -8.3550e-02,  5.4998e-02,  1.7328e-01,
         1.3662e-01, -1.1730e-01,  2.7846e-01,  2.0722e-01,  3.0053e-01,
         5.0410e-02,  3.9533e-02,  1.3043e-01,  5.9432e-02, -3.8900e-01,
         6.0785e-02, -4.4969e-02,  7.5304e-02, -7.1652e-02,  8.3423e-02,
         1.6109e-02, -2.5103e-01, -3.9902e-01, -1.7271e-01, -4.5931e-01,
        -2.5918e-01, -3.5961e-01, -1.8777e-01,  5.6777e-02, -9.4722e-02,
         2.7032e-02, -3.5458e-01, -5.1983e-01, -1.6823e-01,  1.0010e-01,
        -3.0106e-01, -4.3190e-01, -4.2816e-01, -2.8911e-01, -2.1072e-01,
        -1.6314e-01, -4.4411e-01, -6.3439e-01, -1.8423e-01, -2.3412e-01,
        -2.7189e-01, -1.9488e-01, -2.4200e-01, -2.5281e-01, -3.8377e-01,
        -3.2865e-01, -5.7055e-01, -9.0068e-01, -6.1281e-01, -7.0472e-01,
        -5.0634e-01, -4.2751e-01, -3.9559e-01, -1.0865e+00, -8.4829e-01,
        -5.2699e-01, -6.3805e-01, -8.0537e-01, -7.2069e-01, -8.2921e-01,
        -7.0589e-01, -7.0473e-01, -8.2906e-01, -9.7657e-01, -9.7031e-01,
        -9.2001e-01, -7.2209e-01, -8.3623e-01, -7.8918e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1475e+00, -9.8818e-01, -1.3735e+00, -2.3876e+00, -2.6018e+00,
        -2.9361e+00, -2.7479e+00, -2.6003e+00, -2.8055e+00, -2.7730e+00,
        -2.4258e+00, -2.6560e+00, -2.5279e+00, -2.4977e+00, -2.4238e+00,
        -2.3932e+00, -2.1565e+00, -2.3300e+00, -2.3215e+00, -2.3482e+00,
        -2.2681e+00, -2.1679e+00, -8.4854e-01, -1.6857e-01, -1.6567e+00,
        -1.7955e+00, -1.5361e+00, -1.6857e+00, -1.5963e+00, -1.3938e+00,
        -1.8740e+00, -1.9181e+00, -1.5851e+00, -1.9429e+00, -1.6787e+00,
        -1.5787e+00, -1.5823e+00, -1.6824e+00, -1.5581e+00, -1.5641e+00,
        -1.5137e+00, -1.4394e+00, -1.4131e+00, -1.5562e+00, -1.7109e+00,
        -1.4750e+00, -1.4203e+00, -1.5497e+00, -1.6694e+00, -1.7141e+00,
        -1.3802e+00, -1.5377e+00, -1.4972e+00, -1.8726e+00, -1.2685e+00,
        -1.2545e+00, -1.0967e+00, -1.5956e+00, -1.4118e+00, -1.3803e+00,
        -1.2379e+00, -1.5240e+00, -1.0790e+00, -1.2971e+00, -1.5178e+00,
        -1.2066e+00,  1.2498e+00,  1.3567e+00,  2.0829e+00,  2.4860e+00,
         5.1175e+00,  1.0172e-02,  2.9290e-01,  1.8429e-01, -5.7464e-02,
         1.7200e-01,  2.2396e-01,  2.7848e-01,  3.1843e-01,  3.1428e-01,
         2.7339e-01,  4.1555e-01,  3.4729e-01,  2.9285e-01,  4.5452e-01,
         1.2673e-01, -3.2243e-02,  1.9303e-01,  5.0848e-01,  2.7847e-01,
         1.5826e-01,  1.3581e-01,  2.3975e-01,  2.7948e-01,  3.8677e-01,
         9.1655e-02,  2.2303e-01,  2.9829e-01,  2.8011e-01,  2.1156e-01,
         2.8185e-01,  4.0215e-01,  2.3005e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1338e-01,  4.3022e-02,  1.2999e-01,  1.8992e-01,
         2.6048e-01,  3.2169e-01,  1.8894e-01,  1.9055e+00,  2.9271e+00,
         4.0062e+00,  6.7558e-01,  9.6677e-01,  9.2379e-01,  9.8528e-01,
         9.8236e-01,  9.4529e-01,  8.8406e-01,  9.0035e-01,  8.1987e-01,
         8.2321e-01,  7.3322e-01,  7.9443e-01,  9.0579e-01,  5.6602e-01,
         6.0520e-01,  7.9481e-01,  4.7346e-01,  7.9688e-01,  2.9215e-01,
         8.5638e-01,  7.0099e-01,  8.9412e-01,  9.0588e-01,  8.6176e-01,
         1.0227e+00,  1.0376e+00,  5.9171e-01,  9.4039e-01,  1.0227e+00,
         1.0616e+00,  6.0200e-01,  7.6569e-01,  9.3526e-01,  1.1190e+00,
         5.4270e-01,  9.5437e-01,  7.2019e-01,  9.2229e-01,  8.7946e-01,
         4.7956e-01,  8.3006e-01,  8.3797e-01,  8.3750e-01,  9.5048e-01,
         7.9559e-01,  5.3499e-01,  9.5356e-01,  8.9397e-01,  4.9671e-01,
         9.2122e-01,  6.8028e-01,  6.6018e-01,  8.0574e-01,  9.3356e-01,
         9.5995e-01,  6.7257e-01,  6.8717e-01,  8.6882e-01,  8.5598e-01,
         1.0292e+00,  9.4935e-01,  9.4282e-01,  8.7745e-01,  1.0777e+00,
         8.4999e-01,  8.6702e-01,  4.1136e-01,  5.4990e-01,  8.2335e-01,
         9.4810e-01,  1.0048e+00,  8.5702e-01,  7.0316e-01,  5.8277e-01,
         7.7685e-01,  4.7662e-01,  3.6623e-01,  8.1172e-01,  5.0153e-01,
         6.6959e-01,  8.1509e-01,  6.3383e-01,  3.7992e-01,  7.2900e-01,
         3.2439e-01,  8.3580e-01,  6.7167e-01,  8.8581e-01,  7.6802e-01,
         7.4021e-01,  7.2866e-01,  3.4397e-01,  5.9167e-01,  7.0661e-01,
         5.6349e-01,  6.1295e-01,  6.9250e-01,  5.9110e-01,  5.4507e-01,
         6.1438e-01,  7.0168e-01,  7.5783e-01,  4.3423e-01,  5.9785e-01,
         7.2789e-01,  6.1556e-01,  6.8944e-01,  6.1029e-01,  8.3093e-01,
         5.8043e-01,  4.5092e-01,  6.1460e-01,  6.2031e-01,  7.1719e-01,
         6.4113e-01,  6.5508e-01,  1.8848e-01,  7.9324e-01,  3.0145e-01,
         7.2668e-01,  6.2018e-01,  6.7171e-01,  7.3638e-01,  8.1462e-01,
         7.2381e-01,  6.4260e-01,  7.7320e-01,  2.8524e-01,  6.7155e-01,
         4.0728e-01,  6.5854e-01,  5.6592e-01,  4.7057e-01,  1.5975e-01,
         5.0701e-01,  5.4689e-01,  5.9110e-01,  4.4312e-01,  1.7834e-01,
         1.0747e-01,  2.0306e-01,  1.1103e-03,  4.3039e-01,  4.1176e-01,
         4.4235e-01,  3.6746e-01,  2.7049e-01,  3.2414e-01,  3.0596e-01,
        -8.0855e-02,  3.5121e-01,  3.2598e-01,  4.9623e-01,  5.2668e-02,
         1.1295e-01,  1.6585e-01,  3.9908e-01,  3.8625e-01,  1.6978e-01,
        -2.7719e-02,  2.2381e-01,  4.4236e-01,  2.5872e-01,  1.3586e-01,
         2.0529e-01,  1.6049e-01,  1.8861e-01,  2.1334e-01, -1.2885e-02,
         3.7293e-01,  3.8231e-01,  3.0134e-01,  1.5049e-01,  3.1205e-01,
         2.7964e-01,  2.7449e-01,  1.8045e-01,  3.0090e-01,  2.4051e-01,
         1.7669e-01,  1.2764e-02,  5.8585e-03,  2.4307e-01,  1.7532e-01,
         1.9444e-01, -3.0138e-01,  2.5948e-01,  4.9796e-01,  3.1841e-01,
         4.0596e-01,  2.8941e-01,  2.8722e-01,  4.4814e-01,  2.7416e-01,
         3.5395e-01,  1.4261e-01,  2.8312e-01,  3.3659e-01,  3.5065e-01,
        -4.4493e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.3804e-01,
        -1.4868e-01,  3.3410e-01, -1.2859e-01,  2.6987e-01,  2.8148e-01,
         2.4693e-01,  2.2661e-01,  8.5923e-02,  7.8760e-02,  2.3463e-01,
         1.8394e-01,  3.5328e-01, -8.3550e-02,  5.4998e-02,  1.7328e-01,
         1.3662e-01, -1.1730e-01,  2.7846e-01,  2.0722e-01,  3.0053e-01,
         5.0410e-02,  3.9533e-02,  1.3043e-01,  5.6121e-02, -3.8900e-01,
         6.2027e-02, -4.4969e-02,  7.5304e-02, -7.1652e-02,  8.3423e-02,
         1.6109e-02, -2.5103e-01, -4.3320e-01, -1.7271e-01, -4.5931e-01,
        -2.5918e-01, -3.5961e-01, -1.8777e-01,  5.6777e-02, -9.4722e-02,
         2.7032e-02, -3.5458e-01, -5.1983e-01, -1.6823e-01,  1.0010e-01,
        -3.1689e-01, -4.3190e-01, -4.2816e-01, -2.8911e-01, -2.1072e-01,
        -1.6314e-01, -3.8447e-01, -6.3439e-01, -1.8423e-01, -2.3412e-01,
        -2.7189e-01, -1.9488e-01, -2.4200e-01, -2.5281e-01, -3.8377e-01,
        -3.3317e-01, -5.7055e-01, -9.0068e-01, -5.9353e-01, -7.0472e-01,
        -5.0634e-01, -4.2751e-01, -3.9559e-01, -1.0865e+00, -8.4829e-01,
        -5.2699e-01, -6.3805e-01, -9.2498e-01, -7.2069e-01, -8.2921e-01,
        -7.0589e-01, -7.0473e-01, -9.4607e-01, -9.7657e-01, -9.7031e-01,
        -9.2001e-01, -7.2209e-01, -8.3623e-01, -7.8918e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809352
t6: 1641198809352
state_values: tensor([1.1272, 1.0251, 1.1061, 1.2438, 1.3850, 1.4865, 1.5568, 1.6041, 1.6379,
        1.6895, 1.7652, 1.7493, 1.7679, 1.7815, 1.8315, 1.9692, 1.9357, 1.9034,
        1.9512, 1.9750, 2.0072, 2.0111, 1.8770, 1.8209, 1.9495, 1.9754, 1.9927,
        2.0676, 2.0391, 2.0444, 2.2227, 2.1838, 2.1940, 2.1378, 2.1283, 2.1325,
        2.1438, 2.1491, 2.1918, 2.1831, 2.2138, 2.2163, 2.2232, 2.3121, 2.2592,
        2.2618, 2.2622, 2.3263, 2.2929, 2.2865, 2.3036, 2.3550, 2.3289, 2.4420,
        2.3903, 2.3737, 2.4067, 2.5259, 2.4348, 2.5014, 2.4436, 2.4307, 2.4281,
        2.4622, 2.4491, 2.4646, 2.3032, 2.2562, 2.2207, 2.2446, 2.1910, 2.3577,
        2.4427, 2.4575, 2.4700, 2.5548, 2.5437, 2.5521, 2.5481, 2.5656, 2.5515,
        2.5522, 2.5708, 2.5685, 2.6002, 2.6896, 2.6193, 2.6106, 2.6475, 2.6776,
        2.6427, 2.7034, 2.6715, 2.6573, 2.6498, 2.6552, 2.7284, 2.7088, 2.6989,
        2.7425, 2.7298, 2.7026, 2.7023, 2.7081, 2.7056, 2.7135, 2.7189, 2.7326,
        2.7382, 2.7430, 2.7850, 2.8162, 2.8188, 2.6739, 2.5882, 2.5288, 2.6693,
        2.7694, 2.7613, 2.7891, 2.7868, 2.7842, 2.7959, 2.7973, 2.8749, 2.8363,
        2.9159, 2.8643, 2.8443, 2.8436, 2.9882, 2.9306, 2.8786, 2.9117, 3.0961,
        2.9845, 3.0315, 2.9652, 2.9663, 2.9874, 2.9436, 2.9300, 2.9207, 2.9400,
        2.9563, 2.9587, 2.9357, 2.9414, 3.0106, 2.9679, 3.1237, 3.0245, 2.9922,
        2.9781, 2.9768, 3.1478, 3.0890, 3.0884, 3.0298, 3.0141, 3.0121, 3.0160,
        3.0092, 3.0284, 3.0086, 3.0316, 3.0326, 3.0974, 3.0485, 3.0337, 3.0410,
        3.0436, 3.0480, 3.0778, 3.0573, 3.0463, 3.0489, 3.0786, 3.0758, 3.0702,
        3.0593, 3.1124, 3.0778, 3.0791, 3.0761, 3.0849, 3.0730, 3.1371, 3.2049,
        3.2192, 3.1447, 3.1211, 3.1036, 3.1715, 3.1353, 3.2265, 3.2101, 3.2297,
        3.1731, 3.2042, 3.1705, 3.1573, 3.1580, 3.1688, 3.2040, 3.2341, 3.2184,
        3.1895, 3.1763, 3.2189, 3.2831, 3.2088, 3.2027, 3.2524, 3.2801, 3.2218,
        3.2060, 3.1965, 3.1981, 3.2610, 3.2169, 3.2864, 3.2700, 3.2449, 3.2184,
        3.2135, 3.2160, 3.2985, 3.2414, 3.2569, 3.2627, 3.2762, 3.2398, 3.2632,
        3.2378, 3.2531, 3.2401, 3.2340, 3.2523, 3.2317, 3.2339, 3.2492, 3.2548,
        3.2502, 3.2432, 3.2510, 3.2464, 3.2934, 3.2742, 3.2680, 3.3060, 3.3225,
        3.3110, 3.3264, 3.4152, 3.3235, 3.4401, 3.3392, 3.3417, 3.3537, 3.3594,
        3.3829, 3.4281, 3.3830, 3.3899, 3.3464, 3.3869, 3.3431, 3.3722, 3.3352,
        3.4500, 3.3553, 3.3564, 3.3659, 3.4578, 3.3745, 3.3585, 3.3689, 3.3464,
        3.4798, 3.4414, 3.4706, 3.4071, 3.3728, 3.3524, 3.3544, 3.3555, 3.3586,
        3.3583, 3.3966, 3.3725, 3.4009, 3.3761, 3.4228, 3.4241, 3.3885, 3.3827,
        3.3754, 3.3713, 3.3817, 3.3825, 3.4452, 3.4673, 3.4152, 3.4010, 3.3946,
        3.3972, 3.4754, 3.4350, 3.4998, 3.4517, 3.5255, 3.4693, 3.4413, 3.4563,
        3.5797, 3.4698, 3.4327, 3.4228, 3.4739, 3.4390, 3.4771, 3.4395, 3.4351,
        3.4290, 3.4297, 3.4252, 3.5313, 3.4693, 3.5329, 3.4695, 3.4421, 3.4498,
        3.4461, 3.5015, 3.4627, 3.4470, 3.4417, 3.4563, 3.4582, 3.5281, 3.5377,
        3.4931, 3.4926, 3.4688, 3.4637, 3.5597, 3.4977, 3.4806, 3.4892, 3.4847,
        3.5811, 3.5053, 3.5821, 3.6836, 3.5550, 3.5247, 3.5135, 3.4989, 3.5071,
        3.4889, 3.6246, 3.5350, 3.5060, 3.4850, 3.4954, 3.6375, 3.6477, 3.5766,
        3.5885, 3.5479, 3.5257, 3.5139, 3.5172, 3.5129, 3.5121, 3.5545, 3.5452,
        3.5452, 3.5741, 3.5398, 3.6609, 3.5644, 3.5452, 3.5349, 3.5362, 3.5582,
        3.5692, 3.7391, 3.6014, 3.6012, 3.6084, 3.5677, 3.6465, 3.6854, 3.6356,
        3.5950, 3.5797, 3.6768, 3.6783, 3.6597, 3.5944, 3.6254, 3.6111],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809357
t8: 1641198809357
t9: 1641198809357
t10: 1641198809367
t11: 1641198809369
t12: 1641198809369
t1: 1641198809369
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809380
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9960, 1.0099, 0.9214, 0.9565, 0.9268, 0.9982, 0.9836, 0.9230, 0.9962,
        1.0513, 0.9209, 1.0016, 1.0088, 0.9712, 0.9986, 0.9140, 0.8836, 0.8675,
        0.9568, 1.0143, 1.0226, 0.9913, 1.0048, 0.9534, 1.0128, 0.9957, 0.9285,
        0.9979, 0.9985, 0.8612, 0.9077, 1.0901, 0.9134, 1.0209, 1.0374, 0.9904,
        1.0572, 0.9339, 1.0133, 0.9895, 0.9963, 0.9993, 0.9479, 0.9624, 0.9817,
        1.0220, 0.9306, 0.9995, 1.0752, 0.9734, 0.9641, 0.9936, 0.9780, 0.9951,
        1.0146, 0.9500, 0.9521, 1.0763, 0.8835, 0.9858, 0.9888, 0.9877, 0.9717,
        1.0265, 0.9730, 1.0209, 1.0798, 1.0557, 0.9768, 1.0341, 1.1236, 0.9775,
        1.0146, 1.0495, 0.8809, 1.0052, 1.0154, 0.9957, 0.9845, 1.0167, 0.9971,
        0.9908, 1.0125, 0.9634, 0.9719, 0.9738, 1.0391, 0.8875, 1.0006, 0.9767,
        0.9114, 1.0003, 1.0043, 1.0248, 1.0749, 0.8864, 1.0020, 1.0004, 0.9565,
        0.9946, 1.0060, 1.0427, 0.9981, 1.1130, 1.1017, 1.1044, 1.0451, 1.0086,
        0.9950, 1.0104, 1.0381, 1.0245, 0.9930, 1.0976, 1.1050, 1.1703, 0.9374,
        0.9854, 1.0114, 1.0001, 1.0018, 1.0036, 1.0048, 0.9627, 0.9935, 0.9697,
        0.9859, 1.0325, 1.1311, 0.7849, 0.9934, 0.9221, 0.9120, 0.8611, 0.9717,
        1.0578, 1.0010, 1.0088, 0.9791, 1.0003, 1.0144, 1.1413, 0.9686, 0.9814,
        0.9993, 1.0140, 1.0710, 0.9233, 0.9983, 1.0484, 0.9989, 0.9829, 1.0563,
        1.0138, 0.8861, 0.9718, 1.0557, 0.9836, 1.0111, 1.0657, 1.1146, 1.0012,
        0.9950, 0.9976, 0.9743, 1.0098, 0.9867, 0.9927, 1.0046, 0.9972, 1.0202,
        1.0304, 0.9721, 1.0008, 0.9999, 1.0007, 0.9877, 0.9984, 0.9926, 1.0148,
        0.9542, 1.0706, 1.1264, 1.0359, 0.9970, 0.9974, 1.0122, 1.0433, 1.0281,
        0.9823, 1.0975, 1.5400, 0.8228, 1.0050, 0.9745, 1.0100, 1.0528, 0.9762,
        0.9144, 1.0931, 0.9961, 1.0270, 0.9865, 0.9919, 1.0065, 1.0098, 1.0178,
        1.0646, 0.9236, 0.9722, 0.9802, 0.9917, 0.9434, 0.9806, 0.9916, 1.0090,
        1.0351, 1.0739, 0.9157, 1.0007, 0.9904, 1.0119, 1.0003, 1.0006, 1.0619,
        1.0813, 0.8751, 0.9869, 0.9921, 0.9865, 0.9840, 1.0852, 0.9125, 1.0810,
        0.9756, 1.0113, 1.0086, 0.9927, 1.0005, 1.0011, 0.9985, 0.9940, 1.0227,
        1.0277, 1.0449, 1.0083, 0.9929, 0.9927, 1.0220, 0.9446, 0.9912, 1.0090,
        1.0002, 0.9616, 0.9468, 0.8183, 1.0296, 0.9729, 0.9753, 0.9875, 0.9840,
        1.0018, 1.0076, 0.9950, 1.0791, 0.8394, 1.0388, 0.9673, 1.0361, 0.8411,
        0.9817, 0.9906, 0.9819, 0.9537, 0.9605, 1.0308, 0.9601, 1.0312, 0.8909,
        0.9864, 1.0684, 0.9932, 1.0197, 1.2556, 1.0264, 1.0055, 1.0003, 1.0063,
        0.9758, 1.0003, 0.9949, 1.0068, 0.9520, 1.0007, 1.0011, 1.0612, 1.0972,
        1.0225, 1.0012, 1.0001, 1.0022, 0.9478, 0.9998, 0.9967, 1.0245, 1.0308,
        0.9378, 1.0050, 1.0177, 1.0038, 0.9799, 1.0036, 0.9995, 0.9634, 0.9032,
        0.9801, 1.1120, 1.7170, 0.8537, 1.0186, 0.9624, 0.9667, 1.0068, 1.0095,
        1.0036, 1.0088, 0.9674, 0.9759, 0.9924, 0.9819, 1.0093, 1.0924, 1.0852,
        0.9303, 0.9957, 1.0219, 1.0152, 0.9981, 1.0012, 1.0093, 1.0296, 1.0022,
        0.9921, 1.1643, 1.0511, 0.8862, 0.9959, 0.9892, 0.9773, 0.9995, 0.9520,
        0.9557, 0.8375, 1.0454, 1.0007, 1.0339, 1.0346, 0.9927, 0.9988, 1.0030,
        0.9369, 1.0035, 1.0379, 1.0072, 1.0512, 0.8680, 0.9660, 1.0044, 0.9772,
        0.9999, 1.0215, 1.2665, 0.9821, 1.0177, 1.0044, 0.9843, 1.0034, 1.0016,
        0.9819, 1.0039, 0.8961, 0.9924, 1.0618, 1.1319, 1.0466, 0.9843, 1.0041,
        1.0453, 1.1021, 0.9634, 0.9884, 1.0217, 0.7895, 1.0139, 1.0212, 1.0009,
        1.0479, 0.7905, 1.0145, 1.0394, 1.0014, 0.9592, 0.9890, 0.9205],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809384
t4: 1641198809384
surr1, surr2: tensor([-3.1488e+00, -9.9067e-01, -1.3617e+00, -2.3676e+00, -2.5451e+00,
        -2.9392e+00, -2.7374e+00, -2.5534e+00, -2.8140e+00, -2.8001e+00,
        -2.4121e+00, -2.6613e+00, -2.5112e+00, -2.5110e+00, -2.4490e+00,
        -2.3807e+00, -2.1171e+00, -2.2459e+00, -2.3071e+00, -2.3467e+00,
        -2.2674e+00, -2.1662e+00, -8.4856e-01, -1.6836e-01, -1.6561e+00,
        -1.7966e+00, -1.5428e+00, -1.6863e+00, -1.5975e+00, -1.3337e+00,
        -1.8675e+00, -1.9216e+00, -1.5855e+00, -1.9424e+00, -1.6808e+00,
        -1.5795e+00, -1.5732e+00, -1.6899e+00, -1.5558e+00, -1.5706e+00,
        -1.5155e+00, -1.4399e+00, -1.4254e+00, -1.5577e+00, -1.7129e+00,
        -1.4757e+00, -1.4283e+00, -1.5536e+00, -1.6510e+00, -1.7166e+00,
        -1.3836e+00, -1.5336e+00, -1.5063e+00, -1.8741e+00, -1.2642e+00,
        -1.2612e+00, -1.1003e+00, -1.6062e+00, -1.3859e+00, -1.3816e+00,
        -1.2305e+00, -1.5243e+00, -1.0794e+00, -1.2892e+00, -1.5216e+00,
        -1.2035e+00,  1.2367e+00,  1.3488e+00,  2.0941e+00,  2.5277e+00,
         5.2271e+00,  1.0238e-02,  2.9192e-01,  1.8226e-01, -5.6243e-02,
         1.7206e-01,  2.2488e-01,  2.7870e-01,  3.1874e-01,  3.1421e-01,
         2.7350e-01,  4.1582e-01,  3.4682e-01,  2.9413e-01,  4.5593e-01,
         1.2701e-01, -3.2194e-02,  1.9034e-01,  5.0648e-01,  2.7838e-01,
         1.5919e-01,  1.3584e-01,  2.3966e-01,  2.7937e-01,  3.8543e-01,
         9.0269e-02,  2.2304e-01,  2.9845e-01,  2.8080e-01,  2.1144e-01,
         2.8177e-01,  4.0178e-01,  2.3011e-01,  3.9032e-01, -1.2273e-01,
         8.9029e-02, -1.1271e-01,  4.2768e-02,  1.2930e-01,  1.9118e-01,
         2.6284e-01,  3.2238e-01,  1.8795e-01,  1.9013e+00,  2.9405e+00,
         4.2622e+00,  6.8254e-01,  9.6376e-01,  9.2828e-01,  9.8540e-01,
         9.8046e-01,  9.4481e-01,  8.8357e-01,  9.0598e-01,  8.1947e-01,
         8.3197e-01,  7.3402e-01,  7.9227e-01,  9.3137e-01,  4.9364e-01,
         6.0443e-01,  7.8659e-01,  4.7545e-01,  7.6242e-01,  2.9161e-01,
         8.6437e-01,  7.0099e-01,  8.9613e-01,  9.0516e-01,  8.6180e-01,
         1.0224e+00,  1.0765e+00,  5.9226e-01,  9.4121e-01,  1.0242e+00,
         1.0475e+00,  6.0000e-01,  7.7131e-01,  9.3510e-01,  1.1429e+00,
         5.4260e-01,  9.4981e-01,  7.1938e-01,  9.2216e-01,  8.6588e-01,
         4.7850e-01,  8.3244e-01,  8.3848e-01,  8.3702e-01,  9.5091e-01,
         8.0619e-01,  5.3511e-01,  9.5451e-01,  8.8151e-01,  4.9817e-01,
         9.1842e-01,  6.8629e-01,  6.5848e-01,  8.0434e-01,  9.3418e-01,
         9.5577e-01,  6.7027e-01,  6.8967e-01,  8.6791e-01,  8.5611e-01,
         1.0291e+00,  9.5202e-01,  9.4326e-01,  8.7885e-01,  1.0759e+00,
         8.5364e-01,  8.6325e-01,  4.2122e-01,  5.4867e-01,  8.2429e-01,
         9.4687e-01,  1.0139e+00,  8.6329e-01,  7.0237e-01,  5.8227e-01,
         7.7510e-01,  6.6727e-01,  3.3481e-01,  8.1089e-01,  5.1231e-01,
         6.7043e-01,  8.1740e-01,  6.3324e-01,  3.8128e-01,  7.3250e-01,
         3.2455e-01,  8.3438e-01,  6.7357e-01,  8.8922e-01,  7.7201e-01,
         7.4002e-01,  7.2585e-01,  3.4330e-01,  5.9318e-01,  7.0808e-01,
         5.6348e-01,  6.1368e-01,  6.9261e-01,  5.8967e-01,  5.4536e-01,
         6.1414e-01,  7.0164e-01,  7.5522e-01,  4.3678e-01,  5.9778e-01,
         7.3502e-01,  6.1510e-01,  6.8950e-01,  6.1032e-01,  8.3101e-01,
         5.7879e-01,  4.3843e-01,  6.1404e-01,  6.2337e-01,  7.1757e-01,
         6.4160e-01,  6.5386e-01,  1.8909e-01,  7.9243e-01,  3.0233e-01,
         7.2410e-01,  6.1737e-01,  6.7349e-01,  7.3567e-01,  8.1400e-01,
         7.2415e-01,  6.4349e-01,  7.6874e-01,  2.8473e-01,  6.6878e-01,
         4.0676e-01,  6.6186e-01,  5.6427e-01,  4.6537e-01,  1.6055e-01,
         5.0829e-01,  5.4806e-01,  5.9327e-01,  4.4709e-01,  1.7832e-01,
         9.7717e-02,  2.0396e-01,  1.1111e-03,  4.2996e-01,  4.1203e-01,
         4.4299e-01,  3.6703e-01,  2.7043e-01,  3.2498e-01,  3.0626e-01,
        -7.5413e-02,  3.5297e-01,  3.2801e-01,  4.9361e-01,  4.9221e-02,
         1.1352e-01,  1.6612e-01,  3.9964e-01,  3.8954e-01,  1.7009e-01,
        -2.7687e-02,  2.2415e-01,  4.4136e-01,  2.5609e-01,  1.3552e-01,
         2.0658e-01,  1.6053e-01,  1.8801e-01,  2.4352e-01, -1.2883e-02,
         3.7282e-01,  3.8132e-01,  2.9894e-01,  1.5141e-01,  3.1160e-01,
         2.8076e-01,  2.7363e-01,  1.8174e-01,  3.0105e-01,  2.4016e-01,
         1.7581e-01,  1.2731e-02,  5.8520e-03,  2.4304e-01,  1.7527e-01,
         1.9336e-01, -3.0388e-01,  2.5946e-01,  4.9538e-01,  3.1755e-01,
         4.0479e-01,  2.9184e-01,  2.8732e-01,  4.5322e-01,  2.7413e-01,
         3.5588e-01,  1.4258e-01,  2.8319e-01,  3.3666e-01,  3.4947e-01,
        -4.4480e-03,  9.3213e-02, -1.5692e-01, -1.0378e-01,  3.3560e-01,
        -1.5029e-01,  3.2910e-01, -1.2844e-01,  2.6867e-01,  2.7998e-01,
         2.4526e-01,  2.3003e-01,  8.5742e-02,  8.0000e-02,  2.3481e-01,
         1.8360e-01,  3.5266e-01, -8.3273e-02,  5.5269e-02,  1.7303e-01,
         1.3479e-01, -1.1718e-01,  2.7861e-01,  2.0754e-01,  3.0300e-01,
         5.0567e-02,  3.9537e-02,  1.3052e-01,  5.9400e-02, -3.8868e-01,
         6.1073e-02, -4.4955e-02,  7.4779e-02, -7.1842e-02,  8.3432e-02,
         1.6221e-02, -2.5211e-01, -4.0312e-01, -1.7124e-01, -4.6036e-01,
        -2.5760e-01, -3.5956e-01, -1.8781e-01,  5.6787e-02, -9.4672e-02,
         2.7326e-02, -3.5555e-01, -5.1898e-01, -1.6822e-01,  9.9746e-02,
        -3.0561e-01, -4.2907e-01, -4.2820e-01, -2.9049e-01, -2.1068e-01,
        -1.6292e-01, -4.4267e-01, -6.3470e-01, -1.8400e-01, -2.3310e-01,
        -2.7417e-01, -1.9506e-01, -2.4253e-01, -2.5375e-01, -3.8373e-01,
        -3.3173e-01, -5.7142e-01, -8.9978e-01, -6.1073e-01, -7.0279e-01,
        -5.0794e-01, -4.2920e-01, -4.0228e-01, -1.0917e+00, -8.5096e-01,
        -5.2624e-01, -6.3773e-01, -8.1141e-01, -7.1566e-01, -8.2954e-01,
        -7.0624e-01, -7.0369e-01, -8.3101e-01, -9.7206e-01, -9.7078e-01,
        -9.1964e-01, -7.2316e-01, -8.3568e-01, -7.8953e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1488e+00, -9.9067e-01, -1.3617e+00, -2.3676e+00, -2.5451e+00,
        -2.9392e+00, -2.7374e+00, -2.5534e+00, -2.8140e+00, -2.8001e+00,
        -2.4121e+00, -2.6613e+00, -2.5112e+00, -2.5110e+00, -2.4490e+00,
        -2.3807e+00, -2.1565e+00, -2.3300e+00, -2.3071e+00, -2.3467e+00,
        -2.2674e+00, -2.1662e+00, -8.4856e-01, -1.6836e-01, -1.6561e+00,
        -1.7966e+00, -1.5428e+00, -1.6863e+00, -1.5975e+00, -1.3938e+00,
        -1.8675e+00, -1.9216e+00, -1.5855e+00, -1.9424e+00, -1.6808e+00,
        -1.5795e+00, -1.5732e+00, -1.6899e+00, -1.5558e+00, -1.5706e+00,
        -1.5155e+00, -1.4399e+00, -1.4254e+00, -1.5577e+00, -1.7129e+00,
        -1.4757e+00, -1.4283e+00, -1.5536e+00, -1.6510e+00, -1.7166e+00,
        -1.3836e+00, -1.5336e+00, -1.5063e+00, -1.8741e+00, -1.2642e+00,
        -1.2612e+00, -1.1003e+00, -1.6062e+00, -1.4118e+00, -1.3816e+00,
        -1.2305e+00, -1.5243e+00, -1.0794e+00, -1.2892e+00, -1.5216e+00,
        -1.2035e+00,  1.2367e+00,  1.3488e+00,  2.0941e+00,  2.5277e+00,
         5.1175e+00,  1.0238e-02,  2.9192e-01,  1.8226e-01, -5.7464e-02,
         1.7206e-01,  2.2488e-01,  2.7870e-01,  3.1874e-01,  3.1421e-01,
         2.7350e-01,  4.1582e-01,  3.4682e-01,  2.9413e-01,  4.5593e-01,
         1.2701e-01, -3.2194e-02,  1.9303e-01,  5.0648e-01,  2.7838e-01,
         1.5919e-01,  1.3584e-01,  2.3966e-01,  2.7937e-01,  3.8543e-01,
         9.1655e-02,  2.2304e-01,  2.9845e-01,  2.8080e-01,  2.1144e-01,
         2.8177e-01,  4.0178e-01,  2.3011e-01,  3.8576e-01, -1.2255e-01,
         8.8676e-02, -1.1271e-01,  4.2768e-02,  1.2930e-01,  1.9118e-01,
         2.6284e-01,  3.2238e-01,  1.8795e-01,  1.9013e+00,  2.9271e+00,
         4.0062e+00,  6.8254e-01,  9.6376e-01,  9.2828e-01,  9.8540e-01,
         9.8046e-01,  9.4481e-01,  8.8357e-01,  9.0598e-01,  8.1947e-01,
         8.3197e-01,  7.3402e-01,  7.9227e-01,  9.0579e-01,  5.6602e-01,
         6.0443e-01,  7.8659e-01,  4.7545e-01,  7.9688e-01,  2.9161e-01,
         8.6437e-01,  7.0099e-01,  8.9613e-01,  9.0516e-01,  8.6180e-01,
         1.0224e+00,  1.0376e+00,  5.9226e-01,  9.4121e-01,  1.0242e+00,
         1.0475e+00,  6.0000e-01,  7.7131e-01,  9.3510e-01,  1.1429e+00,
         5.4260e-01,  9.4981e-01,  7.1938e-01,  9.2216e-01,  8.7946e-01,
         4.7850e-01,  8.3244e-01,  8.3848e-01,  8.3702e-01,  9.5091e-01,
         7.9559e-01,  5.3511e-01,  9.5451e-01,  8.8151e-01,  4.9817e-01,
         9.1842e-01,  6.8629e-01,  6.5848e-01,  8.0434e-01,  9.3418e-01,
         9.5577e-01,  6.7027e-01,  6.8967e-01,  8.6791e-01,  8.5611e-01,
         1.0291e+00,  9.5202e-01,  9.4326e-01,  8.7885e-01,  1.0759e+00,
         8.5364e-01,  8.6325e-01,  4.1136e-01,  5.4867e-01,  8.2429e-01,
         9.4687e-01,  1.0139e+00,  8.6329e-01,  7.0237e-01,  5.8227e-01,
         7.7510e-01,  4.7662e-01,  3.6623e-01,  8.1089e-01,  5.1231e-01,
         6.7043e-01,  8.1740e-01,  6.3324e-01,  3.8128e-01,  7.3250e-01,
         3.2455e-01,  8.3438e-01,  6.7357e-01,  8.8922e-01,  7.7201e-01,
         7.4002e-01,  7.2585e-01,  3.4330e-01,  5.9318e-01,  7.0808e-01,
         5.6348e-01,  6.1368e-01,  6.9261e-01,  5.8967e-01,  5.4536e-01,
         6.1414e-01,  7.0164e-01,  7.5522e-01,  4.3678e-01,  5.9778e-01,
         7.3502e-01,  6.1510e-01,  6.8950e-01,  6.1032e-01,  8.3101e-01,
         5.7879e-01,  4.5092e-01,  6.1404e-01,  6.2337e-01,  7.1757e-01,
         6.4160e-01,  6.5386e-01,  1.8909e-01,  7.9243e-01,  3.0233e-01,
         7.2410e-01,  6.1737e-01,  6.7349e-01,  7.3567e-01,  8.1400e-01,
         7.2415e-01,  6.4349e-01,  7.6874e-01,  2.8473e-01,  6.6878e-01,
         4.0676e-01,  6.6186e-01,  5.6427e-01,  4.6537e-01,  1.6055e-01,
         5.0829e-01,  5.4806e-01,  5.9327e-01,  4.4709e-01,  1.7832e-01,
         1.0747e-01,  2.0396e-01,  1.1111e-03,  4.2996e-01,  4.1203e-01,
         4.4299e-01,  3.6703e-01,  2.7043e-01,  3.2498e-01,  3.0626e-01,
        -8.0855e-02,  3.5297e-01,  3.2801e-01,  4.9361e-01,  5.2668e-02,
         1.1352e-01,  1.6612e-01,  3.9964e-01,  3.8954e-01,  1.7009e-01,
        -2.7687e-02,  2.2415e-01,  4.4136e-01,  2.5872e-01,  1.3552e-01,
         2.0658e-01,  1.6053e-01,  1.8801e-01,  2.1334e-01, -1.2883e-02,
         3.7282e-01,  3.8132e-01,  2.9894e-01,  1.5141e-01,  3.1160e-01,
         2.8076e-01,  2.7363e-01,  1.8174e-01,  3.0105e-01,  2.4016e-01,
         1.7581e-01,  1.2731e-02,  5.8520e-03,  2.4304e-01,  1.7527e-01,
         1.9336e-01, -3.0388e-01,  2.5946e-01,  4.9538e-01,  3.1755e-01,
         4.0479e-01,  2.9184e-01,  2.8732e-01,  4.5322e-01,  2.7413e-01,
         3.5588e-01,  1.4258e-01,  2.8319e-01,  3.3666e-01,  3.4947e-01,
        -4.4480e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.3560e-01,
        -1.5029e-01,  3.2910e-01, -1.2844e-01,  2.6867e-01,  2.7998e-01,
         2.4526e-01,  2.3003e-01,  8.5742e-02,  8.0000e-02,  2.3481e-01,
         1.8360e-01,  3.5266e-01, -8.3273e-02,  5.5269e-02,  1.7303e-01,
         1.3479e-01, -1.1718e-01,  2.7861e-01,  2.0754e-01,  3.0300e-01,
         5.0567e-02,  3.9537e-02,  1.3052e-01,  5.6121e-02, -3.8868e-01,
         6.2027e-02, -4.4955e-02,  7.4779e-02, -7.1842e-02,  8.3432e-02,
         1.6221e-02, -2.5211e-01, -4.3320e-01, -1.7124e-01, -4.6036e-01,
        -2.5760e-01, -3.5956e-01, -1.8781e-01,  5.6787e-02, -9.4672e-02,
         2.7326e-02, -3.5555e-01, -5.1898e-01, -1.6822e-01,  9.9746e-02,
        -3.1689e-01, -4.2907e-01, -4.2820e-01, -2.9049e-01, -2.1068e-01,
        -1.6292e-01, -3.8447e-01, -6.3470e-01, -1.8400e-01, -2.3310e-01,
        -2.7417e-01, -1.9506e-01, -2.4253e-01, -2.5375e-01, -3.8373e-01,
        -3.3317e-01, -5.7142e-01, -8.9978e-01, -5.9353e-01, -7.0279e-01,
        -5.0794e-01, -4.2920e-01, -4.0228e-01, -1.0896e+00, -8.5096e-01,
        -5.2624e-01, -6.3773e-01, -9.2498e-01, -7.1566e-01, -8.2954e-01,
        -7.0624e-01, -7.0369e-01, -9.4607e-01, -9.7206e-01, -9.7078e-01,
        -9.1964e-01, -7.2316e-01, -8.3568e-01, -7.8953e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809394
t6: 1641198809394
state_values: tensor([1.0450, 0.9338, 1.0048, 1.1341, 1.2680, 1.3645, 1.4306, 1.4750, 1.5068,
        1.5554, 1.6272, 1.6117, 1.6287, 1.6415, 1.6889, 1.8178, 1.7881, 1.7573,
        1.8025, 1.8245, 1.8539, 1.8579, 1.7297, 1.6754, 1.7990, 1.8246, 1.8415,
        1.9118, 1.8864, 1.8915, 2.0546, 2.0199, 2.0293, 1.9788, 1.9701, 1.9743,
        1.9851, 1.9908, 2.0300, 2.0228, 2.0512, 2.0540, 2.0608, 2.1432, 2.0951,
        2.0976, 2.0986, 2.1581, 2.1278, 2.1226, 2.1385, 2.1872, 2.1630, 2.2721,
        2.2227, 2.2070, 2.2387, 2.3536, 2.2671, 2.3309, 2.2758, 2.2638, 2.2614,
        2.2945, 2.2828, 2.2978, 2.1467, 2.1042, 2.0713, 2.0946, 2.0437, 2.1999,
        2.2801, 2.2953, 2.3080, 2.3898, 2.3797, 2.3880, 2.3843, 2.4014, 2.3883,
        2.3892, 2.4073, 2.4056, 2.4363, 2.5226, 2.4561, 2.4476, 2.4832, 2.5127,
        2.4797, 2.5380, 2.5080, 2.4947, 2.4877, 2.4934, 2.5634, 2.5455, 2.5366,
        2.5777, 2.5662, 2.5411, 2.5412, 2.5470, 2.5454, 2.5532, 2.5588, 2.5720,
        2.5777, 2.5825, 2.6200, 2.6468, 2.6493, 2.5178, 2.4350, 2.3781, 2.5142,
        2.6102, 2.6039, 2.6278, 2.6262, 2.6244, 2.6346, 2.6362, 2.7021, 2.6700,
        2.7375, 2.6942, 2.6775, 2.6773, 2.8037, 2.7516, 2.7079, 2.7362, 2.9092,
        2.8024, 2.8475, 2.7838, 2.7849, 2.8054, 2.7654, 2.7541, 2.7468, 2.7635,
        2.7779, 2.7803, 2.7613, 2.7665, 2.8308, 2.7902, 2.9402, 2.8454, 2.8145,
        2.8009, 2.7998, 2.9647, 2.9088, 2.9083, 2.8520, 2.8370, 2.8355, 2.8397,
        2.8332, 2.8518, 2.8336, 2.8556, 2.8570, 2.9196, 2.8730, 2.8588, 2.8658,
        2.8689, 2.8733, 2.9021, 2.8827, 2.8723, 2.8749, 2.9037, 2.9014, 2.8961,
        2.8860, 2.9372, 2.9048, 2.9061, 2.9033, 2.9118, 2.9006, 2.9624, 3.0281,
        3.0422, 2.9707, 2.9481, 2.9316, 2.9968, 2.9625, 3.0502, 3.0348, 3.0537,
        2.9996, 3.0293, 2.9974, 2.9845, 2.9854, 2.9958, 3.0299, 3.0592, 3.0443,
        3.0169, 3.0042, 3.0451, 3.1073, 3.0362, 3.0300, 3.0780, 3.1050, 3.0492,
        3.0338, 3.0248, 3.0268, 3.0873, 3.0452, 3.1120, 3.0965, 3.0724, 3.0469,
        3.0425, 3.0451, 3.1246, 3.0700, 3.0848, 3.0905, 3.1036, 3.0693, 3.0915,
        3.0676, 3.0821, 3.0699, 3.0641, 3.0817, 3.0621, 3.0643, 3.0791, 3.0847,
        3.0808, 3.0741, 3.0818, 3.0775, 3.1227, 3.1045, 3.0989, 3.1353, 3.1513,
        3.1404, 3.1552, 3.2409, 3.1532, 3.2646, 3.1685, 3.1704, 3.1819, 3.1875,
        3.2102, 3.2539, 3.2107, 3.2172, 3.1759, 3.2145, 3.1726, 3.2005, 3.1654,
        3.2759, 3.1853, 3.1858, 3.1950, 3.2837, 3.2041, 3.1884, 3.1983, 3.1769,
        3.3049, 3.2686, 3.2967, 3.2356, 3.2026, 3.1834, 3.1851, 3.1862, 3.1894,
        3.1894, 3.2262, 3.2033, 3.2306, 3.2071, 3.2519, 3.2532, 3.2192, 3.2137,
        3.2069, 3.2029, 3.2129, 3.2137, 3.2747, 3.2959, 3.2458, 3.2321, 3.2260,
        3.2286, 3.3038, 3.2651, 3.3274, 3.2813, 3.3523, 3.2984, 3.2712, 3.2856,
        3.4043, 3.2993, 3.2638, 3.2544, 3.3032, 3.2704, 3.3066, 3.2710, 3.2665,
        3.2607, 3.2614, 3.2572, 3.3594, 3.3001, 3.3610, 3.3002, 3.2738, 3.2813,
        3.2779, 3.3312, 3.2941, 3.2792, 3.2740, 3.2879, 3.2897, 3.3572, 3.3666,
        3.3238, 3.3231, 3.3008, 3.2957, 3.3880, 3.3286, 3.3121, 3.3202, 3.3160,
        3.4089, 3.3368, 3.4101, 3.5036, 3.3844, 3.3550, 3.3441, 3.3300, 3.3379,
        3.3206, 3.4508, 3.3655, 3.3374, 3.3171, 3.3274, 3.4635, 3.4732, 3.4057,
        3.4169, 3.3779, 3.3566, 3.3457, 3.3487, 3.3446, 3.3440, 3.3847, 3.3759,
        3.3760, 3.4038, 3.3709, 3.4854, 3.3953, 3.3765, 3.3667, 3.3680, 3.3890,
        3.3996, 3.5554, 3.4314, 3.4306, 3.4375, 3.3988, 3.4742, 3.5089, 3.4640,
        3.4249, 3.4103, 3.5014, 3.5031, 3.4867, 3.4246, 3.4542, 3.4406],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809399
t8: 1641198809399
t9: 1641198809399
t10: 1641198809409
t11: 1641198809411
t12: 1641198809411
t1: 1641198809411
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809422
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9964, 1.0120, 0.9138, 0.9473, 0.9013, 0.9993, 0.9791, 0.9039, 0.9997,
        1.0639, 0.9131, 1.0037, 1.0011, 0.9771, 1.0104, 0.9126, 0.8796, 0.8710,
        0.9494, 1.0106, 1.0212, 0.9911, 1.0048, 0.9511, 1.0133, 0.9965, 0.9339,
        0.9984, 0.9993, 0.8616, 0.9084, 1.0913, 0.9163, 1.0206, 1.0394, 0.9910,
        1.0494, 0.9399, 1.0112, 0.9945, 0.9979, 0.9997, 0.9583, 0.9612, 0.9828,
        1.0227, 0.9366, 1.0030, 1.0620, 0.9751, 0.9670, 0.9907, 0.9848, 0.9961,
        1.0104, 0.9563, 0.9573, 1.0769, 0.8896, 0.9877, 0.9841, 0.9880, 0.9723,
        1.0191, 0.9761, 1.0173, 1.0667, 1.0479, 0.9828, 1.0521, 1.1214, 0.9848,
        1.0104, 1.0360, 0.8922, 1.0055, 1.0200, 0.9973, 0.9864, 1.0163, 0.9975,
        0.9916, 1.0109, 0.9681, 0.9757, 0.9726, 1.0372, 0.8912, 0.9946, 0.9771,
        0.9166, 1.0006, 1.0040, 1.0245, 1.0706, 0.8942, 1.0019, 1.0010, 0.9621,
        0.9936, 1.0058, 1.0418, 0.9984, 1.1047, 1.0946, 1.0911, 1.0376, 1.0017,
        0.9888, 1.0179, 1.0486, 1.0267, 0.9872, 1.0906, 1.0983, 1.1448, 0.9490,
        0.9821, 1.0167, 1.0002, 0.9994, 1.0029, 1.0041, 0.9693, 0.9931, 0.9809,
        0.9876, 1.0300, 1.1248, 0.7942, 0.9938, 0.9123, 0.9171, 0.8646, 0.9709,
        1.0690, 1.0010, 1.0108, 0.9777, 1.0003, 1.0143, 1.1418, 0.9698, 0.9828,
        1.0014, 0.9972, 1.0660, 0.9317, 0.9980, 1.0724, 0.9988, 0.9782, 1.0551,
        1.0136, 0.8949, 0.9714, 1.0589, 0.9854, 1.0106, 1.0670, 1.1098, 1.0014,
        0.9962, 0.9811, 0.9778, 1.0057, 0.9969, 0.9895, 1.0025, 0.9979, 1.0151,
        1.0263, 0.9766, 0.9993, 1.0000, 1.0005, 0.9909, 0.9989, 0.9945, 1.0126,
        0.9590, 1.0655, 1.1191, 1.0332, 0.9982, 0.9958, 1.0225, 1.0503, 1.0294,
        0.9819, 1.0912, 1.5320, 0.8271, 1.0050, 0.9974, 1.0140, 1.0551, 0.9776,
        0.9175, 1.1012, 0.9966, 1.0248, 0.9902, 0.9971, 1.0129, 1.0091, 1.0142,
        1.0620, 0.9267, 0.9752, 0.9796, 0.9933, 0.9431, 0.9764, 0.9933, 1.0086,
        1.0346, 1.0697, 0.9220, 1.0007, 1.0007, 1.0103, 1.0003, 1.0006, 1.0626,
        1.0775, 0.8824, 0.9861, 0.9975, 0.9885, 0.9860, 1.0834, 0.9167, 1.0770,
        0.9793, 1.0064, 1.0031, 0.9956, 0.9992, 1.0002, 0.9990, 0.9955, 1.0157,
        1.0255, 1.0390, 1.0067, 0.9986, 0.9890, 1.0087, 0.9510, 0.9949, 1.0114,
        1.0045, 0.9723, 0.9479, 0.8250, 1.0252, 0.9733, 0.9736, 0.9878, 0.9852,
        0.9989, 1.0070, 0.9975, 1.0824, 0.8421, 1.0412, 0.9738, 1.0300, 0.8548,
        0.9858, 0.9930, 0.9842, 0.9640, 0.9601, 1.0294, 0.9618, 1.0284, 0.9077,
        0.9856, 1.0745, 0.9937, 1.0169, 1.2549, 1.0261, 1.0052, 0.9973, 0.9969,
        0.9827, 0.9983, 0.9994, 1.0029, 0.9604, 1.0011, 0.9978, 1.0551, 1.0915,
        1.0210, 1.0010, 0.9997, 0.9956, 0.9573, 0.9998, 0.9912, 1.0212, 1.0274,
        0.9478, 1.0052, 1.0305, 1.0036, 0.9848, 1.0033, 0.9997, 0.9632, 0.8985,
        0.9782, 1.1014, 1.7028, 0.8576, 1.0120, 0.9744, 0.9508, 1.0052, 1.0041,
        0.9973, 1.0008, 0.9843, 0.9716, 1.0097, 0.9816, 1.0076, 1.0911, 1.0808,
        0.9368, 0.9935, 1.0055, 1.0138, 0.9987, 1.0029, 1.0201, 1.0331, 1.0022,
        0.9927, 1.1654, 1.0498, 0.8935, 0.9952, 0.9815, 0.9803, 0.9996, 0.9597,
        0.9622, 0.8480, 1.0360, 1.0012, 1.0269, 1.0346, 0.9930, 0.9990, 1.0024,
        0.9487, 1.0037, 1.0357, 1.0071, 1.0469, 0.8829, 0.9650, 1.0045, 0.9822,
        0.9996, 1.0210, 1.2641, 0.9829, 1.0162, 0.9993, 0.9938, 1.0045, 1.0041,
        0.9865, 1.0036, 0.9069, 0.9910, 1.0607, 1.1274, 1.0435, 0.9878, 1.0088,
        1.0678, 1.1027, 0.9669, 0.9865, 1.0228, 0.7975, 1.0065, 1.0215, 1.0014,
        1.0470, 0.7917, 1.0086, 1.0394, 1.0010, 0.9601, 0.9881, 0.9197],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809426
t4: 1641198809426
surr1, surr2: tensor([-3.1501e+00, -9.9268e-01, -1.3504e+00, -2.3448e+00, -2.4753e+00,
        -2.9423e+00, -2.7248e+00, -2.5005e+00, -2.8237e+00, -2.8337e+00,
        -2.3917e+00, -2.6669e+00, -2.4920e+00, -2.5264e+00, -2.4779e+00,
        -2.3771e+00, -2.1076e+00, -2.2549e+00, -2.2893e+00, -2.3380e+00,
        -2.2645e+00, -2.1658e+00, -8.4856e-01, -1.6796e-01, -1.6569e+00,
        -1.7979e+00, -1.5517e+00, -1.6871e+00, -1.5987e+00, -1.3343e+00,
        -1.8689e+00, -1.9237e+00, -1.5904e+00, -1.9420e+00, -1.6839e+00,
        -1.5805e+00, -1.5616e+00, -1.7008e+00, -1.5526e+00, -1.5787e+00,
        -1.5179e+00, -1.4406e+00, -1.4411e+00, -1.5556e+00, -1.7148e+00,
        -1.4766e+00, -1.4375e+00, -1.5591e+00, -1.6307e+00, -1.7197e+00,
        -1.3877e+00, -1.5290e+00, -1.5168e+00, -1.8759e+00, -1.2590e+00,
        -1.2695e+00, -1.1063e+00, -1.6072e+00, -1.3955e+00, -1.3842e+00,
        -1.2247e+00, -1.5248e+00, -1.0801e+00, -1.2799e+00, -1.5264e+00,
        -1.1993e+00,  1.2217e+00,  1.3388e+00,  2.1068e+00,  2.5717e+00,
         5.2172e+00,  1.0315e-02,  2.9069e-01,  1.7991e-01, -5.6964e-02,
         1.7212e-01,  2.2588e-01,  2.7915e-01,  3.1937e-01,  3.1408e-01,
         2.7363e-01,  4.1615e-01,  3.4627e-01,  2.9556e-01,  4.5768e-01,
         1.2685e-01, -3.2136e-02,  1.9114e-01,  5.0347e-01,  2.7852e-01,
         1.6010e-01,  1.3589e-01,  2.3960e-01,  2.7930e-01,  3.8390e-01,
         9.1067e-02,  2.2302e-01,  2.9861e-01,  2.8244e-01,  2.1123e-01,
         2.8171e-01,  4.0144e-01,  2.3019e-01,  3.8740e-01, -1.2195e-01,
         8.7961e-02, -1.1190e-01,  4.2473e-02,  1.2849e-01,  1.9259e-01,
         2.6550e-01,  3.2309e-01,  1.8685e-01,  1.8892e+00,  2.9226e+00,
         4.1695e+00,  6.9097e-01,  9.6047e-01,  9.3307e-01,  9.8544e-01,
         9.7815e-01,  9.4419e-01,  8.8299e-01,  9.1218e-01,  8.1909e-01,
         8.4157e-01,  7.3530e-01,  7.9034e-01,  9.2621e-01,  4.9945e-01,
         6.0467e-01,  7.7827e-01,  4.7815e-01,  7.6553e-01,  2.9140e-01,
         8.7350e-01,  7.0099e-01,  8.9790e-01,  9.0383e-01,  8.6184e-01,
         1.0222e+00,  1.0770e+00,  5.9300e-01,  9.4254e-01,  1.0263e+00,
         1.0301e+00,  5.9718e-01,  7.7834e-01,  9.3485e-01,  1.1691e+00,
         5.4256e-01,  9.4527e-01,  7.1862e-01,  9.2197e-01,  8.7444e-01,
         4.7831e-01,  8.3498e-01,  8.4007e-01,  8.3663e-01,  9.5211e-01,
         8.0267e-01,  5.3524e-01,  9.5558e-01,  8.6690e-01,  4.9995e-01,
         9.1473e-01,  6.9337e-01,  6.5632e-01,  8.0265e-01,  9.3483e-01,
         9.5106e-01,  6.6761e-01,  6.9286e-01,  8.6663e-01,  8.5621e-01,
         1.0289e+00,  9.5503e-01,  9.4377e-01,  8.8051e-01,  1.0736e+00,
         8.5793e-01,  8.5913e-01,  4.1849e-01,  5.4720e-01,  8.2527e-01,
         9.4540e-01,  1.0242e+00,  8.6908e-01,  7.0331e-01,  5.8201e-01,
         7.7062e-01,  6.6380e-01,  3.3656e-01,  8.1090e-01,  5.2434e-01,
         6.7309e-01,  8.1919e-01,  6.3413e-01,  3.8254e-01,  7.3788e-01,
         3.2474e-01,  8.3262e-01,  6.7605e-01,  8.9391e-01,  7.7689e-01,
         7.3950e-01,  7.2330e-01,  3.4247e-01,  5.9515e-01,  7.1021e-01,
         5.6312e-01,  6.1464e-01,  6.9234e-01,  5.8713e-01,  5.4631e-01,
         6.1392e-01,  7.0131e-01,  7.5228e-01,  4.3981e-01,  5.9777e-01,
         7.4264e-01,  6.1417e-01,  6.8954e-01,  6.1034e-01,  8.3153e-01,
         5.7676e-01,  4.4210e-01,  6.1350e-01,  6.2676e-01,  7.1901e-01,
         6.4285e-01,  6.5280e-01,  1.8995e-01,  7.8951e-01,  3.0345e-01,
         7.2063e-01,  6.1404e-01,  6.7549e-01,  7.3469e-01,  8.1321e-01,
         7.2450e-01,  6.4448e-01,  7.6348e-01,  2.8411e-01,  6.6504e-01,
         4.0611e-01,  6.6563e-01,  5.6220e-01,  4.5932e-01,  1.6164e-01,
         5.1018e-01,  5.4937e-01,  5.9582e-01,  4.5204e-01,  1.7854e-01,
         9.8517e-02,  2.0309e-01,  1.1116e-03,  4.2923e-01,  4.1218e-01,
         4.4355e-01,  3.6596e-01,  2.7029e-01,  3.2578e-01,  3.0718e-01,
        -7.5653e-02,  3.5380e-01,  3.3019e-01,  4.9071e-01,  5.0023e-02,
         1.1400e-01,  1.6652e-01,  4.0056e-01,  3.9373e-01,  1.7003e-01,
        -2.7650e-02,  2.2456e-01,  4.4012e-01,  2.6095e-01,  1.3541e-01,
         2.0777e-01,  1.6061e-01,  1.8749e-01,  2.4338e-01, -1.2879e-02,
         3.7268e-01,  3.8018e-01,  2.9614e-01,  1.5249e-01,  3.1095e-01,
         2.8204e-01,  2.7258e-01,  1.8335e-01,  3.0116e-01,  2.3936e-01,
         1.7479e-01,  1.2665e-02,  5.8432e-03,  2.4300e-01,  1.7520e-01,
         1.9209e-01, -3.0692e-01,  2.5946e-01,  4.9263e-01,  3.1654e-01,
         4.0344e-01,  2.9494e-01,  2.8737e-01,  4.5890e-01,  2.7407e-01,
         3.5764e-01,  1.4253e-01,  2.8324e-01,  3.3659e-01,  3.4764e-01,
        -4.4391e-03,  9.2329e-02, -1.5563e-01, -1.0425e-01,  3.3343e-01,
        -1.5216e-01,  3.2368e-01, -1.2823e-01,  2.6724e-01,  2.7823e-01,
         2.4331e-01,  2.3405e-01,  8.5361e-02,  8.1398e-02,  2.3474e-01,
         1.8329e-01,  3.5224e-01, -8.2933e-02,  5.5658e-02,  1.7265e-01,
         1.3263e-01, -1.1702e-01,  2.7878e-01,  2.0790e-01,  3.0626e-01,
         5.0739e-02,  3.9538e-02,  1.3060e-01,  5.9456e-02, -3.8822e-01,
         6.1578e-02, -4.4923e-02,  7.4192e-02, -7.2059e-02,  8.3443e-02,
         1.6353e-02, -2.5382e-01, -4.0817e-01, -1.6969e-01, -4.6061e-01,
        -2.5586e-01, -3.5955e-01, -1.8786e-01,  5.6797e-02, -9.4610e-02,
         2.7668e-02, -3.5560e-01, -5.1792e-01, -1.6820e-01,  9.9337e-02,
        -3.1089e-01, -4.2862e-01, -4.2824e-01, -2.9199e-01, -2.1062e-01,
        -1.6284e-01, -4.4183e-01, -6.3518e-01, -1.8372e-01, -2.3192e-01,
        -2.7682e-01, -1.9526e-01, -2.4314e-01, -2.5494e-01, -3.8360e-01,
        -3.3573e-01, -5.7059e-01, -8.9881e-01, -6.0830e-01, -7.0066e-01,
        -5.0974e-01, -4.3123e-01, -4.1094e-01, -1.0923e+00, -8.5397e-01,
        -5.2527e-01, -6.3839e-01, -8.1959e-01, -7.1048e-01, -8.2983e-01,
        -7.0662e-01, -7.0312e-01, -8.3222e-01, -9.6635e-01, -9.7076e-01,
        -9.1929e-01, -7.2385e-01, -8.3495e-01, -7.8885e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1501e+00, -9.9268e-01, -1.3504e+00, -2.3448e+00, -2.4753e+00,
        -2.9423e+00, -2.7248e+00, -2.5005e+00, -2.8237e+00, -2.8337e+00,
        -2.3917e+00, -2.6669e+00, -2.4920e+00, -2.5264e+00, -2.4779e+00,
        -2.3771e+00, -2.1565e+00, -2.3300e+00, -2.2893e+00, -2.3380e+00,
        -2.2645e+00, -2.1658e+00, -8.4856e-01, -1.6796e-01, -1.6569e+00,
        -1.7979e+00, -1.5517e+00, -1.6871e+00, -1.5987e+00, -1.3938e+00,
        -1.8689e+00, -1.9237e+00, -1.5904e+00, -1.9420e+00, -1.6839e+00,
        -1.5805e+00, -1.5616e+00, -1.7008e+00, -1.5526e+00, -1.5787e+00,
        -1.5179e+00, -1.4406e+00, -1.4411e+00, -1.5556e+00, -1.7148e+00,
        -1.4766e+00, -1.4375e+00, -1.5591e+00, -1.6307e+00, -1.7197e+00,
        -1.3877e+00, -1.5290e+00, -1.5168e+00, -1.8759e+00, -1.2590e+00,
        -1.2695e+00, -1.1063e+00, -1.6072e+00, -1.4118e+00, -1.3842e+00,
        -1.2247e+00, -1.5248e+00, -1.0801e+00, -1.2799e+00, -1.5264e+00,
        -1.1993e+00,  1.2217e+00,  1.3388e+00,  2.1068e+00,  2.5717e+00,
         5.1175e+00,  1.0315e-02,  2.9069e-01,  1.7991e-01, -5.7464e-02,
         1.7212e-01,  2.2588e-01,  2.7915e-01,  3.1937e-01,  3.1408e-01,
         2.7363e-01,  4.1615e-01,  3.4627e-01,  2.9556e-01,  4.5768e-01,
         1.2685e-01, -3.2136e-02,  1.9303e-01,  5.0347e-01,  2.7852e-01,
         1.6010e-01,  1.3589e-01,  2.3960e-01,  2.7930e-01,  3.8390e-01,
         9.1655e-02,  2.2302e-01,  2.9861e-01,  2.8244e-01,  2.1123e-01,
         2.8171e-01,  4.0144e-01,  2.3019e-01,  3.8576e-01, -1.2195e-01,
         8.7961e-02, -1.1190e-01,  4.2473e-02,  1.2849e-01,  1.9259e-01,
         2.6550e-01,  3.2309e-01,  1.8685e-01,  1.8892e+00,  2.9226e+00,
         4.0062e+00,  6.9097e-01,  9.6047e-01,  9.3307e-01,  9.8544e-01,
         9.7815e-01,  9.4419e-01,  8.8299e-01,  9.1218e-01,  8.1909e-01,
         8.4157e-01,  7.3530e-01,  7.9034e-01,  9.0579e-01,  5.6602e-01,
         6.0467e-01,  7.7827e-01,  4.7815e-01,  7.9688e-01,  2.9140e-01,
         8.7350e-01,  7.0099e-01,  8.9790e-01,  9.0383e-01,  8.6184e-01,
         1.0222e+00,  1.0376e+00,  5.9300e-01,  9.4254e-01,  1.0263e+00,
         1.0301e+00,  5.9718e-01,  7.7834e-01,  9.3485e-01,  1.1691e+00,
         5.4256e-01,  9.4527e-01,  7.1862e-01,  9.2197e-01,  8.7946e-01,
         4.7831e-01,  8.3498e-01,  8.4007e-01,  8.3663e-01,  9.5211e-01,
         7.9559e-01,  5.3524e-01,  9.5558e-01,  8.6690e-01,  4.9995e-01,
         9.1473e-01,  6.9337e-01,  6.5632e-01,  8.0265e-01,  9.3483e-01,
         9.5106e-01,  6.6761e-01,  6.9286e-01,  8.6663e-01,  8.5621e-01,
         1.0289e+00,  9.5503e-01,  9.4377e-01,  8.8051e-01,  1.0736e+00,
         8.5793e-01,  8.5913e-01,  4.1136e-01,  5.4720e-01,  8.2527e-01,
         9.4540e-01,  1.0242e+00,  8.6908e-01,  7.0331e-01,  5.8201e-01,
         7.7062e-01,  4.7662e-01,  3.6623e-01,  8.1090e-01,  5.2434e-01,
         6.7309e-01,  8.1919e-01,  6.3413e-01,  3.8254e-01,  7.3710e-01,
         3.2474e-01,  8.3262e-01,  6.7605e-01,  8.9391e-01,  7.7689e-01,
         7.3950e-01,  7.2330e-01,  3.4247e-01,  5.9515e-01,  7.1021e-01,
         5.6312e-01,  6.1464e-01,  6.9234e-01,  5.8713e-01,  5.4631e-01,
         6.1392e-01,  7.0131e-01,  7.5228e-01,  4.3981e-01,  5.9777e-01,
         7.4264e-01,  6.1417e-01,  6.8954e-01,  6.1034e-01,  8.3153e-01,
         5.7676e-01,  4.5092e-01,  6.1350e-01,  6.2676e-01,  7.1901e-01,
         6.4285e-01,  6.5280e-01,  1.8995e-01,  7.8951e-01,  3.0345e-01,
         7.2063e-01,  6.1404e-01,  6.7549e-01,  7.3469e-01,  8.1321e-01,
         7.2450e-01,  6.4448e-01,  7.6348e-01,  2.8411e-01,  6.6504e-01,
         4.0611e-01,  6.6563e-01,  5.6220e-01,  4.5932e-01,  1.6164e-01,
         5.1018e-01,  5.4937e-01,  5.9582e-01,  4.5204e-01,  1.7854e-01,
         1.0747e-01,  2.0309e-01,  1.1116e-03,  4.2923e-01,  4.1218e-01,
         4.4355e-01,  3.6596e-01,  2.7029e-01,  3.2578e-01,  3.0718e-01,
        -8.0855e-02,  3.5380e-01,  3.3019e-01,  4.9071e-01,  5.2668e-02,
         1.1400e-01,  1.6652e-01,  4.0056e-01,  3.9373e-01,  1.7003e-01,
        -2.7650e-02,  2.2456e-01,  4.4012e-01,  2.6095e-01,  1.3541e-01,
         2.0777e-01,  1.6061e-01,  1.8749e-01,  2.1334e-01, -1.2879e-02,
         3.7268e-01,  3.8018e-01,  2.9614e-01,  1.5249e-01,  3.1095e-01,
         2.8204e-01,  2.7258e-01,  1.8335e-01,  3.0116e-01,  2.3936e-01,
         1.7479e-01,  1.2665e-02,  5.8432e-03,  2.4300e-01,  1.7520e-01,
         1.9209e-01, -3.0692e-01,  2.5946e-01,  4.9263e-01,  3.1654e-01,
         4.0344e-01,  2.9494e-01,  2.8737e-01,  4.5890e-01,  2.7407e-01,
         3.5764e-01,  1.4253e-01,  2.8324e-01,  3.3659e-01,  3.4823e-01,
        -4.4391e-03,  9.2211e-02, -1.0053e-01, -1.0941e-01,  3.3343e-01,
        -1.5216e-01,  3.2368e-01, -1.2823e-01,  2.6724e-01,  2.7823e-01,
         2.4331e-01,  2.3405e-01,  8.5361e-02,  8.1398e-02,  2.3474e-01,
         1.8329e-01,  3.5224e-01, -8.2933e-02,  5.5658e-02,  1.7265e-01,
         1.3263e-01, -1.1702e-01,  2.7878e-01,  2.0790e-01,  3.0626e-01,
         5.0739e-02,  3.9538e-02,  1.3060e-01,  5.6121e-02, -3.8822e-01,
         6.2027e-02, -4.4923e-02,  7.4192e-02, -7.2059e-02,  8.3443e-02,
         1.6353e-02, -2.5382e-01, -4.3320e-01, -1.6969e-01, -4.6061e-01,
        -2.5586e-01, -3.5955e-01, -1.8786e-01,  5.6797e-02, -9.4610e-02,
         2.7668e-02, -3.5560e-01, -5.1792e-01, -1.6820e-01,  9.9337e-02,
        -3.1689e-01, -4.2862e-01, -4.2824e-01, -2.9199e-01, -2.1062e-01,
        -1.6284e-01, -3.8447e-01, -6.3518e-01, -1.8372e-01, -2.3192e-01,
        -2.7682e-01, -1.9526e-01, -2.4314e-01, -2.5494e-01, -3.8360e-01,
        -3.3573e-01, -5.7059e-01, -8.9881e-01, -5.9353e-01, -7.0066e-01,
        -5.0974e-01, -4.3123e-01, -4.1094e-01, -1.0896e+00, -8.5397e-01,
        -5.2527e-01, -6.3839e-01, -9.2498e-01, -7.1048e-01, -8.2983e-01,
        -7.0662e-01, -7.0312e-01, -9.4607e-01, -9.6635e-01, -9.7076e-01,
        -9.1929e-01, -7.2385e-01, -8.3495e-01, -7.8885e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809436
t6: 1641198809436
state_values: tensor([0.9765, 0.8575, 0.9267, 1.0484, 1.1725, 1.2648, 1.3279, 1.3705, 1.4011,
        1.4475, 1.5166, 1.5018, 1.5178, 1.5302, 1.5758, 1.7001, 1.6716, 1.6421,
        1.6854, 1.7067, 1.7351, 1.7390, 1.6147, 1.5640, 1.6817, 1.7073, 1.7239,
        1.7922, 1.7680, 1.7730, 1.9306, 1.8980, 1.9071, 1.8587, 1.8502, 1.8545,
        1.8650, 1.8712, 1.9093, 1.9028, 1.9305, 1.9336, 1.9404, 2.0195, 1.9747,
        1.9771, 1.9784, 2.0355, 2.0075, 2.0030, 2.0184, 2.0646, 2.0430, 2.1438,
        2.0991, 2.0852, 2.1147, 2.2224, 2.1422, 2.2012, 2.1509, 2.1405, 2.1386,
        2.1694, 2.1596, 2.1737, 2.0315, 1.9904, 1.9586, 1.9814, 1.9326, 2.0852,
        2.1620, 2.1770, 2.1894, 2.2651, 2.2559, 2.2640, 2.2609, 2.2774, 2.2654,
        2.2666, 2.2840, 2.2828, 2.3128, 2.3967, 2.3330, 2.3249, 2.3594, 2.3883,
        2.3568, 2.4134, 2.3847, 2.3720, 2.3656, 2.3716, 2.4395, 2.4226, 2.4141,
        2.4542, 2.4434, 2.4194, 2.4198, 2.4256, 2.4247, 2.4325, 2.4385, 2.4515,
        2.4573, 2.4622, 2.4986, 2.5249, 2.5277, 2.4001, 2.3222, 2.2704, 2.3975,
        2.4910, 2.4857, 2.5090, 2.5077, 2.5063, 2.5163, 2.5181, 2.5822, 2.5515,
        2.6169, 2.5753, 2.5594, 2.5595, 2.6778, 2.6318, 2.5900, 2.6172, 2.7689,
        2.6783, 2.7167, 2.6628, 2.6639, 2.6817, 2.6466, 2.6359, 2.6294, 2.6455,
        2.6596, 2.6621, 2.6445, 2.6496, 2.7067, 2.6724, 2.8012, 2.7197, 2.6936,
        2.6824, 2.6818, 2.8256, 2.7751, 2.7747, 2.7271, 2.7146, 2.7136, 2.7176,
        2.7125, 2.7286, 2.7135, 2.7325, 2.7340, 2.7876, 2.7482, 2.7363, 2.7424,
        2.7453, 2.7494, 2.7742, 2.7580, 2.7493, 2.7517, 2.7765, 2.7747, 2.7703,
        2.7620, 2.8058, 2.7786, 2.7800, 2.7778, 2.7852, 2.7760, 2.8303, 2.8935,
        2.9074, 2.8384, 2.8172, 2.8033, 2.8638, 2.8315, 2.9159, 2.9012, 2.9196,
        2.8675, 2.8961, 2.8657, 2.8530, 2.8539, 2.8641, 2.8973, 2.9259, 2.9116,
        2.8854, 2.8730, 2.9127, 2.9731, 2.9045, 2.8984, 2.9449, 2.9713, 2.9174,
        2.9025, 2.8939, 2.8961, 2.9548, 2.9141, 2.9788, 2.9640, 2.9407, 2.9160,
        2.9120, 2.9147, 2.9917, 2.9392, 2.9534, 2.9590, 2.9718, 2.9391, 2.9604,
        2.9377, 2.9515, 2.9399, 2.9343, 2.9514, 2.9326, 2.9348, 2.9491, 2.9546,
        2.9514, 2.9449, 2.9526, 2.9484, 2.9922, 2.9748, 2.9697, 3.0048, 3.0203,
        3.0098, 3.0242, 3.1074, 3.0228, 3.1301, 3.0378, 3.0392, 3.0504, 3.0558,
        3.0779, 3.1203, 3.0786, 3.0848, 3.0453, 3.0824, 3.0420, 3.0689, 3.0354,
        3.1423, 3.0549, 3.0551, 3.0640, 3.1500, 3.0735, 3.0579, 3.0674, 3.0469,
        3.1705, 3.1358, 3.1630, 3.1039, 3.0721, 3.0537, 3.0553, 3.0564, 3.0595,
        3.0598, 3.0954, 3.0733, 3.0998, 3.0772, 3.1205, 3.1219, 3.0891, 3.0839,
        3.0775, 3.0736, 3.0832, 3.0840, 3.1435, 3.1641, 3.1155, 3.1023, 3.0963,
        3.0989, 3.1717, 3.1344, 3.1947, 3.1501, 3.2188, 3.1667, 3.1403, 3.1541,
        3.2690, 3.1679, 3.1337, 3.1248, 3.1717, 3.1405, 3.1752, 3.1412, 3.1365,
        3.1310, 3.1317, 3.1277, 3.2266, 3.1696, 3.2282, 3.1696, 3.1439, 3.1514,
        3.1482, 3.1996, 3.1639, 3.1498, 3.1445, 3.1580, 3.1597, 3.2251, 3.2344,
        3.1929, 3.1921, 3.1711, 3.1659, 3.2552, 3.1979, 3.1819, 3.1897, 3.1855,
        3.2757, 3.2064, 3.2770, 3.3675, 3.2524, 3.2238, 3.2130, 3.1993, 3.2069,
        3.1903, 3.3160, 3.2343, 3.2068, 3.1871, 3.1972, 3.3286, 3.3386, 3.2733,
        3.2839, 3.2460, 3.2256, 3.2154, 3.2180, 3.2142, 3.2137, 3.2530, 3.2446,
        3.2447, 3.2716, 3.2399, 3.3503, 3.2640, 3.2456, 3.2362, 3.2374, 3.2576,
        3.2679, 3.4187, 3.2993, 3.2981, 3.3046, 3.2677, 3.3403, 3.3740, 3.3306,
        3.2925, 3.2786, 3.3667, 3.3685, 3.3526, 3.2925, 3.3209, 3.3078],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809440
t8: 1641198809440
t9: 1641198809441
t10: 1641198809451
t11: 1641198809453
t12: 1641198809453
t1: 1641198809453
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809464
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9966, 1.0131, 0.9075, 0.9380, 0.8758, 1.0002, 0.9746, 0.8857, 1.0028,
        1.0754, 0.9031, 1.0055, 0.9948, 0.9820, 1.0187, 0.9159, 0.8751, 0.8731,
        0.9399, 1.0044, 1.0189, 0.9916, 1.0048, 0.9478, 1.0149, 0.9970, 0.9372,
        0.9990, 0.9997, 0.8584, 0.9136, 1.0932, 0.9208, 1.0208, 1.0419, 0.9914,
        1.0429, 0.9450, 1.0093, 0.9990, 0.9991, 1.0000, 0.9649, 0.9575, 0.9834,
        1.0235, 0.9405, 1.0045, 1.0516, 0.9763, 0.9694, 0.9882, 0.9907, 0.9966,
        1.0077, 0.9608, 0.9576, 1.0706, 0.8937, 0.9898, 0.9814, 0.9886, 0.9726,
        1.0145, 0.9788, 1.0142, 1.0562, 1.0416, 0.9876, 1.0670, 1.1087, 0.9919,
        1.0069, 1.0239, 0.9026, 1.0055, 1.0236, 0.9985, 0.9882, 1.0155, 0.9978,
        0.9920, 1.0099, 0.9717, 0.9769, 0.9656, 1.0359, 0.8928, 0.9888, 0.9785,
        0.9200, 1.0010, 1.0040, 1.0241, 1.0681, 0.9002, 1.0016, 1.0013, 0.9664,
        0.9922, 1.0059, 1.0402, 0.9986, 1.0994, 1.0891, 1.0799, 1.0310, 0.9950,
        0.9827, 1.0250, 1.0577, 1.0287, 0.9831, 1.0867, 1.0946, 1.1265, 0.9594,
        0.9791, 1.0214, 1.0001, 0.9974, 1.0024, 1.0036, 0.9748, 0.9933, 0.9895,
        0.9898, 1.0285, 1.1207, 0.8000, 0.9962, 0.8994, 0.9213, 0.8631, 0.9718,
        1.0784, 1.0010, 1.0120, 0.9747, 1.0004, 1.0147, 1.1458, 0.9704, 0.9838,
        1.0030, 0.9845, 1.0616, 0.9384, 0.9978, 1.0943, 0.9990, 0.9746, 1.0545,
        1.0135, 0.9001, 0.9731, 1.0615, 0.9877, 1.0104, 1.0695, 1.1073, 1.0016,
        0.9971, 0.9670, 0.9807, 1.0026, 1.0057, 0.9863, 1.0007, 0.9984, 1.0112,
        1.0230, 0.9802, 0.9982, 1.0000, 1.0003, 0.9932, 0.9992, 0.9956, 1.0115,
        0.9620, 1.0650, 1.1142, 1.0310, 0.9992, 0.9943, 1.0319, 1.0548, 1.0344,
        0.9817, 1.0865, 1.5286, 0.8294, 1.0070, 1.0172, 1.0203, 1.0604, 0.9803,
        0.9191, 1.1131, 0.9970, 1.0234, 0.9934, 1.0022, 1.0172, 1.0093, 1.0130,
        1.0604, 0.9281, 0.9753, 0.9774, 0.9949, 0.9445, 0.9707, 0.9959, 1.0085,
        1.0341, 1.0668, 0.9276, 1.0009, 1.0087, 1.0087, 1.0003, 1.0006, 1.0648,
        1.0750, 0.8887, 0.9859, 1.0020, 0.9897, 0.9874, 1.0792, 0.9192, 1.0778,
        0.9820, 1.0025, 0.9981, 0.9983, 0.9980, 0.9993, 0.9994, 0.9967, 1.0111,
        1.0238, 1.0344, 1.0052, 1.0036, 0.9860, 0.9966, 0.9563, 0.9974, 1.0130,
        1.0074, 0.9780, 0.9504, 0.8283, 1.0100, 0.9730, 0.9709, 0.9870, 0.9847,
        0.9932, 1.0063, 0.9989, 1.0904, 0.8435, 1.0451, 0.9793, 1.0259, 0.8656,
        0.9815, 0.9950, 0.9858, 0.9708, 0.9547, 1.0284, 0.9628, 1.0272, 0.9212,
        0.9873, 1.0778, 0.9944, 1.0153, 1.2580, 1.0260, 1.0049, 0.9945, 0.9879,
        0.9891, 0.9966, 1.0034, 1.0004, 0.9667, 1.0002, 0.9958, 1.0509, 1.0879,
        1.0197, 1.0008, 0.9993, 0.9898, 0.9653, 0.9998, 0.9869, 1.0184, 1.0247,
        0.9562, 1.0052, 1.0404, 1.0033, 0.9869, 1.0029, 0.9998, 0.9620, 0.8898,
        0.9726, 1.0935, 1.6970, 0.8599, 1.0078, 0.9852, 0.9373, 1.0036, 0.9991,
        0.9913, 0.9932, 1.0001, 0.9651, 1.0248, 0.9801, 1.0063, 1.0880, 1.0779,
        0.9421, 0.9919, 0.9909, 1.0125, 0.9992, 1.0043, 1.0298, 1.0352, 1.0021,
        0.9929, 1.1726, 1.0493, 0.8984, 0.9939, 0.9744, 0.9828, 0.9997, 0.9647,
        0.9681, 0.8555, 1.0214, 0.9991, 1.0203, 1.0348, 0.9931, 0.9991, 1.0018,
        0.9588, 0.9980, 1.0344, 1.0070, 1.0438, 0.8969, 0.9765, 1.0046, 0.9866,
        0.9992, 1.0219, 1.2666, 0.9832, 1.0151, 0.9946, 1.0027, 1.0056, 1.0059,
        0.9892, 1.0034, 0.9129, 0.9795, 1.0606, 1.1243, 1.0407, 0.9909, 1.0129,
        1.0874, 1.0960, 0.9691, 0.9840, 1.0276, 0.8024, 0.9956, 1.0220, 1.0018,
        1.0477, 0.7897, 0.9996, 1.0388, 1.0009, 0.9601, 0.9867, 0.9141],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809468
t4: 1641198809468
surr1, surr2: tensor([-3.1509e+00, -9.9380e-01, -1.3411e+00, -2.3216e+00, -2.4051e+00,
        -2.9449e+00, -2.7123e+00, -2.4500e+00, -2.8324e+00, -2.8644e+00,
        -2.3655e+00, -2.6715e+00, -2.4761e+00, -2.5390e+00, -2.4982e+00,
        -2.3858e+00, -2.0969e+00, -2.2603e+00, -2.2666e+00, -2.3236e+00,
        -2.2593e+00, -2.1669e+00, -8.4854e-01, -1.6738e-01, -1.6595e+00,
        -1.7988e+00, -1.5572e+00, -1.6882e+00, -1.5995e+00, -1.3294e+00,
        -1.8798e+00, -1.9271e+00, -1.5983e+00, -1.9422e+00, -1.6880e+00,
        -1.5811e+00, -1.5519e+00, -1.7101e+00, -1.5497e+00, -1.5858e+00,
        -1.5197e+00, -1.4410e+00, -1.4511e+00, -1.5498e+00, -1.7157e+00,
        -1.4778e+00, -1.4435e+00, -1.5614e+00, -1.6148e+00, -1.7218e+00,
        -1.3913e+00, -1.5252e+00, -1.5259e+00, -1.8770e+00, -1.2557e+00,
        -1.2755e+00, -1.1066e+00, -1.5978e+00, -1.4019e+00, -1.3872e+00,
        -1.2214e+00, -1.5258e+00, -1.0805e+00, -1.2742e+00, -1.5306e+00,
        -1.1956e+00,  1.2097e+00,  1.3308e+00,  2.1172e+00,  2.6083e+00,
         5.1579e+00,  1.0389e-02,  2.8970e-01,  1.7782e-01, -5.7631e-02,
         1.7212e-01,  2.2669e-01,  2.7947e-01,  3.1994e-01,  3.1384e-01,
         2.7370e-01,  4.1632e-01,  3.4593e-01,  2.9668e-01,  4.5828e-01,
         1.2594e-01, -3.2095e-02,  1.9149e-01,  5.0054e-01,  2.7890e-01,
         1.6069e-01,  1.3594e-01,  2.3960e-01,  2.7919e-01,  3.8301e-01,
         9.1674e-02,  2.2296e-01,  2.9871e-01,  2.8370e-01,  2.1094e-01,
         2.8176e-01,  4.0082e-01,  2.3024e-01,  3.8555e-01, -1.2133e-01,
         8.7055e-02, -1.1119e-01,  4.2191e-02,  1.2770e-01,  1.9394e-01,
         2.6779e-01,  3.2372e-01,  1.8606e-01,  1.8825e+00,  2.9126e+00,
         4.1027e+00,  6.9859e-01,  9.5758e-01,  9.3741e-01,  9.8539e-01,
         9.7619e-01,  9.4366e-01,  8.8251e-01,  9.1738e-01,  8.1931e-01,
         8.4903e-01,  7.3693e-01,  7.8924e-01,  9.2285e-01,  5.0314e-01,
         6.0613e-01,  7.6726e-01,  4.8029e-01,  7.6425e-01,  2.9166e-01,
         8.8122e-01,  7.0099e-01,  8.9894e-01,  9.0106e-01,  8.6188e-01,
         1.0227e+00,  1.0808e+00,  5.9335e-01,  9.4351e-01,  1.0280e+00,
         1.0169e+00,  5.9472e-01,  7.8391e-01,  9.3461e-01,  1.1930e+00,
         5.4266e-01,  9.4184e-01,  7.1821e-01,  9.2191e-01,  8.7955e-01,
         4.7912e-01,  8.3702e-01,  8.4205e-01,  8.3642e-01,  9.5431e-01,
         8.0086e-01,  5.3532e-01,  9.5644e-01,  8.5444e-01,  5.0144e-01,
         9.1186e-01,  6.9949e-01,  6.5424e-01,  8.0119e-01,  9.3529e-01,
         9.4741e-01,  6.6548e-01,  6.9544e-01,  8.6565e-01,  8.5624e-01,
         1.0287e+00,  9.5726e-01,  9.4405e-01,  8.8152e-01,  1.0724e+00,
         8.6065e-01,  8.5874e-01,  4.1668e-01,  5.4608e-01,  8.2608e-01,
         9.4398e-01,  1.0337e+00,  8.7279e-01,  7.0669e-01,  5.8192e-01,
         7.6734e-01,  6.6233e-01,  3.3752e-01,  8.1253e-01,  5.3474e-01,
         6.7726e-01,  8.2329e-01,  6.3592e-01,  3.8322e-01,  7.4585e-01,
         3.2486e-01,  8.3143e-01,  6.7823e-01,  8.9846e-01,  7.8024e-01,
         7.3965e-01,  7.2248e-01,  3.4197e-01,  5.9603e-01,  7.1029e-01,
         5.6187e-01,  6.1562e-01,  6.9339e-01,  5.8371e-01,  5.4774e-01,
         6.1381e-01,  7.0095e-01,  7.5024e-01,  4.4246e-01,  5.9789e-01,
         7.4861e-01,  6.1317e-01,  6.8955e-01,  6.1034e-01,  8.3327e-01,
         5.7542e-01,  4.4525e-01,  6.1342e-01,  6.2959e-01,  7.1991e-01,
         6.4379e-01,  6.5024e-01,  1.9046e-01,  7.9007e-01,  3.0431e-01,
         7.1781e-01,  6.1094e-01,  6.7727e-01,  7.3378e-01,  8.1247e-01,
         7.2477e-01,  6.4523e-01,  7.6004e-01,  2.8364e-01,  6.6206e-01,
         4.0552e-01,  6.6897e-01,  5.6048e-01,  4.5378e-01,  1.6254e-01,
         5.1146e-01,  5.5023e-01,  5.9754e-01,  4.5472e-01,  1.7900e-01,
         9.8906e-02,  2.0006e-01,  1.1113e-03,  4.2806e-01,  4.1186e-01,
         4.4331e-01,  3.6387e-01,  2.7008e-01,  3.2624e-01,  3.0946e-01,
        -7.5777e-02,  3.5511e-01,  3.3208e-01,  4.8872e-01,  5.0656e-02,
         1.1350e-01,  1.6686e-01,  4.0124e-01,  3.9650e-01,  1.6906e-01,
        -2.7623e-02,  2.2479e-01,  4.3961e-01,  2.6483e-01,  1.3564e-01,
         2.0841e-01,  1.6072e-01,  1.8719e-01,  2.4399e-01, -1.2878e-02,
         3.7258e-01,  3.7911e-01,  2.9348e-01,  1.5348e-01,  3.1042e-01,
         2.8317e-01,  2.7190e-01,  1.8454e-01,  3.0089e-01,  2.3888e-01,
         1.7411e-01,  1.2623e-02,  5.8360e-03,  2.4295e-01,  1.7512e-01,
         1.9098e-01, -3.0947e-01,  2.5946e-01,  4.9054e-01,  3.1568e-01,
         4.0240e-01,  2.9757e-01,  2.8738e-01,  4.6333e-01,  2.7398e-01,
         3.5842e-01,  1.4247e-01,  2.8327e-01,  3.3619e-01,  3.4429e-01,
        -4.4139e-03,  9.1668e-02, -1.5510e-01, -1.0453e-01,  3.3205e-01,
        -1.5385e-01,  3.1909e-01, -1.2803e-01,  2.6592e-01,  2.7656e-01,
         2.4146e-01,  2.3782e-01,  8.4791e-02,  8.2610e-02,  2.3440e-01,
         1.8306e-01,  3.5123e-01, -8.2710e-02,  5.5968e-02,  1.7238e-01,
         1.3071e-01, -1.1687e-01,  2.7891e-01,  2.0819e-01,  3.0916e-01,
         5.0844e-02,  3.9536e-02,  1.3063e-01,  5.9824e-02, -3.8800e-01,
         6.1917e-02, -4.4864e-02,  7.3655e-02, -7.2243e-02,  8.3448e-02,
         1.6438e-02, -2.5539e-01, -4.1178e-01, -1.6730e-01, -4.5963e-01,
        -2.5420e-01, -3.5963e-01, -1.8788e-01,  5.6804e-02, -9.4555e-02,
         2.7965e-02, -3.5360e-01, -5.1724e-01, -1.6819e-01,  9.9041e-02,
        -3.1579e-01, -4.3374e-01, -4.2829e-01, -2.9330e-01, -2.1053e-01,
        -1.6299e-01, -4.4270e-01, -6.3539e-01, -1.8354e-01, -2.3084e-01,
        -2.7929e-01, -1.9548e-01, -2.4357e-01, -2.5566e-01, -3.8353e-01,
        -3.3792e-01, -5.6397e-01, -8.9871e-01, -6.0664e-01, -6.9883e-01,
        -5.1134e-01, -4.3297e-01, -4.1849e-01, -1.0856e+00, -8.5599e-01,
        -5.2393e-01, -6.4142e-01, -8.2467e-01, -7.0276e-01, -8.3021e-01,
        -7.0688e-01, -7.0355e-01, -8.3016e-01, -9.5772e-01, -9.7020e-01,
        -9.1916e-01, -7.2381e-01, -8.3375e-01, -7.8410e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1509e+00, -9.9380e-01, -1.3411e+00, -2.3216e+00, -2.4716e+00,
        -2.9449e+00, -2.7123e+00, -2.4896e+00, -2.8324e+00, -2.8644e+00,
        -2.3655e+00, -2.6715e+00, -2.4761e+00, -2.5390e+00, -2.4982e+00,
        -2.3858e+00, -2.1565e+00, -2.3300e+00, -2.2666e+00, -2.3236e+00,
        -2.2593e+00, -2.1669e+00, -8.4854e-01, -1.6738e-01, -1.6595e+00,
        -1.7988e+00, -1.5572e+00, -1.6882e+00, -1.5995e+00, -1.3938e+00,
        -1.8798e+00, -1.9271e+00, -1.5983e+00, -1.9422e+00, -1.6880e+00,
        -1.5811e+00, -1.5519e+00, -1.7101e+00, -1.5497e+00, -1.5858e+00,
        -1.5197e+00, -1.4410e+00, -1.4511e+00, -1.5498e+00, -1.7157e+00,
        -1.4778e+00, -1.4435e+00, -1.5614e+00, -1.6148e+00, -1.7218e+00,
        -1.3913e+00, -1.5252e+00, -1.5259e+00, -1.8770e+00, -1.2557e+00,
        -1.2755e+00, -1.1066e+00, -1.5978e+00, -1.4118e+00, -1.3872e+00,
        -1.2214e+00, -1.5258e+00, -1.0805e+00, -1.2742e+00, -1.5306e+00,
        -1.1956e+00,  1.2097e+00,  1.3308e+00,  2.1172e+00,  2.6083e+00,
         5.1175e+00,  1.0389e-02,  2.8970e-01,  1.7782e-01, -5.7631e-02,
         1.7212e-01,  2.2669e-01,  2.7947e-01,  3.1994e-01,  3.1384e-01,
         2.7370e-01,  4.1632e-01,  3.4593e-01,  2.9668e-01,  4.5828e-01,
         1.2594e-01, -3.2095e-02,  1.9303e-01,  5.0054e-01,  2.7890e-01,
         1.6069e-01,  1.3594e-01,  2.3960e-01,  2.7919e-01,  3.8301e-01,
         9.1674e-02,  2.2296e-01,  2.9871e-01,  2.8370e-01,  2.1094e-01,
         2.8176e-01,  4.0082e-01,  2.3024e-01,  3.8555e-01, -1.2133e-01,
         8.7055e-02, -1.1119e-01,  4.2191e-02,  1.2770e-01,  1.9394e-01,
         2.6779e-01,  3.2372e-01,  1.8606e-01,  1.8825e+00,  2.9126e+00,
         4.0062e+00,  6.9859e-01,  9.5758e-01,  9.3741e-01,  9.8539e-01,
         9.7619e-01,  9.4366e-01,  8.8251e-01,  9.1738e-01,  8.1931e-01,
         8.4903e-01,  7.3693e-01,  7.8924e-01,  9.0579e-01,  5.6602e-01,
         6.0613e-01,  7.6775e-01,  4.8029e-01,  7.9688e-01,  2.9166e-01,
         8.8122e-01,  7.0099e-01,  8.9894e-01,  9.0106e-01,  8.6188e-01,
         1.0227e+00,  1.0376e+00,  5.9335e-01,  9.4351e-01,  1.0280e+00,
         1.0169e+00,  5.9472e-01,  7.8391e-01,  9.3461e-01,  1.1930e+00,
         5.4266e-01,  9.4184e-01,  7.1821e-01,  9.2191e-01,  8.7955e-01,
         4.7912e-01,  8.3702e-01,  8.4205e-01,  8.3642e-01,  9.5431e-01,
         7.9559e-01,  5.3532e-01,  9.5644e-01,  8.5444e-01,  5.0144e-01,
         9.1186e-01,  6.9949e-01,  6.5424e-01,  8.0119e-01,  9.3529e-01,
         9.4741e-01,  6.6548e-01,  6.9544e-01,  8.6565e-01,  8.5624e-01,
         1.0287e+00,  9.5726e-01,  9.4405e-01,  8.8152e-01,  1.0724e+00,
         8.6065e-01,  8.5874e-01,  4.1136e-01,  5.4608e-01,  8.2608e-01,
         9.4398e-01,  1.0337e+00,  8.7279e-01,  7.0669e-01,  5.8192e-01,
         7.6734e-01,  4.7662e-01,  3.6623e-01,  8.1253e-01,  5.3474e-01,
         6.7726e-01,  8.2329e-01,  6.3592e-01,  3.8322e-01,  7.3710e-01,
         3.2486e-01,  8.3143e-01,  6.7823e-01,  8.9846e-01,  7.8024e-01,
         7.3965e-01,  7.2248e-01,  3.4197e-01,  5.9603e-01,  7.1029e-01,
         5.6187e-01,  6.1562e-01,  6.9339e-01,  5.8371e-01,  5.4774e-01,
         6.1381e-01,  7.0095e-01,  7.5024e-01,  4.4246e-01,  5.9789e-01,
         7.4861e-01,  6.1317e-01,  6.8955e-01,  6.1034e-01,  8.3327e-01,
         5.7542e-01,  4.5092e-01,  6.1342e-01,  6.2959e-01,  7.1991e-01,
         6.4379e-01,  6.5024e-01,  1.9046e-01,  7.9007e-01,  3.0431e-01,
         7.1781e-01,  6.1094e-01,  6.7727e-01,  7.3378e-01,  8.1247e-01,
         7.2477e-01,  6.4523e-01,  7.6004e-01,  2.8364e-01,  6.6206e-01,
         4.0552e-01,  6.6897e-01,  5.6048e-01,  4.5378e-01,  1.6254e-01,
         5.1146e-01,  5.5023e-01,  5.9754e-01,  4.5472e-01,  1.7900e-01,
         1.0747e-01,  2.0006e-01,  1.1113e-03,  4.2806e-01,  4.1186e-01,
         4.4331e-01,  3.6387e-01,  2.7008e-01,  3.2624e-01,  3.0946e-01,
        -8.0855e-02,  3.5511e-01,  3.3208e-01,  4.8872e-01,  5.2668e-02,
         1.1350e-01,  1.6686e-01,  4.0124e-01,  3.9650e-01,  1.6906e-01,
        -2.7623e-02,  2.2479e-01,  4.3961e-01,  2.6483e-01,  1.3564e-01,
         2.0841e-01,  1.6072e-01,  1.8719e-01,  2.1334e-01, -1.2878e-02,
         3.7258e-01,  3.7911e-01,  2.9348e-01,  1.5348e-01,  3.1042e-01,
         2.8317e-01,  2.7190e-01,  1.8454e-01,  3.0089e-01,  2.3888e-01,
         1.7411e-01,  1.2623e-02,  5.8360e-03,  2.4295e-01,  1.7512e-01,
         1.9098e-01, -3.0947e-01,  2.5946e-01,  4.9054e-01,  3.1568e-01,
         4.0240e-01,  2.9757e-01,  2.8738e-01,  4.6333e-01,  2.7398e-01,
         3.5842e-01,  1.4247e-01,  2.8327e-01,  3.3619e-01,  3.4823e-01,
        -4.4139e-03,  9.1668e-02, -1.0053e-01, -1.0941e-01,  3.3205e-01,
        -1.5385e-01,  3.1909e-01, -1.2803e-01,  2.6592e-01,  2.7656e-01,
         2.4146e-01,  2.3782e-01,  8.4791e-02,  8.2610e-02,  2.3440e-01,
         1.8306e-01,  3.5123e-01, -8.2710e-02,  5.5968e-02,  1.7238e-01,
         1.3071e-01, -1.1687e-01,  2.7891e-01,  2.0819e-01,  3.0916e-01,
         5.0844e-02,  3.9536e-02,  1.3063e-01,  5.6121e-02, -3.8800e-01,
         6.2027e-02, -4.4864e-02,  7.3655e-02, -7.2243e-02,  8.3448e-02,
         1.6438e-02, -2.5539e-01, -4.3320e-01, -1.6730e-01, -4.5963e-01,
        -2.5420e-01, -3.5963e-01, -1.8788e-01,  5.6804e-02, -9.4555e-02,
         2.7965e-02, -3.5360e-01, -5.1724e-01, -1.6819e-01,  9.9041e-02,
        -3.1689e-01, -4.3374e-01, -4.2829e-01, -2.9330e-01, -2.1053e-01,
        -1.6299e-01, -3.8447e-01, -6.3539e-01, -1.8354e-01, -2.3084e-01,
        -2.7929e-01, -1.9548e-01, -2.4357e-01, -2.5566e-01, -3.8353e-01,
        -3.3792e-01, -5.6397e-01, -8.9871e-01, -5.9353e-01, -6.9883e-01,
        -5.1134e-01, -4.3297e-01, -4.1849e-01, -1.0856e+00, -8.5599e-01,
        -5.2393e-01, -6.4142e-01, -9.2498e-01, -7.0276e-01, -8.3021e-01,
        -7.0688e-01, -7.0355e-01, -9.4607e-01, -9.5772e-01, -9.7020e-01,
        -9.1916e-01, -7.2381e-01, -8.3375e-01, -7.8410e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809478
t6: 1641198809478
state_values: tensor([0.9247, 0.8015, 0.8682, 0.9860, 1.1068, 1.1950, 1.2556, 1.2973, 1.3279,
        1.3720, 1.4364, 1.4251, 1.4412, 1.4542, 1.4965, 1.6167, 1.5896, 1.5615,
        1.6037, 1.6248, 1.6526, 1.6568, 1.5411, 1.4949, 1.6028, 1.6275, 1.6442,
        1.7110, 1.6881, 1.6932, 1.8469, 1.8159, 1.8249, 1.7783, 1.7702, 1.7748,
        1.7854, 1.7921, 1.8294, 1.8236, 1.8508, 1.8542, 1.8613, 1.9387, 1.8957,
        1.8982, 1.8998, 1.9559, 1.9291, 1.9252, 1.9404, 1.9859, 1.9653, 2.0641,
        2.0208, 2.0076, 2.0364, 2.1396, 2.0643, 2.1214, 2.0732, 2.0634, 2.0617,
        2.0921, 2.0831, 2.0970, 1.9583, 1.9184, 1.8875, 1.9101, 1.8629, 2.0128,
        2.0884, 2.1036, 2.1163, 2.1898, 2.1817, 2.1897, 2.1870, 2.2029, 2.1920,
        2.1934, 2.2102, 2.2096, 2.2376, 2.3168, 2.2575, 2.2502, 2.2823, 2.3096,
        2.2808, 2.3347, 2.3073, 2.2959, 2.2904, 2.2964, 2.3613, 2.3451, 2.3370,
        2.3762, 2.3660, 2.3429, 2.3434, 2.3493, 2.3489, 2.3568, 2.3630, 2.3759,
        2.3817, 2.3868, 2.4224, 2.4482, 2.4511, 2.3289, 2.2547, 2.2044, 2.3278,
        2.4164, 2.4116, 2.4346, 2.4335, 2.4323, 2.4422, 2.4443, 2.5070, 2.4772,
        2.5411, 2.5008, 2.4854, 2.4857, 2.6009, 2.5565, 2.5161, 2.5425, 2.6906,
        2.6025, 2.6399, 2.5874, 2.5885, 2.6060, 2.5718, 2.5616, 2.5556, 2.5713,
        2.5851, 2.5878, 2.5710, 2.5761, 2.6318, 2.5985, 2.7231, 2.6450, 2.6197,
        2.6088, 2.6083, 2.7453, 2.6996, 2.6993, 2.6530, 2.6407, 2.6401, 2.6442,
        2.6392, 2.6550, 2.6407, 2.6591, 2.6608, 2.7132, 2.6749, 2.6634, 2.6694,
        2.6725, 2.6766, 2.7008, 2.6851, 2.6766, 2.6791, 2.7033, 2.7017, 2.6975,
        2.6896, 2.7324, 2.7064, 2.7078, 2.7056, 2.7128, 2.7039, 2.7555, 2.8103,
        2.8221, 2.7629, 2.7445, 2.7314, 2.7855, 2.7577, 2.8305, 2.8178, 2.8337,
        2.7892, 2.8138, 2.7879, 2.7772, 2.7782, 2.7871, 2.8157, 2.8404, 2.8281,
        2.8059, 2.7954, 2.8296, 2.8857, 2.8227, 2.8176, 2.8591, 2.8843, 2.8342,
        2.8215, 2.8144, 2.8164, 2.8691, 2.8322, 2.8920, 2.8780, 2.8559, 2.8341,
        2.8309, 2.8334, 2.9048, 2.8550, 2.8685, 2.8739, 2.8862, 2.8553, 2.8755,
        2.8541, 2.8673, 2.8564, 2.8515, 2.8674, 2.8504, 2.8523, 2.8656, 2.8709,
        2.8681, 2.8619, 2.8694, 2.8655, 2.9072, 2.8906, 2.8860, 2.9193, 2.9341,
        2.9240, 2.9378, 3.0187, 2.9366, 3.0405, 2.9510, 2.9521, 2.9630, 2.9682,
        2.9899, 3.0313, 2.9905, 2.9966, 2.9584, 2.9944, 2.9551, 2.9811, 2.9489,
        3.0529, 2.9680, 2.9679, 2.9765, 3.0605, 2.9862, 2.9709, 2.9800, 2.9602,
        3.0803, 3.0469, 3.0734, 3.0157, 2.9848, 2.9671, 2.9685, 2.9695, 2.9726,
        2.9731, 3.0077, 2.9862, 3.0120, 2.9902, 3.0323, 3.0338, 3.0019, 2.9969,
        2.9907, 2.9868, 2.9961, 2.9969, 3.0553, 3.0753, 3.0278, 3.0150, 3.0092,
        3.0118, 3.0827, 3.0464, 3.1051, 3.0617, 3.1286, 3.0778, 3.0519, 3.0654,
        3.1772, 3.0791, 3.0459, 3.0374, 3.0828, 3.0528, 3.0864, 3.0536, 3.0487,
        3.0433, 3.0440, 3.0402, 3.1366, 3.0812, 3.1382, 3.0811, 3.0560, 3.0634,
        3.0603, 3.1104, 3.0757, 3.0621, 3.0569, 3.0698, 3.0715, 3.1353, 3.1445,
        3.1040, 3.1031, 3.0830, 3.0778, 3.1648, 3.1090, 3.0934, 3.1008, 3.0967,
        3.1847, 3.1177, 3.1862, 3.2742, 3.1623, 3.1343, 3.1236, 3.1101, 3.1175,
        3.1014, 3.2236, 3.1446, 3.1177, 3.0984, 3.1084, 3.2361, 3.2462, 3.1825,
        3.1927, 3.1557, 3.1359, 3.1263, 3.1286, 3.1249, 3.1244, 3.1627, 3.1545,
        3.1546, 3.1809, 3.1500, 3.2573, 3.1739, 3.1558, 3.1467, 3.1478, 3.1674,
        3.1774, 3.3243, 3.2084, 3.2068, 3.2131, 3.1775, 3.2480, 3.2809, 3.2385,
        3.2012, 3.1879, 3.2736, 3.2755, 3.2600, 3.2014, 3.2289, 3.2161],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809482
t8: 1641198809482
t9: 1641198809483
t10: 1641198809493
t11: 1641198809495
t12: 1641198809495
t1: 1641198809495
evaluate_action_mean_shape: torch.Size([404, 1, 1])
state.shape  torch.Size([404, 4])
t2: 1641198809507
logprobs.shape:  torch.Size([404, 1])
old_action_logprobs.shape:  torch.Size([404, 1])
ratios: tensor([0.9964, 1.0126, 0.9035, 0.9319, 0.8624, 1.0005, 0.9739, 0.8793, 1.0037,
        1.0765, 0.8965, 1.0060, 0.9961, 0.9815, 1.0083, 0.9261, 0.8727, 0.8723,
        0.9267, 0.9935, 1.0156, 0.9929, 1.0047, 0.9416, 1.0189, 0.9968, 0.9307,
        0.9998, 0.9996, 0.8458, 0.9238, 1.0960, 0.9258, 1.0214, 1.0453, 0.9905,
        1.0477, 0.9440, 1.0121, 0.9992, 0.9984, 0.9998, 0.9592, 0.9607, 0.9826,
        1.0257, 0.9360, 1.0103, 1.0535, 0.9753, 0.9651, 0.9905, 0.9913, 0.9960,
        1.0101, 0.9587, 0.9411, 1.0538, 0.8937, 0.9921, 0.9814, 0.9881, 0.9695,
        1.0200, 0.9786, 1.0165, 1.0572, 1.0421, 0.9880, 1.0712, 1.0906, 0.9957,
        1.0079, 1.0239, 0.9010, 1.0044, 1.0245, 0.9983, 0.9866, 1.0181, 0.9975,
        0.9908, 1.0116, 0.9697, 0.9657, 0.9506, 1.0367, 0.8860, 0.9798, 0.9811,
        0.9184, 1.0012, 1.0047, 1.0255, 1.0730, 0.8960, 1.0003, 1.0013, 0.9640,
        0.9900, 1.0068, 1.0435, 0.9983, 1.1113, 1.0920, 1.0832, 1.0310, 0.9936,
        0.9806, 1.0274, 1.0584, 1.0282, 0.9840, 1.0918, 1.1001, 1.1366, 0.9612,
        0.9798, 1.0233, 1.0001, 0.9974, 1.0026, 1.0040, 0.9712, 0.9954, 0.9880,
        0.9930, 1.0298, 1.1265, 0.7905, 0.9992, 0.8945, 0.9202, 0.8429, 0.9744,
        1.0837, 1.0010, 1.0121, 0.9700, 1.0004, 1.0162, 1.1563, 0.9685, 0.9814,
        1.0026, 0.9862, 1.0628, 0.9344, 0.9981, 1.1002, 0.9995, 0.9738, 1.0552,
        1.0142, 0.8883, 0.9785, 1.0621, 0.9905, 1.0106, 1.0742, 1.1145, 1.0014,
        0.9965, 0.9678, 0.9806, 1.0043, 1.0067, 0.9883, 1.0006, 0.9982, 1.0139,
        1.0239, 0.9790, 0.9991, 1.0000, 1.0004, 0.9918, 0.9987, 0.9945, 1.0138,
        0.9590, 1.0825, 1.1189, 1.0325, 0.9991, 0.9941, 1.0343, 1.0493, 1.0378,
        0.9820, 1.0855, 1.5525, 0.8216, 1.0181, 1.0234, 1.0286, 1.0622, 0.9857,
        0.9179, 1.1319, 0.9966, 1.0254, 0.9935, 1.0017, 1.0144, 1.0076, 1.0167,
        1.0628, 0.9223, 0.9653, 0.9794, 0.9951, 0.9397, 0.9622, 0.9997, 1.0088,
        1.0366, 1.0717, 0.9247, 1.0018, 1.0070, 1.0062, 1.0002, 1.0005, 1.0708,
        1.0792, 0.8829, 0.9893, 1.0027, 0.9883, 0.9849, 1.0942, 0.9155, 1.0938,
        0.9816, 1.0037, 0.9970, 0.9987, 0.9981, 0.9993, 0.9993, 0.9959, 1.0158,
        1.0247, 1.0374, 1.0053, 1.0033, 0.9867, 0.9964, 0.9549, 0.9936, 1.0118,
        1.0064, 0.9707, 0.9578, 0.8253, 0.9897, 0.9717, 0.9670, 0.9846, 0.9808,
        0.9832, 1.0050, 0.9987, 1.1036, 0.8380, 1.0535, 0.9784, 1.0329, 0.8631,
        0.9789, 0.9953, 0.9849, 0.9657, 0.9515, 1.0291, 0.9605, 1.0318, 0.9195,
        0.9924, 1.0786, 0.9950, 1.0156, 1.2686, 1.0266, 1.0054, 0.9944, 0.9858,
        0.9901, 0.9974, 1.0030, 1.0029, 0.9642, 0.9956, 0.9981, 1.0537, 1.0927,
        1.0206, 1.0009, 0.9993, 0.9899, 0.9647, 0.9999, 0.9879, 1.0189, 1.0264,
        0.9533, 1.0046, 1.0402, 1.0029, 0.9840, 1.0025, 0.9997, 0.9594, 0.8769,
        0.9648, 1.0912, 1.7238, 0.8535, 1.0275, 0.9881, 0.9384, 1.0030, 0.9984,
        0.9897, 0.9913, 1.0028, 0.9604, 1.0282, 0.9804, 1.0064, 1.0929, 1.0817,
        0.9389, 0.9952, 0.9888, 1.0126, 0.9990, 1.0043, 1.0303, 1.0325, 1.0019,
        0.9923, 1.1909, 1.0509, 0.8898, 0.9933, 0.9716, 0.9829, 0.9995, 0.9595,
        0.9773, 0.8515, 1.0020, 0.9931, 1.0155, 1.0356, 0.9922, 0.9989, 1.0021,
        0.9549, 0.9837, 1.0355, 1.0071, 1.0484, 0.8949, 0.9917, 1.0048, 0.9898,
        0.9986, 1.0246, 1.2776, 0.9819, 1.0173, 0.9941, 1.0048, 1.0051, 1.0055,
        0.9866, 1.0043, 0.9030, 0.9535, 1.0621, 1.1311, 1.0420, 0.9907, 1.0129,
        1.0864, 1.0815, 0.9689, 0.9805, 1.0370, 0.7948, 0.9788, 1.0226, 1.0020,
        1.0505, 0.7818, 0.9873, 1.0372, 1.0011, 0.9582, 0.9846, 0.9007],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
ratios.shape:  torch.Size([404])
advantages.shape:  torch.Size([404])
t3: 1641198809511
t4: 1641198809511
surr1, surr2: tensor([-3.1501e+00, -9.9326e-01, -1.3352e+00, -2.3066e+00, -2.3684e+00,
        -2.9457e+00, -2.7103e+00, -2.4323e+00, -2.8350e+00, -2.8673e+00,
        -2.3481e+00, -2.6730e+00, -2.4794e+00, -2.5377e+00, -2.4726e+00,
        -2.4123e+00, -2.0910e+00, -2.2584e+00, -2.2347e+00, -2.2984e+00,
        -2.2520e+00, -2.1698e+00, -8.4849e-01, -1.6628e-01, -1.6661e+00,
        -1.7985e+00, -1.5465e+00, -1.6896e+00, -1.5993e+00, -1.3098e+00,
        -1.9006e+00, -1.9320e+00, -1.6069e+00, -1.9434e+00, -1.6936e+00,
        -1.5797e+00, -1.5590e+00, -1.7081e+00, -1.5540e+00, -1.5861e+00,
        -1.5187e+00, -1.4407e+00, -1.4424e+00, -1.5548e+00, -1.7144e+00,
        -1.4810e+00, -1.4366e+00, -1.5704e+00, -1.6176e+00, -1.7199e+00,
        -1.3851e+00, -1.5288e+00, -1.5268e+00, -1.8757e+00, -1.2586e+00,
        -1.2726e+00, -1.0875e+00, -1.5726e+00, -1.4019e+00, -1.3904e+00,
        -1.2213e+00, -1.5250e+00, -1.0770e+00, -1.2811e+00, -1.5303e+00,
        -1.1984e+00,  1.2107e+00,  1.3314e+00,  2.1181e+00,  2.6185e+00,
         5.0736e+00,  1.0429e-02,  2.9000e-01,  1.7782e-01, -5.7526e-02,
         1.7193e-01,  2.2688e-01,  2.7942e-01,  3.1942e-01,  3.1462e-01,
         2.7361e-01,  4.1585e-01,  3.4651e-01,  2.9605e-01,  4.5300e-01,
         1.2399e-01, -3.2120e-02,  1.9003e-01,  4.9599e-01,  2.7966e-01,
         1.6041e-01,  1.3597e-01,  2.3975e-01,  2.7956e-01,  3.8476e-01,
         9.1245e-02,  2.2266e-01,  2.9870e-01,  2.8301e-01,  2.1047e-01,
         2.8200e-01,  4.0212e-01,  2.3016e-01,  3.8973e-01, -1.2165e-01,
         8.7324e-02, -1.1118e-01,  4.2132e-02,  1.2744e-01,  1.9438e-01,
         2.6797e-01,  3.2354e-01,  1.8625e-01,  1.8913e+00,  2.9273e+00,
         4.1396e+00,  6.9988e-01,  9.5824e-01,  9.3913e-01,  9.8537e-01,
         9.7617e-01,  9.4383e-01,  8.8285e-01,  9.1400e-01,  8.2100e-01,
         8.4769e-01,  7.3929e-01,  7.9024e-01,  9.2761e-01,  4.9716e-01,
         6.0799e-01,  7.6302e-01,  4.7974e-01,  7.4632e-01,  2.9245e-01,
         8.8553e-01,  7.0098e-01,  8.9898e-01,  8.9671e-01,  8.6190e-01,
         1.0242e+00,  1.0907e+00,  5.9222e-01,  9.4119e-01,  1.0275e+00,
         1.0187e+00,  5.9539e-01,  7.8059e-01,  9.3497e-01,  1.1994e+00,
         5.4292e-01,  9.4107e-01,  7.1863e-01,  9.2248e-01,  8.6799e-01,
         4.8180e-01,  8.3751e-01,  8.4438e-01,  8.3660e-01,  9.5848e-01,
         8.0606e-01,  5.3525e-01,  9.5592e-01,  8.5518e-01,  5.0141e-01,
         9.1347e-01,  7.0019e-01,  6.5554e-01,  8.0114e-01,  9.3512e-01,
         9.4991e-01,  6.6606e-01,  6.9456e-01,  8.6642e-01,  8.5624e-01,
         1.0288e+00,  9.5591e-01,  9.4358e-01,  8.8048e-01,  1.0749e+00,
         8.5792e-01,  8.7286e-01,  4.1844e-01,  5.4685e-01,  8.2600e-01,
         9.4372e-01,  1.0361e+00,  8.6824e-01,  7.0905e-01,  5.8208e-01,
         7.6660e-01,  6.7270e-01,  3.3432e-01,  8.2144e-01,  5.3798e-01,
         6.8281e-01,  8.2469e-01,  6.3939e-01,  3.8272e-01,  7.5847e-01,
         3.2473e-01,  8.3308e-01,  6.7831e-01,  8.9804e-01,  7.7807e-01,
         7.3843e-01,  7.2512e-01,  3.4274e-01,  5.9234e-01,  7.0301e-01,
         5.6301e-01,  6.1576e-01,  6.8988e-01,  5.7859e-01,  5.4981e-01,
         6.1400e-01,  7.0261e-01,  7.5365e-01,  4.4108e-01,  5.9848e-01,
         7.4733e-01,  6.1165e-01,  6.8949e-01,  6.1030e-01,  8.3791e-01,
         5.7765e-01,  4.4236e-01,  6.1554e-01,  6.3003e-01,  7.1888e-01,
         6.4219e-01,  6.5927e-01,  1.8969e-01,  8.0176e-01,  3.0416e-01,
         7.1864e-01,  6.1029e-01,  6.7755e-01,  7.3388e-01,  8.1253e-01,
         7.2469e-01,  6.4474e-01,  7.6355e-01,  2.8389e-01,  6.6402e-01,
         4.0556e-01,  6.6881e-01,  5.6087e-01,  4.5371e-01,  1.6229e-01,
         5.0955e-01,  5.4957e-01,  5.9698e-01,  4.5131e-01,  1.8040e-01,
         9.8552e-02,  1.9605e-01,  1.1097e-03,  4.2631e-01,  4.1083e-01,
         4.4157e-01,  3.6023e-01,  2.6975e-01,  3.2618e-01,  3.1320e-01,
        -7.5286e-02,  3.5798e-01,  3.3176e-01,  4.9210e-01,  5.0509e-02,
         1.1321e-01,  1.6691e-01,  4.0086e-01,  3.9443e-01,  1.6850e-01,
        -2.7643e-02,  2.2426e-01,  4.4160e-01,  2.6434e-01,  1.3634e-01,
         2.0856e-01,  1.6081e-01,  1.8724e-01,  2.4604e-01, -1.2886e-02,
         3.7275e-01,  3.7906e-01,  2.9284e-01,  1.5364e-01,  3.1069e-01,
         2.8306e-01,  2.7258e-01,  1.8407e-01,  2.9950e-01,  2.3943e-01,
         1.7457e-01,  1.2679e-02,  5.8412e-03,  2.4297e-01,  1.7513e-01,
         1.9098e-01, -3.0929e-01,  2.5950e-01,  4.9099e-01,  3.1584e-01,
         4.0307e-01,  2.9666e-01,  2.8719e-01,  4.6324e-01,  2.7386e-01,
         3.5735e-01,  1.4242e-01,  2.8326e-01,  3.3528e-01,  3.3930e-01,
        -4.3785e-03,  9.1470e-02, -1.5754e-01, -1.0375e-01,  3.3852e-01,
        -1.5431e-01,  3.1945e-01, -1.2796e-01,  2.6573e-01,  2.7611e-01,
         2.4101e-01,  2.3845e-01,  8.4376e-02,  8.2890e-02,  2.3446e-01,
         1.8308e-01,  3.5282e-01, -8.3003e-02,  5.5779e-02,  1.7295e-01,
         1.3042e-01, -1.1688e-01,  2.7887e-01,  2.0820e-01,  3.0932e-01,
         5.0713e-02,  3.9527e-02,  1.3055e-01,  6.0759e-02, -3.8863e-01,
         6.1322e-02, -4.4837e-02,  7.3448e-02, -7.2249e-02,  8.3436e-02,
         1.6348e-02, -2.5780e-01, -4.0983e-01, -1.6412e-01, -4.5687e-01,
        -2.5300e-01, -3.5989e-01, -1.8770e-01,  5.6792e-02, -9.4580e-02,
         2.7850e-02, -3.4852e-01, -5.1779e-01, -1.6821e-01,  9.9476e-02,
        -3.1510e-01, -4.4048e-01, -4.2834e-01, -2.9422e-01, -2.1041e-01,
        -1.6342e-01, -4.4654e-01, -6.3457e-01, -1.8392e-01, -2.3071e-01,
        -2.7988e-01, -1.9538e-01, -2.4349e-01, -2.5497e-01, -3.8387e-01,
        -3.3427e-01, -5.4900e-01, -9.0004e-01, -6.1033e-01, -6.9967e-01,
        -5.1126e-01, -4.3295e-01, -4.1810e-01, -1.0712e+00, -8.5577e-01,
        -5.2204e-01, -6.4729e-01, -8.1681e-01, -6.9091e-01, -8.3075e-01,
        -7.0702e-01, -7.0545e-01, -8.2185e-01, -9.4591e-01, -9.6871e-01,
        -9.1938e-01, -7.2242e-01, -8.3197e-01, -7.7263e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>) tensor([-3.1501e+00, -9.9326e-01, -1.3352e+00, -2.3066e+00, -2.4716e+00,
        -2.9457e+00, -2.7103e+00, -2.4896e+00, -2.8350e+00, -2.8673e+00,
        -2.3574e+00, -2.6730e+00, -2.4794e+00, -2.5377e+00, -2.4726e+00,
        -2.4123e+00, -2.1565e+00, -2.3300e+00, -2.2347e+00, -2.2984e+00,
        -2.2520e+00, -2.1698e+00, -8.4849e-01, -1.6628e-01, -1.6661e+00,
        -1.7985e+00, -1.5465e+00, -1.6896e+00, -1.5993e+00, -1.3938e+00,
        -1.9006e+00, -1.9320e+00, -1.6069e+00, -1.9434e+00, -1.6936e+00,
        -1.5797e+00, -1.5590e+00, -1.7081e+00, -1.5540e+00, -1.5861e+00,
        -1.5187e+00, -1.4407e+00, -1.4424e+00, -1.5548e+00, -1.7144e+00,
        -1.4810e+00, -1.4366e+00, -1.5704e+00, -1.6176e+00, -1.7199e+00,
        -1.3851e+00, -1.5288e+00, -1.5268e+00, -1.8757e+00, -1.2586e+00,
        -1.2726e+00, -1.0875e+00, -1.5726e+00, -1.4118e+00, -1.3904e+00,
        -1.2213e+00, -1.5250e+00, -1.0770e+00, -1.2811e+00, -1.5303e+00,
        -1.1984e+00,  1.2107e+00,  1.3314e+00,  2.1181e+00,  2.6185e+00,
         5.0736e+00,  1.0429e-02,  2.9000e-01,  1.7782e-01, -5.7526e-02,
         1.7193e-01,  2.2688e-01,  2.7942e-01,  3.1942e-01,  3.1462e-01,
         2.7361e-01,  4.1585e-01,  3.4651e-01,  2.9605e-01,  4.5300e-01,
         1.2399e-01, -3.2120e-02,  1.9303e-01,  4.9599e-01,  2.7966e-01,
         1.6041e-01,  1.3597e-01,  2.3975e-01,  2.7956e-01,  3.8476e-01,
         9.1655e-02,  2.2266e-01,  2.9870e-01,  2.8301e-01,  2.1047e-01,
         2.8200e-01,  4.0212e-01,  2.3016e-01,  3.8576e-01, -1.2165e-01,
         8.7324e-02, -1.1118e-01,  4.2132e-02,  1.2744e-01,  1.9438e-01,
         2.6797e-01,  3.2354e-01,  1.8625e-01,  1.8913e+00,  2.9271e+00,
         4.0062e+00,  6.9988e-01,  9.5824e-01,  9.3913e-01,  9.8537e-01,
         9.7617e-01,  9.4383e-01,  8.8285e-01,  9.1400e-01,  8.2100e-01,
         8.4769e-01,  7.3929e-01,  7.9024e-01,  9.0579e-01,  5.6602e-01,
         6.0799e-01,  7.6775e-01,  4.7974e-01,  7.9688e-01,  2.9245e-01,
         8.8553e-01,  7.0098e-01,  8.9898e-01,  8.9671e-01,  8.6190e-01,
         1.0242e+00,  1.0376e+00,  5.9222e-01,  9.4119e-01,  1.0275e+00,
         1.0187e+00,  5.9539e-01,  7.8059e-01,  9.3497e-01,  1.1992e+00,
         5.4292e-01,  9.4107e-01,  7.1863e-01,  9.2248e-01,  8.7946e-01,
         4.8180e-01,  8.3751e-01,  8.4438e-01,  8.3660e-01,  9.5848e-01,
         7.9559e-01,  5.3525e-01,  9.5592e-01,  8.5518e-01,  5.0141e-01,
         9.1347e-01,  7.0019e-01,  6.5554e-01,  8.0114e-01,  9.3512e-01,
         9.4991e-01,  6.6606e-01,  6.9456e-01,  8.6642e-01,  8.5624e-01,
         1.0288e+00,  9.5591e-01,  9.4358e-01,  8.8048e-01,  1.0749e+00,
         8.5792e-01,  8.7286e-01,  4.1136e-01,  5.4685e-01,  8.2600e-01,
         9.4372e-01,  1.0361e+00,  8.6824e-01,  7.0905e-01,  5.8208e-01,
         7.6660e-01,  4.7662e-01,  3.6623e-01,  8.2144e-01,  5.3798e-01,
         6.8281e-01,  8.2469e-01,  6.3939e-01,  3.8272e-01,  7.3710e-01,
         3.2473e-01,  8.3308e-01,  6.7831e-01,  8.9804e-01,  7.7807e-01,
         7.3843e-01,  7.2512e-01,  3.4274e-01,  5.9234e-01,  7.0301e-01,
         5.6301e-01,  6.1576e-01,  6.8988e-01,  5.7859e-01,  5.4981e-01,
         6.1400e-01,  7.0261e-01,  7.5365e-01,  4.4108e-01,  5.9848e-01,
         7.4733e-01,  6.1165e-01,  6.8949e-01,  6.1030e-01,  8.3791e-01,
         5.7765e-01,  4.5092e-01,  6.1554e-01,  6.3003e-01,  7.1888e-01,
         6.4219e-01,  6.5927e-01,  1.8969e-01,  8.0176e-01,  3.0416e-01,
         7.1864e-01,  6.1029e-01,  6.7755e-01,  7.3388e-01,  8.1253e-01,
         7.2469e-01,  6.4474e-01,  7.6355e-01,  2.8389e-01,  6.6402e-01,
         4.0556e-01,  6.6881e-01,  5.6087e-01,  4.5371e-01,  1.6229e-01,
         5.0955e-01,  5.4957e-01,  5.9698e-01,  4.5131e-01,  1.8040e-01,
         1.0747e-01,  1.9605e-01,  1.1097e-03,  4.2631e-01,  4.1083e-01,
         4.4157e-01,  3.6023e-01,  2.6975e-01,  3.2618e-01,  3.1219e-01,
        -8.0855e-02,  3.5798e-01,  3.3176e-01,  4.9210e-01,  5.2668e-02,
         1.1321e-01,  1.6691e-01,  4.0086e-01,  3.9443e-01,  1.6850e-01,
        -2.7643e-02,  2.2426e-01,  4.4160e-01,  2.6434e-01,  1.3634e-01,
         2.0856e-01,  1.6081e-01,  1.8724e-01,  2.1334e-01, -1.2886e-02,
         3.7275e-01,  3.7906e-01,  2.9284e-01,  1.5364e-01,  3.1069e-01,
         2.8306e-01,  2.7258e-01,  1.8407e-01,  2.9950e-01,  2.3943e-01,
         1.7457e-01,  1.2679e-02,  5.8412e-03,  2.4297e-01,  1.7513e-01,
         1.9098e-01, -3.0929e-01,  2.5950e-01,  4.9099e-01,  3.1584e-01,
         4.0307e-01,  2.9666e-01,  2.8719e-01,  4.6324e-01,  2.7386e-01,
         3.5735e-01,  1.4242e-01,  2.8326e-01,  3.3528e-01,  3.4823e-01,
        -4.3785e-03,  9.1470e-02, -1.0053e-01, -1.0941e-01,  3.3852e-01,
        -1.5431e-01,  3.1945e-01, -1.2796e-01,  2.6573e-01,  2.7611e-01,
         2.4101e-01,  2.3845e-01,  8.4376e-02,  8.2890e-02,  2.3446e-01,
         1.8308e-01,  3.5282e-01, -8.3003e-02,  5.5779e-02,  1.7295e-01,
         1.3042e-01, -1.1688e-01,  2.7887e-01,  2.0820e-01,  3.0932e-01,
         5.0713e-02,  3.9527e-02,  1.3055e-01,  5.6121e-02, -3.8863e-01,
         6.2027e-02, -4.4837e-02,  7.3448e-02, -7.2249e-02,  8.3436e-02,
         1.6348e-02, -2.5780e-01, -4.3320e-01, -1.6412e-01, -4.5687e-01,
        -2.5300e-01, -3.5989e-01, -1.8770e-01,  5.6792e-02, -9.4580e-02,
         2.7850e-02, -3.4852e-01, -5.1779e-01, -1.6821e-01,  9.9476e-02,
        -3.1689e-01, -4.4048e-01, -4.2834e-01, -2.9422e-01, -2.1041e-01,
        -1.6342e-01, -3.8447e-01, -6.3457e-01, -1.8392e-01, -2.3071e-01,
        -2.7988e-01, -1.9538e-01, -2.4349e-01, -2.5497e-01, -3.8387e-01,
        -3.3427e-01, -5.4900e-01, -9.0004e-01, -5.9353e-01, -6.9967e-01,
        -5.1126e-01, -4.3295e-01, -4.1810e-01, -1.0712e+00, -8.5577e-01,
        -5.2204e-01, -6.4729e-01, -9.2498e-01, -6.9091e-01, -8.3075e-01,
        -7.0702e-01, -7.0545e-01, -9.4607e-01, -9.4591e-01, -9.6871e-01,
        -9.1938e-01, -7.2242e-01, -8.3197e-01, -7.7263e-01], device='cuda:0',
       dtype=torch.float64, grad_fn=<MulBackward0>)
t5: 1641198809522
t6: 1641198809522
state_values: tensor([0.8881, 0.7641, 0.8306, 0.9434, 1.0626, 1.1502, 1.2106, 1.2526, 1.2839,
        1.3280, 1.3916, 1.3820, 1.3987, 1.4125, 1.4547, 1.5675, 1.5429, 1.5193,
        1.5589, 1.5793, 1.6064, 1.6112, 1.5037, 1.4591, 1.5655, 1.5899, 1.6060,
        1.6682, 1.6478, 1.6534, 1.8031, 1.7733, 1.7827, 1.7377, 1.7301, 1.7350,
        1.7458, 1.7531, 1.7900, 1.7849, 1.8120, 1.8157, 1.8231, 1.8994, 1.8579,
        1.8606, 1.8626, 1.9179, 1.8922, 1.8888, 1.9040, 1.9489, 1.9292, 2.0266,
        1.9844, 1.9718, 2.0004, 2.1017, 2.0284, 2.0846, 2.0377, 2.0285, 2.0270,
        2.0571, 2.0488, 2.0626, 1.9263, 1.8873, 1.8571, 1.8795, 1.8335, 1.9817,
        2.0563, 2.0718, 2.0847, 2.1572, 2.1495, 2.1576, 2.1552, 2.1709, 2.1606,
        2.1621, 2.1788, 2.1785, 2.2062, 2.2829, 2.2262, 2.2192, 2.2508, 2.2776,
        2.2498, 2.3015, 2.2760, 2.2651, 2.2598, 2.2659, 2.3279, 2.3132, 2.3059,
        2.3427, 2.3334, 2.3123, 2.3132, 2.3188, 2.3190, 2.3266, 2.3327, 2.3450,
        2.3507, 2.3557, 2.3894, 2.4149, 2.4179, 2.3003, 2.2273, 2.1781, 2.3000,
        2.3854, 2.3814, 2.4029, 2.4021, 2.4013, 2.4107, 2.4129, 2.4746, 2.4454,
        2.5083, 2.4688, 2.4538, 2.4543, 2.5672, 2.5237, 2.4844, 2.5102, 2.6557,
        2.5692, 2.6060, 2.5544, 2.5555, 2.5728, 2.5393, 2.5293, 2.5237, 2.5391,
        2.5527, 2.5553, 2.5393, 2.5442, 2.5990, 2.5664, 2.6885, 2.6120, 2.5874,
        2.5766, 2.5762, 2.7107, 2.6660, 2.6656, 2.6203, 2.6082, 2.6077, 2.6120,
        2.6070, 2.6225, 2.6088, 2.6268, 2.6285, 2.6800, 2.6425, 2.6312, 2.6371,
        2.6403, 2.6444, 2.6681, 2.6527, 2.6444, 2.6469, 2.6707, 2.6692, 2.6651,
        2.6574, 2.6995, 2.6743, 2.6757, 2.6735, 2.6805, 2.6718, 2.7224, 2.7763,
        2.7880, 2.7298, 2.7119, 2.6992, 2.7522, 2.7250, 2.7964, 2.7839, 2.7995,
        2.7559, 2.7800, 2.7548, 2.7441, 2.7451, 2.7538, 2.7819, 2.8060, 2.7940,
        2.7724, 2.7622, 2.7956, 2.8465, 2.7890, 2.7838, 2.8231, 2.8455, 2.8002,
        2.7877, 2.7807, 2.7829, 2.8324, 2.7983, 2.8527, 2.8403, 2.8208, 2.8002,
        2.7971, 2.7997, 2.8644, 2.8204, 2.8324, 2.8371, 2.8480, 2.8210, 2.8388,
        2.8201, 2.8317, 2.8221, 2.8176, 2.8319, 2.8164, 2.8183, 2.8304, 2.8351,
        2.8328, 2.8274, 2.8341, 2.8307, 2.8676, 2.8529, 2.8490, 2.8784, 2.8917,
        2.8825, 2.8952, 2.9731, 2.8943, 2.9935, 2.9081, 2.9090, 2.9195, 2.9245,
        2.9452, 2.9848, 2.9457, 2.9515, 2.9151, 2.9495, 2.9119, 2.9368, 2.9060,
        3.0057, 2.9244, 2.9242, 2.9325, 3.0129, 2.9418, 2.9271, 2.9358, 2.9170,
        3.0316, 2.9997, 3.0250, 2.9698, 2.9403, 2.9236, 2.9250, 2.9260, 2.9289,
        2.9295, 2.9627, 2.9421, 2.9669, 2.9460, 2.9863, 2.9876, 2.9571, 2.9525,
        2.9466, 2.9429, 2.9518, 2.9525, 3.0085, 3.0276, 2.9820, 2.9698, 2.9643,
        2.9668, 3.0347, 2.9998, 3.0560, 3.0143, 3.0789, 3.0297, 3.0048, 3.0177,
        3.1262, 3.0307, 2.9991, 2.9911, 3.0346, 3.0060, 3.0380, 3.0067, 3.0021,
        2.9969, 2.9977, 2.9941, 3.0866, 3.0332, 3.0880, 3.0330, 3.0091, 3.0162,
        3.0133, 3.0612, 3.0280, 3.0151, 3.0101, 3.0225, 3.0241, 3.0852, 3.0942,
        3.0550, 3.0541, 3.0351, 3.0301, 3.1140, 3.0598, 3.0449, 3.0520, 3.0481,
        3.1334, 3.0682, 3.1349, 3.2212, 3.1114, 3.0840, 3.0734, 3.0606, 3.0677,
        3.0524, 3.1710, 3.0939, 3.0679, 3.0495, 3.0592, 3.1833, 3.1934, 3.1308,
        3.1407, 3.1044, 3.0852, 3.0762, 3.0784, 3.0749, 3.0745, 3.1113, 3.1033,
        3.1033, 3.1291, 3.0989, 3.2037, 3.1225, 3.1046, 3.0957, 3.0968, 3.1158,
        3.1256, 3.2696, 3.1562, 3.1544, 3.1605, 3.1258, 3.1947, 3.2270, 3.1854,
        3.1487, 3.1358, 3.2197, 3.2216, 3.2063, 3.1488, 3.1757, 3.1631],
       device='cuda:0', grad_fn=<SqueezeBackward0>)
state_values.shape:  torch.Size([404])
old_returns.shape:  torch.Size([404])
t7: 1641198809527
t8: 1641198809527
t9: 1641198809527
t10: 1641198809537
t11: 1641198809539
t12: 1641198809539
OPTIMIZER LR: 0.003
Episode 0 	 Average policy loss, value loss, reward -0.0040511274146671225, 5.095143012977509, 0.027864144013149895
setTime: 1641198718609718
[800.0, 1641198718609718.0]
[800.0, 1641198723632718.0]
[1500.0, 1641198723632718.0]
[1500.0, 1641198728647718.0]
[800.0, 1641198728647718.0]
[800.0, 1641198733663718.0]
[1500.0, 1641198733663718.0]
[1500.0, 1641198738676718.0]
[800.0, 1641198738676718.0]
[800.0, 1641198743689718.0]
[1500.0, 1641198743689718.0]
[1500.0, 1641198748703718.0]
[800.0, 1641198748703718.0]
[800.0, 1641198753716718.0]
[1500.0, 1641198753716718.0]
[1500.0, 1641198758730718.0]
[800.0, 1641198758730718.0]
[800.0, 1641198763743718.0]
[1500.0, 1641198763743718.0]
[1500.0, 1641198768757718.0]
[800.0, 1641198768757718.0]
[800.0, 1641198773776718.0]
[1500.0, 1641198773776718.0]
[1500.0, 1641198778790718.0]
[800.0, 1641198778790718.0]
[800.0, 1641198783803718.0]
[1500.0, 1641198783803718.0]
[1500.0, 1641198788853718.0]
[800.0, 1641198788853718.0]
[800.0, 1641198793866718.0]
[1500.0, 1641198793866718.0]
[1500.0, 1641198798879718.0]
[800.0, 1641198798879718.0]
[800.0, 1641198803893718.0]
[1500.0, 1641198803893718.0]
1 [['DROP', 1641198718684415], ['DROP', 1641198718712278], ['DROP', 1641198718744990], ['DROP', 1641198718777139], [4815, 1641198718778912], [86, 1641198718810380], [87, 1641198718843992], [156, 1641198718877375], [287, 1641198718910909], [357, 1641198718944259], [520, 1641198718977594], [519, 1641198719010900], [552, 1641198719044190], [510, 1641198719077524], [709, 1641198719110927], [675, 1641198719144144], [628, 1641198719177490], [839, 1641198719211110], [671, 1641198719244325], [807, 1641198719277727], [797, 1641198719310832], [702, 1641198719344167], [741, 1641198719377357], [772, 1641198719410949], [1010, 1641198719443966], [623, 1641198719477217], [810, 1641198719510531], [946, 1641198719543908], [720, 1641198719577182], [986, 1641198719610483], [600, 1641198719643683], [958, 1641198719678123], [909, 1641198719711452], [706, 1641198719744725], [1474, 1641198719778233], [882, 1641198719811481], [831, 1641198719844789], [1240, 1641198719878236], [1102, 1641198719911445], [1112, 1641198719944883], [1194, 1641198719978159], [1191, 1641198720011459], [1027, 1641198720044753], [1020, 1641198720078044], [1190, 1641198720111394], [1188, 1641198720144702], [1068, 1641198720178040], [1293, 1641198720211346], [1201, 1641198720244691], [907, 1641198720277935], [1167, 1641198720311290], [1173, 1641198720344588], [1006, 1641198720377896], [1039, 1641198720411251], [1137, 1641198720444606], [1030, 1641198720477924], [1067, 1641198720511293], [1078, 1641198720544577], [1104, 1641198720577880], [1134, 1641198720611350], [1130, 1641198720644524], [1076, 1641198720677822], [999, 1641198720711186], [1043, 1641198720744483], [1138, 1641198720777840], [1022, 1641198720811179], [866, 1641198720844487], [1217, 1641198720877868], [883, 1641198720911113], [974, 1641198720944458], [1123, 1641198720977762], [916, 1641198721011058], [1028, 1641198721044382], [881, 1641198721077726], [1136, 1641198721111067], [942, 1641198721144361], [902, 1641198721177613], [1105, 1641198721210994], [989, 1641198721244307], [1599, 1641198721277707], [1048, 1641198721310948], [1346, 1641198721344290], [1066, 1641198721377504], [1374, 1641198721410831], [1385, 1641198721444196], [1353, 1641198721477546], [1116, 1641198721510863], [1392, 1641198721544215], [1179, 1641198721577489], [1179, 1641198721610750], [1206, 1641198721644222], [1271, 1641198721677518], [1028, 1641198721710910], [1152, 1641198721744183], [1207, 1641198721777547], [977, 1641198721810777], [1144, 1641198721844190], [1342, 1641198721877483], [1146, 1641198721910680], [1003, 1641198721944010], [1362, 1641198721977331], [794, 1641198722010584], [1012, 1641198722043971], [852, 1641198722077425], [1065, 1641198722110758], [842, 1641198722144023], [1035, 1641198722177357], [1126, 1641198722210597], [904, 1641198722243819], [1193, 1641198722277125], [1347, 1641198722310489], [1102, 1641198722343825], [1245, 1641198722377221], [1260, 1641198722410574], [1449, 1641198722444035], [990, 1641198722477285], [1082, 1641198722510567], [1054, 1641198722543855], [1206, 1641198722577198], [992, 1641198722610474], [1099, 1641198722643826], [4039, 1641198722679210], [352, 1641198722710668], [300, 1641198722743963], [239, 1641198722777389], [240, 1641198722810762], [384, 1641198722844209], [570, 1641198722877666], [636, 1641198722911021], [668, 1641198722944395], [817, 1641198722977742], [784, 1641198723011141], [682, 1641198723044530], [767, 1641198723077879], [932, 1641198723111367], [803, 1641198723144734], [687, 1641198723178117], [983, 1641198723211612], [883, 1641198723245078], [1007, 1641198723277412], [943, 1641198723310758], [1155, 1641198723344281], [985, 1641198723377649], [850, 1641198723411214], [1312, 1641198723444569], [1040, 1641198723477931], [909, 1641198723511344], [1248, 1641198723544898], [1198, 1641198723578246], [927, 1641198723611571], [1183, 1641198723645167], [1044, 1641198723677542], [1106, 1641198723710910], [1104, 1641198723744414], [1029, 1641198723777784], [1067, 1641198723811188], [981, 1641198723844577], [1060, 1641198723877983], [1082, 1641198723911304], [921, 1641198723944654], [1315, 1641198723978104], [868, 1641198724011520], [1172, 1641198724044880], [1051, 1641198724078381], [1262, 1641198724112075], [1194, 1641198724144269], [1122, 1641198724177469], [1162, 1641198724210923], [1232, 1641198724244282], [940, 1641198724277758], [1359, 1641198724311200], [1255, 1641198724344611], [1209, 1641198724377976], [1100, 1641198724411350], [1379, 1641198724444873], [1196, 1641198724478126], [1099, 1641198724511457], [1129, 1641198724544913], [1235, 1641198724578278], [980, 1641198724611643], [1143, 1641198724645054], [1043, 1641198724678336], [968, 1641198724710710], [903, 1641198724744056], [1095, 1641198724777438], [1241, 1641198724810853], [905, 1641198724844154], [1158, 1641198724877617], [1270, 1641198724911057], [1111, 1641198724944393], [1190, 1641198724977811], [1120, 1641198725011190], [1061, 1641198725044524], [1285, 1641198725078017], [1321, 1641198725111403], [1263, 1641198725144728], [1237, 1641198725178047], [1193, 1641198725216617], [1320, 1641198725244773], [1106, 1641198725278108], [1452, 1641198725311527], [981, 1641198725344935], [1130, 1641198725378219], [977, 1641198725411623], [1170, 1641198725445016], [1192, 1641198725478432], [1150, 1641198725510828], [1228, 1641198725544197], [1220, 1641198725577533], [1135, 1641198725610933], [1390, 1641198725644309], [1003, 1641198725677621], [1161, 1641198725711045], [996, 1641198725744356], [1280, 1641198725778027], [1118, 1641198725811374], [1128, 1641198725844664], [1143, 1641198725878021], [1216, 1641198725911353], [963, 1641198725944920], [1112, 1641198725978226], [1042, 1641198726011700], [991, 1641198726044355], [874, 1641198726077954], [1159, 1641198726111381], [1192, 1641198726144680], [1142, 1641198726178076], [1110, 1641198726211471], [1051, 1641198726244847], [1126, 1641198726278309], [1166, 1641198726311745], [850, 1641198726344152], [1172, 1641198726377870], [1090, 1641198726411213], [1153, 1641198726444689], [1051, 1641198726478067], [1151, 1641198726511531], [1040, 1641198726544923], [1214, 1641198726578302], [1096, 1641198726611730], [1195, 1641198726644127], [1086, 1641198726677580], [1068, 1641198726710975], [1222, 1641198726756592], [1165, 1641198726777846], [1036, 1641198726811166], [1094, 1641198726844666], [1143, 1641198726878075], [1240, 1641198726911472], [1059, 1641198726944869], [1273, 1641198726978289], [1163, 1641198727011690], [956, 1641198727044034], [1068, 1641198727077466], [1096, 1641198727110946], [1169, 1641198727144381], [1065, 1641198727177713], [1195, 1641198727211207], [1310, 1641198727244578], [1018, 1641198727278033], [1348, 1641198727311481], [1123, 1641198727344825], [1114, 1641198727378224], [1054, 1641198727411608], [1223, 1641198727445072], [1188, 1641198727477510], [998, 1641198727510829], [1083, 1641198727544266], [1170, 1641198727577793], [1110, 1641198727611154], [1177, 1641198727644618], [1092, 1641198727677961], [1223, 1641198727711357], [1002, 1641198727744719], [1159, 1641198727778183], [1033, 1641198727811504], [1075, 1641198727844932], [1178, 1641198727878325], [1107, 1641198727911747], [1048, 1641198727944192], [1233, 1641198727977629], [1069, 1641198728011049], [1030, 1641198728044384], [997, 1641198728077778], [968, 1641198728111228], [1003, 1641198728144560], [860, 1641198728177967], [1117, 1641198728211417], [1093, 1641198728244822], [980, 1641198728278205], [1221, 1641198728311633], [874, 1641198728344016], [965, 1641198728377556], [1132, 1641198728410894], [1240, 1641198728444320], [1158, 1641198728477789], [1056, 1641198728511152], [1135, 1641198728544567], [1258, 1641198728577988], [1085, 1641198728611295], [1496, 1641198728644808], [1035, 1641198728678130], [1086, 1641198728711572], [1009, 1641198728744933], [1208, 1641198728778374], [1238, 1641198728810738], [1043, 1641198728844208], [1033, 1641198728877559], [1076, 1641198728910960], [1015, 1641198728944314], [1354, 1641198728977803], [989, 1641198729011094], [1046, 1641198729044543], [939, 1641198729077987], [1068, 1641198729111390], [1223, 1641198729144770], [786, 1641198729178098], [1207, 1641198729211534], [1106, 1641198729244946], [1077, 1641198729278365], [1138, 1641198729310747], [1125, 1641198729344224], [1006, 1641198729378021], [1050, 1641198729410961], [1370, 1641198729444430], [1212, 1641198729477776], [1008, 1641198729511134], [1256, 1641198729544576], [1027, 1641198729577912], [1136, 1641198729611361], [1221, 1641198729644744], [1007, 1641198729678049], [1237, 1641198729711457], [904, 1641198729744741], [1190, 1641198729778217], [1011, 1641198729811531], [1029, 1641198729844985], [982, 1641198729877463], [1259, 1641198729910918], [896, 1641198729944242], [1302, 1641198729977722], [961, 1641198730011040], [1123, 1641198730044520], [903, 1641198730077809], [1071, 1641198730111185], [1163, 1641198730144578], [942, 1641198730177987], [1165, 1641198730211472], [1020, 1641198730244766], [1048, 1641198730278225], [1140, 1641198730311599], [984, 1641198730343932], [1221, 1641198730377405], [1173, 1641198730410709], [1171, 1641198730444112], [1279, 1641198730477534], [856, 1641198730510850], [1020, 1641198730544269], [1135, 1641198730577633], [1312, 1641198730611163], [1400, 1641198730644529], [985, 1641198730677893], [1194, 1641198730711301], [1091, 1641198730744677], [1346, 1641198730778060], [952, 1641198730811387], [866, 1641198730844810], [1299, 1641198730878252], [1158, 1641198730911587], [1057, 1641198730943938], [903, 1641198730977367], [1057, 1641198731010713], [746, 1641198731044045], [807, 1641198731077372], [1382, 1641198731110908], [1193, 1641198731144321], [1207, 1641198731177713], [1198, 1641198731211088], [1201, 1641198731244482], [1195, 1641198731277914], [1369, 1641198731311338], [1183, 1641198731344704], [1220, 1641198731378095], [948, 1641198731411397], [1303, 1641198731444877], [1152, 1641198731478189], [1149, 1641198731511615], [1336, 1641198731544046], [1344, 1641198731577371], [1137, 1641198731610753], [1404, 1641198731644133], [930, 1641198731677469], [1126, 1641198731710867], [994, 1641198731744278], [1237, 1641198731777689], [1035, 1641198731811001], [973, 1641198731844360], [1330, 1641198731877751], [919, 1641198731911064], [875, 1641198731944482], [1305, 1641198731977933], [965, 1641198732011386], [994, 1641198732044654], [999, 1641198732078075], [1280, 1641198732111615], [1165, 1641198732144944], [999, 1641198732177303], [1352, 1641198732210785], [1289, 1641198732244228], [995, 1641198732277542], [1422, 1641198732310979], [928, 1641198732344255], [1096, 1641198732377677], [936, 1641198732410967], [1142, 1641198732444431], [951, 1641198732477743], [868, 1641198732511154], [1137, 1641198732544732], [1223, 1641198732578015], [988, 1641198732611307], [1462, 1641198732644811], [1007, 1641198732678064], [1024, 1641198732711457], [939, 1641198732744841], [1191, 1641198732778275], [1034, 1641198732810634], [1097, 1641198732844059], [1131, 1641198732877518], [1089, 1641198732910808], [1258, 1641198732944265], [1313, 1641198732977649], [1211, 1641198733011021], [1280, 1641198733044422], [934, 1641198733077722], [1443, 1641198733111201], [942, 1641198733144429], [944, 1641198733177840], [1167, 1641198733211303], [1139, 1641198733244635], [966, 1641198733277999], [1394, 1641198733311401], [923, 1641198733344706], [1123, 1641198733378112], [1077, 1641198733411525], [1129, 1641198733444915], [953, 1641198733478204], [1344, 1641198733510623], [833, 1641198733543933], [1297, 1641198733577399], [831, 1641198733610736], [1171, 1641198733644195], [979, 1641198733677542], [1217, 1641198733710940], [875, 1641198733744296], [1208, 1641198733777712], [1024, 1641198733811045], [1109, 1641198733844441], [1112, 1641198733878008], [1212, 1641198733911500], [1017, 1641198733945023], [1075, 1641198733978149], [758, 1641198734011426], [999, 1641198734044031], [1005, 1641198734077291], [1171, 1641198734110749], [1199, 1641198734144110], [1080, 1641198734177627], [1140, 1641198734210939], [1217, 1641198734244331], [936, 1641198734277755], [1480, 1641198734311224], [1252, 1641198734344650], [1274, 1641198734378007], [972, 1641198734411424], [1243, 1641198734444828], [1116, 1641198734478233], [1195, 1641198734511639], [1008, 1641198734544936], [1196, 1641198734577296], [867, 1641198734610633], [1223, 1641198734644141], [3658, 1641198734680667], [817, 1641198734711637], [648, 1641198734745099], [619, 1641198734778183], [815, 1641198734811611], [486, 1641198734844823], [612, 1641198734878162], [745, 1641198734911757], [695, 1641198734944962], [715, 1641198734978334], [762, 1641198735011655], [656, 1641198735045036], [845, 1641198735078414], [865, 1641198735111768], [893, 1641198735145165], [845, 1641198735178902], [950, 1641198735211986], [1102, 1641198735245291], [894, 1641198735278633], [1211, 1641198735312002], [936, 1641198735345269], [691, 1641198735378644], [845, 1641198735412043], [1223, 1641198735445445], [1043, 1641198735478767], [952, 1641198735512101], [1367, 1641198735545508], [1001, 1641198735578758], [1165, 1641198735612048], [1080, 1641198735645431], [1029, 1641198735678662], [1120, 1641198735712107], [974, 1641198735745407], [1093, 1641198735778823], [1070, 1641198735812026], [911, 1641198735845317], [988, 1641198735878680], [1081, 1641198735911925], [758, 1641198735945226], [988, 1641198735978571], [741, 1641198736011805], [1088, 1641198736045192], [898, 1641198736078461], [1185, 1641198736111906], [1139, 1641198736145189], [1083, 1641198736178486], [964, 1641198736211744], [1010, 1641198736245101], [1182, 1641198736278480], [1295, 1641198736311846], [1032, 1641198736345100], [1139, 1641198736378423], [975, 1641198736411737], [1212, 1641198736445199], [1182, 1641198736478471], [1026, 1641198736511745], [1091, 1641198736545072], [1153, 1641198736578371], [1108, 1641198736611696], [1321, 1641198736645066], [1060, 1641198736678431], [1475, 1641198736711772], [1022, 1641198736745020], [1275, 1641198736778514], [1086, 1641198736811760], [924, 1641198736845067], [1055, 1641198736878479], [1160, 1641198736911835], [1002, 1641198736945193], [1436, 1641198736978702], [1069, 1641198737012004], [1195, 1641198737045328], [996, 1641198737078615], [1337, 1641198737112083], [1149, 1641198737145304], [934, 1641198737178644], [1222, 1641198737212027], [1165, 1641198737245375], [928, 1641198737278560], [1339, 1641198737311968], [694, 1641198737345242], [1163, 1641198737378665], [840, 1641198737411980], [1469, 1641198737445600], [1254, 1641198737478732], [1223, 1641198737512024], [1197, 1641198737544455], [1200, 1641198737577757], [893, 1641198737611151], [1345, 1641198737644484], [938, 1641198737677664], [1118, 1641198737711092], [933, 1641198737745914], [1474, 1641198737778204], [1092, 1641198737811324], [1323, 1641198737844931], [1148, 1641198737878214], [1319, 1641198737911516], [917, 1641198737944760], [1626, 1641198737978233], [803, 1641198738011454], [1144, 1641198738044927], [1069, 1641198738078275], [1200, 1641198738111602], [1187, 1641198738144878], [1239, 1641198738178301], [1232, 1641198738211592], [1302, 1641198738244872], [1051, 1641198738278161], [1543, 1641198738311620], [986, 1641198738344887], [1018, 1641198738378269], [930, 1641198738411521], [1166, 1641198738444902], [1027, 1641198738478120], [1137, 1641198738511446], [1138, 1641198738544751], [1172, 1641198738578108], [1027, 1641198738611554], [4080, 1641198738645625], [408, 1641198738678283], [650, 1641198738711583], [902, 1641198738745067], [949, 1641198738778398], [1096, 1641198738811787], [1075, 1641198738845105], [1166, 1641198738878491], [1078, 1641198738911858], [1196, 1641198738945202], [1239, 1641198738978764], [966, 1641198739011878], [1163, 1641198739045211], [1037, 1641198739078486], [1038, 1641198739111833], [1144, 1641198739145211], [890, 1641198739178402], [1173, 1641198739211787], [1194, 1641198739245029], [965, 1641198739278391], [1185, 1641198739311720], [919, 1641198739345050], [858, 1641198739378331], [921, 1641198739411666], [1313, 1641198739445034], [1011, 1641198739478348], [1165, 1641198739511725], [982, 1641198739545088], [1148, 1641198739578372], [1027, 1641198739611802], [1042, 1641198739645053], [915, 1641198739678408], [872, 1641198739711798], [1050, 1641198739745111], [950, 1641198739778458], [1153, 1641198739811858], [1015, 1641198739845212], [1315, 1641198739878568], [1335, 1641198739911800], [1069, 1641198739945212], [1278, 1641198739978585], [1050, 1641198740011928], [1128, 1641198740045120], [997, 1641198740078424], [1280, 1641198740111814], [1134, 1641198740145036], [1135, 1641198740178370], [1221, 1641198740211719], [1364, 1641198740245036], [1083, 1641198740278387], [1349, 1641198740311809], [956, 1641198740345056], [1052, 1641198740378370], [1251, 1641198740411719], [1035, 1641198740444985], [1072, 1641198740478330], [948, 1641198740511598], [1109, 1641198740545021], [1086, 1641198740578236], [1054, 1641198740612138], [1305, 1641198740645548], [988, 1641198740678452], [1046, 1641198740711803], [887, 1641198740745155], [1054, 1641198740778513], [1189, 1641198740811855], [782, 1641198740845064], [1209, 1641198740878500], [1059, 1641198740911742], [1198, 1641198740945096], [1289, 1641198740978389], [952, 1641198741011616], [1015, 1641198741044971], [798, 1641198741078260], [932, 1641198741111677], [1043, 1641198741144994], [1017, 1641198741178362], [915, 1641198741211625], [1267, 1641198741245080], [1176, 1641198741278592], [1565, 1641198741311831], [1007, 1641198741345211], [1086, 1641198741378430], [1341, 1641198741411933], [1112, 1641198741445105], [1189, 1641198741478560], [1126, 1641198741511811], [1215, 1641198741545161], [1127, 1641198741578348], [1198, 1641198741611726], [1256, 1641198741645113], [1050, 1641198741678431], [1093, 1641198741711669], [1089, 1641198741745043], [1247, 1641198741778450], [1002, 1641198741811796], [1054, 1641198741846556], [1122, 1641198741878558], [1339, 1641198741912003], [1189, 1641198741945353], [1189, 1641198741978612], [856, 1641198742012071], [1149, 1641198742045580], [921, 1641198742078000], [877, 1641198742111411], [1092, 1641198742145167], [894, 1641198742178200], [1126, 1641198742211621], [953, 1641198742244701], [1006, 1641198742278162], [1185, 1641198742311696], [1175, 1641198742345117], [1448, 1641198742378690], [1268, 1641198742411941], [1075, 1641198742445143], [988, 1641198742478385], [1029, 1641198742511814], [1143, 1641198742545086], [1260, 1641198742579053], [1216, 1641198742612241], [1054, 1641198742645878], [1035, 1641198742678018], [1013, 1641198742711384], [868, 1641198742744693], [954, 1641198742778751], [1085, 1641198742811963], [876, 1641198742845290], [1071, 1641198742878722], [932, 1641198742911091], [1152, 1641198742944551], [1204, 1641198742978002], [915, 1641198743011212], [1179, 1641198743044659], [1239, 1641198743077905], [1167, 1641198743111422], [941, 1641198743144686], [1234, 1641198743178002], [1007, 1641198743211294], [1032, 1641198743244695], [926, 1641198743277960], [1475, 1641198743311331], [1189, 1641198743344704], [1114, 1641198743377966], [876, 1641198743411255], [1271, 1641198743444666], [1056, 1641198743477878], [1023, 1641198743511269], [1104, 1641198743545070], [1272, 1641198743578301], [827, 1641198743611445], [1453, 1641198743644927], [1086, 1641198743678379], [1336, 1641198743711608], [1101, 1641198743744900], [1245, 1641198743778488], [914, 1641198743811722], [1071, 1641198743845140], [1142, 1641198743878571], [1232, 1641198743911833], [989, 1641198743945150], [1189, 1641198743978675], [1017, 1641198744012113], [1126, 1641198744045404], [1201, 1641198744078664], [1275, 1641198744111937], [1142, 1641198744145293], [954, 1641198744178629], [1151, 1641198744211997], [1057, 1641198744245339], [1371, 1641198744278800], [1152, 1641198744312043], [1186, 1641198744345526], [1171, 1641198744378753], [1094, 1641198744412172], [1307, 1641198744444579], [1325, 1641198744477846], [1319, 1641198744512274], [965, 1641198744544609], [1143, 1641198744577941], [1150, 1641198744611268], [1071, 1641198744644622], [912, 1641198744677972], [1037, 1641198744711163], [874, 1641198744744450], [898, 1641198744777772], [1252, 1641198744811209], [942, 1641198744844383], [1295, 1641198744877745], [971, 1641198744911019], [1341, 1641198744944593], [1138, 1641198744977875], [1057, 1641198745011179], [988, 1641198745044465], [1274, 1641198745077858], [1073, 1641198745111194], [1110, 1641198745144564], [1235, 1641198745177855], [1318, 1641198745211365], [1044, 1641198745244448], [1071, 1641198745277895], [1614, 1641198745311263], [881, 1641198745344565], [984, 1641198745377832], [1002, 1641198745411209], [1223, 1641198745444579], [1128, 1641198745477930], [1168, 1641198745511247], [1188, 1641198745544593], [1202, 1641198745578027], [1140, 1641198745611339], [1281, 1641198745644683], [1004, 1641198745677967], [1339, 1641198745711269], [922, 1641198745744817], [1100, 1641198745778370], [1175, 1641198745811942], [1169, 1641198745845270], [1340, 1641198745878888], [1095, 1641198745912110], [873, 1641198745945520], [1013, 1641198745979027], [770, 1641198746011321], [1019, 1641198746044877], [916, 1641198746078515], [1131, 1641198746111936], [1147, 1641198746145481], [936, 1641198746178711], [1085, 1641198746212342], [1025, 1641198746245661], [1064, 1641198746277910], [1194, 1641198746311132], [1018, 1641198746344427], [1123, 1641198746377838], [1047, 1641198746411089], [1100, 1641198746444512], [1199, 1641198746477820], [1043, 1641198746511062], [1256, 1641198746544427], [1250, 1641198746577674], [1059, 1641198746611978], [1179, 1641198746645437], [837, 1641198746678718], [897, 1641198746712036], [1031, 1641198746745353], [1110, 1641198746778658], [1100, 1641198746811993], [1141, 1641198746845350], [1143, 1641198746878676], [1259, 1641198746911902], [1129, 1641198746945383], [1417, 1641198746978651], [895, 1641198747011967], [973, 1641198747045218], [1187, 1641198747078590], [1169, 1641198747112039], [952, 1641198747145272], [1184, 1641198747178653], [1353, 1641198747211951], [1141, 1641198747245261], [1156, 1641198747278567], [1188, 1641198747311919], [1102, 1641198747345279], [934, 1641198747378518], [992, 1641198747411831], [1068, 1641198747445268], [1197, 1641198747478559], [995, 1641198747511923], [1153, 1641198747545250], [1145, 1641198747578590], [1061, 1641198747611916], [1411, 1641198747645505], [1073, 1641198747678474], [1179, 1641198747711891], [983, 1641198747745272], [1196, 1641198747778605], [1169, 1641198747811912], [1174, 1641198747845246], [1170, 1641198747880196], [1220, 1641198747911750], [1067, 1641198747945136], [1203, 1641198747978431], [906, 1641198748011930], [982, 1641198748045344], [917, 1641198748078561], [1088, 1641198748111876], [927, 1641198748145195], [1196, 1641198748179077], [955, 1641198748211866], [1177, 1641198748245198], [906, 1641198748279653], [1450, 1641198748313376], [913, 1641198748345253], [1091, 1641198748378715], [1042, 1641198748412391], [901, 1641198748444679], [1233, 1641198748478222], [1123, 1641198748511333], [1260, 1641198748544651], [1208, 1641198748578264], [1179, 1641198748611313], [1350, 1641198748644894], [951, 1641198748678174], [1036, 1641198748711544], [880, 1641198748744733], [1106, 1641198748778301], [1027, 1641198748811569], [1190, 1641198748845014], [977, 1641198748878340], [1213, 1641198748911657], [991, 1641198748945068], [1421, 1641198748978438], [816, 1641198749011578], [1152, 1641198749045145], [989, 1641198749078407], [1204, 1641198749111716], [1288, 1641198749145074], [772, 1641198749178308], [1015, 1641198749211628], [1457, 1641198749245070], [1163, 1641198749278380], [896, 1641198749311681], [955, 1641198749345121], [988, 1641198749378400], [1008, 1641198749411806], [1120, 1641198749445108], [1096, 1641198749478457], [1104, 1641198749511871], [1133, 1641198749545199], [1154, 1641198749578534], [1100, 1641198749611874], [1401, 1641198749645280], [1032, 1641198749678592], [952, 1641198749711785], [1121, 1641198749745158], [997, 1641198749778498], [1122, 1641198749811884], [975, 1641198749845145], [1041, 1641198749878596], [1042, 1641198749911845], [1067, 1641198749945183], [1546, 1641198749978603], [1002, 1641198750011794], [1028, 1641198750045216], [1055, 1641198750078482], [1030, 1641198750111917], [888, 1641198750145167], [892, 1641198750178512], [1152, 1641198750211854], [997, 1641198750245146], [1177, 1641198750278580], [1212, 1641198750311859], [1065, 1641198750345140], [1127, 1641198750378401], [1042, 1641198750411791], [1263, 1641198750445253], [949, 1641198750478468], [928, 1641198750511868], [1180, 1641198750545238], [1201, 1641198750578518], [1281, 1641198750612004], [1327, 1641198750645397], [1158, 1641198750678713], [1046, 1641198750710942], [1308, 1641198750745410], [1056, 1641198750777678], [1098, 1641198750811048], [961, 1641198750844371], [1099, 1641198750877861], [908, 1641198750911003], [1144, 1641198750944452], [1133, 1641198750977754], [918, 1641198751010947], [1051, 1641198751044411], [1235, 1641198751077815], [1232, 1641198751111192], [1204, 1641198751144499], [1194, 1641198751177847], [1073, 1641198751211097], [1229, 1641198751244558], [902, 1641198751277798], [1579, 1641198751311246], [896, 1641198751344472], [1131, 1641198751377892], [1227, 1641198751411167], [1334, 1641198751444606], [1279, 1641198751477770], [987, 1641198751511168], [1331, 1641198751544565], [1115, 1641198751577944], [1008, 1641198751611178], [1412, 1641198751644670], [886, 1641198751677972], [1167, 1641198751711292], [977, 1641198751744627], [1212, 1641198751778042], [1123, 1641198751811264], [1070, 1641198751844602], [1055, 1641198751877891], [1103, 1641198751911190], [910, 1641198751944508], [1628, 1641198751977966], [786, 1641198752011259], [1057, 1641198752044715], [897, 1641198752077906], [1154, 1641198752111354], [1123, 1641198752144700], [1076, 1641198752178056], [1528, 1641198752211520], [1074, 1641198752244714], [1165, 1641198752278064], [1049, 1641198752311289], [894, 1641198752344661], [1041, 1641198752378156], [885, 1641198752411465], [1039, 1641198752444826], [1180, 1641198752478153], [1066, 1641198752511489], [1231, 1641198752544838], [1077, 1641198752578160], [1116, 1641198752611435], [1128, 1641198752644996], [1014, 1641198752678258], [1214, 1641198752711603], [852, 1641198752744864], [969, 1641198752778172], [1231, 1641198752811677], [1002, 1641198752844928], [1487, 1641198752878366], [1371, 1641198752911602], [1116, 1641198752944989], [1305, 1641198752978372], [1128, 1641198753011668], [1005, 1641198753044987], [1064, 1641198753078358], [1000, 1641198753111588], [1100, 1641198753144902], [1088, 1641198753178289], [1336, 1641198753211648], [1124, 1641198753244924], [1135, 1641198753278216], [1381, 1641198753311657], [751, 1641198753344877], [1264, 1641198753378375], [973, 1641198753411670], [980, 1641198753444993], [1115, 1641198753478379], [920, 1641198753511631], [1294, 1641198753545102], [1301, 1641198753578445], [1205, 1641198753611671], [1229, 1641198753645069], [818, 1641198753678237], [1144, 1641198753711752], [818, 1641198753744997], [929, 1641198753778358], [878, 1641198753811703], [1023, 1641198753845156], [1167, 1641198753878517], [1202, 1641198753911844], [1148, 1641198753945198], [1242, 1641198753978499], [889, 1641198754011746], [1060, 1641198754045265], [1201, 1641198754078687], [960, 1641198754111910], [1236, 1641198754145377], [970, 1641198754178631], [1332, 1641198754210933], [1206, 1641198754245290], [1268, 1641198754277682], [1312, 1641198754311139], [1087, 1641198754344411], [1116, 1641198754377797], [1205, 1641198754411095], [1018, 1641198754444336], [957, 1641198754477656], [837, 1641198754510954], [1079, 1641198754544473], [1183, 1641198754577672], [1005, 1641198754611056], [1624, 1641198754644528], [976, 1641198754677826], [1262, 1641198754711207], [993, 1641198754744395], [1041, 1641198754777812], [1181, 1641198754811105], [806, 1641198754844384], [1115, 1641198754877820], [1154, 1641198754911170], [1016, 1641198754944486], [1746, 1641198754978001], [1051, 1641198755011163], [1134, 1641198755044501], [1066, 1641198755077805], [1332, 1641198755111153], [1033, 1641198755144446], [1025, 1641198755177790], [1147, 1641198755211102], [1054, 1641198755244408], [1055, 1641198755277787], [1484, 1641198755311206], [1070, 1641198755344459], [1142, 1641198755377905], [1230, 1641198755411154], [1310, 1641198755444534], [1373, 1641198755477826], [1180, 1641198755511072], [1134, 1641198755544451], [1073, 1641198755577760], [1050, 1641198755611112], [1497, 1641198755644509], [1217, 1641198755677945], [1251, 1641198755711100], [904, 1641198755744399], [1070, 1641198755777836], [994, 1641198755810977], [962, 1641198755844385], [985, 1641198755877799], [1177, 1641198755911136], [833, 1641198755944244], [1088, 1641198755977740], [827, 1641198756011110], [942, 1641198756044204], [837, 1641198756077819], [1099, 1641198756111008], [1078, 1641198756144294], [970, 1641198756177586], [811, 1641198756210818], [1130, 1641198756245578], [1008, 1641198756277825], [1179, 1641198756311040], [983, 1641198756344398], [1402, 1641198756377832], [1221, 1641198756410956], [1239, 1641198756445444], [1194, 1641198756477795], [905, 1641198756510942], [1033, 1641198756544382], [1112, 1641198756577609], [1108, 1641198756611177], [1386, 1641198756644581], [1268, 1641198756677908], [1235, 1641198756711432], [906, 1641198756744649], [1178, 1641198756778205], [972, 1641198756811510], [954, 1641198756844858], [1117, 1641198756878288], [1101, 1641198756911438], [959, 1641198756945110], [1169, 1641198756978552], [1179, 1641198757011659], [1082, 1641198757045065], [1142, 1641198757078349], [1216, 1641198757111617], [992, 1641198757145057], [1017, 1641198757178429], [1226, 1641198757211787], [1055, 1641198757245161], [748, 1641198757278523], [1061, 1641198757311934], [932, 1641198757345394], [1017, 1641198757378697], [1153, 1641198757412015], [1292, 1641198757445265], [1023, 1641198757477674], [1021, 1641198757511103], [1151, 1641198757544616], [1551, 1641198757577804], [579, 1641198757610914], [1158, 1641198757644560], [922, 1641198757678017], [1261, 1641198757711373], [1009, 1641198757744319], [1549, 1641198757777813], [1263, 1641198757811052], [1018, 1641198757844416], [1075, 1641198757877916], [1410, 1641198757911171], [797, 1641198757944473], [1277, 1641198757977943], [1102, 1641198758011302], [1098, 1641198758044621], [1029, 1641198758078075], [1328, 1641198758111404], [1114, 1641198758144775], [1163, 1641198758178131], [1222, 1641198758211358], [1266, 1641198758244668], [1082, 1641198758277963], [1204, 1641198758311231], [942, 1641198758344580], [1077, 1641198758377907], [1000, 1641198758411098], [1190, 1641198758444586], [1151, 1641198758477701], [1069, 1641198758510999], [1006, 1641198758544315], [1088, 1641198758577890], [985, 1641198758611135], [4279, 1641198758645205], [464, 1641198758677830], [659, 1641198758711177], [801, 1641198758744503], [1129, 1641198758778008], [860, 1641198758811123], [1184, 1641198758844626], [1011, 1641198758877989], [1113, 1641198758911286], [1064, 1641198758944677], [1252, 1641198758977959], [922, 1641198759011336], [1348, 1641198759044725], [897, 1641198759077921], [1123, 1641198759111292], [1088, 1641198759144769], [949, 1641198759178021], [1141, 1641198759211474], [1256, 1641198759244630], [982, 1641198759277863], [1077, 1641198759311242], [974, 1641198759344573], [871, 1641198759377951], [969, 1641198759411196], [978, 1641198759444576], [999, 1641198759477834], [1019, 1641198759511327], [1108, 1641198759544677], [1006, 1641198759577925], [1080, 1641198759611335], [976, 1641198759644607], [1002, 1641198759677923], [936, 1641198759711257], [1132, 1641198759744665], [1177, 1641198759777943], [963, 1641198759811346], [1026, 1641198759844740], [1248, 1641198759878138], [1268, 1641198759911468], [1289, 1641198759944874], [1357, 1641198759978214], [963, 1641198760011558], [1379, 1641198760044900], [898, 1641198760078108], [1117, 1641198760111476], [1199, 1641198760144820], [1108, 1641198760178061], [1210, 1641198760211527], [1268, 1641198760244864], [1169, 1641198760278234], [1223, 1641198760311518], [1020, 1641198760344853], [1022, 1641198760378168], [1051, 1641198760411551], [1193, 1641198760444840], [1098, 1641198760478229], [919, 1641198760511506], [1192, 1641198760544939], [1004, 1641198760578221], [1278, 1641198760611553], [1229, 1641198760644840], [1013, 1641198760678191], [1181, 1641198760711512], [972, 1641198760744817], [873, 1641198760778162], [1192, 1641198760811569], [772, 1641198760844764], [1104, 1641198760878215], [1031, 1641198760911529], [1122, 1641198760944956], [1084, 1641198760978274], [1012, 1641198761011552], [862, 1641198761044913], [1011, 1641198761078318], [911, 1641198761111539], [1247, 1641198761145051], [781, 1641198761178258], [1068, 1641198761211731], [1127, 1641198761245063], [1082, 1641198761278437], [1613, 1641198761311875], [1173, 1641198761345183], [1203, 1641198761378731], [1183, 1641198761411834], [1298, 1641198761445210], [1079, 1641198761478523], [1017, 1641198761511840], [1175, 1641198761545321], [1152, 1641198761578700], [1149, 1641198761610972], [1361, 1641198761644392], [1159, 1641198761678670], [1100, 1641198761711962], [978, 1641198761744382], [1345, 1641198761777737], [1132, 1641198761810952], [965, 1641198761844393], [1094, 1641198761877808], [1077, 1641198761911123], [947, 1641198761944417], [1567, 1641198761977802], [1070, 1641198762011066], [974, 1641198762044417], [893, 1641198762077946], [1065, 1641198762111128], [973, 1641198762144707], [843, 1641198762177858], [969, 1641198762211143], [1077, 1641198762244649], [999, 1641198762278011], [1181, 1641198762311402], [1125, 1641198762344776], [1360, 1641198762378303], [1212, 1641198762411567], [1107, 1641198762444879], [1165, 1641198762478261], [960, 1641198762511457], [1011, 1641198762544878], [1092, 1641198762578251], [918, 1641198762611484], [1277, 1641198762645035], [1165, 1641198762678418], [1234, 1641198762711591], [1107, 1641198762744943], [972, 1641198762778132], [1114, 1641198762811454], [827, 1641198762844783], [916, 1641198762878196], [1004, 1641198762911457], [1015, 1641198762944921], [1228, 1641198762978181], [940, 1641198763011374], [1309, 1641198763044864], [860, 1641198763078054], [1231, 1641198763111521], [928, 1641198763144815], [1113, 1641198763178188], [1035, 1641198763211521], [1209, 1641198763244835], [915, 1641198763278109], [1452, 1641198763311483], [936, 1641198763344698], [1424, 1641198763378117], [1185, 1641198763411319], [1210, 1641198763444681], [1060, 1641198763478038], [1032, 1641198763511359], [1103, 1641198763544766], [1014, 1641198763577973], [1038, 1641198763611291], [1319, 1641198763644693], [889, 1641198763677896], [1240, 1641198763711265], [1054, 1641198763744529], [1367, 1641198763778109], [975, 1641198763811480], [1144, 1641198763844899], [1106, 1641198763878072], [1139, 1641198763911415], [1264, 1641198763944660], [1065, 1641198763978013], [1067, 1641198764011391], [1208, 1641198764044651], [1099, 1641198764077967], [1294, 1641198764111258], [1392, 1641198764144614], [869, 1641198764177870], [1088, 1641198764211186], [1265, 1641198764244583], [979, 1641198764277896], [1209, 1641198764311258], [1031, 1641198764344518], [1103, 1641198764378045], [1230, 1641198764411258], [1363, 1641198764444654], [1113, 1641198764477931], [1155, 1641198764511192], [1135, 1641198764544510], [1133, 1641198764577822], [1077, 1641198764611139], [1340, 1641198764644569], [924, 1641198764677731], [879, 1641198764711055], [924, 1641198764744660], [1091, 1641198764777852], [1301, 1641198764811206], [1082, 1641198764844506], [1127, 1641198764877920], [1087, 1641198764911286], [938, 1641198764944634], [1198, 1641198764978144], [866, 1641198765011450], [1132, 1641198765044921], [1129, 1641198765078358], [1239, 1641198765111746], [1213, 1641198765145109], [1131, 1641198765178503], [1223, 1641198765211828], [1149, 1641198765245125], [1126, 1641198765278501], [1558, 1641198765311887], [908, 1641198765345293], [1087, 1641198765378590], [1052, 1641198765410825], [1230, 1641198765445261], [1123, 1641198765478687], [1132, 1641198765510998], [1280, 1641198765544495], [1426, 1641198765577815], [916, 1641198765611060], [1266, 1641198765644546], [1098, 1641198765677876], [1156, 1641198765711196], [1126, 1641198765744593], [1203, 1641198765777953], [1149, 1641198765811102], [1058, 1641198765844505], [1158, 1641198765877862], [1047, 1641198765911095], [770, 1641198765944365], [1033, 1641198765977714], [776, 1641198766011046], [1090, 1641198766044483], [962, 1641198766077989], [1288, 1641198766111222], [1089, 1641198766144567], [881, 1641198766177884], [1112, 1641198766211201], [1218, 1641198766244528], [1118, 1641198766277927], [1177, 1641198766311192], [891, 1641198766344528], [895, 1641198766378057], [1143, 1641198766411323], [1140, 1641198766444722], [1275, 1641198766478158], [994, 1641198766511387], [1010, 1641198766544658], [1170, 1641198766578098], [743, 1641198766611290], [1208, 1641198766644819], [1177, 1641198766678235], [1141, 1641198766711481], [1040, 1641198766744776], [1293, 1641198766778182], [1117, 1641198766811430], [1399, 1641198766844879], [1329, 1641198766878137], [1144, 1641198766911384], [777, 1641198766944711], [1135, 1641198766978040], [934, 1641198767011390], [1208, 1641198767044855], [735, 1641198767078143], [1370, 1641198767111717], [1177, 1641198767145081], [1323, 1641198767178510], [1163, 1641198767211921], [1231, 1641198767245306], [946, 1641198767277635], [1316, 1641198767311138], [988, 1641198767344445], [1113, 1641198767377893], [858, 1641198767411186], [1161, 1641198767449553], [1072, 1641198767479982], [991, 1641198767513467], [1115, 1641198767547325], [1206, 1641198767580424], [1058, 1641198767613901], [1264, 1641198767647719], [1180, 1641198767679946], [1209, 1641198767713919], [1036, 1641198767747818], [1183, 1641198767780454], [1102, 1641198767814061], [1142, 1641198767846338], [1261, 1641198767880763], [1246, 1641198767913434], [1008, 1641198767946701], [1125, 1641198767980358], [1073, 1641198768013857], [817, 1641198768047630], [877, 1641198768080451], [937, 1641198768112784], [1004, 1641198768147243], [795, 1641198768180053], [1252, 1641198768213789], [1159, 1641198768247548], [1145, 1641198768279920], [1208, 1641198768313493], [988, 1641198768347153], [1153, 1641198768380560], [1037, 1641198768413314], [1209, 1641198768447563], [991, 1641198768480071], [942, 1641198768513578], [1147, 1641198768546938], [1297, 1641198768580476], [1110, 1641198768614119], [1579, 1641198768646538], [1093, 1641198768680014], [1001, 1641198768713573], [1372, 1641198768747000], [1010, 1641198768780476], [945, 1641198768813946], [1012, 1641198768846592], [1132, 1641198768880286], [1087, 1641198768913362], [1130, 1641198768946971], [1318, 1641198768980567], [1122, 1641198769012894], [1033, 1641198769047443], [899, 1641198769079748], [1201, 1641198769113310], [973, 1641198769146948], [927, 1641198769180176], [1100, 1641198769214195], [1266, 1641198769247363], [909, 1641198769279772], [1151, 1641198769313377], [974, 1641198769346871], [1081, 1641198769380604], [994, 1641198769414544], [1158, 1641198769447327], [1604, 1641198769480073], [894, 1641198769513654], [1111, 1641198769547003], [1004, 1641198769580440], [990, 1641198769613751], [1166, 1641198769647297], [964, 1641198769679954], [1196, 1641198769713179], [1276, 1641198769746683], [1263, 1641198769780463], [1011, 1641198769813983], [851, 1641198769846999], [1031, 1641198769880709], [1024, 1641198769913268], [869, 1641198769946982], [1435, 1641198769980327], [1048, 1641198770013654], [1186, 1641198770047511], [897, 1641198770079675], [1393, 1641198770113270], [818, 1641198770146411], [1101, 1641198770180022], [1019, 1641198770214057], [1092, 1641198770246435], [898, 1641198770280232], [1235, 1641198770314142], [1046, 1641198770346873], [1303, 1641198770379968], [1084, 1641198770413351], [1386, 1641198770447118], [1009, 1641198770480166], [940, 1641198770513527], [954, 1641198770546986], [1352, 1641198770580670], [908, 1641198770612961], [1184, 1641198770646613], [872, 1641198770679952], [1208, 1641198770713573], [1052, 1641198770747335], [1159, 1641198770779813], [1160, 1641198770813467], [951, 1641198770847543], [1171, 1641198770880247], [1211, 1641198770913907], [1281, 1641198770946433], [990, 1641198770979860], [947, 1641198771013850], [1093, 1641198771046372], [803, 1641198771079748], [1604, 1641198771113989], [1230, 1641198771147171], [1224, 1641198771179885], [1376, 1641198771213287], [1061, 1641198771246723], [788, 1641198771280497], [1445, 1641198771313218], [787, 1641198771346523], [1141, 1641198771380300], [1068, 1641198771413575], [1156, 1641198771447097], [1114, 1641198771480436], [1025, 1641198771514046], [1455, 1641198771546923], [1222, 1641198771580004], [1349, 1641198771614000], [1362, 1641198771647161], [969, 1641198771680658], [1079, 1641198771713649], [1221, 1641198771746957], [951, 1641198771780417], [1054, 1641198771814411], [902, 1641198771847134], [1346, 1641198771880008], [1051, 1641198771913936], [897, 1641198771946471], [1401, 1641198771980067], [630, 1641198772013503], [1112, 1641198772047128], [994, 1641198772080833], [1062, 1641198772113359], [1146, 1641198772146836], [1149, 1641198772180695], [1277, 1641198772213127], [1321, 1641198772246907], [1074, 1641198772280094], [1361, 1641198772314373], [916, 1641198772346956], [1076, 1641198772380256], [877, 1641198772413772], [1154, 1641198772446651], [1184, 1641198772479937], [891, 1641198772513625], [959, 1641198772547093], [1158, 1641198772580551], [1179, 1641198772613592], [1428, 1641198772647091], [902, 1641198772680925], [958, 1641198772719324], [888, 1641198772746994], [920, 1641198772780628], [1040, 1641198772813065], [1100, 1641198772846548], [1218, 1641198772880319], [1086, 1641198772913543], [1530, 1641198772947265], [1232, 1641198772980847], [1244, 1641198773013236], [1217, 1641198773046782], [1006, 1641198773080063], [1175, 1641198773113654], [1134, 1641198773147215], [1095, 1641198773179479], [1135, 1641198773213088], [1129, 1641198773246844], [1147, 1641198773280114], [1465, 1641198773313677], [978, 1641198773347133], [988, 1641198773380468], [1105, 1641198773413280], [904, 1641198773446456], [1034, 1641198773480103], [790, 1641198773513621], [1207, 1641198773547109], [1025, 1641198773580601], [1245, 1641198773613104], [1312, 1641198773646491], [965, 1641198773680173], [1310, 1641198773713437], [990, 1641198773746852], [960, 1641198773780513], [1136, 1641198773812817], [808, 1641198773846416], [1097, 1641198773879773], [1288, 1641198773913353], [980, 1641198773947055], [1214, 1641198773980291], [854, 1641198774013739], [1471, 1641198774046612], [958, 1641198774079767], [1207, 1641198774113400], [895, 1641198774146588], [956, 1641198774180201], [1240, 1641198774214212], [975, 1641198774246222], [1073, 1641198774279806], [1492, 1641198774313703], [895, 1641198774347512], [1429, 1641198774380360], [1028, 1641198774413626], [1097, 1641198774447205], [1151, 1641198774479822], [1042, 1641198774513072], [962, 1641198774546570], [1299, 1641198774580247], [886, 1641198774613567], [1189, 1641198774647184], [1025, 1641198774679571], [1413, 1641198774713160], [1404, 1641198774746894], [1061, 1641198774780099], [1005, 1641198774813578], [837, 1641198774847400], [1009, 1641198774880144], [1063, 1641198774914316], [935, 1641198774946922], [1545, 1641198774980628], [920, 1641198775013444], [1254, 1641198775046610], [1091, 1641198775079955], [1442, 1641198775113792], [1159, 1641198775147138], [1073, 1641198775179699], [1283, 1641198775213138], [1275, 1641198775246600], [972, 1641198775280164], [1425, 1641198775313533], [990, 1641198775347003], [1203, 1641198775380830], [1251, 1641198775413125], [1431, 1641198775446749], [1268, 1641198775479964], [1034, 1641198775513754], [1230, 1641198775547297], [1036, 1641198775579557], [995, 1641198775613045], [1342, 1641198775646782], [960, 1641198775680088], [1353, 1641198775713646], [1053, 1641198775746970], [1308, 1641198775780622], [1066, 1641198775813353], [1009, 1641198775846538], [990, 1641198775879972], [1069, 1641198775913882], [743, 1641198775946034], [899, 1641198775980594], [863, 1641198776012975], [926, 1641198776046482], [1016, 1641198776080158], [1002, 1641198776113472], [1150, 1641198776147249], [888, 1641198776179742], [1188, 1641198776213138], [1200, 1641198776246742], [846, 1641198776280110], [1106, 1641198776313624], [877, 1641198776347374], [1199, 1641198776379607], [1177, 1641198776413137], [1272, 1641198776446850], [1107, 1641198776480138], [1111, 1641198776513687], [1028, 1641198776547027], [1201, 1641198776580520], [865, 1641198776613243], [1348, 1641198776646506], [1088, 1641198776680208], [1120, 1641198776714205], [1057, 1641198776746944], [1484, 1641198776780685], [1115, 1641198776812934], [1094, 1641198776846483], [1192, 1641198776880193], [1068, 1641198776913460], [828, 1641198776947049], [1236, 1641198776980830], [921, 1641198777012998], [1203, 1641198777046763], [801, 1641198777080003], [1425, 1641198777114250], [955, 1641198777147540], [1085, 1641198777179845], [1367, 1641198777213412], [1125, 1641198777247066], [853, 1641198777279486], [1120, 1641198777313236], [744, 1641198777346986], [1045, 1641198777379879], [918, 1641198777413638], [1170, 1641198777446929], [1061, 1641198777480305], [1251, 1641198777514513], [1315, 1641198777547228], [1216, 1641198777580132], [998, 1641198777613297], [1006, 1641198777646756], [1029, 1641198777680645], [980, 1641198777713192], [870, 1641198777747145], [1512, 1641198777781225], [959, 1641198777813461], [1330, 1641198777847173], [1319, 1641198777879383], [1231, 1641198777914312], [1038, 1641198777947394], [1350, 1641198777980053], [1175, 1641198778013572], [1145, 1641198778047302], [966, 1641198778079557], [1307, 1641198778113314], [1124, 1641198778146604], [1069, 1641198778180150], [1232, 1641198778213865], [1403, 1641198778247122], [974, 1641198778280551], [1313, 1641198778313308], [874, 1641198778346557], [1127, 1641198778380155], [909, 1641198778413361], [997, 1641198778446919], [1125, 1641198778480708], [780, 1641198778512897], [1574, 1641198778546667], [916, 1641198778580127], [1247, 1641198778613642], [4143, 1641198778647749], [517, 1641198778679390], [1001, 1641198778713066], [662, 1641198778746680], [1011, 1641198778780037], [898, 1641198778813493], [943, 1641198778847282], [1327, 1641198778879636], [1122, 1641198778913169], [1082, 1641198778946489], [1146, 1641198778979979], [960, 1641198779013756], [1066, 1641198779047011], [910, 1641198779080388], [1069, 1641198779114183], [1075, 1641198779146553], [955, 1641198779179983], [1308, 1641198779213385], [1107, 1641198779246838], [968, 1641198779280544], [1082, 1641198779312964], [1081, 1641198779346764], [916, 1641198779380889], [895, 1641198779413243], [1112, 1641198779446968], [1031, 1641198779480195], [953, 1641198779513950], [1070, 1641198779547185], [1165, 1641198779579913], [1060, 1641198779613676], [1051, 1641198779647344], [801, 1641198779679630], [1066, 1641198779713204], [885, 1641198779746522], [1022, 1641198779780067], [1087, 1641198779813759], [1039, 1641198779847077], [1249, 1641198779880627], [1277, 1641198779913405], [1090, 1641198779947185], [1353, 1641198779981153], [1050, 1641198780013650], [1415, 1641198780047222], [924, 1641198780080681], [1229, 1641198780113045], [1108, 1641198780146464], [1092, 1641198780180176], [1220, 1641198780213477], [1356, 1641198780247080], [1148, 1641198780280359], [1247, 1641198780313901], [1001, 1641198780346667], [1181, 1641198780379831], [993, 1641198780413306], [1022, 1641198780447306], [1040, 1641198780479719], [1014, 1641198780513311], [1071, 1641198780546500], [1299, 1641198780580047], [996, 1641198780613685], [1210, 1641198780647285], [986, 1641198780692933], [1052, 1641198780738244], [1151, 1641198780770796], [918, 1641198780795881], [1225, 1641198780832540], [779, 1641198780863637], [1121, 1641198780894133], [1386, 1641198780950655], [1094, 1641198780976426], [1169, 1641198780996321], [740, 1641198781027296], [1090, 1641198781066859], [729, 1641198781107780], [935, 1641198781128403], [824, 1641198781164446], [1039, 1641198781215994], [1063, 1641198781226507], [1267, 1641198781267672], [1120, 1641198781296238], [1408, 1641198781332945], [996, 1641198781363501], [1233, 1641198781394550], [1252, 1641198781427988], [1272, 1641198781463552], [1179, 1641198781493945], [1154, 1641198781532190], [1211, 1641198781562152], [1241, 1641198781597132], [1138, 1641198781627928], [1391, 1641198781660492], [1091, 1641198781699365], [1084, 1641198781728538], [987, 1641198781764666], [1201, 1641198781794710], [1008, 1641198781830717], [1167, 1641198781861624], [1063, 1641198781897195], [1345, 1641198781930843], [931, 1641198781960614], [1289, 1641198781993707], [850, 1641198782030378], [1107, 1641198782064176], [963, 1641198782092400], [1025, 1641198782129605], [969, 1641198782156619], [862, 1641198782194756], [1181, 1641198782228643], [1217, 1641198782258871], [1219, 1641198782297010], [1367, 1641198782327864], [1050, 1641198782362214], [1125, 1641198782391708], [1005, 1641198782431495], [1043, 1641198782462944], [1112, 1641198782489558], [919, 1641198782534506], [1136, 1641198782556870], [1316, 1641198782600863], [1064, 1641198782626688], [1210, 1641198782666227], [964, 1641198782692024], [949, 1641198782729174], [943, 1641198782760633], [1091, 1641198782789120], [1041, 1641198782824479], [809, 1641198782867068], [1114, 1641198782894561], [931, 1641198782927402], [1237, 1641198782963484], [1444, 1641198782992872], [1047, 1641198783028538], [905, 1641198783062053], [975, 1641198783089788], [1032, 1641198783132253], [1144, 1641198783161878], [894, 1641198783193981], [1177, 1641198783225748], [1144, 1641198783257872], [987, 1641198783298735], [1368, 1641198783322258], [982, 1641198783363006], [1232, 1641198783389254], [923, 1641198783429038], [1147, 1641198783459304], [1109, 1641198783480119], [1291, 1641198783514072], [1016, 1641198783548695], [1091, 1641198783579313], [1115, 1641198783613172], [1285, 1641198783646675], [1091, 1641198783679907], [1307, 1641198783713582], [1227, 1641198783746961], [1154, 1641198783780399], [1201, 1641198783813994], [865, 1641198783846223], [1131, 1641198783879752], [1043, 1641198783913413], [1110, 1641198783946884], [1227, 1641198783980357], [1134, 1641198784012780], [1283, 1641198784047239], [1166, 1641198784079925], [1206, 1641198784113336], [1073, 1641198784147168], [963, 1641198784180311], [1124, 1641198784213947], [1258, 1641198784246960], [958, 1641198784280160], [1278, 1641198784313805], [1069, 1641198784346903], [1208, 1641198784380547], [1366, 1641198784413093], [1294, 1641198784446782], [1260, 1641198784480020], [1071, 1641198784513540], [980, 1641198784546834], [1106, 1641198784580268], [1007, 1641198784614000], [1336, 1641198784646293], [1093, 1641198784679682], [1038, 1641198784713346], [1063, 1641198784746635], [917, 1641198784780136], [939, 1641198784813517], [1008, 1641198784847083], [900, 1641198784879774], [1074, 1641198784912992], [983, 1641198784946599], [1367, 1641198784980356], [908, 1641198785013600], [1358, 1641198785047253], [1365, 1641198785079535], [1157, 1641198785113008], [1217, 1641198785146922], [1063, 1641198785180068], [1119, 1641198785213590], [1112, 1641198785247288], [1038, 1641198785279539], [1549, 1641198785313234], [981, 1641198785346534], [962, 1641198785380107], [1102, 1641198785414300], [1271, 1641198785447023], [1112, 1641198785480566], [1318, 1641198785513363], [996, 1641198785546804], [1212, 1641198785580310], [1021, 1641198785613640], [1219, 1641198785647274], [1158, 1641198785679929], [1222, 1641198785713251], [1086, 1641198785746748], [1161, 1641198785780382], [962, 1641198785813630], [896, 1641198785847236], [1032, 1641198785879537], [1178, 1641198785913096], [960, 1641198785946768], [1216, 1641198785980012], [1002, 1641198786013545], [908, 1641198786047311], [1046, 1641198786079648], [1175, 1641198786113198], [1062, 1641198786146523], [1020, 1641198786180091], [1036, 1641198786213766], [1004, 1641198786246077], [965, 1641198786279578], [1238, 1641198786313302], [1033, 1641198786346626], [1134, 1641198786380207], [1127, 1641198786413477], [1223, 1641198786446988], [1045, 1641198786480645], [927, 1641198786512962], [1021, 1641198786546526], [962, 1641198786580382], [987, 1641198786612903], [1404, 1641198786646950], [1241, 1641198786680480], [1255, 1641198786713872], [1135, 1641198786746562], [1105, 1641198786779813], [1191, 1641198786813316], [1269, 1641198786846979], [863, 1641198786880250], [1043, 1641198786913870], [869, 1641198786946268], [1241, 1641198786979799], [1092, 1641198787013848], [1239, 1641198787046489], [1250, 1641198787080159], [1319, 1641198787113901], [954, 1641198787146198], [1253, 1641198787179774], [1059, 1641198787213067], [1167, 1641198787246761], [1034, 1641198787280384], [1267, 1641198787313678], [966, 1641198787347135], [1208, 1641198787379917], [822, 1641198787413077], [1201, 1641198787446801], [1260, 1641198787480013], [1025, 1641198787513495], [990, 1641198787547564], [1120, 1641198787579486], [1051, 1641198787613060], [1365, 1641198787646827], [1022, 1641198787679992], [1259, 1641198787713603], [1118, 1641198787746826], [1298, 1641198787780488], [947, 1641198787814358], [1095, 1641198787847259], [1167, 1641198787879738], [1092, 1641198787913417], [1047, 1641198787946763], [1352, 1641198787980342], [1031, 1641198788013730], [1076, 1641198788047106], [1007, 1641198788079746], [1030, 1641198788112939], [1027, 1641198788146451], [795, 1641198788180086], [987, 1641198788213799], [1106, 1641198788247281], [822, 1641198788279479], [1267, 1641198788313015], [1105, 1641198788346672], [1187, 1641198788379908], [1097, 1641198788413461], [1250, 1641198788447534], [1228, 1641198788480124], [1067, 1641198788513799], [1262, 1641198788546297], [1150, 1641198788579714], [861, 1641198788613419], [1474, 1641198788646692], [935, 1641198788680157], [1178, 1641198788713957], [973, 1641198788746215], [1242, 1641198788779830], [1062, 1641198788813163], [1144, 1641198788846710], [1167, 1641198788880408], [940, 1641198788912642], [992, 1641198788947205], [1307, 1641198788979987], [803, 1641198789013209], [1172, 1641198789046901], [1159, 1641198789080166], [1348, 1641198789113643], [966, 1641198789147500], [1030, 1641198789179636], [869, 1641198789213411], [1128, 1641198789247549], [922, 1641198789279848], [1217, 1641198789313567], [1104, 1641198789347174], [1041, 1641198789380030], [1181, 1641198789413798], [1243, 1641198789446031], [1144, 1641198789479511], [1113, 1641198789513217], [1150, 1641198789546551], [1415, 1641198789580161], [882, 1641198789613385], [1055, 1641198789646898], [837, 1641198789680626], [1059, 1641198789712878], [922, 1641198789746443], [1451, 1641198789780210], [1125, 1641198789813370], [1141, 1641198789847033], [992, 1641198789879348], [1157, 1641198789912863], [870, 1641198789946515], [1217, 1641198789979698], [774, 1641198790013605], [1147, 1641198790046920], [931, 1641198790080150], [1228, 1641198790113783], [1027, 1641198790145959], [974, 1641198790179439], [1135, 1641198790213215], [1023, 1641198790246432], [1268, 1641198790280045], [1196, 1641198790313661], [799, 1641198790345979], [1042, 1641198790379540], [959, 1641198790412865], [1275, 1641198790446635], [1046, 1641198790480469], [1344, 1641198790513154], [1182, 1641198790546766], [1354, 1641198790580295], [1179, 1641198790612639], [1273, 1641198790646321], [925, 1641198790679605], [1168, 1641198790713109], [870, 1641198790746749], [1008, 1641198790779921], [1219, 1641198790813616], [1155, 1641198790847240], [1212, 1641198790879543], [1152, 1641198790913116], [876, 1641198790946478], [1374, 1641198790980030], [728, 1641198791013580], [1104, 1641198791046283], [848, 1641198791080047], [1247, 1641198791114416], [1134, 1641198791146760], [1135, 1641198791180301], [1189, 1641198791213651], [1548, 1641198791247252], [1148, 1641198791280339], [1311, 1641198791313002], [961, 1641198791346565], [1272, 1641198791380421], [1027, 1641198791412651], [1055, 1641198791446171], [1105, 1641198791479495], [1204, 1641198791513091], [1361, 1641198791546816], [1436, 1641198791580041], [980, 1641198791613556], [1605, 1641198791647372], [865, 1641198791679575], [1177, 1641198791713242], [1086, 1641198791746495], [1026, 1641198791779964], [883, 1641198791813634], [927, 1641198791845973], [919, 1641198791879449], [1212, 1641198791913112], [1068, 1641198791946433], [1425, 1641198791980025], [1008, 1641198792013376], [982, 1641198792046864], [1054, 1641198792079672], [1099, 1641198792112976], [1071, 1641198792146523], [947, 1641198792180213], [1281, 1641198792213662], [1267, 1641198792247159], [1105, 1641198792279455], [1475, 1641198792312983], [842, 1641198792346645], [1094, 1641198792379860], [817, 1641198792413350], [1160, 1641198792447145], [939, 1641198792479296], [966, 1641198792512922], [1022, 1641198792546381], [984, 1641198792579799], [958, 1641198792613513], [1304, 1641198792646774], [1278, 1641198792680310], [1245, 1641198792713084], [1087, 1641198792746412], [974, 1641198792779750], [1234, 1641198792813642], [946, 1641198792846642], [1164, 1641198792880371], [1049, 1641198792912617], [1194, 1641198792947182], [1313, 1641198792979839], [1211, 1641198793013083], [1185, 1641198793046701], [1088, 1641198793079895], [1179, 1641198793113374], [1022, 1641198793147076], [1034, 1641198793179356], [1190, 1641198793212954], [1009, 1641198793246543], [986, 1641198793279781], [1421, 1641198793313372], [871, 1641198793346554], [1296, 1641198793380156], [1145, 1641198793413771], [1102, 1641198793446172], [1043, 1641198793479507], [937, 1641198793513201], [1108, 1641198793546505], [1071, 1641198793580015], [951, 1641198793613346], [1389, 1641198793646889], [944, 1641198793680407], [1217, 1641198793712798], [1083, 1641198793746378], [1004, 1641198793779931], [961, 1641198793813293], [871, 1641198793846887], [1154, 1641198793879297], [1035, 1641198793912806], [944, 1641198793946510], [1260, 1641198793979736], [817, 1641198794013146], [1180, 1641198794047040], [1204, 1641198794079482], [1504, 1641198794113051], [1273, 1641198794146213], [914, 1641198794179772], [1076, 1641198794213427], [1037, 1641198794246708], [1174, 1641198794280416], [1261, 1641198794313606], [1128, 1641198794347170], [1158, 1641198794380192], [1085, 1641198794413476], [1094, 1641198794447100], [1130, 1641198794479744], [901, 1641198794512932], [1303, 1641198794546440], [1027, 1641198794579977], [943, 1641198794613441], [1272, 1641198794647332], [930, 1641198794679337], [1136, 1641198794712894], [983, 1641198794746768], [1532, 1641198794780004], [1340, 1641198794813434], [1130, 1641198794847138], [1331, 1641198794879347], [1007, 1641198794912886], [1029, 1641198794946221], [1177, 1641198794979753], [915, 1641198795013472], [999, 1641198795046802], [1113, 1641198795080167], [1236, 1641198795113918], [1324, 1641198795146341], [1114, 1641198795179752], [1294, 1641198795213111], [1075, 1641198795246536], [1045, 1641198795280307], [1500, 1641198795312524], [949, 1641198795346959], [1155, 1641198795379745], [1180, 1641198795413159], [1275, 1641198795446737], [1257, 1641198795480040], [1075, 1641198795513528], [1328, 1641198795547542], [1442, 1641198795579550], [1036, 1641198795613041], [1144, 1641198795646730], [922, 1641198795680034], [925, 1641198795713624], [991, 1641198795746000], [1187, 1641198795779498], [1075, 1641198795813263], [1147, 1641198795846563], [1083, 1641198795879868], [1077, 1641198795913622], [836, 1641198795945871], [1103, 1641198795979505], [806, 1641198796012826], [719, 1641198796046331], [950, 1641198796080027], [943, 1641198796113346], [982, 1641198796146903], [1165, 1641198796179657], [1087, 1641198796212902], [1249, 1641198796246529], [1147, 1641198796279888], [1237, 1641198796313477], [723, 1641198796347267], [1005, 1641198796379610], [1030, 1641198796413173], [1261, 1641198796446943], [1439, 1641198796480279], [1051, 1641198796513767], [1198, 1641198796546148], [1284, 1641198796579651], [1034, 1641198796613283], [1275, 1641198796646671], [929, 1641198796680070], [909, 1641198796713730], [905, 1641198796746087], [1135, 1641198796779656], [956, 1641198796812974], [1186, 1641198796846459], [1421, 1641198796880191], [1273, 1641198796913398], [997, 1641198796946956], [1338, 1641198796979700], [934, 1641198797012949], [969, 1641198797046596], [938, 1641198797079795], [1206, 1641198797113420], [1078, 1641198797147053], [1157, 1641198797179286], [1067, 1641198797212779], [1096, 1641198797246527], [962, 1641198797279726], [1192, 1641198797313130], [768, 1641198797346527], [950, 1641198797379993], [1030, 1641198797413767], [1075, 1641198797446134], [1188, 1641198797479611], [977, 1641198797513162], [1463, 1641198797546582], [1312, 1641198797580051], [1270, 1641198797613329], [1235, 1641198797646839], [639, 1641198797679410], [1036, 1641198797712820], [827, 1641198797746336], [1280, 1641198797780082], [1174, 1641198797813565], [1052, 1641198797847272], [1328, 1641198797879849], [1277, 1641198797913415], [1091, 1641198797947186], [1431, 1641198797979437], [1049, 1641198798012946], [1311, 1641198798046692], [944, 1641198798079826], [1297, 1641198798113647], [1228, 1641198798146785], [1015, 1641198798180289], [1356, 1641198798213080], [1220, 1641198798246200], [1195, 1641198798279800], [1301, 1641198798313474], [947, 1641198798346772], [1082, 1641198798380307], [939, 1641198798412599], [892, 1641198798445961], [1075, 1641198798479768], [787, 1641198798512928], [1436, 1641198798546694], [1008, 1641198798580139], [994, 1641198798612307], [4503, 1641198798645543], [466, 1641198798677988], [751, 1641198798711261], [818, 1641198798744708], [940, 1641198798778137], [918, 1641198798811369], [1133, 1641198798844885], [1218, 1641198798878194], [967, 1641198798911489], [1234, 1641198798944954], [1241, 1641198798978229], [1043, 1641198799011457], [1095, 1641198799044884], [950, 1641198799078076], [1074, 1641198799111526], [1072, 1641198799144887], [1005, 1641198799178155], [1324, 1641198799211499], [1060, 1641198799244847], [978, 1641198799278077], [1078, 1641198799311471], [978, 1641198799344756], [831, 1641198799378027], [935, 1641198799411392], [1257, 1641198799444728], [814, 1641198799477984], [1163, 1641198799511433], [1007, 1641198799544726], [1103, 1641198799578013], [1067, 1641198799611419], [1125, 1641198799644679], [829, 1641198799677971], [1043, 1641198799711446], [1010, 1641198799744674], [951, 1641198799778158], [1096, 1641198799811496], [895, 1641198799844838], [1323, 1641198799878237], [1204, 1641198799911497], [1255, 1641198799944886], [1219, 1641198799978153], [1075, 1641198800011611], [1137, 1641198800044902], [1097, 1641198800078160], [1158, 1641198800111588], [1125, 1641198800144926], [1168, 1641198800177367], [1372, 1641198800210727], [1249, 1641198800244085], [1026, 1641198800277329], [1209, 1641198800310777], [992, 1641198800344052], [1043, 1641198800377459], [978, 1641198800410694], [1161, 1641198800444181], [981, 1641198800477426], [1118, 1641198800510835], [1136, 1641198800544075], [1099, 1641198800577476], [1107, 1641198800610773], [1315, 1641198800644353], [887, 1641198800677391], [1080, 1641198800710809], [1086, 1641198800744139], [1016, 1641198800777495], [966, 1641198800810753], [886, 1641198800843987], [1046, 1641198800877575], [1177, 1641198800910638], [1214, 1641198800944007], [1196, 1641198800977330], [929, 1641198801010587], [1084, 1641198801044000], [881, 1641198801077279], [925, 1641198801110555], [959, 1641198801143995], [1081, 1641198801177512], [878, 1641198801210698], [1239, 1641198801244182], [1040, 1641198801277547], [1525, 1641198801310871], [1053, 1641198801344185], [1134, 1641198801377513], [1220, 1641198801410858], [1308, 1641198801444161], [1359, 1641198801477544], [1095, 1641198801510815], [1121, 1641198801544179], [1232, 1641198801577544], [1103, 1641198801610820], [1412, 1641198801644287], [1006, 1641198801677549], [1112, 1641198801710826], [1250, 1641198801744185], [1018, 1641198801777480], [1024, 1641198801810759], [1012, 1641198801844151], [1142, 1641198801877652], [1216, 1641198801910932], [1014, 1641198801944281], [1542, 1641198801977597], [806, 1641198802010787], [1046, 1641198802044190], [799, 1641198802077470], [1171, 1641198802110777], [1076, 1641198802144140], [812, 1641198802177666], [1072, 1641198802210840], [1096, 1641198802244153], [937, 1641198802277646], [1107, 1641198802310976], [1155, 1641198802344369], [1399, 1641198802377861], [1400, 1641198802410991], [1128, 1641198802444373], [1031, 1641198802477565], [932, 1641198802510872], [922, 1641198802544199], [1144, 1641198802577546], [953, 1641198802610852], [1166, 1641198802644266], [1154, 1641198802677659], [1118, 1641198802710844], [991, 1641198802744102], [1134, 1641198802777539], [974, 1641198802810696], [980, 1641198802844101], [1064, 1641198802877475], [1184, 1641198802910718], [1001, 1641198802944103], [1129, 1641198802977531], [841, 1641198803010707], [1151, 1641198803044148], [1032, 1641198803077313], [1414, 1641198803110700], [1070, 1641198803144092], [913, 1641198803177409], [1085, 1641198803210738], [1153, 1641198803244080], [904, 1641198803277381], [1254, 1641198803310793], [1229, 1641198803344268], [1108, 1641198803377427], [1071, 1641198803410791], [1170, 1641198803444103], [1379, 1641198803477537], [901, 1641198803510723], [1272, 1641198803544323], [964, 1641198803577471], [1046, 1641198803610995], [1225, 1641198803644243], [1181, 1641198803677625], [1159, 1641198803710809], [1068, 1641198803744217], [1401, 1641198803777652], [1201, 1641198803810905], [1030, 1641198803844228], [1053, 1641198803877658], [1064, 1641198803910920], [1069, 1641198803944311], [1071, 1641198803977547], [1035, 1641198804010991], [1115, 1641198804044359], [1136, 1641198804077695], [1046, 1641198804110847], [1470, 1641198804144406], [966, 1641198804177583], [1273, 1641198804210962], [1078, 1641198804244233], [960, 1641198804277564], [1311, 1641198804310977], [1080, 1641198804344298], [1255, 1641198804377687], [1151, 1641198804410837], [1271, 1641198804444289], [1359, 1641198804477592], [1052, 1641198804510881], [1110, 1641198804544240], [1044, 1641198804577480], [963, 1641198804610820], [1329, 1641198804644213], [809, 1641198804677462], [1166, 1641198804710787], [853, 1641198804743981], [1164, 1641198804777619], [1158, 1641198804810665], [869, 1641198804843984], [1077, 1641198804877386], [1131, 1641198804910729], [1021, 1641198804944122], [1337, 1641198804977550], [948, 1641198805010718], [926, 1641198805044086], [1154, 1641198805077608], [1403, 1641198805110928], [1354, 1641198805144251], [1120, 1641198805177593], [1274, 1641198805210924], [915, 1641198805244138], [1191, 1641198805277540], [1332, 1641198805310835], [944, 1641198805344217], [984, 1641198805377550], [1267, 1641198805410869], [1060, 1641198805444190], [1157, 1641198805477735], [1169, 1641198805510938], [1174, 1641198805544213], [1485, 1641198805577623], [921, 1641198805610849], [1133, 1641198805644197], [1052, 1641198805677585], [1079, 1641198805710861], [1278, 1641198805744217], [1163, 1641198805777549], [1065, 1641198805810805], [936, 1641198805844213], [968, 1641198805877513], [1031, 1641198805910830], [809, 1641198805944114], [1286, 1641198805977578], [886, 1641198806010813], [1204, 1641198806044197], [918, 1641198806077478], [1188, 1641198806110826], [1062, 1641198806144172], [923, 1641198806177572], [1103, 1641198806210900], [1199, 1641198806244283], [965, 1641198806277408], [1179, 1641198806310775], [1086, 1641198806344133], [1009, 1641198806377557], [1287, 1641198806410825], [1048, 1641198806444217], [1038, 1641198806477582], [1044, 1641198806510814], [1120, 1641198806544162], [984, 1641198806577405], [895, 1641198806610761], [1238, 1641198806644238], [1205, 1641198806677624], [1017, 1641198806710713], [1133, 1641198806744149], [1179, 1641198806777578], [1296, 1641198806810896], [1171, 1641198806844183], [1334, 1641198806877598], [1129, 1641198806910807], [955, 1641198806944104], [1161, 1641198806977524], [855, 1641198807010842], [1020, 1641198807044168], [1319, 1641198807077572], [1434, 1641198807111000], [1117, 1641198807144371], [1024, 1641198807177654], [1159, 1641198807211041], [1093, 1641198807244310], [1089, 1641198807277706], [1278, 1641198807311065], [1007, 1641198807344390], [1037, 1641198807377778], [886, 1641198807410970], [1214, 1641198807444452], [1083, 1641198807477783], [1030, 1641198807511084], [1141, 1641198807544439], [1248, 1641198807577840], [1090, 1641198807611165]]
2 [[268000, 1641198718651813], [268000, 1641198718684387], [268000, 1641198718712268], [268000, 1641198718744981], [268000, 1641198718777129], [268000, 1641198718778960], [268000, 1641198719777266], [266000, 1641198720194425], [266000, 1641198722676891], [266000, 1641198722679260], [266000, 1641198734677168], [266000, 1641198734680705]]
3 [[300000, 1641198718609718], [300000, 1641198718609849], [300000, 1641198718651910], [300000, 1641198718807585], [300000, 1641198719659259], [300000, 1641198719768213], [300000, 1641198719906635], [300000, 1641198720065690], [300000, 1641198720269583], [300000, 1641198720501854], [300000, 1641198720733193], [300000, 1641198720963330], [300000, 1641198721166025], [300000, 1641198721368815], [300000, 1641198721570442], [300000, 1641198721799138], [300000, 1641198722005565], [300000, 1641198722230958], [300000, 1641198722433115], [300000, 1641198722657101], [300000, 1641198722892605], [300000, 1641198723127762], [300000, 1641198723354701], [300000, 1641198723555670], [300000, 1641198723757273], [300000, 1641198724295028], [300000, 1641198724424513], [300000, 1641198724627143], [300000, 1641198724827672], [300000, 1641198725029965], [300000, 1641198725234298], [300000, 1641198725457155], [300000, 1641198725659946], [300000, 1641198725861753], [300000, 1641198726091605], [300000, 1641198726293650], [300000, 1641198726522268], [300000, 1641198726726686], [300000, 1641198726929047], [300000, 1641198727156692], [300000, 1641198727358672], [300000, 1641198727560008], [300000, 1641198727763698], [300000, 1641198727965801], [300000, 1641198728196602], [300000, 1641198728398388], [300000, 1641198728604353], [300000, 1641198728834431], [300000, 1641198729030049], [300000, 1641198729239259], [300000, 1641198729468103], [300000, 1641198729670331], [300000, 1641198729897285], [300000, 1641198730131269], [300000, 1641198730337730], [300000, 1641198730563525], [300000, 1641198730767259], [300000, 1641198730998606], [300000, 1641198731201802], [300000, 1641198731403796], [300000, 1641198731633321], [300000, 1641198731831326], [300000, 1641198732032482], [300000, 1641198732237211], [300000, 1641198732471982], [300000, 1641198732692043], [300000, 1641198732894794], [300000, 1641198733105394], [300000, 1641198733325864], [300000, 1641198734207469], [300000, 1641198734528702], [300000, 1641198734813501], [300000, 1641198734894647], [300000, 1641198734953148], [300000, 1641198735158759], [300000, 1641198735390674], [300000, 1641198735596263], [300000, 1641198735829162], [300000, 1641198736058623], [300000, 1641198736259733], [300000, 1641198736460422], [300000, 1641198736663393], [300000, 1641198736893693], [300000, 1641198737125763], [300000, 1641198737327730], [300000, 1641198737531028], [300000, 1641198737734906], [300000, 1641198737965458], [300000, 1641198738168229], [300000, 1641198738395642], [300000, 1641198738602620], [300000, 1641198738832588], [300000, 1641198739068030], [300000, 1641198739267884], [300000, 1641198739470520], [300000, 1641198739697491], [300000, 1641198739903773], [300000, 1641198740132400], [300000, 1641198740336555], [300000, 1641198740566146], [300000, 1641198740765696], [300000, 1641198740971224], [300000, 1641198741202035], [300000, 1641198741436647], [300000, 1641198741634345], [300000, 1641198741866080], [300000, 1641198742073902], [300000, 1641198742295999], [300000, 1641198742580112], [300000, 1641198742793320], [300000, 1641198742995262], [300000, 1641198743197135], [300000, 1641198743426943], [300000, 1641198743633430], [300000, 1641198743860107], [300000, 1641198744064237], [300000, 1641198744493713], [300000, 1641198744921971], [300000, 1641198745027173], [300000, 1641198745227434], [300000, 1641198745456649], [300000, 1641198745659993], [300000, 1641198745863692], [300000, 1641198746093875], [300000, 1641198746324090], [300000, 1641198746557646], [300000, 1641198746758531], [300000, 1641198746963084], [300000, 1641198747192778], [300000, 1641198747427445], [300000, 1641198747668274], [300000, 1641198747870440], [300000, 1641198748096973], [300000, 1641198748301807], [300000, 1641198748504592], [300000, 1641198748732180], [300000, 1641198748934563], [300000, 1641198749137395], [300000, 1641198749367094], [300000, 1641198749567977], [300000, 1641198749768923], [300000, 1641198750004610], [300000, 1641198750232929], [300000, 1641198750430222], [300000, 1641198750638607], [300000, 1641198750866052], [300000, 1641198751064742], [300000, 1641198751268633], [300000, 1641198751469252], [300000, 1641198751671355], [300000, 1641198751901631], [300000, 1641198752133538], [300000, 1641198752330319], [300000, 1641198752535474], [300000, 1641198752759510], [300000, 1641198752961558], [300000, 1641198753192799], [300000, 1641198753394127], [300000, 1641198753596820], [300000, 1641198753825380], [300000, 1641198754059551], [300000, 1641198754291741], [300000, 1641198754525044], [300000, 1641198754726818], [300000, 1641198754929528], [300000, 1641198755160203], [300000, 1641198755392279], [300000, 1641198755591635], [300000, 1641198755795687], [300000, 1641198756026533], [300000, 1641198756259753], [300000, 1641198756461563], [300000, 1641198756691249], [300000, 1641198756893554], [300000, 1641198757096751], [300000, 1641198757326389], [300000, 1641198757557336], [300000, 1641198757765035], [300000, 1641198757999227], [300000, 1641198758198527], [300000, 1641198758397987], [300000, 1641198758602507], [300000, 1641198758803265], [300000, 1641198759030471], [300000, 1641198759231776], [300000, 1641198759464213], [300000, 1641198759666396], [300000, 1641198759899239], [300000, 1641198760133131], [300000, 1641198760333104], [300000, 1641198760564738], [300000, 1641198760767058], [300000, 1641198760967731], [300000, 1641198761169936], [300000, 1641198761399672], [300000, 1641198761601268], [300000, 1641198761803708], [300000, 1641198762006372], [300000, 1641198762230919], [300000, 1641198762464917], [300000, 1641198762667818], [300000, 1641198762889515], [300000, 1641198763091858], [300000, 1641198763330716], [300000, 1641198763560532], [300000, 1641198763791948], [300000, 1641198764026373], [300000, 1641198764257346], [300000, 1641198764460378], [300000, 1641198764663043], [300000, 1641198764891413], [300000, 1641198765094117], [300000, 1641198765296827], [300000, 1641198765524390], [300000, 1641198765727006], [300000, 1641198765927355], [300000, 1641198766160738], [300000, 1641198766394748], [300000, 1641198766595989], [300000, 1641198766824367], [300000, 1641198767057823], [300000, 1641198767260975], [300000, 1641198767463986], [300000, 1641198767665323], [300000, 1641198767868065], [300000, 1641198768097701], [300000, 1641198768301293], [300000, 1641198768502864], [300000, 1641198768702999], [300000, 1641198768906382], [300000, 1641198769136375], [300000, 1641198769335459], [300000, 1641198769563747], [300000, 1641198769768258], [300000, 1641198770006036], [300000, 1641198770234447], [300000, 1641198770468708], [300000, 1641198770667397], [300000, 1641198770899523], [300000, 1641198771101134], [300000, 1641198771338892], [300000, 1641198771569871], [300000, 1641198771768374], [300000, 1641198772002222], [300000, 1641198772201836], [300000, 1641198772400380], [300000, 1641198772608388], [300000, 1641198772825588], [300000, 1641198773026231], [300000, 1641198773228494], [300000, 1641198773431751], [300000, 1641198773661277], [300000, 1641198773896832], [300000, 1641198774127909], [300000, 1641198774330790], [300000, 1641198774559952], [300000, 1641198774762980], [300000, 1641198774996818], [300000, 1641198775224194], [300000, 1641198775426032], [300000, 1641198775659583], [300000, 1641198775895432], [300000, 1641198776128191], [300000, 1641198776360524], [300000, 1641198776592986], [300000, 1641198776797874], [300000, 1641198776997912], [300000, 1641198777227488], [300000, 1641198777459487], [300000, 1641198777695725], [300000, 1641198777904028], [300000, 1641198778133858], [300000, 1641198778336612], [300000, 1641198778572198], [300000, 1641198778803201], [300000, 1641198779010430], [300000, 1641198779235339], [300000, 1641198779434389], [300000, 1641198779669541], [300000, 1641198779903897], [300000, 1641198780135733], [300000, 1641198780337790], [300000, 1641198780566977], [300000, 1641198780790673], [300000, 1641198780999879], [300000, 1641198781246857], [300000, 1641198781448848], [300000, 1641198781648504], [300000, 1641198781850229], [300000, 1641198782050750], [300000, 1641198782253968], [300000, 1641198782484847], [300000, 1641198782689919], [300000, 1641198782908001], [300000, 1641198783148019], [300000, 1641198783376612], [300000, 1641198783595152], [300000, 1641198783798955], [300000, 1641198784022240], [300000, 1641198784225178], [300000, 1641198784428588], [300000, 1641198784657551], [300000, 1641198784861339], [300000, 1641198785063538], [300000, 1641198785265111], [300000, 1641198785494100], [300000, 1641198785697157], [300000, 1641198785925184], [300000, 1641198786127690], [300000, 1641198786330921], [300000, 1641198786562234], [300000, 1641198786792643], [300000, 1641198786996208], [300000, 1641198787199132], [300000, 1641198787428353], [300000, 1641198787662234], [300000, 1641198787867418], [300000, 1641198788102766], [300000, 1641198788336303], [300000, 1641198788531782], [300000, 1641198788739760], [300000, 1641198788968162], [300000, 1641198789202235], [300000, 1641198789436081], [300000, 1641198789637450], [300000, 1641198789839413], [300000, 1641198790067205], [300000, 1641198790266950], [300000, 1641198790472514], [300000, 1641198790673675], [300000, 1641198790902730], [300000, 1641198791137362], [300000, 1641198791337878], [300000, 1641198791542542], [300000, 1641198791769478], [300000, 1641198792005545], [300000, 1641198792236361], [300000, 1641198792467587], [300000, 1641198792670141], [300000, 1641198792892419], [300000, 1641198793091966], [300000, 1641198793295105], [300000, 1641198793527893], [300000, 1641198793730667], [300000, 1641198793959505], [300000, 1641198794164804], [300000, 1641198794393732], [300000, 1641198794628178], [300000, 1641198794830140], [300000, 1641198795059963], [300000, 1641198795262200], [300000, 1641198795462652], [300000, 1641198795665125], [300000, 1641198795897299], [300000, 1641198796128855], [300000, 1641198796329910], [300000, 1641198796558273], [300000, 1641198796759308], [300000, 1641198796960873], [300000, 1641198797193104], [300000, 1641198797428624], [300000, 1641198797658748], [300000, 1641198797870094], [300000, 1641198798070972], [300000, 1641198798300808], [300000, 1641198798498555], [300000, 1641198798737194], [300000, 1641198798930000], [300000, 1641198799132464], [300000, 1641198799365880], [300000, 1641198799599458], [300000, 1641198799832607], [300000, 1641198800062721], [300000, 1641198800269262], [300000, 1641198800496267], [300000, 1641198800698314], [300000, 1641198800933975], [300000, 1641198801129833], [300000, 1641198801334758], [300000, 1641198801560549], [300000, 1641198801766870], [300000, 1641198801963941], [300000, 1641198802166423], [300000, 1641198802367048], [300000, 1641198802595686], [300000, 1641198802798817], [300000, 1641198802994901], [300000, 1641198803224999], [300000, 1641198803427878], [300000, 1641198803658698], [300000, 1641198803861542], [300000, 1641198804090363], [300000, 1641198804323772], [300000, 1641198804556457], [300000, 1641198804790092], [300000, 1641198804993612], [300000, 1641198805197095], [300000, 1641198805424490], [300000, 1641198805656324], [300000, 1641198805856947], [300000, 1641198806061938], [300000, 1641198806292256], [300000, 1641198806525933], [300000, 1641198806757482], [300000, 1641198806958047], [300000, 1641198807162710], [300000, 1641198807393697], [300000, 1641198807595413], [300000, 1641198807642480]]
4 [[800.0, 1641198718609718.0], [800.0, 1641198723632718.0], [1500.0, 1641198723632718.0], [1500.0, 1641198728647718.0], [800.0, 1641198728647718.0], [800.0, 1641198733663718.0], [1500.0, 1641198733663718.0], [1500.0, 1641198738676718.0], [800.0, 1641198738676718.0], [800.0, 1641198743689718.0], [1500.0, 1641198743689718.0], [1500.0, 1641198748703718.0], [800.0, 1641198748703718.0], [800.0, 1641198753716718.0], [1500.0, 1641198753716718.0], [1500.0, 1641198758730718.0], [800.0, 1641198758730718.0], [800.0, 1641198763743718.0], [1500.0, 1641198763743718.0], [1500.0, 1641198768757718.0], [800.0, 1641198768757718.0], [800.0, 1641198773776718.0], [1500.0, 1641198773776718.0], [1500.0, 1641198778790718.0], [800.0, 1641198778790718.0], [800.0, 1641198783803718.0], [1500.0, 1641198783803718.0], [1500.0, 1641198788853718.0], [800.0, 1641198788853718.0], [800.0, 1641198793866718.0], [1500.0, 1641198793866718.0], [1500.0, 1641198798879718.0], [800.0, 1641198798879718.0], [800.0, 1641198803893718.0], [1500.0, 1641198803893718.0]]
1641198718684415
1641198718651813
===================================
target: 1 ; 1641198718684387
DROP ; 1641198718684415 
===================================
target: 2 ; 1641198718712268
DROP ; 1641198718712278 
===================================
target: 3 ; 1641198718744981
DROP ; 1641198718744990 
===================================
target: 4 ; 1641198718777129
DROP ; 1641198718777139 
4815 ; 1641198718778912 
===================================
target: 5 ; 1641198718778960
86 ; 1641198718810380 
87 ; 1641198718843992 
156 ; 1641198718877375 
287 ; 1641198718910909 
357 ; 1641198718944259 
520 ; 1641198718977594 
519 ; 1641198719010900 
552 ; 1641198719044190 
510 ; 1641198719077524 
709 ; 1641198719110927 
675 ; 1641198719144144 
628 ; 1641198719177490 
839 ; 1641198719211110 
671 ; 1641198719244325 
807 ; 1641198719277727 
797 ; 1641198719310832 
702 ; 1641198719344167 
741 ; 1641198719377357 
772 ; 1641198719410949 
1010 ; 1641198719443966 
623 ; 1641198719477217 
810 ; 1641198719510531 
946 ; 1641198719543908 
720 ; 1641198719577182 
986 ; 1641198719610483 
600 ; 1641198719643683 
958 ; 1641198719678123 
909 ; 1641198719711452 
706 ; 1641198719744725 
===================================
target: 6 ; 1641198719777266
1474 ; 1641198719778233 
882 ; 1641198719811481 
831 ; 1641198719844789 
1240 ; 1641198719878236 
1102 ; 1641198719911445 
1112 ; 1641198719944883 
1194 ; 1641198719978159 
1191 ; 1641198720011459 
1027 ; 1641198720044753 
1020 ; 1641198720078044 
1190 ; 1641198720111394 
1188 ; 1641198720144702 
1068 ; 1641198720178040 
===================================
target: 7 ; 1641198720194425
1293 ; 1641198720211346 
1201 ; 1641198720244691 
907 ; 1641198720277935 
1167 ; 1641198720311290 
1173 ; 1641198720344588 
1006 ; 1641198720377896 
1039 ; 1641198720411251 
1137 ; 1641198720444606 
1030 ; 1641198720477924 
1067 ; 1641198720511293 
1078 ; 1641198720544577 
1104 ; 1641198720577880 
1134 ; 1641198720611350 
1130 ; 1641198720644524 
1076 ; 1641198720677822 
999 ; 1641198720711186 
1043 ; 1641198720744483 
1138 ; 1641198720777840 
1022 ; 1641198720811179 
866 ; 1641198720844487 
1217 ; 1641198720877868 
883 ; 1641198720911113 
974 ; 1641198720944458 
1123 ; 1641198720977762 
916 ; 1641198721011058 
1028 ; 1641198721044382 
881 ; 1641198721077726 
1136 ; 1641198721111067 
942 ; 1641198721144361 
902 ; 1641198721177613 
1105 ; 1641198721210994 
989 ; 1641198721244307 
1599 ; 1641198721277707 
1048 ; 1641198721310948 
1346 ; 1641198721344290 
1066 ; 1641198721377504 
1374 ; 1641198721410831 
1385 ; 1641198721444196 
1353 ; 1641198721477546 
1116 ; 1641198721510863 
1392 ; 1641198721544215 
1179 ; 1641198721577489 
1179 ; 1641198721610750 
1206 ; 1641198721644222 
1271 ; 1641198721677518 
1028 ; 1641198721710910 
1152 ; 1641198721744183 
1207 ; 1641198721777547 
977 ; 1641198721810777 
1144 ; 1641198721844190 
1342 ; 1641198721877483 
1146 ; 1641198721910680 
1003 ; 1641198721944010 
1362 ; 1641198721977331 
794 ; 1641198722010584 
1012 ; 1641198722043971 
852 ; 1641198722077425 
1065 ; 1641198722110758 
842 ; 1641198722144023 
1035 ; 1641198722177357 
1126 ; 1641198722210597 
904 ; 1641198722243819 
1193 ; 1641198722277125 
1347 ; 1641198722310489 
1102 ; 1641198722343825 
1245 ; 1641198722377221 
1260 ; 1641198722410574 
1449 ; 1641198722444035 
990 ; 1641198722477285 
1082 ; 1641198722510567 
1054 ; 1641198722543855 
1206 ; 1641198722577198 
992 ; 1641198722610474 
1099 ; 1641198722643826 
===================================
target: 8 ; 1641198722676891
4039 ; 1641198722679210 
===================================
target: 9 ; 1641198722679260
352 ; 1641198722710668 
300 ; 1641198722743963 
239 ; 1641198722777389 
240 ; 1641198722810762 
384 ; 1641198722844209 
570 ; 1641198722877666 
636 ; 1641198722911021 
668 ; 1641198722944395 
817 ; 1641198722977742 
784 ; 1641198723011141 
682 ; 1641198723044530 
767 ; 1641198723077879 
932 ; 1641198723111367 
803 ; 1641198723144734 
687 ; 1641198723178117 
983 ; 1641198723211612 
883 ; 1641198723245078 
1007 ; 1641198723277412 
943 ; 1641198723310758 
1155 ; 1641198723344281 
985 ; 1641198723377649 
850 ; 1641198723411214 
1312 ; 1641198723444569 
1040 ; 1641198723477931 
909 ; 1641198723511344 
1248 ; 1641198723544898 
1198 ; 1641198723578246 
927 ; 1641198723611571 
1183 ; 1641198723645167 
1044 ; 1641198723677542 
1106 ; 1641198723710910 
1104 ; 1641198723744414 
1029 ; 1641198723777784 
1067 ; 1641198723811188 
981 ; 1641198723844577 
1060 ; 1641198723877983 
1082 ; 1641198723911304 
921 ; 1641198723944654 
1315 ; 1641198723978104 
868 ; 1641198724011520 
1172 ; 1641198724044880 
1051 ; 1641198724078381 
1262 ; 1641198724112075 
1194 ; 1641198724144269 
1122 ; 1641198724177469 
1162 ; 1641198724210923 
1232 ; 1641198724244282 
940 ; 1641198724277758 
1359 ; 1641198724311200 
1255 ; 1641198724344611 
1209 ; 1641198724377976 
1100 ; 1641198724411350 
1379 ; 1641198724444873 
1196 ; 1641198724478126 
1099 ; 1641198724511457 
1129 ; 1641198724544913 
1235 ; 1641198724578278 
980 ; 1641198724611643 
1143 ; 1641198724645054 
1043 ; 1641198724678336 
968 ; 1641198724710710 
903 ; 1641198724744056 
1095 ; 1641198724777438 
1241 ; 1641198724810853 
905 ; 1641198724844154 
1158 ; 1641198724877617 
1270 ; 1641198724911057 
1111 ; 1641198724944393 
1190 ; 1641198724977811 
1120 ; 1641198725011190 
1061 ; 1641198725044524 
1285 ; 1641198725078017 
1321 ; 1641198725111403 
1263 ; 1641198725144728 
1237 ; 1641198725178047 
1193 ; 1641198725216617 
1320 ; 1641198725244773 
1106 ; 1641198725278108 
1452 ; 1641198725311527 
981 ; 1641198725344935 
1130 ; 1641198725378219 
977 ; 1641198725411623 
1170 ; 1641198725445016 
1192 ; 1641198725478432 
1150 ; 1641198725510828 
1228 ; 1641198725544197 
1220 ; 1641198725577533 
1135 ; 1641198725610933 
1390 ; 1641198725644309 
1003 ; 1641198725677621 
1161 ; 1641198725711045 
996 ; 1641198725744356 
1280 ; 1641198725778027 
1118 ; 1641198725811374 
1128 ; 1641198725844664 
1143 ; 1641198725878021 
1216 ; 1641198725911353 
963 ; 1641198725944920 
1112 ; 1641198725978226 
1042 ; 1641198726011700 
991 ; 1641198726044355 
874 ; 1641198726077954 
1159 ; 1641198726111381 
1192 ; 1641198726144680 
1142 ; 1641198726178076 
1110 ; 1641198726211471 
1051 ; 1641198726244847 
1126 ; 1641198726278309 
1166 ; 1641198726311745 
850 ; 1641198726344152 
1172 ; 1641198726377870 
1090 ; 1641198726411213 
1153 ; 1641198726444689 
1051 ; 1641198726478067 
1151 ; 1641198726511531 
1040 ; 1641198726544923 
1214 ; 1641198726578302 
1096 ; 1641198726611730 
1195 ; 1641198726644127 
1086 ; 1641198726677580 
1068 ; 1641198726710975 
1222 ; 1641198726756592 
1165 ; 1641198726777846 
1036 ; 1641198726811166 
1094 ; 1641198726844666 
1143 ; 1641198726878075 
1240 ; 1641198726911472 
1059 ; 1641198726944869 
1273 ; 1641198726978289 
1163 ; 1641198727011690 
956 ; 1641198727044034 
1068 ; 1641198727077466 
1096 ; 1641198727110946 
1169 ; 1641198727144381 
1065 ; 1641198727177713 
1195 ; 1641198727211207 
1310 ; 1641198727244578 
1018 ; 1641198727278033 
1348 ; 1641198727311481 
1123 ; 1641198727344825 
1114 ; 1641198727378224 
1054 ; 1641198727411608 
1223 ; 1641198727445072 
1188 ; 1641198727477510 
998 ; 1641198727510829 
1083 ; 1641198727544266 
1170 ; 1641198727577793 
1110 ; 1641198727611154 
1177 ; 1641198727644618 
1092 ; 1641198727677961 
1223 ; 1641198727711357 
1002 ; 1641198727744719 
1159 ; 1641198727778183 
1033 ; 1641198727811504 
1075 ; 1641198727844932 
1178 ; 1641198727878325 
1107 ; 1641198727911747 
1048 ; 1641198727944192 
1233 ; 1641198727977629 
1069 ; 1641198728011049 
1030 ; 1641198728044384 
997 ; 1641198728077778 
968 ; 1641198728111228 
1003 ; 1641198728144560 
860 ; 1641198728177967 
1117 ; 1641198728211417 
1093 ; 1641198728244822 
980 ; 1641198728278205 
1221 ; 1641198728311633 
874 ; 1641198728344016 
965 ; 1641198728377556 
1132 ; 1641198728410894 
1240 ; 1641198728444320 
1158 ; 1641198728477789 
1056 ; 1641198728511152 
1135 ; 1641198728544567 
1258 ; 1641198728577988 
1085 ; 1641198728611295 
1496 ; 1641198728644808 
1035 ; 1641198728678130 
1086 ; 1641198728711572 
1009 ; 1641198728744933 
1208 ; 1641198728778374 
1238 ; 1641198728810738 
1043 ; 1641198728844208 
1033 ; 1641198728877559 
1076 ; 1641198728910960 
1015 ; 1641198728944314 
1354 ; 1641198728977803 
989 ; 1641198729011094 
1046 ; 1641198729044543 
939 ; 1641198729077987 
1068 ; 1641198729111390 
1223 ; 1641198729144770 
786 ; 1641198729178098 
1207 ; 1641198729211534 
1106 ; 1641198729244946 
1077 ; 1641198729278365 
1138 ; 1641198729310747 
1125 ; 1641198729344224 
1006 ; 1641198729378021 
1050 ; 1641198729410961 
1370 ; 1641198729444430 
1212 ; 1641198729477776 
1008 ; 1641198729511134 
1256 ; 1641198729544576 
1027 ; 1641198729577912 
1136 ; 1641198729611361 
1221 ; 1641198729644744 
1007 ; 1641198729678049 
1237 ; 1641198729711457 
904 ; 1641198729744741 
1190 ; 1641198729778217 
1011 ; 1641198729811531 
1029 ; 1641198729844985 
982 ; 1641198729877463 
1259 ; 1641198729910918 
896 ; 1641198729944242 
1302 ; 1641198729977722 
961 ; 1641198730011040 
1123 ; 1641198730044520 
903 ; 1641198730077809 
1071 ; 1641198730111185 
1163 ; 1641198730144578 
942 ; 1641198730177987 
1165 ; 1641198730211472 
1020 ; 1641198730244766 
1048 ; 1641198730278225 
1140 ; 1641198730311599 
984 ; 1641198730343932 
1221 ; 1641198730377405 
1173 ; 1641198730410709 
1171 ; 1641198730444112 
1279 ; 1641198730477534 
856 ; 1641198730510850 
1020 ; 1641198730544269 
1135 ; 1641198730577633 
1312 ; 1641198730611163 
1400 ; 1641198730644529 
985 ; 1641198730677893 
1194 ; 1641198730711301 
1091 ; 1641198730744677 
1346 ; 1641198730778060 
952 ; 1641198730811387 
866 ; 1641198730844810 
1299 ; 1641198730878252 
1158 ; 1641198730911587 
1057 ; 1641198730943938 
903 ; 1641198730977367 
1057 ; 1641198731010713 
746 ; 1641198731044045 
807 ; 1641198731077372 
1382 ; 1641198731110908 
1193 ; 1641198731144321 
1207 ; 1641198731177713 
1198 ; 1641198731211088 
1201 ; 1641198731244482 
1195 ; 1641198731277914 
1369 ; 1641198731311338 
1183 ; 1641198731344704 
1220 ; 1641198731378095 
948 ; 1641198731411397 
1303 ; 1641198731444877 
1152 ; 1641198731478189 
1149 ; 1641198731511615 
1336 ; 1641198731544046 
1344 ; 1641198731577371 
1137 ; 1641198731610753 
1404 ; 1641198731644133 
930 ; 1641198731677469 
1126 ; 1641198731710867 
994 ; 1641198731744278 
1237 ; 1641198731777689 
1035 ; 1641198731811001 
973 ; 1641198731844360 
1330 ; 1641198731877751 
919 ; 1641198731911064 
875 ; 1641198731944482 
1305 ; 1641198731977933 
965 ; 1641198732011386 
994 ; 1641198732044654 
999 ; 1641198732078075 
1280 ; 1641198732111615 
1165 ; 1641198732144944 
999 ; 1641198732177303 
1352 ; 1641198732210785 
1289 ; 1641198732244228 
995 ; 1641198732277542 
1422 ; 1641198732310979 
928 ; 1641198732344255 
1096 ; 1641198732377677 
936 ; 1641198732410967 
1142 ; 1641198732444431 
951 ; 1641198732477743 
868 ; 1641198732511154 
1137 ; 1641198732544732 
1223 ; 1641198732578015 
988 ; 1641198732611307 
1462 ; 1641198732644811 
1007 ; 1641198732678064 
1024 ; 1641198732711457 
939 ; 1641198732744841 
1191 ; 1641198732778275 
1034 ; 1641198732810634 
1097 ; 1641198732844059 
1131 ; 1641198732877518 
1089 ; 1641198732910808 
1258 ; 1641198732944265 
1313 ; 1641198732977649 
1211 ; 1641198733011021 
1280 ; 1641198733044422 
934 ; 1641198733077722 
1443 ; 1641198733111201 
942 ; 1641198733144429 
944 ; 1641198733177840 
1167 ; 1641198733211303 
1139 ; 1641198733244635 
966 ; 1641198733277999 
1394 ; 1641198733311401 
923 ; 1641198733344706 
1123 ; 1641198733378112 
1077 ; 1641198733411525 
1129 ; 1641198733444915 
953 ; 1641198733478204 
1344 ; 1641198733510623 
833 ; 1641198733543933 
1297 ; 1641198733577399 
831 ; 1641198733610736 
1171 ; 1641198733644195 
979 ; 1641198733677542 
1217 ; 1641198733710940 
875 ; 1641198733744296 
1208 ; 1641198733777712 
1024 ; 1641198733811045 
1109 ; 1641198733844441 
1112 ; 1641198733878008 
1212 ; 1641198733911500 
1017 ; 1641198733945023 
1075 ; 1641198733978149 
758 ; 1641198734011426 
999 ; 1641198734044031 
1005 ; 1641198734077291 
1171 ; 1641198734110749 
1199 ; 1641198734144110 
1080 ; 1641198734177627 
1140 ; 1641198734210939 
1217 ; 1641198734244331 
936 ; 1641198734277755 
1480 ; 1641198734311224 
1252 ; 1641198734344650 
1274 ; 1641198734378007 
972 ; 1641198734411424 
1243 ; 1641198734444828 
1116 ; 1641198734478233 
1195 ; 1641198734511639 
1008 ; 1641198734544936 
1196 ; 1641198734577296 
867 ; 1641198734610633 
1223 ; 1641198734644141 
===================================
target: 10 ; 1641198734677168
3658 ; 1641198734680667 
one RTC finished. 
end Time:  1641198810392
